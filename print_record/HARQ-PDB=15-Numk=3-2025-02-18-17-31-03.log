
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0001], device='cuda:0')), ('power', tensor([0.9999], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.27698108553886414
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802683353424
epoch£º0	 i:4 	 global-step:4	 l-p:-0.7105808854103088
epoch£º0	 i:5 	 global-step:5	 l-p:-2.5309500694274902
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505254745483398
epoch£º0	 i:7 	 global-step:7	 l-p:0.4916812479496002
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606582403182983
epoch£º0	 i:9 	 global-step:9	 l-p:0.05680278688669205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.377257376909256 
model_pd.l_d.mean(): -18.863727569580078 
model_pd.lagr.mean(): -18.486469268798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0057], device='cuda:0')), ('power', tensor([0.9952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.377257376909256
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878843605518341
epoch£º1	 i:3 	 global-step:23	 l-p:0.24289998412132263
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980881869792938
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688207864761353
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738249897956848
epoch£º1	 i:7 	 global-step:27	 l-p:0.21875056624412537
epoch£º1	 i:8 	 global-step:28	 l-p:0.22574138641357422
epoch£º1	 i:9 	 global-step:29	 l-p:0.11670563369989395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 5.9885, 6.5145],
        [4.9151, 5.0129, 4.9528],
        [4.9151, 5.7268, 5.9878],
        [4.9151, 4.9970, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13362102210521698 
model_pd.l_d.mean(): -20.285629272460938 
model_pd.lagr.mean(): -20.152008056640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0097], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13362102210521698
epoch£º2	 i:1 	 global-step:41	 l-p:0.10827109217643738
epoch£º2	 i:2 	 global-step:42	 l-p:0.10365663468837738
epoch£º2	 i:3 	 global-step:43	 l-p:0.10691878944635391
epoch£º2	 i:4 	 global-step:44	 l-p:0.11889299005270004
epoch£º2	 i:5 	 global-step:45	 l-p:0.10603377968072891
epoch£º2	 i:6 	 global-step:46	 l-p:0.11256348341703415
epoch£º2	 i:7 	 global-step:47	 l-p:-0.5110002756118774
epoch£º2	 i:8 	 global-step:48	 l-p:0.10819507390260696
epoch£º2	 i:9 	 global-step:49	 l-p:0.11759166419506073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3947, 6.4841, 6.9565],
        [5.3947, 5.4457, 5.4069],
        [5.3947, 5.5007, 5.4350],
        [5.3947, 6.9665, 8.0013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08163749426603317 
model_pd.l_d.mean(): -18.719287872314453 
model_pd.lagr.mean(): -18.637649536132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0105], device='cuda:0')), ('power', tensor([0.9893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08163749426603317
epoch£º3	 i:1 	 global-step:61	 l-p:0.08724100142717361
epoch£º3	 i:2 	 global-step:62	 l-p:0.000394015311030671
epoch£º3	 i:3 	 global-step:63	 l-p:0.12234185636043549
epoch£º3	 i:4 	 global-step:64	 l-p:0.11887037754058838
epoch£º3	 i:5 	 global-step:65	 l-p:0.11695914715528488
epoch£º3	 i:6 	 global-step:66	 l-p:0.11347242444753647
epoch£º3	 i:7 	 global-step:67	 l-p:0.11854049563407898
epoch£º3	 i:8 	 global-step:68	 l-p:0.12620234489440918
epoch£º3	 i:9 	 global-step:69	 l-p:0.12451384216547012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9813, 5.7048, 5.8822],
        [4.9813, 5.8649, 6.1865],
        [4.9813, 6.5220, 7.6084],
        [4.9813, 5.0696, 5.0129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11540452390909195 
model_pd.l_d.mean(): -19.0565185546875 
model_pd.lagr.mean(): -18.94111442565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-19.7592], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11540452390909195
epoch£º4	 i:1 	 global-step:81	 l-p:0.1313905268907547
epoch£º4	 i:2 	 global-step:82	 l-p:0.16934607923030853
epoch£º4	 i:3 	 global-step:83	 l-p:0.13243471086025238
epoch£º4	 i:4 	 global-step:84	 l-p:0.15472617745399475
epoch£º4	 i:5 	 global-step:85	 l-p:0.12686507403850555
epoch£º4	 i:6 	 global-step:86	 l-p:0.1303711235523224
epoch£º4	 i:7 	 global-step:87	 l-p:0.13439692556858063
epoch£º4	 i:8 	 global-step:88	 l-p:0.1405920684337616
epoch£º4	 i:9 	 global-step:89	 l-p:0.16175119578838348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6544, 4.6545, 4.6544],
        [4.6544, 4.6605, 4.6548],
        [4.6544, 4.7478, 4.6909],
        [4.6544, 4.7883, 4.7203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18698807060718536 
model_pd.l_d.mean(): -20.1062068939209 
model_pd.lagr.mean(): -19.919218063354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.8780], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18698807060718536
epoch£º5	 i:1 	 global-step:101	 l-p:0.15168975293636322
epoch£º5	 i:2 	 global-step:102	 l-p:0.16872403025627136
epoch£º5	 i:3 	 global-step:103	 l-p:0.08486352860927582
epoch£º5	 i:4 	 global-step:104	 l-p:0.13325311243534088
epoch£º5	 i:5 	 global-step:105	 l-p:0.15382957458496094
epoch£º5	 i:6 	 global-step:106	 l-p:0.12070516496896744
epoch£º5	 i:7 	 global-step:107	 l-p:0.13906458020210266
epoch£º5	 i:8 	 global-step:108	 l-p:0.13696497678756714
epoch£º5	 i:9 	 global-step:109	 l-p:0.13650542497634888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8654, 4.8931, 4.8703],
        [4.8654, 4.8654, 4.8654],
        [4.8654, 6.2734, 7.2129],
        [4.8654, 4.8702, 4.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13271291553974152 
model_pd.l_d.mean(): -20.583019256591797 
model_pd.lagr.mean(): -20.450305938720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.2438], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13271291553974152
epoch£º6	 i:1 	 global-step:121	 l-p:0.13211940228939056
epoch£º6	 i:2 	 global-step:122	 l-p:0.13903596997261047
epoch£º6	 i:3 	 global-step:123	 l-p:0.15075421333312988
epoch£º6	 i:4 	 global-step:124	 l-p:0.12953267991542816
epoch£º6	 i:5 	 global-step:125	 l-p:0.15238814055919647
epoch£º6	 i:6 	 global-step:126	 l-p:0.13753646612167358
epoch£º6	 i:7 	 global-step:127	 l-p:0.12962596118450165
epoch£º6	 i:8 	 global-step:128	 l-p:0.1311018466949463
epoch£º6	 i:9 	 global-step:129	 l-p:0.12418382614850998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 5.5407, 5.6706],
        [4.8894, 4.9519, 4.9077],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 4.9105, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12779361009597778 
model_pd.l_d.mean(): -20.454792022705078 
model_pd.lagr.mean(): -20.326997756958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.1229], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12779361009597778
epoch£º7	 i:1 	 global-step:141	 l-p:0.1182163804769516
epoch£º7	 i:2 	 global-step:142	 l-p:0.13610458374023438
epoch£º7	 i:3 	 global-step:143	 l-p:0.1380041539669037
epoch£º7	 i:4 	 global-step:144	 l-p:0.1488667130470276
epoch£º7	 i:5 	 global-step:145	 l-p:-0.23913636803627014
epoch£º7	 i:6 	 global-step:146	 l-p:0.12930674850940704
epoch£º7	 i:7 	 global-step:147	 l-p:0.1389794945716858
epoch£º7	 i:8 	 global-step:148	 l-p:0.16634242236614227
epoch£º7	 i:9 	 global-step:149	 l-p:0.14020872116088867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6979, 4.6979, 4.6979],
        [4.6979, 4.6980, 4.6979],
        [4.6979, 5.4962, 5.7741],
        [4.6979, 4.8409, 4.7712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1339530199766159 
model_pd.l_d.mean(): -18.755706787109375 
model_pd.lagr.mean(): -18.621753692626953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-19.5433], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1339530199766159
epoch£º8	 i:1 	 global-step:161	 l-p:0.10942623019218445
epoch£º8	 i:2 	 global-step:162	 l-p:0.13943679630756378
epoch£º8	 i:3 	 global-step:163	 l-p:0.1424546241760254
epoch£º8	 i:4 	 global-step:164	 l-p:0.1277136206626892
epoch£º8	 i:5 	 global-step:165	 l-p:0.1480616182088852
epoch£º8	 i:6 	 global-step:166	 l-p:0.12972120940685272
epoch£º8	 i:7 	 global-step:167	 l-p:0.1317291259765625
epoch£º8	 i:8 	 global-step:168	 l-p:0.140774667263031
epoch£º8	 i:9 	 global-step:169	 l-p:0.14067496359348297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8656, 4.8674, 4.8656],
        [4.8656, 4.8675, 4.8656],
        [4.8656, 5.0135, 4.9413],
        [4.8656, 4.8864, 4.8686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.13530603051185608 
model_pd.l_d.mean(): -17.66032600402832 
model_pd.lagr.mean(): -17.525020599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5548], device='cuda:0')), ('power', tensor([-18.4200], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.13530603051185608
epoch£º9	 i:1 	 global-step:181	 l-p:0.12824425101280212
epoch£º9	 i:2 	 global-step:182	 l-p:0.13838236033916473
epoch£º9	 i:3 	 global-step:183	 l-p:0.13025568425655365
epoch£º9	 i:4 	 global-step:184	 l-p:0.12077601999044418
epoch£º9	 i:5 	 global-step:185	 l-p:0.14665475487709045
epoch£º9	 i:6 	 global-step:186	 l-p:0.16524989902973175
epoch£º9	 i:7 	 global-step:187	 l-p:0.12909722328186035
epoch£º9	 i:8 	 global-step:188	 l-p:0.12787218391895294
epoch£º9	 i:9 	 global-step:189	 l-p:0.10969669371843338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8094, 6.0339, 6.7596],
        [4.8094, 5.6447, 5.9466],
        [4.8094, 4.8145, 4.8097],
        [4.8094, 4.8094, 4.8094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15206678211688995 
model_pd.l_d.mean(): -20.224409103393555 
model_pd.lagr.mean(): -20.072341918945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.9380], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15206678211688995
epoch£º10	 i:1 	 global-step:201	 l-p:0.1391996592283249
epoch£º10	 i:2 	 global-step:202	 l-p:0.11491090804338455
epoch£º10	 i:3 	 global-step:203	 l-p:0.13253697752952576
epoch£º10	 i:4 	 global-step:204	 l-p:0.1327715665102005
epoch£º10	 i:5 	 global-step:205	 l-p:0.1397324502468109
epoch£º10	 i:6 	 global-step:206	 l-p:0.14992031455039978
epoch£º10	 i:7 	 global-step:207	 l-p:0.1646294742822647
epoch£º10	 i:8 	 global-step:208	 l-p:-0.05704508721828461
epoch£º10	 i:9 	 global-step:209	 l-p:0.1418544501066208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7934, 5.3127, 5.3589],
        [4.7934, 6.1000, 6.9314],
        [4.7934, 5.5043, 5.6960],
        [4.7934, 4.8165, 4.7971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11840341240167618 
model_pd.l_d.mean(): -20.091371536254883 
model_pd.lagr.mean(): -19.97296905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.8150], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11840341240167618
epoch£º11	 i:1 	 global-step:221	 l-p:0.1498481184244156
epoch£º11	 i:2 	 global-step:222	 l-p:0.12453683465719223
epoch£º11	 i:3 	 global-step:223	 l-p:0.1274459958076477
epoch£º11	 i:4 	 global-step:224	 l-p:0.14309555292129517
epoch£º11	 i:5 	 global-step:225	 l-p:0.13841676712036133
epoch£º11	 i:6 	 global-step:226	 l-p:0.12570175528526306
epoch£º11	 i:7 	 global-step:227	 l-p:0.20622022449970245
epoch£º11	 i:8 	 global-step:228	 l-p:0.1232600063085556
epoch£º11	 i:9 	 global-step:229	 l-p:0.15526573359966278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9188, 5.3213, 5.2957],
        [4.9188, 4.9555, 4.9265],
        [4.9188, 4.9499, 4.9247],
        [4.9188, 5.0891, 5.0141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12307965755462646 
model_pd.l_d.mean(): -19.340322494506836 
model_pd.lagr.mean(): -19.217243194580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.0808], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12307965755462646
epoch£º12	 i:1 	 global-step:241	 l-p:0.1360098421573639
epoch£º12	 i:2 	 global-step:242	 l-p:0.1200462281703949
epoch£º12	 i:3 	 global-step:243	 l-p:0.13655756413936615
epoch£º12	 i:4 	 global-step:244	 l-p:0.13908500969409943
epoch£º12	 i:5 	 global-step:245	 l-p:0.13875550031661987
epoch£º12	 i:6 	 global-step:246	 l-p:0.13550697267055511
epoch£º12	 i:7 	 global-step:247	 l-p:0.13369400799274445
epoch£º12	 i:8 	 global-step:248	 l-p:0.13904595375061035
epoch£º12	 i:9 	 global-step:249	 l-p:0.09761043637990952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8530, 5.9949, 6.6194],
        [4.8530, 4.8530, 4.8530],
        [4.8530, 4.8563, 4.8532],
        [4.8530, 5.2476, 5.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12329402565956116 
model_pd.l_d.mean(): -18.70701026916504 
model_pd.lagr.mean(): -18.583715438842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-19.4482], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12329402565956116
epoch£º13	 i:1 	 global-step:261	 l-p:0.18997308611869812
epoch£º13	 i:2 	 global-step:262	 l-p:0.1283417046070099
epoch£º13	 i:3 	 global-step:263	 l-p:0.1305587887763977
epoch£º13	 i:4 	 global-step:264	 l-p:0.13821493089199066
epoch£º13	 i:5 	 global-step:265	 l-p:0.14281265437602997
epoch£º13	 i:6 	 global-step:266	 l-p:0.11233367025852203
epoch£º13	 i:7 	 global-step:267	 l-p:0.1348501741886139
epoch£º13	 i:8 	 global-step:268	 l-p:0.1386839896440506
epoch£º13	 i:9 	 global-step:269	 l-p:0.12354252487421036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8309, 4.8310, 4.8309],
        [4.8309, 5.7266, 6.0897],
        [4.8309, 4.8650, 4.8379],
        [4.8309, 5.6607, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1509508639574051 
model_pd.l_d.mean(): -18.576194763183594 
model_pd.lagr.mean(): -18.425243377685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-19.3109], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.1509508639574051
epoch£º14	 i:1 	 global-step:281	 l-p:0.12489312887191772
epoch£º14	 i:2 	 global-step:282	 l-p:0.16707393527030945
epoch£º14	 i:3 	 global-step:283	 l-p:0.14096592366695404
epoch£º14	 i:4 	 global-step:284	 l-p:0.13556167483329773
epoch£º14	 i:5 	 global-step:285	 l-p:0.1230582669377327
epoch£º14	 i:6 	 global-step:286	 l-p:0.11791884899139404
epoch£º14	 i:7 	 global-step:287	 l-p:0.1315152496099472
epoch£º14	 i:8 	 global-step:288	 l-p:0.1315719336271286
epoch£º14	 i:9 	 global-step:289	 l-p:0.015737637877464294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7477, 4.7786, 4.7538],
        [4.7477, 4.7514, 4.7479],
        [4.7477, 4.9185, 4.8467],
        [4.7477, 4.7582, 4.7488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.1557796150445938 
model_pd.l_d.mean(): -20.54967498779297 
model_pd.lagr.mean(): -20.393896102905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2513], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.1557796150445938
epoch£º15	 i:1 	 global-step:301	 l-p:0.16218039393424988
epoch£º15	 i:2 	 global-step:302	 l-p:0.12420011311769485
epoch£º15	 i:3 	 global-step:303	 l-p:0.14429372549057007
epoch£º15	 i:4 	 global-step:304	 l-p:0.15843605995178223
epoch£º15	 i:5 	 global-step:305	 l-p:0.14607249200344086
epoch£º15	 i:6 	 global-step:306	 l-p:0.11911492794752121
epoch£º15	 i:7 	 global-step:307	 l-p:0.14245353639125824
epoch£º15	 i:8 	 global-step:308	 l-p:0.13309884071350098
epoch£º15	 i:9 	 global-step:309	 l-p:0.2072717547416687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 4.8642, 4.8624],
        [4.8623, 4.8884, 4.8668],
        [4.8623, 4.8627, 4.8623],
        [4.8623, 5.2191, 5.1793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12061794847249985 
model_pd.l_d.mean(): -18.991838455200195 
model_pd.lagr.mean(): -18.871219635009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-19.6958], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12061794847249985
epoch£º16	 i:1 	 global-step:321	 l-p:0.11633148044347763
epoch£º16	 i:2 	 global-step:322	 l-p:0.15113627910614014
epoch£º16	 i:3 	 global-step:323	 l-p:0.135508194565773
epoch£º16	 i:4 	 global-step:324	 l-p:0.1245483011007309
epoch£º16	 i:5 	 global-step:325	 l-p:0.11478719115257263
epoch£º16	 i:6 	 global-step:326	 l-p:0.13394208252429962
epoch£º16	 i:7 	 global-step:327	 l-p:0.15472891926765442
epoch£º16	 i:8 	 global-step:328	 l-p:0.13002780079841614
epoch£º16	 i:9 	 global-step:329	 l-p:0.12865927815437317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9090, 5.8365, 6.2249],
        [4.9090, 5.2229, 5.1669],
        [4.9090, 4.9091, 4.9090],
        [4.9090, 5.8243, 6.2004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.16058342158794403 
model_pd.l_d.mean(): -20.177383422851562 
model_pd.lagr.mean(): -20.016799926757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.8588], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.16058342158794403
epoch£º17	 i:1 	 global-step:341	 l-p:0.12124836444854736
epoch£º17	 i:2 	 global-step:342	 l-p:0.13692811131477356
epoch£º17	 i:3 	 global-step:343	 l-p:0.13877074420452118
epoch£º17	 i:4 	 global-step:344	 l-p:0.13086701929569244
epoch£º17	 i:5 	 global-step:345	 l-p:0.12558455765247345
epoch£º17	 i:6 	 global-step:346	 l-p:0.1373773217201233
epoch£º17	 i:7 	 global-step:347	 l-p:0.14227215945720673
epoch£º17	 i:8 	 global-step:348	 l-p:0.13048991560935974
epoch£º17	 i:9 	 global-step:349	 l-p:0.13335996866226196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7508, 4.7988, 4.7634],
        [4.7508, 4.7511, 4.7509],
        [4.7508, 4.8621, 4.8004],
        [4.7508, 4.7632, 4.7522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14244864881038666 
model_pd.l_d.mean(): -20.058279037475586 
model_pd.lagr.mean(): -19.915830612182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8015], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14244864881038666
epoch£º18	 i:1 	 global-step:361	 l-p:0.19642041623592377
epoch£º18	 i:2 	 global-step:362	 l-p:0.13525113463401794
epoch£º18	 i:3 	 global-step:363	 l-p:0.17519474029541016
epoch£º18	 i:4 	 global-step:364	 l-p:0.1319628357887268
epoch£º18	 i:5 	 global-step:365	 l-p:0.13759176433086395
epoch£º18	 i:6 	 global-step:366	 l-p:0.13375209271907806
epoch£º18	 i:7 	 global-step:367	 l-p:0.05589083209633827
epoch£º18	 i:8 	 global-step:368	 l-p:0.1406274288892746
epoch£º18	 i:9 	 global-step:369	 l-p:0.14228591322898865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7707, 4.9428, 4.8715],
        [4.7707, 4.8001, 4.7763],
        [4.7707, 4.7707, 4.7707],
        [4.7707, 4.9752, 4.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.14762669801712036 
model_pd.l_d.mean(): -20.522296905517578 
model_pd.lagr.mean(): -20.374670028686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.2100], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.14762669801712036
epoch£º19	 i:1 	 global-step:381	 l-p:0.14677515625953674
epoch£º19	 i:2 	 global-step:382	 l-p:0.133392795920372
epoch£º19	 i:3 	 global-step:383	 l-p:0.13147056102752686
epoch£º19	 i:4 	 global-step:384	 l-p:0.12496723234653473
epoch£º19	 i:5 	 global-step:385	 l-p:0.14536027610301971
epoch£º19	 i:6 	 global-step:386	 l-p:0.12979179620742798
epoch£º19	 i:7 	 global-step:387	 l-p:0.12461087852716446
epoch£º19	 i:8 	 global-step:388	 l-p:0.10972227156162262
epoch£º19	 i:9 	 global-step:389	 l-p:0.1706894040107727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9244, 4.9271, 4.9245],
        [4.9244, 5.3463, 5.3350],
        [4.9244, 4.9666, 4.9343],
        [4.9244, 4.9853, 4.9425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12436626106500626 
model_pd.l_d.mean(): -20.474315643310547 
model_pd.lagr.mean(): -20.34994888305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.1301], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12436626106500626
epoch£º20	 i:1 	 global-step:401	 l-p:0.12764941155910492
epoch£º20	 i:2 	 global-step:402	 l-p:0.12204652279615402
epoch£º20	 i:3 	 global-step:403	 l-p:0.19470283389091492
epoch£º20	 i:4 	 global-step:404	 l-p:0.14041435718536377
epoch£º20	 i:5 	 global-step:405	 l-p:0.13922637701034546
epoch£º20	 i:6 	 global-step:406	 l-p:0.12474101036787033
epoch£º20	 i:7 	 global-step:407	 l-p:0.13122020661830902
epoch£º20	 i:8 	 global-step:408	 l-p:0.1255127638578415
epoch£º20	 i:9 	 global-step:409	 l-p:0.12827926874160767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9206, 6.3322, 7.2834],
        [4.9206, 4.9820, 4.9390],
        [4.9206, 4.9206, 4.9206],
        [4.9206, 5.0205, 4.9614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13278909027576447 
model_pd.l_d.mean(): -20.31562614440918 
model_pd.lagr.mean(): -20.182836532592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13278909027576447
epoch£º21	 i:1 	 global-step:421	 l-p:0.12889739871025085
epoch£º21	 i:2 	 global-step:422	 l-p:0.13648802042007446
epoch£º21	 i:3 	 global-step:423	 l-p:0.13070148229599
epoch£º21	 i:4 	 global-step:424	 l-p:0.16298241913318634
epoch£º21	 i:5 	 global-step:425	 l-p:0.13520163297653198
epoch£º21	 i:6 	 global-step:426	 l-p:0.15799671411514282
epoch£º21	 i:7 	 global-step:427	 l-p:0.16642926633358002
epoch£º21	 i:8 	 global-step:428	 l-p:0.1887708455324173
epoch£º21	 i:9 	 global-step:429	 l-p:0.13467790186405182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7039, 4.8001, 4.7437],
        [4.7039, 4.9843, 4.9283],
        [4.7039, 4.7060, 4.7040],
        [4.7039, 4.7340, 4.7099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18138854205608368 
model_pd.l_d.mean(): -20.52960968017578 
model_pd.lagr.mean(): -20.348220825195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-21.2463], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18138854205608368
epoch£º22	 i:1 	 global-step:441	 l-p:0.13949167728424072
epoch£º22	 i:2 	 global-step:442	 l-p:0.13412922620773315
epoch£º22	 i:3 	 global-step:443	 l-p:0.1333840936422348
epoch£º22	 i:4 	 global-step:444	 l-p:0.11404075473546982
epoch£º22	 i:5 	 global-step:445	 l-p:0.14604105055332184
epoch£º22	 i:6 	 global-step:446	 l-p:0.11981405317783356
epoch£º22	 i:7 	 global-step:447	 l-p:0.13960476219654083
epoch£º22	 i:8 	 global-step:448	 l-p:0.135689839720726
epoch£º22	 i:9 	 global-step:449	 l-p:0.12130016088485718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0005, 6.4325, 7.3950],
        [5.0005, 5.5550, 5.6191],
        [5.0005, 5.0010, 5.0005],
        [5.0005, 5.9315, 6.3177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.1050717905163765 
model_pd.l_d.mean(): -19.999608993530273 
model_pd.lagr.mean(): -19.8945369720459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-20.6708], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.1050717905163765
epoch£º23	 i:1 	 global-step:461	 l-p:0.13177451491355896
epoch£º23	 i:2 	 global-step:462	 l-p:0.13836413621902466
epoch£º23	 i:3 	 global-step:463	 l-p:0.11578373610973358
epoch£º23	 i:4 	 global-step:464	 l-p:0.1382008194923401
epoch£º23	 i:5 	 global-step:465	 l-p:0.1318320482969284
epoch£º23	 i:6 	 global-step:466	 l-p:0.15632224082946777
epoch£º23	 i:7 	 global-step:467	 l-p:0.12825074791908264
epoch£º23	 i:8 	 global-step:468	 l-p:0.14314305782318115
epoch£º23	 i:9 	 global-step:469	 l-p:0.14557769894599915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8297, 5.2773, 5.2875],
        [4.8297, 5.1130, 5.0544],
        [4.8297, 4.9098, 4.8588],
        [4.8297, 4.8297, 4.8297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13809999823570251 
model_pd.l_d.mean(): -20.95723533630371 
model_pd.lagr.mean(): -20.819135665893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4010], device='cuda:0')), ('power', tensor([-21.5958], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.13809999823570251
epoch£º24	 i:1 	 global-step:481	 l-p:0.14638927578926086
epoch£º24	 i:2 	 global-step:482	 l-p:0.12133976817131042
epoch£º24	 i:3 	 global-step:483	 l-p:0.12021716684103012
epoch£º24	 i:4 	 global-step:484	 l-p:0.14195884764194489
epoch£º24	 i:5 	 global-step:485	 l-p:0.11674609035253525
epoch£º24	 i:6 	 global-step:486	 l-p:0.14352989196777344
epoch£º24	 i:7 	 global-step:487	 l-p:0.14084389805793762
epoch£º24	 i:8 	 global-step:488	 l-p:0.18713420629501343
epoch£º24	 i:9 	 global-step:489	 l-p:0.12445337325334549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 6.0145, 6.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13400670886039734 
model_pd.l_d.mean(): -18.3502140045166 
model_pd.lagr.mean(): -18.21620750427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5760], device='cuda:0')), ('power', tensor([-19.1391], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13400670886039734
epoch£º25	 i:1 	 global-step:501	 l-p:0.13481579720973969
epoch£º25	 i:2 	 global-step:502	 l-p:0.14243881404399872
epoch£º25	 i:3 	 global-step:503	 l-p:0.12411126494407654
epoch£º25	 i:4 	 global-step:504	 l-p:0.38716593384742737
epoch£º25	 i:5 	 global-step:505	 l-p:0.1474734991788864
epoch£º25	 i:6 	 global-step:506	 l-p:0.13004808127880096
epoch£º25	 i:7 	 global-step:507	 l-p:0.1287354826927185
epoch£º25	 i:8 	 global-step:508	 l-p:0.14900624752044678
epoch£º25	 i:9 	 global-step:509	 l-p:0.12858454883098602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8317, 5.0213, 4.9503],
        [4.8317, 5.4288, 5.5392],
        [4.8317, 4.8906, 4.8494],
        [4.8317, 5.3466, 5.3990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15572410821914673 
model_pd.l_d.mean(): -20.57049560546875 
model_pd.lagr.mean(): -20.414772033691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15572410821914673
epoch£º26	 i:1 	 global-step:521	 l-p:0.13858217000961304
epoch£º26	 i:2 	 global-step:522	 l-p:0.1395915448665619
epoch£º26	 i:3 	 global-step:523	 l-p:0.12689198553562164
epoch£º26	 i:4 	 global-step:524	 l-p:0.005153598729521036
epoch£º26	 i:5 	 global-step:525	 l-p:0.15632960200309753
epoch£º26	 i:6 	 global-step:526	 l-p:0.14128828048706055
epoch£º26	 i:7 	 global-step:527	 l-p:0.1435181200504303
epoch£º26	 i:8 	 global-step:528	 l-p:0.14050215482711792
epoch£º26	 i:9 	 global-step:529	 l-p:0.12953272461891174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8516, 4.9670, 4.9046],
        [4.8516, 5.1808, 5.1367],
        [4.8516, 4.8516, 4.8516],
        [4.8516, 4.8541, 4.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12485390156507492 
model_pd.l_d.mean(): -18.25809097290039 
model_pd.lagr.mean(): -18.133237838745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5422], device='cuda:0')), ('power', tensor([-19.0114], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.12485390156507492
epoch£º27	 i:1 	 global-step:541	 l-p:0.13130289316177368
epoch£º27	 i:2 	 global-step:542	 l-p:0.11955615133047104
epoch£º27	 i:3 	 global-step:543	 l-p:0.14031007885932922
epoch£º27	 i:4 	 global-step:544	 l-p:0.13030825555324554
epoch£º27	 i:5 	 global-step:545	 l-p:0.13881301879882812
epoch£º27	 i:6 	 global-step:546	 l-p:0.13480719923973083
epoch£º27	 i:7 	 global-step:547	 l-p:0.14729687571525574
epoch£º27	 i:8 	 global-step:548	 l-p:0.38011831045150757
epoch£º27	 i:9 	 global-step:549	 l-p:0.13564205169677734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8318, 5.0296, 4.9593],
        [4.8318, 5.4246, 5.5333],
        [4.8318, 4.8796, 4.8444],
        [4.8318, 6.1340, 6.9726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13329292833805084 
model_pd.l_d.mean(): -19.907880783081055 
model_pd.lagr.mean(): -19.774587631225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.6358], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13329292833805084
epoch£º28	 i:1 	 global-step:561	 l-p:0.16080474853515625
epoch£º28	 i:2 	 global-step:562	 l-p:0.12945449352264404
epoch£º28	 i:3 	 global-step:563	 l-p:0.12828055024147034
epoch£º28	 i:4 	 global-step:564	 l-p:0.13639262318611145
epoch£º28	 i:5 	 global-step:565	 l-p:0.16054658591747284
epoch£º28	 i:6 	 global-step:566	 l-p:0.12322445958852768
epoch£º28	 i:7 	 global-step:567	 l-p:0.12307044863700867
epoch£º28	 i:8 	 global-step:568	 l-p:0.12756824493408203
epoch£º28	 i:9 	 global-step:569	 l-p:0.13800300657749176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0150, 5.0150, 5.0150],
        [5.0150, 5.2199, 5.1464],
        [5.0150, 5.6643, 5.8007],
        [5.0150, 5.0193, 5.0152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.13617472350597382 
model_pd.l_d.mean(): -19.668392181396484 
model_pd.lagr.mean(): -19.532217025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-20.3400], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.13617472350597382
epoch£º29	 i:1 	 global-step:581	 l-p:0.10737068206071854
epoch£º29	 i:2 	 global-step:582	 l-p:0.11962787061929703
epoch£º29	 i:3 	 global-step:583	 l-p:0.13836073875427246
epoch£º29	 i:4 	 global-step:584	 l-p:0.1320611983537674
epoch£º29	 i:5 	 global-step:585	 l-p:0.1399259865283966
epoch£º29	 i:6 	 global-step:586	 l-p:0.2121720314025879
epoch£º29	 i:7 	 global-step:587	 l-p:0.1248352900147438
epoch£º29	 i:8 	 global-step:588	 l-p:0.12869445979595184
epoch£º29	 i:9 	 global-step:589	 l-p:0.13239255547523499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 4.8349, 4.8334],
        [4.8334, 6.1192, 6.9391],
        [4.8334, 5.0449, 4.9758],
        [4.8334, 6.0008, 6.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.15087294578552246 
model_pd.l_d.mean(): -18.95175552368164 
model_pd.lagr.mean(): -18.80088233947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5613], device='cuda:0')), ('power', tensor([-19.7322], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.15087294578552246
epoch£º30	 i:1 	 global-step:601	 l-p:0.12216286361217499
epoch£º30	 i:2 	 global-step:602	 l-p:0.12749528884887695
epoch£º30	 i:3 	 global-step:603	 l-p:0.012068686075508595
epoch£º30	 i:4 	 global-step:604	 l-p:0.13114161789417267
epoch£º30	 i:5 	 global-step:605	 l-p:0.16880197823047638
epoch£º30	 i:6 	 global-step:606	 l-p:0.1379488855600357
epoch£º30	 i:7 	 global-step:607	 l-p:0.1307259351015091
epoch£º30	 i:8 	 global-step:608	 l-p:0.1297648400068283
epoch£º30	 i:9 	 global-step:609	 l-p:0.15520355105400085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7230, 4.7230, 4.7230],
        [4.7230, 4.7309, 4.7237],
        [4.7230, 6.0393, 6.9206],
        [4.7230, 4.7768, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13403606414794922 
model_pd.l_d.mean(): -18.069232940673828 
model_pd.lagr.mean(): -17.935195922851562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6331], device='cuda:0')), ('power', tensor([-18.9134], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13403606414794922
epoch£º31	 i:1 	 global-step:621	 l-p:0.17098064720630646
epoch£º31	 i:2 	 global-step:622	 l-p:0.12065848708152771
epoch£º31	 i:3 	 global-step:623	 l-p:0.13284112513065338
epoch£º31	 i:4 	 global-step:624	 l-p:0.13170944154262543
epoch£º31	 i:5 	 global-step:625	 l-p:0.1265157163143158
epoch£º31	 i:6 	 global-step:626	 l-p:0.14885328710079193
epoch£º31	 i:7 	 global-step:627	 l-p:0.2063962072134018
epoch£º31	 i:8 	 global-step:628	 l-p:0.14710360765457153
epoch£º31	 i:9 	 global-step:629	 l-p:0.1636175513267517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 6.2453, 7.2551],
        [4.8148, 5.7434, 6.1608],
        [4.8148, 4.8712, 4.8316],
        [4.8148, 4.8162, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13486388325691223 
model_pd.l_d.mean(): -19.91747283935547 
model_pd.lagr.mean(): -19.782608032226562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.6232], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13486388325691223
epoch£º32	 i:1 	 global-step:641	 l-p:0.14056812226772308
epoch£º32	 i:2 	 global-step:642	 l-p:0.15270821750164032
epoch£º32	 i:3 	 global-step:643	 l-p:0.12265190482139587
epoch£º32	 i:4 	 global-step:644	 l-p:0.17840047180652618
epoch£º32	 i:5 	 global-step:645	 l-p:0.13909025490283966
epoch£º32	 i:6 	 global-step:646	 l-p:0.15400303900241852
epoch£º32	 i:7 	 global-step:647	 l-p:0.2805700898170471
epoch£º32	 i:8 	 global-step:648	 l-p:0.11922380328178406
epoch£º32	 i:9 	 global-step:649	 l-p:0.1317543387413025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9064, 4.9066, 4.9064],
        [4.9064, 5.1508, 5.0846],
        [4.9064, 4.9070, 4.9064],
        [4.9064, 5.1510, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11790164560079575 
model_pd.l_d.mean(): -20.35063934326172 
model_pd.lagr.mean(): -20.232738494873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4456], device='cuda:0')), ('power', tensor([-21.0282], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11790164560079575
epoch£º33	 i:1 	 global-step:661	 l-p:0.20176950097084045
epoch£º33	 i:2 	 global-step:662	 l-p:0.1389603614807129
epoch£º33	 i:3 	 global-step:663	 l-p:0.11760108917951584
epoch£º33	 i:4 	 global-step:664	 l-p:0.14047464728355408
epoch£º33	 i:5 	 global-step:665	 l-p:0.13330680131912231
epoch£º33	 i:6 	 global-step:666	 l-p:0.14870111644268036
epoch£º33	 i:7 	 global-step:667	 l-p:0.15926694869995117
epoch£º33	 i:8 	 global-step:668	 l-p:0.13533762097358704
epoch£º33	 i:9 	 global-step:669	 l-p:0.19962242245674133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 5.3299, 5.4378],
        [4.7523, 4.7524, 4.7523],
        [4.7523, 5.0221, 4.9659],
        [4.7523, 4.8057, 4.7678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.15001283586025238 
model_pd.l_d.mean(): -20.410314559936523 
model_pd.lagr.mean(): -20.26030158996582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-21.1281], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.15001283586025238
epoch£º34	 i:1 	 global-step:681	 l-p:0.14427445828914642
epoch£º34	 i:2 	 global-step:682	 l-p:0.13363514840602875
epoch£º34	 i:3 	 global-step:683	 l-p:0.12760819494724274
epoch£º34	 i:4 	 global-step:684	 l-p:0.15792132914066315
epoch£º34	 i:5 	 global-step:685	 l-p:0.14213691651821136
epoch£º34	 i:6 	 global-step:686	 l-p:0.14004305005073547
epoch£º34	 i:7 	 global-step:687	 l-p:0.13657212257385254
epoch£º34	 i:8 	 global-step:688	 l-p:0.12412823736667633
epoch£º34	 i:9 	 global-step:689	 l-p:0.17167556285858154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.3872, 5.4008],
        [4.9326, 4.9326, 4.9326],
        [4.9326, 5.0646, 4.9986],
        [4.9326, 6.2882, 7.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.11937922239303589 
model_pd.l_d.mean(): -19.902976989746094 
model_pd.lagr.mean(): -19.783597946166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.6084], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.11937922239303589
epoch£º35	 i:1 	 global-step:701	 l-p:0.1334604173898697
epoch£º35	 i:2 	 global-step:702	 l-p:0.15160147845745087
epoch£º35	 i:3 	 global-step:703	 l-p:0.1281462013721466
epoch£º35	 i:4 	 global-step:704	 l-p:0.115984246134758
epoch£º35	 i:5 	 global-step:705	 l-p:0.1319243460893631
epoch£º35	 i:6 	 global-step:706	 l-p:0.1484459936618805
epoch£º35	 i:7 	 global-step:707	 l-p:0.1274743229150772
epoch£º35	 i:8 	 global-step:708	 l-p:0.10740560293197632
epoch£º35	 i:9 	 global-step:709	 l-p:0.12258850038051605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8828, 4.8829, 4.8828],
        [4.8828, 4.8828, 4.8828],
        [4.8828, 5.5177, 5.6592],
        [4.8828, 4.9286, 4.8947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.1355714350938797 
model_pd.l_d.mean(): -19.0329647064209 
model_pd.lagr.mean(): -18.89739418029785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-19.7650], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.1355714350938797
epoch£º36	 i:1 	 global-step:721	 l-p:0.11875178664922714
epoch£º36	 i:2 	 global-step:722	 l-p:0.1439608931541443
epoch£º36	 i:3 	 global-step:723	 l-p:0.11945179849863052
epoch£º36	 i:4 	 global-step:724	 l-p:0.13529907166957855
epoch£º36	 i:5 	 global-step:725	 l-p:0.15217405557632446
epoch£º36	 i:6 	 global-step:726	 l-p:0.15418706834316254
epoch£º36	 i:7 	 global-step:727	 l-p:0.13982172310352325
epoch£º36	 i:8 	 global-step:728	 l-p:-0.24995221197605133
epoch£º36	 i:9 	 global-step:729	 l-p:0.13167326152324677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8259, 4.8204],
        [4.8199, 6.0991, 6.9193],
        [4.8199, 5.2085, 5.1949],
        [4.8199, 5.3073, 5.3491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.12874707579612732 
model_pd.l_d.mean(): -18.535175323486328 
model_pd.lagr.mean(): -18.40642738342285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.3218], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.12874707579612732
epoch£º37	 i:1 	 global-step:741	 l-p:0.12683479487895966
epoch£º37	 i:2 	 global-step:742	 l-p:0.23318243026733398
epoch£º37	 i:3 	 global-step:743	 l-p:0.12618303298950195
epoch£º37	 i:4 	 global-step:744	 l-p:0.14028604328632355
epoch£º37	 i:5 	 global-step:745	 l-p:0.13209828734397888
epoch£º37	 i:6 	 global-step:746	 l-p:0.12678880989551544
epoch£º37	 i:7 	 global-step:747	 l-p:0.11798848956823349
epoch£º37	 i:8 	 global-step:748	 l-p:0.12296485900878906
epoch£º37	 i:9 	 global-step:749	 l-p:0.1188797801733017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9074, 5.0170, 4.9567],
        [4.9074, 4.9725, 4.9285],
        [4.9074, 4.9326, 4.9120],
        [4.9074, 4.9696, 4.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.1629640907049179 
model_pd.l_d.mean(): -19.890819549560547 
model_pd.lagr.mean(): -19.727855682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4836], device='cuda:0')), ('power', tensor([-20.6022], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.1629640907049179
epoch£º38	 i:1 	 global-step:761	 l-p:0.14156998693943024
epoch£º38	 i:2 	 global-step:762	 l-p:0.12921111285686493
epoch£º38	 i:3 	 global-step:763	 l-p:0.12469562888145447
epoch£º38	 i:4 	 global-step:764	 l-p:0.1333252638578415
epoch£º38	 i:5 	 global-step:765	 l-p:0.123685821890831
epoch£º38	 i:6 	 global-step:766	 l-p:0.1768995225429535
epoch£º38	 i:7 	 global-step:767	 l-p:0.13975360989570618
epoch£º38	 i:8 	 global-step:768	 l-p:0.14744094014167786
epoch£º38	 i:9 	 global-step:769	 l-p:0.12311678379774094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7669, 4.7669, 4.7669],
        [4.7669, 5.8097, 6.3630],
        [4.7669, 4.7669, 4.7669],
        [4.7669, 4.7688, 4.7670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13791783154010773 
model_pd.l_d.mean(): -20.780925750732422 
model_pd.lagr.mean(): -20.643007278442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.4596], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13791783154010773
epoch£º39	 i:1 	 global-step:781	 l-p:0.1335582733154297
epoch£º39	 i:2 	 global-step:782	 l-p:0.10831554979085922
epoch£º39	 i:3 	 global-step:783	 l-p:0.1387975513935089
epoch£º39	 i:4 	 global-step:784	 l-p:0.16921554505825043
epoch£º39	 i:5 	 global-step:785	 l-p:0.1925193965435028
epoch£º39	 i:6 	 global-step:786	 l-p:0.1455140858888626
epoch£º39	 i:7 	 global-step:787	 l-p:0.14439640939235687
epoch£º39	 i:8 	 global-step:788	 l-p:0.13691388070583344
epoch£º39	 i:9 	 global-step:789	 l-p:0.12353178858757019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9624, 4.9626, 4.9624],
        [4.9624, 4.9809, 4.9652],
        [4.9624, 5.0747, 5.0134],
        [4.9624, 4.9699, 4.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.11194141954183578 
model_pd.l_d.mean(): -20.549823760986328 
model_pd.lagr.mean(): -20.437881469726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112], device='cuda:0')), ('power', tensor([-21.1944], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.11194141954183578
epoch£º40	 i:1 	 global-step:801	 l-p:0.1301330178976059
epoch£º40	 i:2 	 global-step:802	 l-p:0.11617773026227951
epoch£º40	 i:3 	 global-step:803	 l-p:0.1514836996793747
epoch£º40	 i:4 	 global-step:804	 l-p:0.12384548038244247
epoch£º40	 i:5 	 global-step:805	 l-p:0.11679033935070038
epoch£º40	 i:6 	 global-step:806	 l-p:0.12567056715488434
epoch£º40	 i:7 	 global-step:807	 l-p:0.19722571969032288
epoch£º40	 i:8 	 global-step:808	 l-p:0.1291155368089676
epoch£º40	 i:9 	 global-step:809	 l-p:0.145138680934906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9346, 4.9372, 4.9347],
        [4.9346, 4.9349, 4.9346],
        [4.9346, 5.1178, 5.0478],
        [4.9346, 5.0489, 4.9874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1423845738172531 
model_pd.l_d.mean(): -20.475444793701172 
model_pd.lagr.mean(): -20.333059310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.1287], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.1423845738172531
epoch£º41	 i:1 	 global-step:821	 l-p:0.12187822163105011
epoch£º41	 i:2 	 global-step:822	 l-p:0.11777930706739426
epoch£º41	 i:3 	 global-step:823	 l-p:0.13271351158618927
epoch£º41	 i:4 	 global-step:824	 l-p:0.2276717722415924
epoch£º41	 i:5 	 global-step:825	 l-p:0.12362243235111237
epoch£º41	 i:6 	 global-step:826	 l-p:0.13086804747581482
epoch£º41	 i:7 	 global-step:827	 l-p:0.12329774349927902
epoch£º41	 i:8 	 global-step:828	 l-p:0.1310683637857437
epoch£º41	 i:9 	 global-step:829	 l-p:0.16322478652000427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 5.1116, 5.0574],
        [4.8334, 4.8334, 4.8334],
        [4.8334, 6.1941, 7.1179],
        [4.8334, 4.8490, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12239082157611847 
model_pd.l_d.mean(): -18.991313934326172 
model_pd.lagr.mean(): -18.86892318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5236], device='cuda:0')), ('power', tensor([-19.7336], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.12239082157611847
epoch£º42	 i:1 	 global-step:841	 l-p:0.13513097167015076
epoch£º42	 i:2 	 global-step:842	 l-p:0.13941870629787445
epoch£º42	 i:3 	 global-step:843	 l-p:0.21289879083633423
epoch£º42	 i:4 	 global-step:844	 l-p:0.15255485475063324
epoch£º42	 i:5 	 global-step:845	 l-p:0.1472131907939911
epoch£º42	 i:6 	 global-step:846	 l-p:0.1349879503250122
epoch£º42	 i:7 	 global-step:847	 l-p:0.09510646760463715
epoch£º42	 i:8 	 global-step:848	 l-p:0.13840553164482117
epoch£º42	 i:9 	 global-step:849	 l-p:0.16126468777656555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7678, 5.9701, 6.7115],
        [4.7678, 5.3089, 5.3940],
        [4.7678, 5.0661, 5.0204],
        [4.7678, 5.9103, 6.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.06238723546266556 
model_pd.l_d.mean(): -19.928783416748047 
model_pd.lagr.mean(): -19.866395950317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.6763], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.06238723546266556
epoch£º43	 i:1 	 global-step:861	 l-p:0.14900676906108856
epoch£º43	 i:2 	 global-step:862	 l-p:0.13461311161518097
epoch£º43	 i:3 	 global-step:863	 l-p:0.132412850856781
epoch£º43	 i:4 	 global-step:864	 l-p:0.14031066000461578
epoch£º43	 i:5 	 global-step:865	 l-p:0.12816911935806274
epoch£º43	 i:6 	 global-step:866	 l-p:0.13030605018138885
epoch£º43	 i:7 	 global-step:867	 l-p:0.14207783341407776
epoch£º43	 i:8 	 global-step:868	 l-p:0.12005610018968582
epoch£º43	 i:9 	 global-step:869	 l-p:0.14094310998916626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8878, 5.0136, 4.9502],
        [4.8878, 4.8948, 4.8884],
        [4.8878, 4.8901, 4.8879],
        [4.8878, 4.8879, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13652320206165314 
model_pd.l_d.mean(): -20.439420700073242 
model_pd.lagr.mean(): -20.302898406982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1046], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13652320206165314
epoch£º44	 i:1 	 global-step:881	 l-p:0.13807985186576843
epoch£º44	 i:2 	 global-step:882	 l-p:-0.03193362057209015
epoch£º44	 i:3 	 global-step:883	 l-p:0.1403774470090866
epoch£º44	 i:4 	 global-step:884	 l-p:0.1372583508491516
epoch£º44	 i:5 	 global-step:885	 l-p:0.12938593327999115
epoch£º44	 i:6 	 global-step:886	 l-p:0.1125878319144249
epoch£º44	 i:7 	 global-step:887	 l-p:0.15188312530517578
epoch£º44	 i:8 	 global-step:888	 l-p:0.16582638025283813
epoch£º44	 i:9 	 global-step:889	 l-p:0.12697607278823853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8619, 4.8619, 4.8619],
        [4.8619, 4.8629, 4.8619],
        [4.8619, 5.0938, 5.0297],
        [4.8619, 4.8619, 4.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.1325896680355072 
model_pd.l_d.mean(): -20.828224182128906 
model_pd.lagr.mean(): -20.695634841918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.4759], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.1325896680355072
epoch£º45	 i:1 	 global-step:901	 l-p:0.13080772757530212
epoch£º45	 i:2 	 global-step:902	 l-p:0.1115826666355133
epoch£º45	 i:3 	 global-step:903	 l-p:0.1257406324148178
epoch£º45	 i:4 	 global-step:904	 l-p:0.1495400369167328
epoch£º45	 i:5 	 global-step:905	 l-p:-0.007568149361759424
epoch£º45	 i:6 	 global-step:906	 l-p:0.14164216816425323
epoch£º45	 i:7 	 global-step:907	 l-p:0.14175434410572052
epoch£º45	 i:8 	 global-step:908	 l-p:0.16079366207122803
epoch£º45	 i:9 	 global-step:909	 l-p:0.12867426872253418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8885, 4.8845],
        [4.8843, 4.9106, 4.8892],
        [4.8843, 4.8843, 4.8843],
        [4.8843, 4.9100, 4.8891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11819026619195938 
model_pd.l_d.mean(): -19.682544708251953 
model_pd.lagr.mean(): -19.564353942871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.4005], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11819026619195938
epoch£º46	 i:1 	 global-step:921	 l-p:0.24162887036800385
epoch£º46	 i:2 	 global-step:922	 l-p:0.1281239092350006
epoch£º46	 i:3 	 global-step:923	 l-p:0.12558497488498688
epoch£º46	 i:4 	 global-step:924	 l-p:0.14086292684078217
epoch£º46	 i:5 	 global-step:925	 l-p:0.11604674160480499
epoch£º46	 i:6 	 global-step:926	 l-p:0.14509223401546478
epoch£º46	 i:7 	 global-step:927	 l-p:0.13225089013576508
epoch£º46	 i:8 	 global-step:928	 l-p:0.129102423787117
epoch£º46	 i:9 	 global-step:929	 l-p:0.1555924266576767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8283, 4.8840, 4.8452],
        [4.8283, 4.9345, 4.8765],
        [4.8283, 4.8301, 4.8284],
        [4.8283, 5.1694, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.12015414983034134 
model_pd.l_d.mean(): -18.42574119567871 
model_pd.lagr.mean(): -18.305587768554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-19.1709], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.12015414983034134
epoch£º47	 i:1 	 global-step:941	 l-p:0.1428690254688263
epoch£º47	 i:2 	 global-step:942	 l-p:0.16499654948711395
epoch£º47	 i:3 	 global-step:943	 l-p:0.13090990483760834
epoch£º47	 i:4 	 global-step:944	 l-p:0.13788239657878876
epoch£º47	 i:5 	 global-step:945	 l-p:0.11366187781095505
epoch£º47	 i:6 	 global-step:946	 l-p:0.13483524322509766
epoch£º47	 i:7 	 global-step:947	 l-p:0.13506491482257843
epoch£º47	 i:8 	 global-step:948	 l-p:0.13370691239833832
epoch£º47	 i:9 	 global-step:949	 l-p:0.1701141744852066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7862, 5.1243, 5.0940],
        [4.7862, 4.9667, 4.9006],
        [4.7862, 4.7862, 4.7862],
        [4.7862, 5.0162, 4.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17270848155021667 
model_pd.l_d.mean(): -20.852392196655273 
model_pd.lagr.mean(): -20.679683685302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.5241], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17270848155021667
epoch£º48	 i:1 	 global-step:961	 l-p:0.12782059609889984
epoch£º48	 i:2 	 global-step:962	 l-p:0.1354924738407135
epoch£º48	 i:3 	 global-step:963	 l-p:0.12013496458530426
epoch£º48	 i:4 	 global-step:964	 l-p:0.1380750685930252
epoch£º48	 i:5 	 global-step:965	 l-p:0.21219410002231598
epoch£º48	 i:6 	 global-step:966	 l-p:0.12402303516864777
epoch£º48	 i:7 	 global-step:967	 l-p:0.12953273952007294
epoch£º48	 i:8 	 global-step:968	 l-p:0.11851298809051514
epoch£º48	 i:9 	 global-step:969	 l-p:0.11745252460241318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9221, 4.9565, 4.9297],
        [4.9221, 4.9234, 4.9221],
        [4.9221, 5.0805, 5.0129],
        [4.9221, 4.9283, 4.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12594278156757355 
model_pd.l_d.mean(): -20.845367431640625 
model_pd.lagr.mean(): -20.719425201416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3920], device='cuda:0')), ('power', tensor([-21.4735], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12594278156757355
epoch£º49	 i:1 	 global-step:981	 l-p:0.1394570916891098
epoch£º49	 i:2 	 global-step:982	 l-p:0.1596858948469162
epoch£º49	 i:3 	 global-step:983	 l-p:0.13263100385665894
epoch£º49	 i:4 	 global-step:984	 l-p:0.14662575721740723
epoch£º49	 i:5 	 global-step:985	 l-p:0.14537814259529114
epoch£º49	 i:6 	 global-step:986	 l-p:0.139382466673851
epoch£º49	 i:7 	 global-step:987	 l-p:0.19168727099895477
epoch£º49	 i:8 	 global-step:988	 l-p:0.11981597542762756
epoch£º49	 i:9 	 global-step:989	 l-p:0.14000684022903442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8120, 5.6421, 5.9736],
        [4.8120, 4.8121, 4.8120],
        [4.8120, 5.1291, 5.0902],
        [4.8120, 4.8138, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13871052861213684 
model_pd.l_d.mean(): -20.09473419189453 
model_pd.lagr.mean(): -19.956024169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.8194], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.13871052861213684
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13916277885437012
epoch£º50	 i:2 	 global-step:1002	 l-p:0.1439143270254135
epoch£º50	 i:3 	 global-step:1003	 l-p:0.1403169482946396
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12623260915279388
epoch£º50	 i:5 	 global-step:1005	 l-p:0.12198817729949951
epoch£º50	 i:6 	 global-step:1006	 l-p:0.13390587270259857
epoch£º50	 i:7 	 global-step:1007	 l-p:0.1457759290933609
epoch£º50	 i:8 	 global-step:1008	 l-p:0.12407294660806656
epoch£º50	 i:9 	 global-step:1009	 l-p:0.13592977821826935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 5.5699, 5.6900],
        [4.9641, 6.2928, 7.1561],
        [4.9641, 4.9642, 4.9641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11433085799217224 
model_pd.l_d.mean(): -18.682985305786133 
model_pd.lagr.mean(): -18.568655014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.4081], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11433085799217224
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10962005704641342
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11620579659938812
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12692952156066895
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12341010570526123
epoch£º51	 i:5 	 global-step:1025	 l-p:0.12480384856462479
epoch£º51	 i:6 	 global-step:1026	 l-p:0.13821548223495483
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18906226754188538
epoch£º51	 i:8 	 global-step:1028	 l-p:0.15353068709373474
epoch£º51	 i:9 	 global-step:1029	 l-p:0.15874595940113068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7054, 5.2840, 5.4070],
        [4.7054, 5.1214, 5.1349],
        [4.7054, 4.7539, 4.7193],
        [4.7054, 4.7054, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.1433434933423996 
model_pd.l_d.mean(): -20.545391082763672 
model_pd.lagr.mean(): -20.402048110961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.2487], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.1433434933423996
epoch£º52	 i:1 	 global-step:1041	 l-p:0.15665395557880402
epoch£º52	 i:2 	 global-step:1042	 l-p:0.1393285095691681
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14809924364089966
epoch£º52	 i:4 	 global-step:1044	 l-p:0.04069837927818298
epoch£º52	 i:5 	 global-step:1045	 l-p:0.146022230386734
epoch£º52	 i:6 	 global-step:1046	 l-p:0.1635284423828125
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.1482287049293518
epoch£º52	 i:8 	 global-step:1048	 l-p:0.1962914615869522
epoch£º52	 i:9 	 global-step:1049	 l-p:0.10795873403549194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6464, 4.6543, 4.6471],
        [4.6464, 4.6464, 4.6464],
        [4.6464, 4.6470, 4.6464],
        [4.6464, 4.6468, 4.6464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.1533750742673874 
model_pd.l_d.mean(): -19.069425582885742 
model_pd.lagr.mean(): -18.91604995727539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-19.8513], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.1533750742673874
epoch£º53	 i:1 	 global-step:1061	 l-p:0.13261757791042328
epoch£º53	 i:2 	 global-step:1062	 l-p:0.13291135430335999
epoch£º53	 i:3 	 global-step:1063	 l-p:-0.6126742362976074
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12235696613788605
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11968023329973221
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12427115440368652
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12091784924268723
epoch£º53	 i:8 	 global-step:1068	 l-p:0.13446833193302155
epoch£º53	 i:9 	 global-step:1069	 l-p:0.13642695546150208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0137, 5.0137, 5.0137],
        [5.0137, 5.7297, 5.9356],
        [5.0137, 5.7817, 6.0332],
        [5.0137, 5.1209, 5.0616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1283033937215805 
model_pd.l_d.mean(): -20.593904495239258 
model_pd.lagr.mean(): -20.465600967407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2171], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1283033937215805
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12306027114391327
epoch£º54	 i:2 	 global-step:1082	 l-p:0.12654834985733032
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11655143648386002
epoch£º54	 i:4 	 global-step:1084	 l-p:0.1251312643289566
epoch£º54	 i:5 	 global-step:1085	 l-p:0.14126050472259521
epoch£º54	 i:6 	 global-step:1086	 l-p:0.04986928030848503
epoch£º54	 i:7 	 global-step:1087	 l-p:0.15599000453948975
epoch£º54	 i:8 	 global-step:1088	 l-p:0.13251297175884247
epoch£º54	 i:9 	 global-step:1089	 l-p:0.14125102758407593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8611, 5.5963, 5.8361],
        [4.8611, 5.8973, 6.4395],
        [4.8611, 5.2947, 5.3095],
        [4.8611, 5.6053, 5.8533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.15564410388469696 
model_pd.l_d.mean(): -20.037389755249023 
model_pd.lagr.mean(): -19.881746292114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-20.7506], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.15564410388469696
epoch£º55	 i:1 	 global-step:1101	 l-p:-0.7835656404495239
epoch£º55	 i:2 	 global-step:1102	 l-p:0.1493179202079773
epoch£º55	 i:3 	 global-step:1103	 l-p:0.1556423455476761
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10965937376022339
epoch£º55	 i:5 	 global-step:1105	 l-p:0.11472291499376297
epoch£º55	 i:6 	 global-step:1106	 l-p:0.1134544163942337
epoch£º55	 i:7 	 global-step:1107	 l-p:0.1212015300989151
epoch£º55	 i:8 	 global-step:1108	 l-p:0.1133359894156456
epoch£º55	 i:9 	 global-step:1109	 l-p:0.14994369447231293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9237, 6.2317, 7.0814],
        [4.9237, 4.9238, 4.9237],
        [4.9237, 5.1621, 5.0997],
        [4.9237, 5.0004, 4.9521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.18314486742019653 
model_pd.l_d.mean(): -20.000200271606445 
model_pd.lagr.mean(): -19.817054748535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.7045], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.18314486742019653
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13572968542575836
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12788935005664825
epoch£º56	 i:3 	 global-step:1123	 l-p:0.12440969049930573
epoch£º56	 i:4 	 global-step:1124	 l-p:0.1221800372004509
epoch£º56	 i:5 	 global-step:1125	 l-p:0.12963566184043884
epoch£º56	 i:6 	 global-step:1126	 l-p:0.13438677787780762
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14420662820339203
epoch£º56	 i:8 	 global-step:1128	 l-p:0.1660284399986267
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14165887236595154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 5.8063, 6.2925],
        [4.8277, 5.9211, 6.5343],
        [4.8277, 4.8278, 4.8277],
        [4.8277, 4.8937, 4.8504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.16100572049617767 
model_pd.l_d.mean(): -20.344396591186523 
model_pd.lagr.mean(): -20.183391571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.16100572049617767
epoch£º57	 i:1 	 global-step:1141	 l-p:0.13890351355075836
epoch£º57	 i:2 	 global-step:1142	 l-p:0.16307629644870758
epoch£º57	 i:3 	 global-step:1143	 l-p:0.05947475880384445
epoch£º57	 i:4 	 global-step:1144	 l-p:0.12492973357439041
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14687642455101013
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14737454056739807
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11315146833658218
epoch£º57	 i:8 	 global-step:1148	 l-p:0.12686260044574738
epoch£º57	 i:9 	 global-step:1149	 l-p:0.11401433497667313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9294, 5.2412, 5.1980],
        [4.9294, 5.1477, 5.0827],
        [4.9294, 6.1823, 6.9652],
        [4.9294, 4.9294, 4.9294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.1307823359966278 
model_pd.l_d.mean(): -20.614959716796875 
model_pd.lagr.mean(): -20.484176635742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.1307823359966278
epoch£º58	 i:1 	 global-step:1161	 l-p:0.13997144997119904
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12424609810113907
epoch£º58	 i:3 	 global-step:1163	 l-p:0.13025431334972382
epoch£º58	 i:4 	 global-step:1164	 l-p:0.15095208585262299
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13278956711292267
epoch£º58	 i:6 	 global-step:1166	 l-p:0.13467715680599213
epoch£º58	 i:7 	 global-step:1167	 l-p:0.14396964013576508
epoch£º58	 i:8 	 global-step:1168	 l-p:0.15477602183818817
epoch£º58	 i:9 	 global-step:1169	 l-p:0.08777429163455963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6159, 4.9342, 4.9072],
        [4.6159, 4.6179, 4.6160],
        [4.6159, 4.6159, 4.6159],
        [4.6159, 4.7686, 4.7074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.13096044957637787 
model_pd.l_d.mean(): -18.834449768066406 
model_pd.lagr.mean(): -18.703489303588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5796], device='cuda:0')), ('power', tensor([-19.6323], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.13096044957637787
epoch£º59	 i:1 	 global-step:1181	 l-p:0.1460946947336197
epoch£º59	 i:2 	 global-step:1182	 l-p:0.16557689011096954
epoch£º59	 i:3 	 global-step:1183	 l-p:0.3579862415790558
epoch£º59	 i:4 	 global-step:1184	 l-p:0.15379057824611664
epoch£º59	 i:5 	 global-step:1185	 l-p:0.1806543916463852
epoch£º59	 i:6 	 global-step:1186	 l-p:0.13994161784648895
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12406089156866074
epoch£º59	 i:8 	 global-step:1188	 l-p:0.13059887290000916
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13422556221485138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0033, 5.1118, 5.0527],
        [5.0033, 5.0033, 5.0033],
        [5.0033, 5.0053, 5.0034],
        [5.0033, 5.3503, 5.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15150202810764313 
model_pd.l_d.mean(): -18.20465087890625 
model_pd.lagr.mean(): -18.05314826965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5589], device='cuda:0')), ('power', tensor([-18.9745], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.15150202810764313
epoch£º60	 i:1 	 global-step:1201	 l-p:0.13278667628765106
epoch£º60	 i:2 	 global-step:1202	 l-p:0.113423652946949
epoch£º60	 i:3 	 global-step:1203	 l-p:0.11557361483573914
epoch£º60	 i:4 	 global-step:1204	 l-p:0.12958915531635284
epoch£º60	 i:5 	 global-step:1205	 l-p:0.1215687170624733
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12599119544029236
epoch£º60	 i:7 	 global-step:1207	 l-p:0.11328809708356857
epoch£º60	 i:8 	 global-step:1208	 l-p:0.14462272822856903
epoch£º60	 i:9 	 global-step:1209	 l-p:0.11627897620201111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8475, 4.8475, 4.8475],
        [4.8475, 5.4710, 5.6208],
        [4.8475, 4.8540, 4.8480],
        [4.8475, 5.5587, 5.7823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15131473541259766 
model_pd.l_d.mean(): -19.231124877929688 
model_pd.lagr.mean(): -19.079811096191406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-19.9577], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15131473541259766
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14555738866329193
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12608492374420166
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17639899253845215
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13829809427261353
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11723329871892929
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13532577455043793
epoch£º61	 i:7 	 global-step:1227	 l-p:0.1299356073141098
epoch£º61	 i:8 	 global-step:1228	 l-p:0.13775873184204102
epoch£º61	 i:9 	 global-step:1229	 l-p:0.15251046419143677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8236, 4.8550, 4.8305],
        [4.8236, 5.7478, 6.1805],
        [4.8236, 4.9538, 4.8922],
        [4.8236, 5.0357, 4.9733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.1619023084640503 
model_pd.l_d.mean(): -19.955684661865234 
model_pd.lagr.mean(): -19.79378318786621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.6929], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.1619023084640503
epoch£º62	 i:1 	 global-step:1241	 l-p:0.13106369972229004
epoch£º62	 i:2 	 global-step:1242	 l-p:0.1082867905497551
epoch£º62	 i:3 	 global-step:1243	 l-p:0.12257884442806244
epoch£º62	 i:4 	 global-step:1244	 l-p:0.17873001098632812
epoch£º62	 i:5 	 global-step:1245	 l-p:0.1262366771697998
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13354627788066864
epoch£º62	 i:7 	 global-step:1247	 l-p:0.13899317383766174
epoch£º62	 i:8 	 global-step:1248	 l-p:0.13777513802051544
epoch£º62	 i:9 	 global-step:1249	 l-p:0.134012833237648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9335, 4.9361, 4.9336],
        [4.9335, 6.2783, 7.1770],
        [4.9335, 6.2831, 7.1876],
        [4.9335, 4.9335, 4.9335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.13272613286972046 
model_pd.l_d.mean(): -20.04277992248535 
model_pd.lagr.mean(): -19.910053253173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4594], device='cuda:0')), ('power', tensor([-20.7311], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.13272613286972046
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14917342364788055
epoch£º63	 i:2 	 global-step:1262	 l-p:0.036927614361047745
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15349185466766357
epoch£º63	 i:4 	 global-step:1264	 l-p:0.11600939184427261
epoch£º63	 i:5 	 global-step:1265	 l-p:0.13936841487884521
epoch£º63	 i:6 	 global-step:1266	 l-p:0.143493190407753
epoch£º63	 i:7 	 global-step:1267	 l-p:0.18242774903774261
epoch£º63	 i:8 	 global-step:1268	 l-p:0.11514616012573242
epoch£º63	 i:9 	 global-step:1269	 l-p:0.021065788343548775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6644, 4.6660, 4.6644],
        [4.6644, 4.6644, 4.6644],
        [4.6644, 4.6905, 4.6697],
        [4.6644, 4.6644, 4.6644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.13140460848808289 
model_pd.l_d.mean(): -19.788150787353516 
model_pd.lagr.mean(): -19.65674591064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.5652], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.13140460848808289
epoch£º64	 i:1 	 global-step:1281	 l-p:0.14742060005664825
epoch£º64	 i:2 	 global-step:1282	 l-p:0.25166618824005127
epoch£º64	 i:3 	 global-step:1283	 l-p:0.22086967527866364
epoch£º64	 i:4 	 global-step:1284	 l-p:0.1113511323928833
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16896970570087433
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13798902928829193
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10178247839212418
epoch£º64	 i:8 	 global-step:1288	 l-p:0.14322662353515625
epoch£º64	 i:9 	 global-step:1289	 l-p:0.1156531274318695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9468, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0190, 6.5946],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12496849149465561 
model_pd.l_d.mean(): -20.00864028930664 
model_pd.lagr.mean(): -19.8836727142334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7012], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12496849149465561
epoch£º65	 i:1 	 global-step:1301	 l-p:0.1392752081155777
epoch£º65	 i:2 	 global-step:1302	 l-p:0.13500240445137024
epoch£º65	 i:3 	 global-step:1303	 l-p:0.1367240995168686
epoch£º65	 i:4 	 global-step:1304	 l-p:0.1211332455277443
epoch£º65	 i:5 	 global-step:1305	 l-p:0.16671256721019745
epoch£º65	 i:6 	 global-step:1306	 l-p:0.13533923029899597
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14534617960453033
epoch£º65	 i:8 	 global-step:1308	 l-p:0.1482006013393402
epoch£º65	 i:9 	 global-step:1309	 l-p:0.28089016675949097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7085, 4.7085, 4.7085],
        [4.7085, 5.6968, 6.2182],
        [4.7085, 5.2369, 5.3288],
        [4.7085, 4.7323, 4.7130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14083252847194672 
model_pd.l_d.mean(): -20.467388153076172 
model_pd.lagr.mean(): -20.326555252075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4909], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14083252847194672
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1394396424293518
epoch£º66	 i:2 	 global-step:1322	 l-p:0.1322019100189209
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12715041637420654
epoch£º66	 i:4 	 global-step:1324	 l-p:0.16594399511814117
epoch£º66	 i:5 	 global-step:1325	 l-p:0.13365468382835388
epoch£º66	 i:6 	 global-step:1326	 l-p:0.13651485741138458
epoch£º66	 i:7 	 global-step:1327	 l-p:0.18176259100437164
epoch£º66	 i:8 	 global-step:1328	 l-p:0.1300732046365738
epoch£º66	 i:9 	 global-step:1329	 l-p:0.15339335799217224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8988, 4.8988, 4.8988],
        [4.8988, 4.8988, 4.8988],
        [4.8988, 5.8759, 6.3563],
        [4.8988, 5.6692, 5.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.16616816818714142 
model_pd.l_d.mean(): -20.004396438598633 
model_pd.lagr.mean(): -19.838228225708008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4795], device='cuda:0')), ('power', tensor([-20.7127], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.16616816818714142
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11301668733358383
epoch£º67	 i:2 	 global-step:1342	 l-p:0.12897974252700806
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12960121035575867
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13698658347129822
epoch£º67	 i:5 	 global-step:1345	 l-p:0.2112143635749817
epoch£º67	 i:6 	 global-step:1346	 l-p:0.13393069803714752
epoch£º67	 i:7 	 global-step:1347	 l-p:0.1287914663553238
epoch£º67	 i:8 	 global-step:1348	 l-p:0.1446351855993271
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10992677509784698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8884, 6.1549, 6.9688],
        [4.8884, 6.2119, 7.0961],
        [4.8884, 4.8982, 4.8895],
        [4.8884, 5.9383, 6.4999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.12088736891746521 
model_pd.l_d.mean(): -19.09025001525879 
model_pd.lagr.mean(): -18.969362258911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-19.8516], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.12088736891746521
epoch£º68	 i:1 	 global-step:1361	 l-p:0.3598368763923645
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15067926049232483
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11881549656391144
epoch£º68	 i:4 	 global-step:1364	 l-p:0.12765422463417053
epoch£º68	 i:5 	 global-step:1365	 l-p:0.12374517321586609
epoch£º68	 i:6 	 global-step:1366	 l-p:0.13096413016319275
epoch£º68	 i:7 	 global-step:1367	 l-p:0.15561915934085846
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11845092475414276
epoch£º68	 i:9 	 global-step:1369	 l-p:0.16050860285758972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8355, 4.9999, 4.9361],
        [4.8355, 5.9071, 6.5025],
        [4.8355, 5.0575, 4.9975],
        [4.8355, 4.8355, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.15231019258499146 
model_pd.l_d.mean(): -18.61509132385254 
model_pd.lagr.mean(): -18.46278190612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5706], device='cuda:0')), ('power', tensor([-19.4014], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.15231019258499146
epoch£º69	 i:1 	 global-step:1381	 l-p:0.1318371742963791
epoch£º69	 i:2 	 global-step:1382	 l-p:-0.1990482062101364
epoch£º69	 i:3 	 global-step:1383	 l-p:0.15426772832870483
epoch£º69	 i:4 	 global-step:1384	 l-p:0.12037506699562073
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12480597198009491
epoch£º69	 i:6 	 global-step:1386	 l-p:0.11934346705675125
epoch£º69	 i:7 	 global-step:1387	 l-p:0.13282181322574615
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12728847563266754
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12115193903446198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9306, 6.3162, 7.2714],
        [4.9306, 4.9583, 4.9362],
        [4.9306, 4.9306, 4.9306],
        [4.9306, 6.2265, 7.0700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10400024056434631 
model_pd.l_d.mean(): -17.98586082458496 
model_pd.lagr.mean(): -17.881860733032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5762], device='cuda:0')), ('power', tensor([-18.7710], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10400024056434631
epoch£º70	 i:1 	 global-step:1401	 l-p:0.1265614926815033
epoch£º70	 i:2 	 global-step:1402	 l-p:0.12904788553714752
epoch£º70	 i:3 	 global-step:1403	 l-p:0.1404656022787094
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15240797400474548
epoch£º70	 i:5 	 global-step:1405	 l-p:0.18527743220329285
epoch£º70	 i:6 	 global-step:1406	 l-p:0.16522452235221863
epoch£º70	 i:7 	 global-step:1407	 l-p:0.10285830497741699
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12662185728549957
epoch£º70	 i:9 	 global-step:1409	 l-p:0.15000930428504944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8210, 5.0123, 4.9496],
        [4.8210, 4.8240, 4.8211],
        [4.8210, 4.8210, 4.8209],
        [4.8210, 5.4243, 5.5656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.137946218252182 
model_pd.l_d.mean(): -20.32706642150879 
model_pd.lagr.mean(): -20.189119338989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-21.0375], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.137946218252182
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1292579025030136
epoch£º71	 i:2 	 global-step:1422	 l-p:0.09325931966304779
epoch£º71	 i:3 	 global-step:1423	 l-p:0.12163189798593521
epoch£º71	 i:4 	 global-step:1424	 l-p:0.16066591441631317
epoch£º71	 i:5 	 global-step:1425	 l-p:0.13914188742637634
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14205442368984222
epoch£º71	 i:7 	 global-step:1427	 l-p:0.16101832687854767
epoch£º71	 i:8 	 global-step:1428	 l-p:0.14341895282268524
epoch£º71	 i:9 	 global-step:1429	 l-p:0.15501436591148376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8509, 4.8509, 4.8509],
        [4.8509, 4.8509, 4.8509],
        [4.8509, 4.8554, 4.8512],
        [4.8509, 4.8509, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12231584638357162 
model_pd.l_d.mean(): -19.196983337402344 
model_pd.lagr.mean(): -19.07466697692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12231584638357162
epoch£º72	 i:1 	 global-step:1441	 l-p:0.1467251181602478
epoch£º72	 i:2 	 global-step:1442	 l-p:0.13569651544094086
epoch£º72	 i:3 	 global-step:1443	 l-p:0.14201034605503082
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14734192192554474
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11704432219266891
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15368573367595673
epoch£º72	 i:7 	 global-step:1447	 l-p:0.1330123096704483
epoch£º72	 i:8 	 global-step:1448	 l-p:0.15642118453979492
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.04157199710607529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8868, 4.8868, 4.8868],
        [4.8868, 5.0443, 4.9803],
        [4.8868, 5.8553, 6.3325],
        [4.8868, 5.1080, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1317950189113617 
model_pd.l_d.mean(): -19.311697006225586 
model_pd.lagr.mean(): -19.179901123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-20.0034], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.1317950189113617
epoch£º73	 i:1 	 global-step:1461	 l-p:0.1439758986234665
epoch£º73	 i:2 	 global-step:1462	 l-p:0.22139160335063934
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11550027877092361
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12857310473918915
epoch£º73	 i:5 	 global-step:1465	 l-p:0.1209404468536377
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13365910947322845
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14521679282188416
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10627502202987671
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12822923064231873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9009, 4.9188, 4.9037],
        [4.9009, 5.2702, 5.2564],
        [4.9009, 5.6302, 5.8714],
        [4.9009, 5.1214, 5.0604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13521282374858856 
model_pd.l_d.mean(): -19.75572967529297 
model_pd.lagr.mean(): -19.62051773071289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.4845], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13521282374858856
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11785311251878738
epoch£º74	 i:2 	 global-step:1482	 l-p:0.1448005884885788
epoch£º74	 i:3 	 global-step:1483	 l-p:0.1340390294790268
epoch£º74	 i:4 	 global-step:1484	 l-p:0.1252591609954834
epoch£º74	 i:5 	 global-step:1485	 l-p:0.18119235336780548
epoch£º74	 i:6 	 global-step:1486	 l-p:0.1846732646226883
epoch£º74	 i:7 	 global-step:1487	 l-p:0.10869599878787994
epoch£º74	 i:8 	 global-step:1488	 l-p:0.1622314602136612
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13272583484649658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8920, 5.2251, 5.1962],
        [4.8920, 5.1021, 5.0402],
        [4.8920, 4.8920, 4.8920],
        [4.8920, 5.6745, 5.9659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13643082976341248 
model_pd.l_d.mean(): -20.096933364868164 
model_pd.lagr.mean(): -19.96050262451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.13643082976341248
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12809038162231445
epoch£º75	 i:2 	 global-step:1502	 l-p:0.14080646634101868
epoch£º75	 i:3 	 global-step:1503	 l-p:0.14227813482284546
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13767971098423004
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11957219988107681
epoch£º75	 i:6 	 global-step:1506	 l-p:0.16405922174453735
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13617169857025146
epoch£º75	 i:8 	 global-step:1508	 l-p:0.09892752766609192
epoch£º75	 i:9 	 global-step:1509	 l-p:0.14535866677761078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8378, 4.8383, 4.8378],
        [4.8378, 4.8378, 4.8378],
        [4.8378, 5.5728, 5.8274],
        [4.8378, 5.0654, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12408145517110825 
model_pd.l_d.mean(): -19.41663360595703 
model_pd.lagr.mean(): -19.292552947998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1678], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12408145517110825
epoch£º76	 i:1 	 global-step:1521	 l-p:0.1384497731924057
epoch£º76	 i:2 	 global-step:1522	 l-p:0.2667454183101654
epoch£º76	 i:3 	 global-step:1523	 l-p:0.12712189555168152
epoch£º76	 i:4 	 global-step:1524	 l-p:0.11962025612592697
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11829335987567902
epoch£º76	 i:6 	 global-step:1526	 l-p:0.11112578958272934
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11747156083583832
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12500230967998505
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16276682913303375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9340, 4.9511, 4.9366],
        [4.9340, 4.9343, 4.9340],
        [4.9340, 4.9758, 4.9450],
        [4.9340, 5.0350, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12634335458278656 
model_pd.l_d.mean(): -20.453214645385742 
model_pd.lagr.mean(): -20.326871871948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.1179], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12634335458278656
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11698202043771744
epoch£º77	 i:2 	 global-step:1542	 l-p:0.15160928666591644
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10698415338993073
epoch£º77	 i:4 	 global-step:1544	 l-p:0.13720323145389557
epoch£º77	 i:5 	 global-step:1545	 l-p:0.19167126715183258
epoch£º77	 i:6 	 global-step:1546	 l-p:0.1266685277223587
epoch£º77	 i:7 	 global-step:1547	 l-p:0.17548424005508423
epoch£º77	 i:8 	 global-step:1548	 l-p:0.20012500882148743
epoch£º77	 i:9 	 global-step:1549	 l-p:0.1402096152305603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8063, 5.3959, 5.5314],
        [4.8063, 5.6717, 6.0568],
        [4.8063, 4.9062, 4.8523],
        [4.8063, 4.8570, 4.8217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14393089711666107 
model_pd.l_d.mean(): -19.923086166381836 
model_pd.lagr.mean(): -19.779155731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.6659], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14393089711666107
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13526515662670135
epoch£º78	 i:2 	 global-step:1562	 l-p:0.1694849729537964
epoch£º78	 i:3 	 global-step:1563	 l-p:0.13437655568122864
epoch£º78	 i:4 	 global-step:1564	 l-p:0.16377481818199158
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12506096065044403
epoch£º78	 i:6 	 global-step:1566	 l-p:0.08515775203704834
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13314375281333923
epoch£º78	 i:8 	 global-step:1568	 l-p:0.14538826048374176
epoch£º78	 i:9 	 global-step:1569	 l-p:0.14050458371639252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8254, 5.4681, 5.6451],
        [4.8254, 4.8254, 4.8254],
        [4.8254, 4.8270, 4.8254],
        [4.8254, 4.8296, 4.8257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14757391810417175 
model_pd.l_d.mean(): -20.45145606994629 
model_pd.lagr.mean(): -20.303882598876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1535], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14757391810417175
epoch£º79	 i:1 	 global-step:1581	 l-p:0.1718907356262207
epoch£º79	 i:2 	 global-step:1582	 l-p:0.13692834973335266
epoch£º79	 i:3 	 global-step:1583	 l-p:0.13718940317630768
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12769103050231934
epoch£º79	 i:5 	 global-step:1585	 l-p:0.19374814629554749
epoch£º79	 i:6 	 global-step:1586	 l-p:0.2300565540790558
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14395688474178314
epoch£º79	 i:8 	 global-step:1588	 l-p:0.15542563796043396
epoch£º79	 i:9 	 global-step:1589	 l-p:0.101958729326725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8129, 6.1495, 7.0742],
        [4.8129, 4.8290, 4.8153],
        [4.8129, 4.8129, 4.8129],
        [4.8129, 5.7480, 6.2052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.1707744002342224 
model_pd.l_d.mean(): -19.077165603637695 
model_pd.lagr.mean(): -18.906391143798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8170], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.1707744002342224
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12721002101898193
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13936732709407806
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12071206420660019
epoch£º80	 i:4 	 global-step:1604	 l-p:0.24613438546657562
epoch£º80	 i:5 	 global-step:1605	 l-p:0.13247404992580414
epoch£º80	 i:6 	 global-step:1606	 l-p:0.1286044716835022
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13222673535346985
epoch£º80	 i:8 	 global-step:1608	 l-p:0.12685227394104004
epoch£º80	 i:9 	 global-step:1609	 l-p:0.13117049634456635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 4.9184, 4.9130],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 5.8766, 6.3501],
        [4.9125, 4.9827, 4.9382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.1223609670996666 
model_pd.l_d.mean(): -20.22054672241211 
model_pd.lagr.mean(): -20.098186492919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.1223609670996666
epoch£º81	 i:1 	 global-step:1621	 l-p:0.1438048928976059
epoch£º81	 i:2 	 global-step:1622	 l-p:0.13110938668251038
epoch£º81	 i:3 	 global-step:1623	 l-p:0.15585504472255707
epoch£º81	 i:4 	 global-step:1624	 l-p:0.16523334383964539
epoch£º81	 i:5 	 global-step:1625	 l-p:0.5328431725502014
epoch£º81	 i:6 	 global-step:1626	 l-p:0.13285210728645325
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.012270164676010609
epoch£º81	 i:8 	 global-step:1628	 l-p:-1.4957623481750488
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13620352745056152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 4.7941, 4.7523],
        [4.7284, 5.3703, 5.5580],
        [4.7284, 4.9282, 4.8703],
        [4.7284, 5.7550, 6.3262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.2518194615840912 
model_pd.l_d.mean(): -20.29587745666504 
model_pd.lagr.mean(): -20.044057846069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-21.0370], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.2518194615840912
epoch£º82	 i:1 	 global-step:1641	 l-p:0.13042472302913666
epoch£º82	 i:2 	 global-step:1642	 l-p:0.1534264087677002
epoch£º82	 i:3 	 global-step:1643	 l-p:0.1669299453496933
epoch£º82	 i:4 	 global-step:1644	 l-p:0.13605597615242004
epoch£º82	 i:5 	 global-step:1645	 l-p:0.1375766545534134
epoch£º82	 i:6 	 global-step:1646	 l-p:0.15046600997447968
epoch£º82	 i:7 	 global-step:1647	 l-p:0.1373671591281891
epoch£º82	 i:8 	 global-step:1648	 l-p:0.1285077929496765
epoch£º82	 i:9 	 global-step:1649	 l-p:0.13814496994018555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9474, 5.8657, 6.2876],
        [4.9474, 5.1824, 5.1241],
        [4.9474, 4.9480, 4.9474],
        [4.9474, 5.4525, 5.5167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11888937652111053 
model_pd.l_d.mean(): -19.251291275024414 
model_pd.lagr.mean(): -19.132402420043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9706], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11888937652111053
epoch£º83	 i:1 	 global-step:1661	 l-p:0.10654883831739426
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13716962933540344
epoch£º83	 i:3 	 global-step:1663	 l-p:0.13136906921863556
epoch£º83	 i:4 	 global-step:1664	 l-p:0.1272442787885666
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12627343833446503
epoch£º83	 i:6 	 global-step:1666	 l-p:0.13229280710220337
epoch£º83	 i:7 	 global-step:1667	 l-p:0.1546819806098938
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.19513079524040222
epoch£º83	 i:9 	 global-step:1669	 l-p:0.13777320086956024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8804, 4.8806, 4.8804],
        [4.8804, 4.8888, 4.8813],
        [4.8804, 4.9535, 4.9081],
        [4.8804, 5.1099, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13601094484329224 
model_pd.l_d.mean(): -20.28618812561035 
model_pd.lagr.mean(): -20.150177001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-20.9916], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13601094484329224
epoch£º84	 i:1 	 global-step:1681	 l-p:0.12280208617448807
epoch£º84	 i:2 	 global-step:1682	 l-p:0.13972362875938416
epoch£º84	 i:3 	 global-step:1683	 l-p:0.1486397385597229
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12543368339538574
epoch£º84	 i:5 	 global-step:1685	 l-p:0.1465696543455124
epoch£º84	 i:6 	 global-step:1686	 l-p:0.18466202914714813
epoch£º84	 i:7 	 global-step:1687	 l-p:0.06746786087751389
epoch£º84	 i:8 	 global-step:1688	 l-p:0.13643641769886017
epoch£º84	 i:9 	 global-step:1689	 l-p:0.1347390115261078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8291, 5.5559, 5.8086],
        [4.8291, 5.2917, 5.3377],
        [4.8291, 5.6603, 6.0119],
        [4.8291, 5.1527, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.16141633689403534 
model_pd.l_d.mean(): -20.015277862548828 
model_pd.lagr.mean(): -19.85386085510254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.16141633689403534
epoch£º85	 i:1 	 global-step:1701	 l-p:0.13557304441928864
epoch£º85	 i:2 	 global-step:1702	 l-p:0.09140540659427643
epoch£º85	 i:3 	 global-step:1703	 l-p:0.15843750536441803
epoch£º85	 i:4 	 global-step:1704	 l-p:0.1532386988401413
epoch£º85	 i:5 	 global-step:1705	 l-p:0.16742467880249023
epoch£º85	 i:6 	 global-step:1706	 l-p:0.11873393505811691
epoch£º85	 i:7 	 global-step:1707	 l-p:0.12003077566623688
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13899649679660797
epoch£º85	 i:9 	 global-step:1709	 l-p:0.12462016940116882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9069, 5.7601, 6.1237],
        [4.9069, 4.9165, 4.9079],
        [4.9069, 4.9069, 4.9069],
        [4.9069, 5.2179, 5.1828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.5805625319480896 
model_pd.l_d.mean(): -17.353965759277344 
model_pd.lagr.mean(): -16.77340316772461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6600], device='cuda:0')), ('power', tensor([-18.2177], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.5805625319480896
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11675955355167389
epoch£º86	 i:2 	 global-step:1722	 l-p:0.13129578530788422
epoch£º86	 i:3 	 global-step:1723	 l-p:0.1409791111946106
epoch£º86	 i:4 	 global-step:1724	 l-p:0.1256081908941269
epoch£º86	 i:5 	 global-step:1725	 l-p:0.1254468709230423
epoch£º86	 i:6 	 global-step:1726	 l-p:0.13080766797065735
epoch£º86	 i:7 	 global-step:1727	 l-p:0.1361200511455536
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14059136807918549
epoch£º86	 i:9 	 global-step:1729	 l-p:0.19269855320453644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6877, 4.7752, 4.7262],
        [4.6877, 4.6917, 4.6880],
        [4.6877, 4.6881, 4.6877],
        [4.6877, 5.1693, 5.2391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.15297558903694153 
model_pd.l_d.mean(): -19.670848846435547 
model_pd.lagr.mean(): -19.517873764038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5793], device='cuda:0')), ('power', tensor([-20.4775], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.15297558903694153
epoch£º87	 i:1 	 global-step:1741	 l-p:0.1362951397895813
epoch£º87	 i:2 	 global-step:1742	 l-p:0.1360076516866684
epoch£º87	 i:3 	 global-step:1743	 l-p:0.17059370875358582
epoch£º87	 i:4 	 global-step:1744	 l-p:0.056285060942173004
epoch£º87	 i:5 	 global-step:1745	 l-p:0.010130052454769611
epoch£º87	 i:6 	 global-step:1746	 l-p:0.1307860165834427
epoch£º87	 i:7 	 global-step:1747	 l-p:0.2216227799654007
epoch£º87	 i:8 	 global-step:1748	 l-p:0.1490994542837143
epoch£º87	 i:9 	 global-step:1749	 l-p:0.146593376994133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8849, 4.8849, 4.8849],
        [4.8849, 5.1777, 5.1376],
        [4.8849, 4.9856, 4.9315],
        [4.8849, 4.8902, 4.8853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.1538381576538086 
model_pd.l_d.mean(): -20.64870834350586 
model_pd.lagr.mean(): -20.494869232177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.3139], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.1538381576538086
epoch£º88	 i:1 	 global-step:1761	 l-p:0.12668690085411072
epoch£º88	 i:2 	 global-step:1762	 l-p:0.12922844290733337
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13921025395393372
epoch£º88	 i:4 	 global-step:1764	 l-p:0.1316681206226349
epoch£º88	 i:5 	 global-step:1765	 l-p:0.14828191697597504
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14860354363918304
epoch£º88	 i:7 	 global-step:1767	 l-p:0.1481906771659851
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11828800290822983
epoch£º88	 i:9 	 global-step:1769	 l-p:0.11790735274553299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7805, 5.7454, 6.2442],
        [4.7805, 4.8045, 4.7853],
        [4.7805, 4.8130, 4.7883],
        [4.7805, 4.9300, 4.8700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11386770009994507 
model_pd.l_d.mean(): -19.35983657836914 
model_pd.lagr.mean(): -19.245969772338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5823], device='cuda:0')), ('power', tensor([-20.1662], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11386770009994507
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15337346494197845
epoch£º89	 i:2 	 global-step:1782	 l-p:0.1326213926076889
epoch£º89	 i:3 	 global-step:1783	 l-p:0.1501663327217102
epoch£º89	 i:4 	 global-step:1784	 l-p:0.21535521745681763
epoch£º89	 i:5 	 global-step:1785	 l-p:0.1411198526620865
epoch£º89	 i:6 	 global-step:1786	 l-p:0.14539694786071777
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.179179385304451
epoch£º89	 i:8 	 global-step:1788	 l-p:0.11794482916593552
epoch£º89	 i:9 	 global-step:1789	 l-p:0.12753519415855408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9758, 6.0879, 6.7186],
        [4.9758, 4.9758, 4.9758],
        [4.9758, 4.9759, 4.9758],
        [4.9758, 5.0137, 4.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.16870048642158508 
model_pd.l_d.mean(): -19.37898826599121 
model_pd.lagr.mean(): -19.21028709411621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-20.0688], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.16870048642158508
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11208037286996841
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12222913652658463
epoch£º90	 i:3 	 global-step:1803	 l-p:0.1191619262099266
epoch£º90	 i:4 	 global-step:1804	 l-p:0.12066423147916794
epoch£º90	 i:5 	 global-step:1805	 l-p:0.13653209805488586
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12312018126249313
epoch£º90	 i:7 	 global-step:1807	 l-p:0.15758772194385529
epoch£º90	 i:8 	 global-step:1808	 l-p:0.14703166484832764
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13849376142024994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7746, 4.7746, 4.7746],
        [4.7746, 4.7749, 4.7746],
        [4.7746, 4.7805, 4.7751],
        [4.7746, 5.5271, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13219788670539856 
model_pd.l_d.mean(): -19.322614669799805 
model_pd.lagr.mean(): -19.19041633605957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.0467], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13219788670539856
epoch£º91	 i:1 	 global-step:1821	 l-p:0.18661437928676605
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2720528244972229
epoch£º91	 i:3 	 global-step:1823	 l-p:0.13594070076942444
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14104914665222168
epoch£º91	 i:5 	 global-step:1825	 l-p:0.16389480233192444
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10022510588169098
epoch£º91	 i:7 	 global-step:1827	 l-p:0.12935689091682434
epoch£º91	 i:8 	 global-step:1828	 l-p:0.1309659481048584
epoch£º91	 i:9 	 global-step:1829	 l-p:0.1526566594839096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8991, 4.8991, 4.8991],
        [4.8991, 5.9394, 6.5036],
        [4.8991, 5.0452, 4.9839],
        [4.8991, 4.9582, 4.9189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.1324738711118698 
model_pd.l_d.mean(): -19.615846633911133 
model_pd.lagr.mean(): -19.483373641967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.3764], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.1324738711118698
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13203531503677368
epoch£º92	 i:2 	 global-step:1842	 l-p:0.11994165927171707
epoch£º92	 i:3 	 global-step:1843	 l-p:-1.655489206314087
epoch£º92	 i:4 	 global-step:1844	 l-p:0.13775260746479034
epoch£º92	 i:5 	 global-step:1845	 l-p:0.14413271844387054
epoch£º92	 i:6 	 global-step:1846	 l-p:0.15493109822273254
epoch£º92	 i:7 	 global-step:1847	 l-p:0.14308778941631317
epoch£º92	 i:8 	 global-step:1848	 l-p:0.10595329105854034
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12840686738491058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9038, 4.9038, 4.9038],
        [4.9038, 5.2691, 5.2589],
        [4.9038, 5.3029, 5.3097],
        [4.9038, 5.1776, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12621328234672546 
model_pd.l_d.mean(): -19.67447853088379 
model_pd.lagr.mean(): -19.54826545715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.3844], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12621328234672546
epoch£º93	 i:1 	 global-step:1861	 l-p:0.13031183183193207
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12462878972291946
epoch£º93	 i:3 	 global-step:1863	 l-p:0.14523981511592865
epoch£º93	 i:4 	 global-step:1864	 l-p:0.15493367612361908
epoch£º93	 i:5 	 global-step:1865	 l-p:0.18874400854110718
epoch£º93	 i:6 	 global-step:1866	 l-p:0.14262650907039642
epoch£º93	 i:7 	 global-step:1867	 l-p:0.14957451820373535
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.16091659665107727
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12663030624389648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9840, 5.5303, 5.6246],
        [4.9840, 4.9840, 4.9840],
        [4.9840, 5.0285, 4.9964],
        [4.9840, 4.9985, 4.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1490868777036667 
model_pd.l_d.mean(): -20.69589614868164 
model_pd.lagr.mean(): -20.546810150146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.3255], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1490868777036667
epoch£º94	 i:1 	 global-step:1881	 l-p:0.11324475705623627
epoch£º94	 i:2 	 global-step:1882	 l-p:0.17031793296337128
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12375140190124512
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12355717271566391
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11606147140264511
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11329998075962067
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12080718576908112
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13050830364227295
epoch£º94	 i:9 	 global-step:1889	 l-p:0.1564171016216278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8952, 4.8953, 4.8952],
        [4.8952, 4.8952, 4.8952],
        [4.8952, 4.8966, 4.8952],
        [4.8952, 4.9106, 4.8975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12278853356838226 
model_pd.l_d.mean(): -19.264141082763672 
model_pd.lagr.mean(): -19.1413516998291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-20.0075], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.12278853356838226
epoch£º95	 i:1 	 global-step:1901	 l-p:0.14851941168308258
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13646860420703888
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12951666116714478
epoch£º95	 i:4 	 global-step:1904	 l-p:0.1277352273464203
epoch£º95	 i:5 	 global-step:1905	 l-p:0.2341075837612152
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13593213260173798
epoch£º95	 i:7 	 global-step:1907	 l-p:0.1373540759086609
epoch£º95	 i:8 	 global-step:1908	 l-p:0.13590270280838013
epoch£º95	 i:9 	 global-step:1909	 l-p:0.3693356215953827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7908, 5.9199, 6.6059],
        [4.7908, 4.7914, 4.7908],
        [4.7908, 5.5351, 5.8141],
        [4.7908, 4.7908, 4.7908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.104037806391716 
model_pd.l_d.mean(): -18.36155128479004 
model_pd.lagr.mean(): -18.25751304626465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6453], device='cuda:0')), ('power', tensor([-19.2214], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.104037806391716
epoch£º96	 i:1 	 global-step:1921	 l-p:0.1520959585905075
epoch£º96	 i:2 	 global-step:1922	 l-p:0.12140302360057831
epoch£º96	 i:3 	 global-step:1923	 l-p:0.1341206282377243
epoch£º96	 i:4 	 global-step:1924	 l-p:0.1384296715259552
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13624054193496704
epoch£º96	 i:6 	 global-step:1926	 l-p:0.1773214340209961
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12007715553045273
epoch£º96	 i:8 	 global-step:1928	 l-p:0.13987641036510468
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12690073251724243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.5216, 5.5699],
        [5.0394, 5.9005, 6.2611],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 6.3564, 7.2213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.14717690646648407 
model_pd.l_d.mean(): -20.29983139038086 
model_pd.lagr.mean(): -20.15265464782715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-20.9457], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.14717690646648407
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12230207026004791
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10776960849761963
epoch£º97	 i:3 	 global-step:1943	 l-p:0.14829911291599274
epoch£º97	 i:4 	 global-step:1944	 l-p:0.18553394079208374
epoch£º97	 i:5 	 global-step:1945	 l-p:0.1393761932849884
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11608055979013443
epoch£º97	 i:7 	 global-step:1947	 l-p:0.13259322941303253
epoch£º97	 i:8 	 global-step:1948	 l-p:0.1432722955942154
epoch£º97	 i:9 	 global-step:1949	 l-p:0.12731698155403137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 5.0911, 5.0908],
        [4.7284, 4.7284, 4.7284],
        [4.7284, 4.7677, 4.7392],
        [4.7284, 5.3218, 5.4769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13279277086257935 
model_pd.l_d.mean(): -20.321035385131836 
model_pd.lagr.mean(): -20.188241958618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-21.0655], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13279277086257935
epoch£º98	 i:1 	 global-step:1961	 l-p:0.14916253089904785
epoch£º98	 i:2 	 global-step:1962	 l-p:0.12943869829177856
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.03075987659394741
epoch£º98	 i:4 	 global-step:1964	 l-p:0.17162875831127167
epoch£º98	 i:5 	 global-step:1965	 l-p:0.18133407831192017
epoch£º98	 i:6 	 global-step:1966	 l-p:0.13635525107383728
epoch£º98	 i:7 	 global-step:1967	 l-p:0.1489669382572174
epoch£º98	 i:8 	 global-step:1968	 l-p:0.12327384948730469
epoch£º98	 i:9 	 global-step:1969	 l-p:0.1371772587299347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9241, 4.9241],
        [4.9241, 4.9242, 4.9241],
        [4.9241, 5.3863, 5.4309],
        [4.9241, 4.9241, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.1452411413192749 
model_pd.l_d.mean(): -19.399749755859375 
model_pd.lagr.mean(): -19.25450897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4568], device='cuda:0')), ('power', tensor([-20.0783], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.1452411413192749
epoch£º99	 i:1 	 global-step:1981	 l-p:0.14091704785823822
epoch£º99	 i:2 	 global-step:1982	 l-p:0.11232917755842209
epoch£º99	 i:3 	 global-step:1983	 l-p:0.14403554797172546
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12521061301231384
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.04700805991888046
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12293677031993866
epoch£º99	 i:7 	 global-step:1987	 l-p:0.1597287803888321
epoch£º99	 i:8 	 global-step:1988	 l-p:0.12660229206085205
epoch£º99	 i:9 	 global-step:1989	 l-p:0.1395108848810196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8764, 4.8770, 4.8764],
        [4.8764, 5.3748, 5.4471],
        [4.8764, 4.9642, 4.9146],
        [4.8764, 4.8764, 4.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.16194237768650055 
model_pd.l_d.mean(): -20.743160247802734 
model_pd.lagr.mean(): -20.581218719482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.4021], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.16194237768650055
epoch£º100	 i:1 	 global-step:2001	 l-p:0.13343743979930878
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11211985349655151
epoch£º100	 i:3 	 global-step:2003	 l-p:0.1176290512084961
epoch£º100	 i:4 	 global-step:2004	 l-p:0.1396699845790863
epoch£º100	 i:5 	 global-step:2005	 l-p:0.14430533349514008
epoch£º100	 i:6 	 global-step:2006	 l-p:0.13814540207386017
epoch£º100	 i:7 	 global-step:2007	 l-p:2.386664867401123
epoch£º100	 i:8 	 global-step:2008	 l-p:-0.6520603895187378
epoch£º100	 i:9 	 global-step:2009	 l-p:0.1707194447517395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8123, 4.8712, 4.8328],
        [4.8123, 4.8124, 4.8123],
        [4.8123, 4.8144, 4.8124],
        [4.8123, 4.8123, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.13606294989585876 
model_pd.l_d.mean(): -20.10523223876953 
model_pd.lagr.mean(): -19.96916961669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.13606294989585876
epoch£º101	 i:1 	 global-step:2021	 l-p:0.11696895211935043
epoch£º101	 i:2 	 global-step:2022	 l-p:0.10562201589345932
epoch£º101	 i:3 	 global-step:2023	 l-p:0.04097531735897064
epoch£º101	 i:4 	 global-step:2024	 l-p:0.13702093064785004
epoch£º101	 i:5 	 global-step:2025	 l-p:0.1337171345949173
epoch£º101	 i:6 	 global-step:2026	 l-p:0.11803590506315231
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12559863924980164
epoch£º101	 i:8 	 global-step:2028	 l-p:0.14421936869621277
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1384148746728897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9760, 5.6923, 5.9283],
        [4.9760, 5.6971, 5.9374],
        [4.9760, 6.2049, 6.9787],
        [4.9760, 5.1470, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11542589217424393 
model_pd.l_d.mean(): -20.078229904174805 
model_pd.lagr.mean(): -19.962804794311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.7715], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11542589217424393
epoch£º102	 i:1 	 global-step:2041	 l-p:0.1633380800485611
epoch£º102	 i:2 	 global-step:2042	 l-p:0.015343794599175453
epoch£º102	 i:3 	 global-step:2043	 l-p:0.14636167883872986
epoch£º102	 i:4 	 global-step:2044	 l-p:0.14009661972522736
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13479095697402954
epoch£º102	 i:6 	 global-step:2046	 l-p:0.14273516833782196
epoch£º102	 i:7 	 global-step:2047	 l-p:0.127495676279068
epoch£º102	 i:8 	 global-step:2048	 l-p:0.14488917589187622
epoch£º102	 i:9 	 global-step:2049	 l-p:0.23212866485118866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6879, 4.7791, 4.7302],
        [4.6879, 5.0149, 5.0012],
        [4.6879, 4.7728, 4.7256],
        [4.6879, 4.6979, 4.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.12454941123723984 
model_pd.l_d.mean(): -19.899656295776367 
model_pd.lagr.mean(): -19.77510643005371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5609], device='cuda:0')), ('power', tensor([-20.6900], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.12454941123723984
epoch£º103	 i:1 	 global-step:2061	 l-p:0.07462118566036224
epoch£º103	 i:2 	 global-step:2062	 l-p:0.15335878729820251
epoch£º103	 i:3 	 global-step:2063	 l-p:0.161056786775589
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13235916197299957
epoch£º103	 i:5 	 global-step:2065	 l-p:0.137505441904068
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13019506633281708
epoch£º103	 i:7 	 global-step:2067	 l-p:0.15221762657165527
epoch£º103	 i:8 	 global-step:2068	 l-p:0.1304822713136673
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12791956961154938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9658, 4.9676, 4.9658],
        [4.9658, 5.1130, 5.0521],
        [4.9658, 5.0240, 4.9853],
        [4.9658, 5.8964, 6.3411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12891139090061188 
model_pd.l_d.mean(): -20.71617317199707 
model_pd.lagr.mean(): -20.587261199951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3973], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12891139090061188
epoch£º104	 i:1 	 global-step:2081	 l-p:0.3811608850955963
epoch£º104	 i:2 	 global-step:2082	 l-p:0.1530897319316864
epoch£º104	 i:3 	 global-step:2083	 l-p:0.15865300595760345
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12984681129455566
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12669934332370758
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12576140463352203
epoch£º104	 i:7 	 global-step:2087	 l-p:0.14561447501182556
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12466844171285629
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13258738815784454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8395, 4.8395, 4.8395],
        [4.8395, 4.8395, 4.8395],
        [4.8395, 5.0425, 4.9861],
        [4.8395, 5.3607, 5.4540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.1607753336429596 
model_pd.l_d.mean(): -20.348169326782227 
model_pd.lagr.mean(): -20.187393188476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.0498], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.1607753336429596
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16111865639686584
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12355157732963562
epoch£º105	 i:3 	 global-step:2103	 l-p:0.13996337354183197
epoch£º105	 i:4 	 global-step:2104	 l-p:0.14281654357910156
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18187816441059113
epoch£º105	 i:6 	 global-step:2106	 l-p:0.16427941620349884
epoch£º105	 i:7 	 global-step:2107	 l-p:0.1664828509092331
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07361260056495667
epoch£º105	 i:9 	 global-step:2109	 l-p:0.1287342607975006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9102, 4.9131, 4.9104],
        [4.9102, 5.6171, 5.8542],
        [4.9102, 4.9287, 4.9134],
        [4.9102, 5.8249, 6.2619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13306106626987457 
model_pd.l_d.mean(): -18.520103454589844 
model_pd.lagr.mean(): -18.387042999267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-19.2983], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13306106626987457
epoch£º106	 i:1 	 global-step:2121	 l-p:0.14283111691474915
epoch£º106	 i:2 	 global-step:2122	 l-p:0.133017435669899
epoch£º106	 i:3 	 global-step:2123	 l-p:0.1334732174873352
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12378203123807907
epoch£º106	 i:5 	 global-step:2125	 l-p:0.1292262077331543
epoch£º106	 i:6 	 global-step:2126	 l-p:-0.8459764122962952
epoch£º106	 i:7 	 global-step:2127	 l-p:0.134750634431839
epoch£º106	 i:8 	 global-step:2128	 l-p:0.13276420533657074
epoch£º106	 i:9 	 global-step:2129	 l-p:0.11549320071935654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9003, 4.9029, 4.9004],
        [4.9003, 4.9054, 4.9007],
        [4.9003, 5.9268, 6.4850],
        [4.9003, 5.7418, 6.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.1452323943376541 
model_pd.l_d.mean(): -19.375402450561523 
model_pd.lagr.mean(): -19.23016929626465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.1036], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.1452323943376541
epoch£º107	 i:1 	 global-step:2141	 l-p:0.12108190357685089
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.02908065728843212
epoch£º107	 i:3 	 global-step:2143	 l-p:0.1228322684764862
epoch£º107	 i:4 	 global-step:2144	 l-p:0.12857888638973236
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13161130249500275
epoch£º107	 i:6 	 global-step:2146	 l-p:0.17274081707000732
epoch£º107	 i:7 	 global-step:2147	 l-p:0.1509426236152649
epoch£º107	 i:8 	 global-step:2148	 l-p:0.1269291490316391
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1382361650466919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8342, 4.8253],
        [4.8240, 4.8242, 4.8240],
        [4.8240, 4.9191, 4.8684],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13067962229251862 
model_pd.l_d.mean(): -18.854278564453125 
model_pd.lagr.mean(): -18.72359848022461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-19.6127], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13067962229251862
epoch£º108	 i:1 	 global-step:2161	 l-p:0.16373340785503387
epoch£º108	 i:2 	 global-step:2162	 l-p:0.10531415045261383
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13608883321285248
epoch£º108	 i:4 	 global-step:2164	 l-p:0.13890936970710754
epoch£º108	 i:5 	 global-step:2165	 l-p:0.15491658449172974
epoch£º108	 i:6 	 global-step:2166	 l-p:0.12788431346416473
epoch£º108	 i:7 	 global-step:2167	 l-p:0.12684443593025208
epoch£º108	 i:8 	 global-step:2168	 l-p:0.1336350291967392
epoch£º108	 i:9 	 global-step:2169	 l-p:0.16246402263641357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9291, 5.0128, 4.9646],
        [4.9291, 4.9306, 4.9291],
        [4.9291, 6.1867, 7.0080],
        [4.9291, 4.9291, 4.9291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11599687486886978 
model_pd.l_d.mean(): -20.014297485351562 
model_pd.lagr.mean(): -19.898300170898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.7177], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11599687486886978
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13467492163181305
epoch£º109	 i:2 	 global-step:2182	 l-p:0.13122142851352692
epoch£º109	 i:3 	 global-step:2183	 l-p:0.14612068235874176
epoch£º109	 i:4 	 global-step:2184	 l-p:0.39356258511543274
epoch£º109	 i:5 	 global-step:2185	 l-p:0.1397668868303299
epoch£º109	 i:6 	 global-step:2186	 l-p:0.14309754967689514
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12363619357347488
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13457900285720825
epoch£º109	 i:9 	 global-step:2189	 l-p:0.11706529557704926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9407, 4.9412, 4.9407],
        [4.9407, 4.9407, 4.9407],
        [4.9407, 4.9417, 4.9407],
        [4.9407, 4.9612, 4.9445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.6603273153305054 
model_pd.l_d.mean(): -20.809101104736328 
model_pd.lagr.mean(): -20.148773193359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.4399], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.6603273153305054
epoch£º110	 i:1 	 global-step:2201	 l-p:0.13157068192958832
epoch£º110	 i:2 	 global-step:2202	 l-p:0.09853442758321762
epoch£º110	 i:3 	 global-step:2203	 l-p:0.134680837392807
epoch£º110	 i:4 	 global-step:2204	 l-p:0.14398711919784546
epoch£º110	 i:5 	 global-step:2205	 l-p:0.1349381059408188
epoch£º110	 i:6 	 global-step:2206	 l-p:0.13921263813972473
epoch£º110	 i:7 	 global-step:2207	 l-p:0.1334948092699051
epoch£º110	 i:8 	 global-step:2208	 l-p:0.23316019773483276
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21813857555389404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8588, 4.8631, 4.8591],
        [4.8588, 5.5366, 5.7565],
        [4.8588, 5.3721, 5.4610],
        [4.8588, 5.7227, 6.1183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.17197895050048828 
model_pd.l_d.mean(): -20.088855743408203 
model_pd.lagr.mean(): -19.91687774658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.17197895050048828
epoch£º111	 i:1 	 global-step:2221	 l-p:0.13511916995048523
epoch£º111	 i:2 	 global-step:2222	 l-p:0.4400756061077118
epoch£º111	 i:3 	 global-step:2223	 l-p:0.12737306952476501
epoch£º111	 i:4 	 global-step:2224	 l-p:0.11795725673437119
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12329532951116562
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11508488655090332
epoch£º111	 i:7 	 global-step:2227	 l-p:0.14042510092258453
epoch£º111	 i:8 	 global-step:2228	 l-p:0.13586284220218658
epoch£º111	 i:9 	 global-step:2229	 l-p:0.11055301129817963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9609, 5.0253, 4.9843],
        [4.9609, 5.0233, 4.9831],
        [4.9609, 4.9609, 4.9609],
        [4.9609, 4.9831, 4.9652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14736345410346985 
model_pd.l_d.mean(): -20.548364639282227 
model_pd.lagr.mean(): -20.4010009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4242], device='cuda:0')), ('power', tensor([-21.2062], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14736345410346985
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.12840348482131958
epoch£º112	 i:2 	 global-step:2242	 l-p:0.1549401432275772
epoch£º112	 i:3 	 global-step:2243	 l-p:0.13487344980239868
epoch£º112	 i:4 	 global-step:2244	 l-p:0.1346540004014969
epoch£º112	 i:5 	 global-step:2245	 l-p:0.15541304647922516
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11933120340108871
epoch£º112	 i:7 	 global-step:2247	 l-p:0.16730709373950958
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13128694891929626
epoch£º112	 i:9 	 global-step:2249	 l-p:0.13383840024471283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7995, 4.7995, 4.7995],
        [4.7995, 5.4674, 5.6862],
        [4.7995, 4.8595, 4.8211],
        [4.7995, 4.7995, 4.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.09870366752147675 
model_pd.l_d.mean(): -19.498804092407227 
model_pd.lagr.mean(): -19.400100708007812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5252], device='cuda:0')), ('power', tensor([-20.2484], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.09870366752147675
epoch£º113	 i:1 	 global-step:2261	 l-p:0.1366472840309143
epoch£º113	 i:2 	 global-step:2262	 l-p:0.1301097422838211
epoch£º113	 i:3 	 global-step:2263	 l-p:0.14400134980678558
epoch£º113	 i:4 	 global-step:2264	 l-p:0.1389940083026886
epoch£º113	 i:5 	 global-step:2265	 l-p:0.1642477810382843
epoch£º113	 i:6 	 global-step:2266	 l-p:0.532466471195221
epoch£º113	 i:7 	 global-step:2267	 l-p:0.1376623511314392
epoch£º113	 i:8 	 global-step:2268	 l-p:0.14826390147209167
epoch£º113	 i:9 	 global-step:2269	 l-p:-1.4471471309661865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8028, 4.8028, 4.8028],
        [4.8028, 4.8043, 4.8029],
        [4.8028, 4.8036, 4.8028],
        [4.8028, 5.6696, 6.0778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.12600794434547424 
model_pd.l_d.mean(): -19.596534729003906 
model_pd.lagr.mean(): -19.47052764892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-20.3751], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.12600794434547424
epoch£º114	 i:1 	 global-step:2281	 l-p:0.1414308100938797
epoch£º114	 i:2 	 global-step:2282	 l-p:0.14236295223236084
epoch£º114	 i:3 	 global-step:2283	 l-p:0.15760155022144318
epoch£º114	 i:4 	 global-step:2284	 l-p:0.1312251091003418
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13056355714797974
epoch£º114	 i:6 	 global-step:2286	 l-p:0.12767860293388367
epoch£º114	 i:7 	 global-step:2287	 l-p:0.1333434134721756
epoch£º114	 i:8 	 global-step:2288	 l-p:0.08849403262138367
epoch£º114	 i:9 	 global-step:2289	 l-p:0.13609187304973602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8589, 4.8589, 4.8589],
        [4.8589, 4.8590, 4.8589],
        [4.8589, 4.8589, 4.8589],
        [4.8589, 5.0529, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.12731653451919556 
model_pd.l_d.mean(): -20.264650344848633 
model_pd.lagr.mean(): -20.137332916259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765], device='cuda:0')), ('power', tensor([-20.9728], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.12731653451919556
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11605178564786911
epoch£º115	 i:2 	 global-step:2302	 l-p:0.12111927568912506
epoch£º115	 i:3 	 global-step:2303	 l-p:0.6387292146682739
epoch£º115	 i:4 	 global-step:2304	 l-p:0.17218255996704102
epoch£º115	 i:5 	 global-step:2305	 l-p:0.1311643421649933
epoch£º115	 i:6 	 global-step:2306	 l-p:0.13663874566555023
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13648080825805664
epoch£º115	 i:8 	 global-step:2308	 l-p:0.4432796835899353
epoch£º115	 i:9 	 global-step:2309	 l-p:0.19258631765842438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7769, 4.8521, 4.8082],
        [4.7769, 4.8114, 4.7860],
        [4.7769, 4.7770, 4.7769],
        [4.7769, 4.9688, 4.9146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11802554130554199 
model_pd.l_d.mean(): -19.752229690551758 
model_pd.lagr.mean(): -19.634204864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-20.5174], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11802554130554199
epoch£º116	 i:1 	 global-step:2321	 l-p:0.1410110890865326
epoch£º116	 i:2 	 global-step:2322	 l-p:0.3659423291683197
epoch£º116	 i:3 	 global-step:2323	 l-p:0.15108567476272583
epoch£º116	 i:4 	 global-step:2324	 l-p:0.10309196263551712
epoch£º116	 i:5 	 global-step:2325	 l-p:0.17403937876224518
epoch£º116	 i:6 	 global-step:2326	 l-p:0.15331502258777618
epoch£º116	 i:7 	 global-step:2327	 l-p:0.1373858004808426
epoch£º116	 i:8 	 global-step:2328	 l-p:0.1091376543045044
epoch£º116	 i:9 	 global-step:2329	 l-p:0.13312026858329773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0272, 5.0282, 5.0272],
        [5.0272, 5.0892, 5.0491],
        [5.0272, 5.0272, 5.0272],
        [5.0272, 5.0272, 5.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12944625318050385 
model_pd.l_d.mean(): -20.764846801757812 
model_pd.lagr.mean(): -20.635400772094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3770], device='cuda:0')), ('power', tensor([-21.3768], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12944625318050385
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13372130692005157
epoch£º117	 i:2 	 global-step:2342	 l-p:0.1344398409128189
epoch£º117	 i:3 	 global-step:2343	 l-p:0.1146756112575531
epoch£º117	 i:4 	 global-step:2344	 l-p:0.14236095547676086
epoch£º117	 i:5 	 global-step:2345	 l-p:0.12434528768062592
epoch£º117	 i:6 	 global-step:2346	 l-p:0.28003332018852234
epoch£º117	 i:7 	 global-step:2347	 l-p:0.14010927081108093
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.27930083870887756
epoch£º117	 i:9 	 global-step:2349	 l-p:0.15428170561790466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7120, 5.8614, 6.5996],
        [4.7120, 5.0060, 4.9815],
        [4.7120, 4.7311, 4.7156],
        [4.7120, 4.7120, 4.7120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12218302488327026 
model_pd.l_d.mean(): -19.74301528930664 
model_pd.lagr.mean(): -19.620832443237305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5612], device='cuda:0')), ('power', tensor([-20.5320], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12218302488327026
epoch£º118	 i:1 	 global-step:2361	 l-p:0.14129891991615295
epoch£º118	 i:2 	 global-step:2362	 l-p:0.14146052300930023
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19656425714492798
epoch£º118	 i:4 	 global-step:2364	 l-p:0.14453710615634918
epoch£º118	 i:5 	 global-step:2365	 l-p:0.15234260261058807
epoch£º118	 i:6 	 global-step:2366	 l-p:0.15165875852108002
epoch£º118	 i:7 	 global-step:2367	 l-p:-3.647312641143799
epoch£º118	 i:8 	 global-step:2368	 l-p:0.4914165139198303
epoch£º118	 i:9 	 global-step:2369	 l-p:0.18057934939861298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8881, 5.0809, 5.0246],
        [4.8881, 4.8959, 4.8889],
        [4.8881, 5.4221, 5.5268],
        [4.8881, 5.5573, 5.7707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16676445305347443 
model_pd.l_d.mean(): -20.724170684814453 
model_pd.lagr.mean(): -20.557405471801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.3858], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16676445305347443
epoch£º119	 i:1 	 global-step:2381	 l-p:0.13332176208496094
epoch£º119	 i:2 	 global-step:2382	 l-p:0.12292829900979996
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11854095011949539
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11453159898519516
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11662672460079193
epoch£º119	 i:6 	 global-step:2386	 l-p:0.12515948712825775
epoch£º119	 i:7 	 global-step:2387	 l-p:0.14094819128513336
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.12446156144142151
epoch£º119	 i:9 	 global-step:2389	 l-p:0.14612095057964325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 4.9117, 4.9116],
        [4.9116, 4.9118, 4.9116],
        [4.9116, 4.9125, 4.9117],
        [4.9116, 5.3191, 5.3397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.13102635741233826 
model_pd.l_d.mean(): -20.306060791015625 
model_pd.lagr.mean(): -20.175033569335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0005], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.13102635741233826
epoch£º120	 i:1 	 global-step:2401	 l-p:0.12613467872142792
epoch£º120	 i:2 	 global-step:2402	 l-p:0.1277753859758377
epoch£º120	 i:3 	 global-step:2403	 l-p:0.15212275087833405
epoch£º120	 i:4 	 global-step:2404	 l-p:0.1474153697490692
epoch£º120	 i:5 	 global-step:2405	 l-p:0.1271064281463623
epoch£º120	 i:6 	 global-step:2406	 l-p:0.14593710005283356
epoch£º120	 i:7 	 global-step:2407	 l-p:0.126295804977417
epoch£º120	 i:8 	 global-step:2408	 l-p:0.08761698752641678
epoch£º120	 i:9 	 global-step:2409	 l-p:0.04906053468585014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 4.9189, 4.8649],
        [4.7389, 4.7646, 4.7447],
        [4.7389, 4.7389, 4.7389],
        [4.7389, 4.7394, 4.7389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.14721235632896423 
model_pd.l_d.mean(): -19.512348175048828 
model_pd.lagr.mean(): -19.365135192871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.2340], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.14721235632896423
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10670050233602524
epoch£º121	 i:2 	 global-step:2422	 l-p:0.13416694104671478
epoch£º121	 i:3 	 global-step:2423	 l-p:0.13525794446468353
epoch£º121	 i:4 	 global-step:2424	 l-p:0.20098601281642914
epoch£º121	 i:5 	 global-step:2425	 l-p:0.12883533537387848
epoch£º121	 i:6 	 global-step:2426	 l-p:0.13878285884857178
epoch£º121	 i:7 	 global-step:2427	 l-p:0.16520404815673828
epoch£º121	 i:8 	 global-step:2428	 l-p:0.1252584457397461
epoch£º121	 i:9 	 global-step:2429	 l-p:0.13324213027954102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9520, 4.9786, 4.9578],
        [4.9520, 4.9616, 4.9531],
        [4.9520, 4.9520, 4.9520],
        [4.9520, 4.9587, 4.9526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.1325395554304123 
model_pd.l_d.mean(): -18.616865158081055 
model_pd.lagr.mean(): -18.484325408935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.1325395554304123
epoch£º122	 i:1 	 global-step:2441	 l-p:0.11314928531646729
epoch£º122	 i:2 	 global-step:2442	 l-p:0.14786964654922485
epoch£º122	 i:3 	 global-step:2443	 l-p:0.13475358486175537
epoch£º122	 i:4 	 global-step:2444	 l-p:0.13365696370601654
epoch£º122	 i:5 	 global-step:2445	 l-p:0.11102622002363205
epoch£º122	 i:6 	 global-step:2446	 l-p:0.14292213320732117
epoch£º122	 i:7 	 global-step:2447	 l-p:0.13064834475517273
epoch£º122	 i:8 	 global-step:2448	 l-p:0.1158924251794815
epoch£º122	 i:9 	 global-step:2449	 l-p:0.13310976326465607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7854, 4.7858, 4.7854],
        [4.7854, 4.8368, 4.8026],
        [4.7854, 4.7911, 4.7859],
        [4.7854, 4.7854, 4.7854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13925926387310028 
model_pd.l_d.mean(): -20.524927139282227 
model_pd.lagr.mean(): -20.38566780090332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.2382], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13925926387310028
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13565759360790253
epoch£º123	 i:2 	 global-step:2462	 l-p:0.17591224610805511
epoch£º123	 i:3 	 global-step:2463	 l-p:0.14319176971912384
epoch£º123	 i:4 	 global-step:2464	 l-p:0.06843767315149307
epoch£º123	 i:5 	 global-step:2465	 l-p:0.1453811079263687
epoch£º123	 i:6 	 global-step:2466	 l-p:0.21024833619594574
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14650897681713104
epoch£º123	 i:8 	 global-step:2468	 l-p:5.308215141296387
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08912108838558197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6756, 4.6759, 4.6756],
        [4.6756, 5.3672, 5.6271],
        [4.6756, 4.7073, 4.6838],
        [4.6756, 4.7228, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.2198965847492218 
model_pd.l_d.mean(): -20.144580841064453 
model_pd.lagr.mean(): -19.924684524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.9062], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.2198965847492218
epoch£º124	 i:1 	 global-step:2481	 l-p:-0.18225984275341034
epoch£º124	 i:2 	 global-step:2482	 l-p:0.1810000240802765
epoch£º124	 i:3 	 global-step:2483	 l-p:0.1314888596534729
epoch£º124	 i:4 	 global-step:2484	 l-p:0.14065949618816376
epoch£º124	 i:5 	 global-step:2485	 l-p:0.10328514128923416
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11696942895650864
epoch£º124	 i:7 	 global-step:2487	 l-p:0.10890630632638931
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11589916050434113
epoch£º124	 i:9 	 global-step:2489	 l-p:0.11167439818382263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2932, 5.3083, 5.2954],
        [5.2932, 6.7116, 7.6682],
        [5.2932, 5.3123, 5.2964],
        [5.2932, 5.7490, 5.7753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.105833038687706 
model_pd.l_d.mean(): -19.020742416381836 
model_pd.lagr.mean(): -18.91490936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3972], device='cuda:0')), ('power', tensor([-19.6343], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.105833038687706
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11200861632823944
epoch£º125	 i:2 	 global-step:2502	 l-p:0.11222091317176819
epoch£º125	 i:3 	 global-step:2503	 l-p:0.12959755957126617
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11511239409446716
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12890280783176422
epoch£º125	 i:6 	 global-step:2506	 l-p:0.12400825321674347
epoch£º125	 i:7 	 global-step:2507	 l-p:0.19420327246189117
epoch£º125	 i:8 	 global-step:2508	 l-p:0.14393100142478943
epoch£º125	 i:9 	 global-step:2509	 l-p:0.15763477981090546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6927, 5.3561, 5.5890],
        [4.6927, 5.1113, 5.1548],
        [4.6927, 4.6927, 4.6927],
        [4.6927, 4.6977, 4.6931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.14195846021175385 
model_pd.l_d.mean(): -20.695846557617188 
model_pd.lagr.mean(): -20.55388832092285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-21.4088], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.14195846021175385
epoch£º126	 i:1 	 global-step:2521	 l-p:0.16387248039245605
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16865576803684235
epoch£º126	 i:3 	 global-step:2523	 l-p:0.1756865680217743
epoch£º126	 i:4 	 global-step:2524	 l-p:0.1473517119884491
epoch£º126	 i:5 	 global-step:2525	 l-p:0.1555628776550293
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11348117887973785
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17040595412254333
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.046231772750616074
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.20496833324432373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6878, 4.7974, 4.7464],
        [4.6878, 4.7628, 4.7200],
        [4.6878, 4.8728, 4.8213],
        [4.6878, 5.4594, 5.7949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09849166870117188 
model_pd.l_d.mean(): -19.73218536376953 
model_pd.lagr.mean(): -19.63369369506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5375], device='cuda:0')), ('power', tensor([-20.4968], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09849166870117188
epoch£º127	 i:1 	 global-step:2541	 l-p:0.2437490075826645
epoch£º127	 i:2 	 global-step:2542	 l-p:0.1380278319120407
epoch£º127	 i:3 	 global-step:2543	 l-p:0.14092092216014862
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11358053982257843
epoch£º127	 i:5 	 global-step:2545	 l-p:0.14621244370937347
epoch£º127	 i:6 	 global-step:2546	 l-p:0.11908034235239029
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11689645797014236
epoch£º127	 i:8 	 global-step:2548	 l-p:0.1189693808555603
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11203265935182571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2055, 5.6831, 5.7288],
        [5.2055, 5.3938, 5.3313],
        [5.2055, 5.2084, 5.2056],
        [5.2055, 5.2062, 5.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09624256193637848 
model_pd.l_d.mean(): -19.658735275268555 
model_pd.lagr.mean(): -19.56249237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-20.3178], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09624256193637848
epoch£º128	 i:1 	 global-step:2561	 l-p:0.12331900000572205
epoch£º128	 i:2 	 global-step:2562	 l-p:0.12177818268537521
epoch£º128	 i:3 	 global-step:2563	 l-p:0.19872236251831055
epoch£º128	 i:4 	 global-step:2564	 l-p:0.14209303259849548
epoch£º128	 i:5 	 global-step:2565	 l-p:0.14447639882564545
epoch£º128	 i:6 	 global-step:2566	 l-p:0.1566612869501114
epoch£º128	 i:7 	 global-step:2567	 l-p:0.12645626068115234
epoch£º128	 i:8 	 global-step:2568	 l-p:0.141159325838089
epoch£º128	 i:9 	 global-step:2569	 l-p:-0.056067295372486115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7560, 4.7567, 4.7560],
        [4.7560, 4.7727, 4.7589],
        [4.7560, 4.7560, 4.7560],
        [4.7560, 4.8818, 4.8281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.11933334916830063 
model_pd.l_d.mean(): -19.98590660095215 
model_pd.lagr.mean(): -19.866573333740234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-20.7640], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.11933334916830063
epoch£º129	 i:1 	 global-step:2581	 l-p:0.2378825694322586
epoch£º129	 i:2 	 global-step:2582	 l-p:0.14636625349521637
epoch£º129	 i:3 	 global-step:2583	 l-p:0.15546658635139465
epoch£º129	 i:4 	 global-step:2584	 l-p:0.1300397664308548
epoch£º129	 i:5 	 global-step:2585	 l-p:0.1259794980287552
epoch£º129	 i:6 	 global-step:2586	 l-p:0.15479010343551636
epoch£º129	 i:7 	 global-step:2587	 l-p:0.1506296992301941
epoch£º129	 i:8 	 global-step:2588	 l-p:0.14670534431934357
epoch£º129	 i:9 	 global-step:2589	 l-p:0.1279129832983017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8930, 5.0316, 4.9750],
        [4.8930, 4.8930, 4.8930],
        [4.8930, 4.8930, 4.8930],
        [4.8930, 4.8941, 4.8931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.053324516862630844 
model_pd.l_d.mean(): -18.751680374145508 
model_pd.lagr.mean(): -18.69835662841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5354], device='cuda:0')), ('power', tensor([-19.5035], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.053324516862630844
epoch£º130	 i:1 	 global-step:2601	 l-p:0.16439981758594513
epoch£º130	 i:2 	 global-step:2602	 l-p:0.13260617852210999
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13836759328842163
epoch£º130	 i:4 	 global-step:2604	 l-p:0.12867464125156403
epoch£º130	 i:5 	 global-step:2605	 l-p:0.1563529521226883
epoch£º130	 i:6 	 global-step:2606	 l-p:0.12385083734989166
epoch£º130	 i:7 	 global-step:2607	 l-p:0.2065650075674057
epoch£º130	 i:8 	 global-step:2608	 l-p:0.12918607890605927
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13685591518878937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9149, 4.9149, 4.9149],
        [4.9149, 4.9903, 4.9460],
        [4.9149, 4.9150, 4.9149],
        [4.9149, 4.9149, 4.9149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13942649960517883 
model_pd.l_d.mean(): -19.717121124267578 
model_pd.lagr.mean(): -19.577693939208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-20.4417], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.13942649960517883
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16038867831230164
epoch£º131	 i:2 	 global-step:2622	 l-p:0.15112608671188354
epoch£º131	 i:3 	 global-step:2623	 l-p:0.08014137297868729
epoch£º131	 i:4 	 global-step:2624	 l-p:0.12559624016284943
epoch£º131	 i:5 	 global-step:2625	 l-p:0.14877261221408844
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13280616700649261
epoch£º131	 i:7 	 global-step:2627	 l-p:0.1597038358449936
epoch£º131	 i:8 	 global-step:2628	 l-p:0.12524399161338806
epoch£º131	 i:9 	 global-step:2629	 l-p:0.1552509218454361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9342, 4.9342, 4.9342],
        [4.9342, 6.0200, 6.6522],
        [4.9342, 4.9342, 4.9342],
        [4.9342, 5.3588, 5.3916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.11712399870157242 
model_pd.l_d.mean(): -19.119667053222656 
model_pd.lagr.mean(): -19.00254249572754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-19.8686], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.11712399870157242
epoch£º132	 i:1 	 global-step:2641	 l-p:0.11805981397628784
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12227487564086914
epoch£º132	 i:3 	 global-step:2643	 l-p:0.12845905125141144
epoch£º132	 i:4 	 global-step:2644	 l-p:0.1399652510881424
epoch£º132	 i:5 	 global-step:2645	 l-p:0.12983585894107819
epoch£º132	 i:6 	 global-step:2646	 l-p:0.13265493512153625
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.32529982924461365
epoch£º132	 i:8 	 global-step:2648	 l-p:0.1253235936164856
epoch£º132	 i:9 	 global-step:2649	 l-p:0.16938437521457672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9633, 4.9633, 4.9633],
        [4.9633, 4.9633, 4.9633],
        [4.9633, 5.2694, 5.2421],
        [4.9633, 5.0746, 5.0208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14211539924144745 
model_pd.l_d.mean(): -18.585342407226562 
model_pd.lagr.mean(): -18.443227767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-19.3119], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14211539924144745
epoch£º133	 i:1 	 global-step:2661	 l-p:0.12338615953922272
epoch£º133	 i:2 	 global-step:2662	 l-p:0.13563095033168793
epoch£º133	 i:3 	 global-step:2663	 l-p:0.1345861256122589
epoch£º133	 i:4 	 global-step:2664	 l-p:0.13398751616477966
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12387882173061371
epoch£º133	 i:6 	 global-step:2666	 l-p:0.157276451587677
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10742223262786865
epoch£º133	 i:8 	 global-step:2668	 l-p:0.1594962775707245
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13607896864414215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9565, 5.2468, 5.2143],
        [4.9565, 5.0353, 4.9898],
        [4.9565, 4.9721, 4.9590],
        [4.9565, 4.9589, 4.9566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.1390286535024643 
model_pd.l_d.mean(): -20.33600425720215 
model_pd.lagr.mean(): -20.196975708007812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.0090], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.1390286535024643
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12881210446357727
epoch£º134	 i:2 	 global-step:2682	 l-p:0.12976299226284027
epoch£º134	 i:3 	 global-step:2683	 l-p:0.1936628669500351
epoch£º134	 i:4 	 global-step:2684	 l-p:0.12775294482707977
epoch£º134	 i:5 	 global-step:2685	 l-p:0.1152525320649147
epoch£º134	 i:6 	 global-step:2686	 l-p:0.12981438636779785
epoch£º134	 i:7 	 global-step:2687	 l-p:0.11707895249128342
epoch£º134	 i:8 	 global-step:2688	 l-p:0.11491487920284271
epoch£º134	 i:9 	 global-step:2689	 l-p:0.14984308183193207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9485, 4.9485],
        [4.9485, 6.0586, 6.7180],
        [4.9485, 5.0732, 5.0177],
        [4.9485, 5.8683, 6.3215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12527760863304138 
model_pd.l_d.mean(): -18.7529239654541 
model_pd.lagr.mean(): -18.62764549255371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.5188], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.12527760863304138
epoch£º135	 i:1 	 global-step:2701	 l-p:0.0957324206829071
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12516283988952637
epoch£º135	 i:3 	 global-step:2703	 l-p:0.17985141277313232
epoch£º135	 i:4 	 global-step:2704	 l-p:0.12893866002559662
epoch£º135	 i:5 	 global-step:2705	 l-p:0.15726517140865326
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16908147931098938
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11557485163211823
epoch£º135	 i:8 	 global-step:2708	 l-p:0.12886245548725128
epoch£º135	 i:9 	 global-step:2709	 l-p:0.14108557999134064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9325, 4.9326, 4.9325],
        [4.9325, 5.0752, 5.0185],
        [4.9325, 4.9332, 4.9325],
        [4.9325, 5.1043, 5.0475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.14106392860412598 
model_pd.l_d.mean(): -20.93817710876465 
model_pd.lagr.mean(): -20.7971134185791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3873], device='cuda:0')), ('power', tensor([-21.5626], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.14106392860412598
epoch£º136	 i:1 	 global-step:2721	 l-p:0.14095750451087952
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15687569975852966
epoch£º136	 i:3 	 global-step:2723	 l-p:0.12047090381383896
epoch£º136	 i:4 	 global-step:2724	 l-p:0.14837253093719482
epoch£º136	 i:5 	 global-step:2725	 l-p:0.11636907607316971
epoch£º136	 i:6 	 global-step:2726	 l-p:0.18499526381492615
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1346064805984497
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17907841503620148
epoch£º136	 i:9 	 global-step:2729	 l-p:0.11643952131271362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9626, 4.9626, 4.9626],
        [4.9626, 4.9791, 4.9654],
        [4.9626, 4.9626, 4.9626],
        [4.9626, 5.9679, 6.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.21663297712802887 
model_pd.l_d.mean(): -20.639198303222656 
model_pd.lagr.mean(): -20.855831146240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.2908], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.21663297712802887
epoch£º137	 i:1 	 global-step:2741	 l-p:0.13477911055088043
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11629187315702438
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12511730194091797
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11656071245670319
epoch£º137	 i:5 	 global-step:2745	 l-p:0.13058969378471375
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11374780535697937
epoch£º137	 i:7 	 global-step:2747	 l-p:0.10907169431447983
epoch£º137	 i:8 	 global-step:2748	 l-p:0.14096371829509735
epoch£º137	 i:9 	 global-step:2749	 l-p:0.1456715166568756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9449, 5.0152, 4.9730],
        [4.9449, 4.9449, 4.9449],
        [4.9449, 4.9449, 4.9449],
        [4.9449, 4.9449, 4.9449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.1319803148508072 
model_pd.l_d.mean(): -20.644489288330078 
model_pd.lagr.mean(): -20.512508392333984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153], device='cuda:0')), ('power', tensor([-21.2943], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.1319803148508072
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12657447159290314
epoch£º138	 i:2 	 global-step:2762	 l-p:0.1394597589969635
epoch£º138	 i:3 	 global-step:2763	 l-p:0.2631090581417084
epoch£º138	 i:4 	 global-step:2764	 l-p:0.15144698321819305
epoch£º138	 i:5 	 global-step:2765	 l-p:0.1341102123260498
epoch£º138	 i:6 	 global-step:2766	 l-p:0.0628577396273613
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.004374017473310232
epoch£º138	 i:8 	 global-step:2768	 l-p:0.13681162893772125
epoch£º138	 i:9 	 global-step:2769	 l-p:0.2063019573688507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8998, 5.8801, 6.4066],
        [4.8998, 6.0689, 6.8069],
        [4.8998, 4.8998, 4.8998],
        [4.8998, 4.9018, 4.8999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.08526557683944702 
model_pd.l_d.mean(): -18.715988159179688 
model_pd.lagr.mean(): -18.630722045898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-19.4725], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.08526557683944702
epoch£º139	 i:1 	 global-step:2781	 l-p:0.1494760662317276
epoch£º139	 i:2 	 global-step:2782	 l-p:0.13185448944568634
epoch£º139	 i:3 	 global-step:2783	 l-p:0.13533230125904083
epoch£º139	 i:4 	 global-step:2784	 l-p:0.12864293158054352
epoch£º139	 i:5 	 global-step:2785	 l-p:0.11740446090698242
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13436494767665863
epoch£º139	 i:7 	 global-step:2787	 l-p:0.12016472220420837
epoch£º139	 i:8 	 global-step:2788	 l-p:0.1696145385503769
epoch£º139	 i:9 	 global-step:2789	 l-p:0.16297422349452972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8771, 5.9064, 6.4901],
        [4.8771, 4.9107, 4.8860],
        [4.8771, 5.4737, 5.6371],
        [4.8771, 5.9257, 6.5308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.142275869846344 
model_pd.l_d.mean(): -20.090476989746094 
model_pd.lagr.mean(): -19.948200225830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.8152], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.142275869846344
epoch£º140	 i:1 	 global-step:2801	 l-p:0.1365613043308258
epoch£º140	 i:2 	 global-step:2802	 l-p:0.13427935540676117
epoch£º140	 i:3 	 global-step:2803	 l-p:0.17217904329299927
epoch£º140	 i:4 	 global-step:2804	 l-p:0.10805230587720871
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13066789507865906
epoch£º140	 i:6 	 global-step:2806	 l-p:0.1372573971748352
epoch£º140	 i:7 	 global-step:2807	 l-p:0.136882945895195
epoch£º140	 i:8 	 global-step:2808	 l-p:0.12624163925647736
epoch£º140	 i:9 	 global-step:2809	 l-p:0.12929630279541016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0309, 5.0310, 5.0309],
        [5.0309, 5.0310, 5.0309],
        [5.0309, 5.0309, 5.0309],
        [5.0309, 5.1604, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13617879152297974 
model_pd.l_d.mean(): -20.141679763793945 
model_pd.lagr.mean(): -20.00550079345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13617879152297974
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11883926391601562
epoch£º141	 i:2 	 global-step:2822	 l-p:0.4117157757282257
epoch£º141	 i:3 	 global-step:2823	 l-p:0.12850074470043182
epoch£º141	 i:4 	 global-step:2824	 l-p:0.13709670305252075
epoch£º141	 i:5 	 global-step:2825	 l-p:0.12967857718467712
epoch£º141	 i:6 	 global-step:2826	 l-p:0.13021968305110931
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12636341154575348
epoch£º141	 i:8 	 global-step:2828	 l-p:0.04820303991436958
epoch£º141	 i:9 	 global-step:2829	 l-p:0.19942182302474976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7061, 4.7986, 4.7516],
        [4.7061, 4.8646, 4.8123],
        [4.7061, 4.8480, 4.7954],
        [4.7061, 4.7066, 4.7062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.4738732874393463 
model_pd.l_d.mean(): -20.265745162963867 
model_pd.lagr.mean(): -19.791872024536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-21.0234], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.4738732874393463
epoch£º142	 i:1 	 global-step:2841	 l-p:0.16610309481620789
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14317570626735687
epoch£º142	 i:3 	 global-step:2843	 l-p:-0.3124043047428131
epoch£º142	 i:4 	 global-step:2844	 l-p:0.1526600569486618
epoch£º142	 i:5 	 global-step:2845	 l-p:0.15588565170764923
epoch£º142	 i:6 	 global-step:2846	 l-p:0.11304543167352676
epoch£º142	 i:7 	 global-step:2847	 l-p:0.1181846559047699
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12784036993980408
epoch£º142	 i:9 	 global-step:2849	 l-p:0.11206839233636856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.9433, 6.2307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11717037111520767 
model_pd.l_d.mean(): -18.95305824279785 
model_pd.lagr.mean(): -18.835887908935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-19.6299], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11717037111520767
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11353204399347305
epoch£º143	 i:2 	 global-step:2862	 l-p:0.11672637611627579
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13291290402412415
epoch£º143	 i:4 	 global-step:2864	 l-p:0.12743252515792847
epoch£º143	 i:5 	 global-step:2865	 l-p:0.12337855249643326
epoch£º143	 i:6 	 global-step:2866	 l-p:0.1572784185409546
epoch£º143	 i:7 	 global-step:2867	 l-p:0.14086012542247772
epoch£º143	 i:8 	 global-step:2868	 l-p:0.10172508656978607
epoch£º143	 i:9 	 global-step:2869	 l-p:0.17130881547927856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 5.8968, 6.6537],
        [4.7389, 5.2827, 5.4187],
        [4.7389, 4.8285, 4.7820],
        [4.7389, 4.7441, 4.7394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13801893591880798 
model_pd.l_d.mean(): -19.54551124572754 
model_pd.lagr.mean(): -19.40749168395996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-20.2945], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13801893591880798
epoch£º144	 i:1 	 global-step:2881	 l-p:0.1373155117034912
epoch£º144	 i:2 	 global-step:2882	 l-p:0.8934991359710693
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11640788614749908
epoch£º144	 i:4 	 global-step:2884	 l-p:0.04436486214399338
epoch£º144	 i:5 	 global-step:2885	 l-p:0.17069755494594574
epoch£º144	 i:6 	 global-step:2886	 l-p:0.15110236406326294
epoch£º144	 i:7 	 global-step:2887	 l-p:0.09981081634759903
epoch£º144	 i:8 	 global-step:2888	 l-p:0.15679913759231567
epoch£º144	 i:9 	 global-step:2889	 l-p:0.12231796234846115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.7362, 5.9853],
        [5.0241, 5.8313, 6.1681],
        [5.0241, 5.0329, 5.0251],
        [5.0241, 6.1065, 6.7281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12291698157787323 
model_pd.l_d.mean(): -20.473691940307617 
model_pd.lagr.mean(): -20.35077476501465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.1255], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12291698157787323
epoch£º145	 i:1 	 global-step:2901	 l-p:0.19279196858406067
epoch£º145	 i:2 	 global-step:2902	 l-p:0.118537537753582
epoch£º145	 i:3 	 global-step:2903	 l-p:0.12223241478204727
epoch£º145	 i:4 	 global-step:2904	 l-p:0.13435567915439606
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11446306854486465
epoch£º145	 i:6 	 global-step:2906	 l-p:0.13535846769809723
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10579967498779297
epoch£º145	 i:8 	 global-step:2908	 l-p:0.14327220618724823
epoch£º145	 i:9 	 global-step:2909	 l-p:0.1431712508201599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8910, 4.8910, 4.8910],
        [4.8910, 4.9224, 4.8990],
        [4.8910, 5.1415, 5.1011],
        [4.8910, 4.9501, 4.9128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.15299585461616516 
model_pd.l_d.mean(): -19.637550354003906 
model_pd.lagr.mean(): -19.484554290771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.3594], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.15299585461616516
epoch£º146	 i:1 	 global-step:2921	 l-p:0.16732461750507355
epoch£º146	 i:2 	 global-step:2922	 l-p:0.13910110294818878
epoch£º146	 i:3 	 global-step:2923	 l-p:0.13617804646492004
epoch£º146	 i:4 	 global-step:2924	 l-p:-0.02170967124402523
epoch£º146	 i:5 	 global-step:2925	 l-p:0.14187955856323242
epoch£º146	 i:6 	 global-step:2926	 l-p:0.16152764856815338
epoch£º146	 i:7 	 global-step:2927	 l-p:0.1425894945859909
epoch£º146	 i:8 	 global-step:2928	 l-p:0.17254094779491425
epoch£º146	 i:9 	 global-step:2929	 l-p:0.1941574364900589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8221, 4.9905, 4.9371],
        [4.8221, 4.8221, 4.8221],
        [4.8221, 4.8659, 4.8358],
        [4.8221, 5.0702, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13663087785243988 
model_pd.l_d.mean(): -20.784469604492188 
model_pd.lagr.mean(): -20.647838592529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-21.4616], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13663087785243988
epoch£º147	 i:1 	 global-step:2941	 l-p:0.1303853988647461
epoch£º147	 i:2 	 global-step:2942	 l-p:0.1330980807542801
epoch£º147	 i:3 	 global-step:2943	 l-p:0.1609722226858139
epoch£º147	 i:4 	 global-step:2944	 l-p:0.13943734765052795
epoch£º147	 i:5 	 global-step:2945	 l-p:0.147186279296875
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05772809684276581
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11968771368265152
epoch£º147	 i:8 	 global-step:2948	 l-p:0.1423853039741516
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12378420680761337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0169, 5.0381, 5.0212],
        [5.0169, 5.0267, 5.0182],
        [5.0169, 6.1193, 6.7655],
        [5.0169, 5.0169, 5.0169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12316081672906876 
model_pd.l_d.mean(): -20.50149917602539 
model_pd.lagr.mean(): -20.378337860107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.1461], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12316081672906876
epoch£º148	 i:1 	 global-step:2961	 l-p:0.15079808235168457
epoch£º148	 i:2 	 global-step:2962	 l-p:0.12832878530025482
epoch£º148	 i:3 	 global-step:2963	 l-p:0.16931934654712677
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1370006650686264
epoch£º148	 i:5 	 global-step:2965	 l-p:0.11860263347625732
epoch£º148	 i:6 	 global-step:2966	 l-p:0.1417667716741562
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10499867796897888
epoch£º148	 i:8 	 global-step:2968	 l-p:0.12056655436754227
epoch£º148	 i:9 	 global-step:2969	 l-p:0.13967251777648926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 4.9000, 4.8622],
        [4.8391, 4.8681, 4.8463],
        [4.8391, 5.8638, 6.4534],
        [4.8391, 5.2855, 5.3448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.1945420205593109 
model_pd.l_d.mean(): -18.755502700805664 
model_pd.lagr.mean(): -18.56096076965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-19.5561], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.1945420205593109
epoch£º149	 i:1 	 global-step:2981	 l-p:0.1428433358669281
epoch£º149	 i:2 	 global-step:2982	 l-p:0.23036272823810577
epoch£º149	 i:3 	 global-step:2983	 l-p:0.13947328925132751
epoch£º149	 i:4 	 global-step:2984	 l-p:0.10395738482475281
epoch£º149	 i:5 	 global-step:2985	 l-p:0.14260472357273102
epoch£º149	 i:6 	 global-step:2986	 l-p:0.13735908269882202
epoch£º149	 i:7 	 global-step:2987	 l-p:0.17951171100139618
epoch£º149	 i:8 	 global-step:2988	 l-p:0.116239532828331
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14684642851352692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8550, 5.1441, 5.1190],
        [4.8550, 5.2683, 5.3059],
        [4.8550, 5.1379, 5.1104],
        [4.8550, 4.8568, 4.8551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.134211465716362 
model_pd.l_d.mean(): -18.662302017211914 
model_pd.lagr.mean(): -18.528091430664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-19.4304], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.134211465716362
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17186173796653748
epoch£º150	 i:2 	 global-step:3002	 l-p:0.06388652324676514
epoch£º150	 i:3 	 global-step:3003	 l-p:0.12168869376182556
epoch£º150	 i:4 	 global-step:3004	 l-p:0.12090489268302917
epoch£º150	 i:5 	 global-step:3005	 l-p:0.12666304409503937
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12320195883512497
epoch£º150	 i:7 	 global-step:3007	 l-p:0.1272990107536316
epoch£º150	 i:8 	 global-step:3008	 l-p:0.1284324675798416
epoch£º150	 i:9 	 global-step:3009	 l-p:0.16401296854019165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9225, 5.2712, 5.2696],
        [4.9225, 4.9225, 4.9225],
        [4.9225, 5.6900, 6.0042],
        [4.9225, 5.3899, 5.4578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13014642894268036 
model_pd.l_d.mean(): -20.736303329467773 
model_pd.lagr.mean(): -20.606157302856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.3896], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13014642894268036
epoch£º151	 i:1 	 global-step:3021	 l-p:0.1409524530172348
epoch£º151	 i:2 	 global-step:3022	 l-p:0.22005122900009155
epoch£º151	 i:3 	 global-step:3023	 l-p:0.01967337727546692
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3461624085903168
epoch£º151	 i:5 	 global-step:3025	 l-p:0.15860989689826965
epoch£º151	 i:6 	 global-step:3026	 l-p:0.14770470559597015
epoch£º151	 i:7 	 global-step:3027	 l-p:0.06821256875991821
epoch£º151	 i:8 	 global-step:3028	 l-p:0.14477187395095825
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12967777252197266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9094, 5.0421, 4.9876],
        [4.9094, 5.8391, 6.3169],
        [4.9094, 5.0815, 5.0270],
        [4.9094, 5.0765, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.09883890300989151 
model_pd.l_d.mean(): -19.85956382751465 
model_pd.lagr.mean(): -19.760725021362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-20.6108], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.09883890300989151
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11932844668626785
epoch£º152	 i:2 	 global-step:3042	 l-p:0.13062433898448944
epoch£º152	 i:3 	 global-step:3043	 l-p:0.12601648271083832
epoch£º152	 i:4 	 global-step:3044	 l-p:0.1348685771226883
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12301627546548843
epoch£º152	 i:6 	 global-step:3046	 l-p:0.12868323922157288
epoch£º152	 i:7 	 global-step:3047	 l-p:0.14273801445960999
epoch£º152	 i:8 	 global-step:3048	 l-p:0.16293640434741974
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12502311170101166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9620, 4.9658, 4.9623],
        [4.9620, 4.9621, 4.9620],
        [4.9620, 4.9723, 4.9634],
        [4.9620, 4.9620, 4.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.17590755224227905 
model_pd.l_d.mean(): -19.374603271484375 
model_pd.lagr.mean(): -19.19869613647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.0581], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.17590755224227905
epoch£º153	 i:1 	 global-step:3061	 l-p:0.12960660457611084
epoch£º153	 i:2 	 global-step:3062	 l-p:0.0671481192111969
epoch£º153	 i:3 	 global-step:3063	 l-p:0.14171746373176575
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13708819448947906
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11809132248163223
epoch£º153	 i:6 	 global-step:3066	 l-p:0.14354462921619415
epoch£º153	 i:7 	 global-step:3067	 l-p:0.12174450606107712
epoch£º153	 i:8 	 global-step:3068	 l-p:0.14105159044265747
epoch£º153	 i:9 	 global-step:3069	 l-p:0.17224232852458954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7396, 5.3093, 5.4700],
        [4.7396, 4.8730, 4.8210],
        [4.7396, 4.7610, 4.7442],
        [4.7396, 4.7435, 4.7400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.14142736792564392 
model_pd.l_d.mean(): -18.744365692138672 
model_pd.lagr.mean(): -18.602937698364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5893], device='cuda:0')), ('power', tensor([-19.5512], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.14142736792564392
epoch£º154	 i:1 	 global-step:3081	 l-p:0.13062703609466553
epoch£º154	 i:2 	 global-step:3082	 l-p:0.20047400891780853
epoch£º154	 i:3 	 global-step:3083	 l-p:0.13998126983642578
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12102975696325302
epoch£º154	 i:5 	 global-step:3085	 l-p:0.15692982077598572
epoch£º154	 i:6 	 global-step:3086	 l-p:0.1341116726398468
epoch£º154	 i:7 	 global-step:3087	 l-p:0.14187900722026825
epoch£º154	 i:8 	 global-step:3088	 l-p:0.3711170554161072
epoch£º154	 i:9 	 global-step:3089	 l-p:0.1302367001771927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0545, 5.6953, 5.8847],
        [5.0545, 5.6068, 5.7261],
        [5.0545, 5.0970, 5.0672],
        [5.0545, 5.3768, 5.3577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.1293618530035019 
model_pd.l_d.mean(): -19.950233459472656 
model_pd.lagr.mean(): -19.820871353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.6240], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.1293618530035019
epoch£º155	 i:1 	 global-step:3101	 l-p:0.2467799335718155
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13076251745224
epoch£º155	 i:3 	 global-step:3103	 l-p:0.12569096684455872
epoch£º155	 i:4 	 global-step:3104	 l-p:0.11777126044034958
epoch£º155	 i:5 	 global-step:3105	 l-p:0.15362000465393066
epoch£º155	 i:6 	 global-step:3106	 l-p:0.1274392157793045
epoch£º155	 i:7 	 global-step:3107	 l-p:0.16637367010116577
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12189832329750061
epoch£º155	 i:9 	 global-step:3109	 l-p:0.13681373000144958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8870, 5.9778, 6.6364],
        [4.8870, 6.0009, 6.6857],
        [4.8870, 5.0347, 4.9802],
        [4.8870, 5.9745, 6.6293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12619300186634064 
model_pd.l_d.mean(): -19.98308753967285 
model_pd.lagr.mean(): -19.856895446777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.7141], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12619300186634064
epoch£º156	 i:1 	 global-step:3121	 l-p:0.15135562419891357
epoch£º156	 i:2 	 global-step:3122	 l-p:0.1253344863653183
epoch£º156	 i:3 	 global-step:3123	 l-p:0.07849434018135071
epoch£º156	 i:4 	 global-step:3124	 l-p:0.14664863049983978
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13940595090389252
epoch£º156	 i:6 	 global-step:3126	 l-p:0.1545710265636444
epoch£º156	 i:7 	 global-step:3127	 l-p:0.13001002371311188
epoch£º156	 i:8 	 global-step:3128	 l-p:0.1219903901219368
epoch£º156	 i:9 	 global-step:3129	 l-p:0.14602874219417572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9445, 5.0063, 4.9679],
        [4.9445, 5.4315, 5.5134],
        [4.9445, 5.8217, 6.2434],
        [4.9445, 5.1717, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.07942657172679901 
model_pd.l_d.mean(): -20.169748306274414 
model_pd.lagr.mean(): -20.090322494506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.8787], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.07942657172679901
epoch£º157	 i:1 	 global-step:3141	 l-p:0.115088552236557
epoch£º157	 i:2 	 global-step:3142	 l-p:0.16387219727039337
epoch£º157	 i:3 	 global-step:3143	 l-p:0.12627920508384705
epoch£º157	 i:4 	 global-step:3144	 l-p:0.14505954086780548
epoch£º157	 i:5 	 global-step:3145	 l-p:0.1890605092048645
epoch£º157	 i:6 	 global-step:3146	 l-p:0.13292492926120758
epoch£º157	 i:7 	 global-step:3147	 l-p:0.15368561446666718
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14091920852661133
epoch£º157	 i:9 	 global-step:3149	 l-p:0.20758506655693054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8970, 4.8969],
        [4.8969, 4.9047, 4.8979],
        [4.8969, 4.8969, 4.8969],
        [4.8969, 5.2925, 5.3195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.14894498884677887 
model_pd.l_d.mean(): -19.583826065063477 
model_pd.lagr.mean(): -19.43488121032715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.14894498884677887
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13469460606575012
epoch£º158	 i:2 	 global-step:3162	 l-p:0.12151515483856201
epoch£º158	 i:3 	 global-step:3163	 l-p:0.12073793262243271
epoch£º158	 i:4 	 global-step:3164	 l-p:0.1266389787197113
epoch£º158	 i:5 	 global-step:3165	 l-p:0.13047605752944946
epoch£º158	 i:6 	 global-step:3166	 l-p:0.15539728105068207
epoch£º158	 i:7 	 global-step:3167	 l-p:0.14183993637561798
epoch£º158	 i:8 	 global-step:3168	 l-p:0.1379714161157608
epoch£º158	 i:9 	 global-step:3169	 l-p:0.2774495482444763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8206, 4.9537, 4.9008],
        [4.8206, 4.8785, 4.8421],
        [4.8206, 4.9201, 4.8713],
        [4.8206, 4.8523, 4.8290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.4207479655742645 
model_pd.l_d.mean(): -20.48779296875 
model_pd.lagr.mean(): -20.067045211791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4829], device='cuda:0')), ('power', tensor([-21.2049], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.4207479655742645
epoch£º159	 i:1 	 global-step:3181	 l-p:0.13949799537658691
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13686883449554443
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.4399047791957855
epoch£º159	 i:4 	 global-step:3184	 l-p:0.15449292957782745
epoch£º159	 i:5 	 global-step:3185	 l-p:0.13250203430652618
epoch£º159	 i:6 	 global-step:3186	 l-p:0.12135162204504013
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13584278523921967
epoch£º159	 i:8 	 global-step:3188	 l-p:0.1244821548461914
epoch£º159	 i:9 	 global-step:3189	 l-p:0.18456466495990753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8676, 4.9223, 4.8871],
        [4.8676, 5.5552, 5.8065],
        [4.8676, 4.8676, 4.8676],
        [4.8676, 4.8681, 4.8676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.197275310754776 
model_pd.l_d.mean(): -20.888959884643555 
model_pd.lagr.mean(): -20.69168472290039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.5441], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.197275310754776
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11667167395353317
epoch£º160	 i:2 	 global-step:3202	 l-p:0.1261681169271469
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13175858557224274
epoch£º160	 i:4 	 global-step:3204	 l-p:0.1416420340538025
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.07889789342880249
epoch£º160	 i:6 	 global-step:3206	 l-p:0.1282547563314438
epoch£º160	 i:7 	 global-step:3207	 l-p:0.13265754282474518
epoch£º160	 i:8 	 global-step:3208	 l-p:0.13389471173286438
epoch£º160	 i:9 	 global-step:3209	 l-p:0.15662942826747894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9397, 5.0731, 5.0187],
        [4.9397, 5.0032, 4.9642],
        [4.9397, 5.5149, 5.6643],
        [4.9397, 5.5890, 5.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.16019673645496368 
model_pd.l_d.mean(): -20.868980407714844 
model_pd.lagr.mean(): -20.708784103393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4005], device='cuda:0')), ('power', tensor([-21.5061], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.16019673645496368
epoch£º161	 i:1 	 global-step:3221	 l-p:0.13114680349826813
epoch£º161	 i:2 	 global-step:3222	 l-p:0.2832969129085541
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11708594858646393
epoch£º161	 i:4 	 global-step:3224	 l-p:0.03606124967336655
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19147345423698425
epoch£º161	 i:6 	 global-step:3226	 l-p:0.13080160319805145
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14572256803512573
epoch£º161	 i:8 	 global-step:3228	 l-p:0.17050908505916595
epoch£º161	 i:9 	 global-step:3229	 l-p:0.1671193689107895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6408, 5.1010, 5.1898],
        [4.6408, 4.6412, 4.6408],
        [4.6408, 4.7158, 4.6742],
        [4.6408, 5.2162, 5.3959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12155498564243317 
model_pd.l_d.mean(): -20.335569381713867 
model_pd.lagr.mean(): -20.214014053344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-21.1222], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12155498564243317
epoch£º162	 i:1 	 global-step:3241	 l-p:0.15463899075984955
epoch£º162	 i:2 	 global-step:3242	 l-p:0.16063500940799713
epoch£º162	 i:3 	 global-step:3243	 l-p:0.15913347899913788
epoch£º162	 i:4 	 global-step:3244	 l-p:0.07124535739421844
epoch£º162	 i:5 	 global-step:3245	 l-p:0.12858659029006958
epoch£º162	 i:6 	 global-step:3246	 l-p:0.19601352512836456
epoch£º162	 i:7 	 global-step:3247	 l-p:0.08601455390453339
epoch£º162	 i:8 	 global-step:3248	 l-p:0.5686051249504089
epoch£º162	 i:9 	 global-step:3249	 l-p:0.12522856891155243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9430, 5.8061, 6.2165],
        [4.9430, 4.9430, 4.9430],
        [4.9430, 5.2567, 5.2406],
        [4.9430, 4.9430, 4.9430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14484092593193054 
model_pd.l_d.mean(): -18.54429817199707 
model_pd.lagr.mean(): -18.399457931518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-19.3250], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14484092593193054
epoch£º163	 i:1 	 global-step:3261	 l-p:0.117344431579113
epoch£º163	 i:2 	 global-step:3262	 l-p:0.11954858154058456
epoch£º163	 i:3 	 global-step:3263	 l-p:0.17113158106803894
epoch£º163	 i:4 	 global-step:3264	 l-p:0.13175177574157715
epoch£º163	 i:5 	 global-step:3265	 l-p:0.13822363317012787
epoch£º163	 i:6 	 global-step:3266	 l-p:0.1168263778090477
epoch£º163	 i:7 	 global-step:3267	 l-p:0.14107738435268402
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12590806186199188
epoch£º163	 i:9 	 global-step:3269	 l-p:0.1319456398487091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9566, 5.1772, 5.1298],
        [4.9566, 4.9606, 4.9570],
        [4.9566, 4.9575, 4.9567],
        [4.9566, 4.9793, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.12746627628803253 
model_pd.l_d.mean(): -20.163833618164062 
model_pd.lagr.mean(): -20.036367416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.12746627628803253
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12813541293144226
epoch£º164	 i:2 	 global-step:3282	 l-p:0.14661958813667297
epoch£º164	 i:3 	 global-step:3283	 l-p:0.11124435812234879
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14926470816135406
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12970693409442902
epoch£º164	 i:6 	 global-step:3286	 l-p:0.1297234147787094
epoch£º164	 i:7 	 global-step:3287	 l-p:0.14510852098464966
epoch£º164	 i:8 	 global-step:3288	 l-p:0.15020164847373962
epoch£º164	 i:9 	 global-step:3289	 l-p:0.14789101481437683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8951, 4.8951, 4.8951],
        [4.8951, 5.6363, 5.9356],
        [4.8951, 5.4144, 5.5263],
        [4.8951, 5.0772, 5.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13100244104862213 
model_pd.l_d.mean(): -20.2044677734375 
model_pd.lagr.mean(): -20.07346534729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.9135], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13100244104862213
epoch£º165	 i:1 	 global-step:3301	 l-p:0.13213133811950684
epoch£º165	 i:2 	 global-step:3302	 l-p:0.16264930367469788
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14334644377231598
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.016632337123155594
epoch£º165	 i:5 	 global-step:3305	 l-p:0.40678608417510986
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12462004274129868
epoch£º165	 i:7 	 global-step:3307	 l-p:0.14270301163196564
epoch£º165	 i:8 	 global-step:3308	 l-p:0.015078501775860786
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13115748763084412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8702, 4.8718, 4.8703],
        [4.8702, 4.9372, 4.8972],
        [4.8702, 4.9465, 4.9033],
        [4.8702, 4.8703, 4.8702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.1429573893547058 
model_pd.l_d.mean(): -18.978986740112305 
model_pd.lagr.mean(): -18.836029052734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-19.7437], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.1429573893547058
epoch£º166	 i:1 	 global-step:3321	 l-p:0.13214069604873657
epoch£º166	 i:2 	 global-step:3322	 l-p:0.14452147483825684
epoch£º166	 i:3 	 global-step:3323	 l-p:0.14302785694599152
epoch£º166	 i:4 	 global-step:3324	 l-p:0.1264314353466034
epoch£º166	 i:5 	 global-step:3325	 l-p:0.1114988848567009
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12366359680891037
epoch£º166	 i:7 	 global-step:3327	 l-p:0.13026826083660126
epoch£º166	 i:8 	 global-step:3328	 l-p:0.11892315000295639
epoch£º166	 i:9 	 global-step:3329	 l-p:0.8405590653419495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0056, 5.0056, 5.0056],
        [5.0056, 5.1580, 5.1023],
        [5.0056, 6.2000, 6.9611],
        [5.0056, 5.0174, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13492725789546967 
model_pd.l_d.mean(): -19.458099365234375 
model_pd.lagr.mean(): -19.323171615600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-20.1630], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13492725789546967
epoch£º167	 i:1 	 global-step:3341	 l-p:0.13003431260585785
epoch£º167	 i:2 	 global-step:3342	 l-p:0.127092644572258
epoch£º167	 i:3 	 global-step:3343	 l-p:0.13898780941963196
epoch£º167	 i:4 	 global-step:3344	 l-p:0.13273771107196808
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12146519124507904
epoch£º167	 i:6 	 global-step:3346	 l-p:0.3702574372291565
epoch£º167	 i:7 	 global-step:3347	 l-p:0.07087266445159912
epoch£º167	 i:8 	 global-step:3348	 l-p:0.048020485788583755
epoch£º167	 i:9 	 global-step:3349	 l-p:0.12065762281417847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8241, 4.8246, 4.8241],
        [4.8241, 5.8396, 6.4302],
        [4.8241, 4.8241, 4.8241],
        [4.8241, 4.8241, 4.8241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.1283147782087326 
model_pd.l_d.mean(): -18.6581974029541 
model_pd.lagr.mean(): -18.529882431030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.4495], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.1283147782087326
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12601330876350403
epoch£º168	 i:2 	 global-step:3362	 l-p:0.14647716283798218
epoch£º168	 i:3 	 global-step:3363	 l-p:0.14453889429569244
epoch£º168	 i:4 	 global-step:3364	 l-p:0.14144767820835114
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11997029185295105
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11652997881174088
epoch£º168	 i:7 	 global-step:3367	 l-p:0.14143246412277222
epoch£º168	 i:8 	 global-step:3368	 l-p:0.1515544354915619
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12819327414035797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8907, 4.8909, 4.8907],
        [4.8907, 4.9091, 4.8942],
        [4.8907, 5.0211, 4.9678],
        [4.8907, 4.9412, 4.9078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13750511407852173 
model_pd.l_d.mean(): -20.71617317199707 
model_pd.lagr.mean(): -20.57866859436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3809], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13750511407852173
epoch£º169	 i:1 	 global-step:3381	 l-p:0.1419399082660675
epoch£º169	 i:2 	 global-step:3382	 l-p:0.1336933672428131
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.10756998509168625
epoch£º169	 i:4 	 global-step:3384	 l-p:0.14054587483406067
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09748193621635437
epoch£º169	 i:6 	 global-step:3386	 l-p:0.051760222762823105
epoch£º169	 i:7 	 global-step:3387	 l-p:0.15242229402065277
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12539184093475342
epoch£º169	 i:9 	 global-step:3389	 l-p:0.14870378375053406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9487, 5.1440, 5.0929],
        [4.9487, 4.9496, 4.9488],
        [4.9487, 4.9724, 4.9539],
        [4.9487, 4.9494, 4.9487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.15095825493335724 
model_pd.l_d.mean(): -18.792579650878906 
model_pd.lagr.mean(): -18.641620635986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5342], device='cuda:0')), ('power', tensor([-19.5436], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.15095825493335724
epoch£º170	 i:1 	 global-step:3401	 l-p:0.14094679057598114
epoch£º170	 i:2 	 global-step:3402	 l-p:0.1380879133939743
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12541784346103668
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12352348119020462
epoch£º170	 i:5 	 global-step:3405	 l-p:0.43123096227645874
epoch£º170	 i:6 	 global-step:3406	 l-p:0.12052592635154724
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12275056540966034
epoch£º170	 i:8 	 global-step:3408	 l-p:0.13010525703430176
epoch£º170	 i:9 	 global-step:3409	 l-p:0.15123935043811798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9569, 5.4455, 5.5327],
        [4.9569, 4.9583, 4.9570],
        [4.9569, 4.9820, 4.9626],
        [4.9569, 4.9569, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11723906546831131 
model_pd.l_d.mean(): -18.685253143310547 
model_pd.lagr.mean(): -18.56801414489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.4504], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11723906546831131
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483363598585129
epoch£º171	 i:2 	 global-step:3422	 l-p:0.1653948724269867
epoch£º171	 i:3 	 global-step:3423	 l-p:0.14427317678928375
epoch£º171	 i:4 	 global-step:3424	 l-p:0.07828862220048904
epoch£º171	 i:5 	 global-step:3425	 l-p:0.11680763214826584
epoch£º171	 i:6 	 global-step:3426	 l-p:0.13464246690273285
epoch£º171	 i:7 	 global-step:3427	 l-p:0.25303253531455994
epoch£º171	 i:8 	 global-step:3428	 l-p:0.1385194957256317
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15589293837547302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8074, 4.8079, 4.8074],
        [4.8074, 4.8128, 4.8080],
        [4.8074, 5.0545, 5.0202],
        [4.8074, 4.8074, 4.8074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.11588346213102341 
model_pd.l_d.mean(): -20.79198455810547 
model_pd.lagr.mean(): -20.676101684570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.4793], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.11588346213102341
epoch£º172	 i:1 	 global-step:3441	 l-p:0.19621795415878296
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09903980046510696
epoch£º172	 i:3 	 global-step:3443	 l-p:-0.16531981527805328
epoch£º172	 i:4 	 global-step:3444	 l-p:0.12688666582107544
epoch£º172	 i:5 	 global-step:3445	 l-p:0.13663195073604584
epoch£º172	 i:6 	 global-step:3446	 l-p:0.14092276990413666
epoch£º172	 i:7 	 global-step:3447	 l-p:0.17890223860740662
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12066928297281265
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12979933619499207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1353, 6.2637, 6.9316],
        [5.1353, 5.2357, 5.1844],
        [5.1353, 5.8315, 6.0680],
        [5.1353, 5.2478, 5.1941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11810120940208435 
model_pd.l_d.mean(): -19.4683837890625 
model_pd.lagr.mean(): -19.350282669067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.1655], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11810120940208435
epoch£º173	 i:1 	 global-step:3461	 l-p:0.09931252896785736
epoch£º173	 i:2 	 global-step:3462	 l-p:0.12842634320259094
epoch£º173	 i:3 	 global-step:3463	 l-p:0.14312173426151276
epoch£º173	 i:4 	 global-step:3464	 l-p:0.125008225440979
epoch£º173	 i:5 	 global-step:3465	 l-p:0.13909463584423065
epoch£º173	 i:6 	 global-step:3466	 l-p:0.13274358212947845
epoch£º173	 i:7 	 global-step:3467	 l-p:0.09155543893575668
epoch£º173	 i:8 	 global-step:3468	 l-p:0.0922703817486763
epoch£º173	 i:9 	 global-step:3469	 l-p:0.14170847833156586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6642, 5.4158, 5.7591],
        [4.6642, 4.6757, 4.6659],
        [4.6642, 4.7299, 4.6908],
        [4.6642, 4.7831, 4.7331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.15752002596855164 
model_pd.l_d.mean(): -20.25859260559082 
model_pd.lagr.mean(): -20.101072311401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5425], device='cuda:0')), ('power', tensor([-21.0341], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.15752002596855164
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13991931080818176
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10787804424762726
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15758273005485535
epoch£º174	 i:4 	 global-step:3484	 l-p:0.1577698141336441
epoch£º174	 i:5 	 global-step:3485	 l-p:-0.5037062764167786
epoch£º174	 i:6 	 global-step:3486	 l-p:0.18976084887981415
epoch£º174	 i:7 	 global-step:3487	 l-p:0.13604091107845306
epoch£º174	 i:8 	 global-step:3488	 l-p:0.12962894141674042
epoch£º174	 i:9 	 global-step:3489	 l-p:0.1185666024684906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 5.1444, 5.1419],
        [5.1417, 5.1532, 5.1434],
        [5.1417, 5.1957, 5.1601],
        [5.1417, 5.2253, 5.1784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12143217772245407 
model_pd.l_d.mean(): -20.606821060180664 
model_pd.lagr.mean(): -20.485389709472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3579], device='cuda:0')), ('power', tensor([-21.1976], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12143217772245407
epoch£º175	 i:1 	 global-step:3501	 l-p:0.12045691907405853
epoch£º175	 i:2 	 global-step:3502	 l-p:0.1572062373161316
epoch£º175	 i:3 	 global-step:3503	 l-p:0.19582484662532806
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13158617913722992
epoch£º175	 i:5 	 global-step:3505	 l-p:0.1183004379272461
epoch£º175	 i:6 	 global-step:3506	 l-p:0.11533771455287933
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12438452243804932
epoch£º175	 i:8 	 global-step:3508	 l-p:0.12497077882289886
epoch£º175	 i:9 	 global-step:3509	 l-p:0.1243445947766304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9240, 4.9240, 4.9240],
        [4.9240, 4.9424, 4.9275],
        [4.9240, 5.4519, 5.5723],
        [4.9240, 5.1305, 5.0825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.0948229506611824 
model_pd.l_d.mean(): -19.311126708984375 
model_pd.lagr.mean(): -19.2163028717041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.0412], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.0948229506611824
epoch£º176	 i:1 	 global-step:3521	 l-p:0.17333002388477325
epoch£º176	 i:2 	 global-step:3522	 l-p:0.13698290288448334
epoch£º176	 i:3 	 global-step:3523	 l-p:0.13736121356487274
epoch£º176	 i:4 	 global-step:3524	 l-p:0.10094720870256424
epoch£º176	 i:5 	 global-step:3525	 l-p:0.5056532621383667
epoch£º176	 i:6 	 global-step:3526	 l-p:0.138876810669899
epoch£º176	 i:7 	 global-step:3527	 l-p:0.13509739935398102
epoch£º176	 i:8 	 global-step:3528	 l-p:-0.29989445209503174
epoch£º176	 i:9 	 global-step:3529	 l-p:0.15770690143108368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8006, 5.8640, 6.5153],
        [4.8006, 4.8011, 4.8006],
        [4.8006, 4.8962, 4.8482],
        [4.8006, 4.8008, 4.8006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11441051214933395 
model_pd.l_d.mean(): -19.830095291137695 
model_pd.lagr.mean(): -19.71568489074707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5278], device='cuda:0')), ('power', tensor([-20.5859], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.11441051214933395
epoch£º177	 i:1 	 global-step:3541	 l-p:0.1329599916934967
epoch£º177	 i:2 	 global-step:3542	 l-p:0.17259563505649567
epoch£º177	 i:3 	 global-step:3543	 l-p:0.11052850633859634
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13878847658634186
epoch£º177	 i:5 	 global-step:3545	 l-p:0.10793787986040115
epoch£º177	 i:6 	 global-step:3546	 l-p:0.13080623745918274
epoch£º177	 i:7 	 global-step:3547	 l-p:0.12541237473487854
epoch£º177	 i:8 	 global-step:3548	 l-p:0.14895318448543549
epoch£º177	 i:9 	 global-step:3549	 l-p:0.19635459780693054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9393, 4.9392],
        [4.9392, 4.9392, 4.9392],
        [4.9392, 4.9392, 4.9392],
        [4.9392, 5.0080, 4.9668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.19865332543849945 
model_pd.l_d.mean(): -18.319686889648438 
model_pd.lagr.mean(): -18.12103271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5402], device='cuda:0')), ('power', tensor([-19.0717], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.19865332543849945
epoch£º178	 i:1 	 global-step:3561	 l-p:0.14010098576545715
epoch£º178	 i:2 	 global-step:3562	 l-p:0.13761605322360992
epoch£º178	 i:3 	 global-step:3563	 l-p:0.11380922049283981
epoch£º178	 i:4 	 global-step:3564	 l-p:0.129417285323143
epoch£º178	 i:5 	 global-step:3565	 l-p:0.153206005692482
epoch£º178	 i:6 	 global-step:3566	 l-p:0.12530779838562012
epoch£º178	 i:7 	 global-step:3567	 l-p:0.28490397334098816
epoch£º178	 i:8 	 global-step:3568	 l-p:0.12286129593849182
epoch£º178	 i:9 	 global-step:3569	 l-p:0.07828634232282639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 4.8158, 4.8148],
        [4.8148, 4.8352, 4.8188],
        [4.8148, 4.8340, 4.8185],
        [4.8148, 4.8152, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.17446307837963104 
model_pd.l_d.mean(): -20.570770263671875 
model_pd.lagr.mean(): -20.39630699157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.2748], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.17446307837963104
epoch£º179	 i:1 	 global-step:3581	 l-p:0.5431086421012878
epoch£º179	 i:2 	 global-step:3582	 l-p:0.1261366754770279
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12480618804693222
epoch£º179	 i:4 	 global-step:3584	 l-p:0.1499737948179245
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1307569444179535
epoch£º179	 i:6 	 global-step:3586	 l-p:0.16031096875667572
epoch£º179	 i:7 	 global-step:3587	 l-p:0.1263057142496109
epoch£º179	 i:8 	 global-step:3588	 l-p:0.10533857345581055
epoch£º179	 i:9 	 global-step:3589	 l-p:0.20112742483615875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8976, 4.8969],
        [4.8969, 4.8974, 4.8969],
        [4.8969, 5.0855, 5.0351],
        [4.8969, 4.8969, 4.8969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12802115082740784 
model_pd.l_d.mean(): -20.887521743774414 
model_pd.lagr.mean(): -20.75950050354004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4122], device='cuda:0')), ('power', tensor([-21.5367], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12802115082740784
epoch£º180	 i:1 	 global-step:3601	 l-p:0.1490710824728012
epoch£º180	 i:2 	 global-step:3602	 l-p:-2.026139259338379
epoch£º180	 i:3 	 global-step:3603	 l-p:0.20708972215652466
epoch£º180	 i:4 	 global-step:3604	 l-p:-0.07756469398736954
epoch£º180	 i:5 	 global-step:3605	 l-p:0.16001178324222565
epoch£º180	 i:6 	 global-step:3606	 l-p:0.265902042388916
epoch£º180	 i:7 	 global-step:3607	 l-p:0.12818320095539093
epoch£º180	 i:8 	 global-step:3608	 l-p:0.13517844676971436
epoch£º180	 i:9 	 global-step:3609	 l-p:0.1342034637928009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9307, 4.9373, 4.9315],
        [4.9307, 5.0032, 4.9606],
        [4.9307, 5.3230, 5.3525],
        [4.9307, 5.5854, 5.8101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.15092483162879944 
model_pd.l_d.mean(): -20.478107452392578 
model_pd.lagr.mean(): -20.32718276977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.15092483162879944
epoch£º181	 i:1 	 global-step:3621	 l-p:0.1997084766626358
epoch£º181	 i:2 	 global-step:3622	 l-p:0.12427333742380142
epoch£º181	 i:3 	 global-step:3623	 l-p:0.12848824262619019
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13530632853507996
epoch£º181	 i:5 	 global-step:3625	 l-p:0.10631797462701797
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13396626710891724
epoch£º181	 i:7 	 global-step:3627	 l-p:0.12668347358703613
epoch£º181	 i:8 	 global-step:3628	 l-p:0.17239564657211304
epoch£º181	 i:9 	 global-step:3629	 l-p:0.19612574577331543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9397, 4.9477, 4.9406],
        [4.9397, 4.9893, 4.9559],
        [4.9397, 5.0616, 5.0083],
        [4.9397, 4.9397, 4.9397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12426803261041641 
model_pd.l_d.mean(): -20.257476806640625 
model_pd.lagr.mean(): -20.133209228515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.9271], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12426803261041641
epoch£º182	 i:1 	 global-step:3641	 l-p:0.14273275434970856
epoch£º182	 i:2 	 global-step:3642	 l-p:0.12838952243328094
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12142157554626465
epoch£º182	 i:4 	 global-step:3644	 l-p:0.21484936773777008
epoch£º182	 i:5 	 global-step:3645	 l-p:0.0847955271601677
epoch£º182	 i:6 	 global-step:3646	 l-p:0.18100211024284363
epoch£º182	 i:7 	 global-step:3647	 l-p:0.20537039637565613
epoch£º182	 i:8 	 global-step:3648	 l-p:0.14274351298809052
epoch£º182	 i:9 	 global-step:3649	 l-p:0.11360768973827362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0608, 5.3787, 5.3633],
        [5.0608, 5.2013, 5.1454],
        [5.0608, 5.0690, 5.0618],
        [5.0608, 5.7898, 6.0674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12625838816165924 
model_pd.l_d.mean(): -20.34090232849121 
model_pd.lagr.mean(): -20.214643478393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-20.9890], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12625838816165924
epoch£º183	 i:1 	 global-step:3661	 l-p:0.1262773871421814
epoch£º183	 i:2 	 global-step:3662	 l-p:0.13033930957317352
epoch£º183	 i:3 	 global-step:3663	 l-p:0.1254248321056366
epoch£º183	 i:4 	 global-step:3664	 l-p:0.19432960450649261
epoch£º183	 i:5 	 global-step:3665	 l-p:0.41489332914352417
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14069297909736633
epoch£º183	 i:7 	 global-step:3667	 l-p:-1.4833486080169678
epoch£º183	 i:8 	 global-step:3668	 l-p:0.12895740568637848
epoch£º183	 i:9 	 global-step:3669	 l-p:0.12877023220062256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9053, 4.9743, 4.9328],
        [4.9053, 5.6722, 6.0029],
        [4.9053, 4.9373, 4.9133],
        [4.9053, 4.9053, 4.9053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.1409459412097931 
model_pd.l_d.mean(): -19.021957397460938 
model_pd.lagr.mean(): -18.881011962890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5307], device='cuda:0')), ('power', tensor([-19.7719], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.1409459412097931
epoch£º184	 i:1 	 global-step:3681	 l-p:0.16310305893421173
epoch£º184	 i:2 	 global-step:3682	 l-p:0.1296585649251938
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12198346853256226
epoch£º184	 i:4 	 global-step:3684	 l-p:0.1301991492509842
epoch£º184	 i:5 	 global-step:3685	 l-p:0.1005682423710823
epoch£º184	 i:6 	 global-step:3686	 l-p:0.1361878663301468
epoch£º184	 i:7 	 global-step:3687	 l-p:0.1213773712515831
epoch£º184	 i:8 	 global-step:3688	 l-p:0.1144363284111023
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09776870161294937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9300, 5.0471, 4.9944],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9445, 4.9324],
        [4.9300, 5.6585, 5.9507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12937955558300018 
model_pd.l_d.mean(): -19.37190818786621 
model_pd.lagr.mean(): -19.242528915405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.1168], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12937955558300018
epoch£º185	 i:1 	 global-step:3701	 l-p:0.23997457325458527
epoch£º185	 i:2 	 global-step:3702	 l-p:0.13698847591876984
epoch£º185	 i:3 	 global-step:3703	 l-p:0.14871154725551605
epoch£º185	 i:4 	 global-step:3704	 l-p:0.14519670605659485
epoch£º185	 i:5 	 global-step:3705	 l-p:0.014165286906063557
epoch£º185	 i:6 	 global-step:3706	 l-p:0.1442718505859375
epoch£º185	 i:7 	 global-step:3707	 l-p:0.16219966113567352
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11549375206232071
epoch£º185	 i:9 	 global-step:3709	 l-p:-2.4760243892669678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8943, 4.8943, 4.8943],
        [4.8943, 5.2418, 5.2494],
        [4.8943, 5.1997, 5.1866],
        [4.8943, 4.9970, 4.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1323605179786682 
model_pd.l_d.mean(): -20.221113204956055 
model_pd.lagr.mean(): -20.08875274658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.9387], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.1323605179786682
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13774167001247406
epoch£º186	 i:2 	 global-step:3722	 l-p:0.19764307141304016
epoch£º186	 i:3 	 global-step:3723	 l-p:0.18740786612033844
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10521461069583893
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11362509429454803
epoch£º186	 i:6 	 global-step:3726	 l-p:0.13410884141921997
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13956256210803986
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12081103771924973
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11620941013097763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1181, 5.1181],
        [5.1181, 5.2981, 5.2423],
        [5.1181, 5.4358, 5.4184],
        [5.1181, 5.1181, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.19263936579227448 
model_pd.l_d.mean(): -19.39535903930664 
model_pd.lagr.mean(): -19.20271873474121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.0695], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.19263936579227448
epoch£º187	 i:1 	 global-step:3741	 l-p:0.14034852385520935
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11663980036973953
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13576145470142365
epoch£º187	 i:4 	 global-step:3744	 l-p:0.13870888948440552
epoch£º187	 i:5 	 global-step:3745	 l-p:0.12621374428272247
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05964856967329979
epoch£º187	 i:7 	 global-step:3747	 l-p:0.09832178056240082
epoch£º187	 i:8 	 global-step:3748	 l-p:0.10230270028114319
epoch£º187	 i:9 	 global-step:3749	 l-p:0.14388780295848846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6942, 5.3895, 5.6812],
        [4.6942, 4.7562, 4.7171],
        [4.6942, 4.6942, 4.6942],
        [4.6942, 5.3784, 5.6598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.14990803599357605 
model_pd.l_d.mean(): -20.689380645751953 
model_pd.lagr.mean(): -20.539472579956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-21.4257], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.14990803599357605
epoch£º188	 i:1 	 global-step:3761	 l-p:0.13251429796218872
epoch£º188	 i:2 	 global-step:3762	 l-p:0.12465716153383255
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14234909415245056
epoch£º188	 i:4 	 global-step:3764	 l-p:0.08901586383581161
epoch£º188	 i:5 	 global-step:3765	 l-p:0.184434175491333
epoch£º188	 i:6 	 global-step:3766	 l-p:0.16651365160942078
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10966464132070541
epoch£º188	 i:8 	 global-step:3768	 l-p:0.17314910888671875
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11730661243200302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0383, 6.0355, 6.5817],
        [5.0383, 6.0516, 6.6148],
        [5.0383, 5.0383, 5.0383],
        [5.0383, 5.0383, 5.0383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10281127691268921 
model_pd.l_d.mean(): -19.67401885986328 
model_pd.lagr.mean(): -19.57120704650879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.3667], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10281127691268921
epoch£º189	 i:1 	 global-step:3781	 l-p:0.21021537482738495
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11469388753175735
epoch£º189	 i:3 	 global-step:3783	 l-p:0.12662677466869354
epoch£º189	 i:4 	 global-step:3784	 l-p:0.1222328320145607
epoch£º189	 i:5 	 global-step:3785	 l-p:0.10520993918180466
epoch£º189	 i:6 	 global-step:3786	 l-p:0.12324406206607819
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11035798490047455
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11788429319858551
epoch£º189	 i:9 	 global-step:3789	 l-p:0.13425403833389282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0345, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.13342036306858063 
model_pd.l_d.mean(): -20.220495223999023 
model_pd.lagr.mean(): -20.087074279785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.13342036306858063
epoch£º190	 i:1 	 global-step:3801	 l-p:0.13924382627010345
epoch£º190	 i:2 	 global-step:3802	 l-p:0.1330270916223526
epoch£º190	 i:3 	 global-step:3803	 l-p:0.07740665227174759
epoch£º190	 i:4 	 global-step:3804	 l-p:0.24378085136413574
epoch£º190	 i:5 	 global-step:3805	 l-p:0.1390099674463272
epoch£º190	 i:6 	 global-step:3806	 l-p:0.2035360038280487
epoch£º190	 i:7 	 global-step:3807	 l-p:0.1521420031785965
epoch£º190	 i:8 	 global-step:3808	 l-p:0.15095637738704681
epoch£º190	 i:9 	 global-step:3809	 l-p:0.1582360863685608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6779, 5.5109, 5.9397],
        [4.6779, 4.7165, 4.6879],
        [4.6779, 4.6779, 4.6779],
        [4.6779, 4.6779, 4.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1652076691389084 
model_pd.l_d.mean(): -18.843477249145508 
model_pd.lagr.mean(): -18.67827033996582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6520], device='cuda:0')), ('power', tensor([-19.7154], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.1652076691389084
epoch£º191	 i:1 	 global-step:3821	 l-p:0.20706914365291595
epoch£º191	 i:2 	 global-step:3822	 l-p:0.021380241960287094
epoch£º191	 i:3 	 global-step:3823	 l-p:0.13560552895069122
epoch£º191	 i:4 	 global-step:3824	 l-p:0.16084976494312286
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10943154245615005
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12196225672960281
epoch£º191	 i:7 	 global-step:3827	 l-p:0.1422569900751114
epoch£º191	 i:8 	 global-step:3828	 l-p:0.15518265962600708
epoch£º191	 i:9 	 global-step:3829	 l-p:0.12967228889465332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9334, 4.9493, 4.9359],
        [4.9334, 4.9430, 4.9346],
        [4.9334, 5.7662, 6.1599],
        [4.9334, 4.9561, 4.9378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19202683866024017 
model_pd.l_d.mean(): -20.20292091369629 
model_pd.lagr.mean(): -20.010894775390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9085], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19202683866024017
epoch£º192	 i:1 	 global-step:3841	 l-p:0.14703048765659332
epoch£º192	 i:2 	 global-step:3842	 l-p:0.4417523145675659
epoch£º192	 i:3 	 global-step:3843	 l-p:0.12885189056396484
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5393063426017761
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13068172335624695
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14514081180095673
epoch£º192	 i:7 	 global-step:3847	 l-p:0.1557137370109558
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12739820778369904
epoch£º192	 i:9 	 global-step:3849	 l-p:0.13173949718475342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9371, 5.7055, 6.0370],
        [4.9371, 4.9514, 4.9392],
        [4.9371, 4.9388, 4.9372],
        [4.9371, 4.9372, 4.9371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.136252760887146 
model_pd.l_d.mean(): -20.545209884643555 
model_pd.lagr.mean(): -20.40895652770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.136252760887146
epoch£º193	 i:1 	 global-step:3861	 l-p:0.13793151080608368
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05907156318426132
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.08938592672348022
epoch£º193	 i:4 	 global-step:3864	 l-p:0.132924884557724
epoch£º193	 i:5 	 global-step:3865	 l-p:0.14665207266807556
epoch£º193	 i:6 	 global-step:3866	 l-p:0.13393472135066986
epoch£º193	 i:7 	 global-step:3867	 l-p:0.10342172533273697
epoch£º193	 i:8 	 global-step:3868	 l-p:0.15017607808113098
epoch£º193	 i:9 	 global-step:3869	 l-p:0.1121799647808075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7177, 4.7177, 4.7177],
        [4.7177, 5.1899, 5.2912],
        [4.7177, 4.8766, 4.8255],
        [4.7177, 4.7416, 4.7220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.09613670408725739 
model_pd.l_d.mean(): -20.923107147216797 
model_pd.lagr.mean(): -20.82697105407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.6310], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.09613670408725739
epoch£º194	 i:1 	 global-step:3881	 l-p:0.1433710902929306
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.01775800622999668
epoch£º194	 i:3 	 global-step:3883	 l-p:0.133452370762825
epoch£º194	 i:4 	 global-step:3884	 l-p:0.1358608454465866
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11681064963340759
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12126418948173523
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1419041007757187
epoch£º194	 i:8 	 global-step:3888	 l-p:0.13041618466377258
epoch£º194	 i:9 	 global-step:3889	 l-p:1.3035950660705566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1023, 5.1715, 5.1289],
        [5.1023, 5.6085, 5.7044],
        [5.1023, 5.1047, 5.1025],
        [5.1023, 5.5111, 5.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11473313719034195 
model_pd.l_d.mean(): -20.305158615112305 
model_pd.lagr.mean(): -20.190425872802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4129], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11473313719034195
epoch£º195	 i:1 	 global-step:3901	 l-p:0.11911598592996597
epoch£º195	 i:2 	 global-step:3902	 l-p:0.12007228285074234
epoch£º195	 i:3 	 global-step:3903	 l-p:0.12040776759386063
epoch£º195	 i:4 	 global-step:3904	 l-p:0.16078369319438934
epoch£º195	 i:5 	 global-step:3905	 l-p:0.1342783272266388
epoch£º195	 i:6 	 global-step:3906	 l-p:0.14203350245952606
epoch£º195	 i:7 	 global-step:3907	 l-p:0.22434990108013153
epoch£º195	 i:8 	 global-step:3908	 l-p:0.1144995465874672
epoch£º195	 i:9 	 global-step:3909	 l-p:0.10724256187677383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7941, 4.7941, 4.7941],
        [4.7941, 4.7984, 4.7944],
        [4.7941, 4.7943, 4.7941],
        [4.7941, 4.7972, 4.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.1374271810054779 
model_pd.l_d.mean(): -19.624454498291016 
model_pd.lagr.mean(): -19.487028121948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5740], device='cuda:0')), ('power', tensor([-20.4252], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.1374271810054779
epoch£º196	 i:1 	 global-step:3921	 l-p:0.098869189620018
epoch£º196	 i:2 	 global-step:3922	 l-p:0.08999872952699661
epoch£º196	 i:3 	 global-step:3923	 l-p:0.09527857601642609
epoch£º196	 i:4 	 global-step:3924	 l-p:0.13066275417804718
epoch£º196	 i:5 	 global-step:3925	 l-p:0.1557721048593521
epoch£º196	 i:6 	 global-step:3926	 l-p:0.12409152090549469
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12654946744441986
epoch£º196	 i:8 	 global-step:3928	 l-p:0.1358465701341629
epoch£º196	 i:9 	 global-step:3929	 l-p:0.12398520112037659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9352, 4.9355, 4.9352],
        [4.9352, 5.3408, 5.3816],
        [4.9352, 4.9352, 4.9352],
        [4.9352, 4.9481, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.18533872067928314 
model_pd.l_d.mean(): -19.068614959716797 
model_pd.lagr.mean(): -18.883275985717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-19.8258], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.18533872067928314
epoch£º197	 i:1 	 global-step:3941	 l-p:0.19377467036247253
epoch£º197	 i:2 	 global-step:3942	 l-p:0.15182214975357056
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11366229504346848
epoch£º197	 i:4 	 global-step:3944	 l-p:0.0008105849847197533
epoch£º197	 i:5 	 global-step:3945	 l-p:0.1342543214559555
epoch£º197	 i:6 	 global-step:3946	 l-p:0.1288842409849167
epoch£º197	 i:7 	 global-step:3947	 l-p:0.133231058716774
epoch£º197	 i:8 	 global-step:3948	 l-p:0.13210125267505646
epoch£º197	 i:9 	 global-step:3949	 l-p:0.1207284927368164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9713, 4.9908, 4.9746],
        [4.9713, 4.9714, 4.9713],
        [4.9713, 4.9713, 4.9713],
        [4.9713, 4.9721, 4.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12789763510227203 
model_pd.l_d.mean(): -20.525915145874023 
model_pd.lagr.mean(): -20.39801788330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.1876], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.12789763510227203
epoch£º198	 i:1 	 global-step:3961	 l-p:0.13781246542930603
epoch£º198	 i:2 	 global-step:3962	 l-p:0.14969584345817566
epoch£º198	 i:3 	 global-step:3963	 l-p:0.12293335050344467
epoch£º198	 i:4 	 global-step:3964	 l-p:0.10889233648777008
epoch£º198	 i:5 	 global-step:3965	 l-p:0.127914696931839
epoch£º198	 i:6 	 global-step:3966	 l-p:0.14037944376468658
epoch£º198	 i:7 	 global-step:3967	 l-p:0.2756952941417694
epoch£º198	 i:8 	 global-step:3968	 l-p:0.14134447276592255
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12410267442464828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9915, 4.9923, 4.9916],
        [4.9915, 5.0016, 4.9927],
        [4.9915, 4.9916, 4.9915],
        [4.9915, 5.1046, 5.0507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.16566681861877441 
model_pd.l_d.mean(): -19.046707153320312 
model_pd.lagr.mean(): -18.881040573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-19.7598], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.16566681861877441
epoch£º199	 i:1 	 global-step:3981	 l-p:0.11751819401979446
epoch£º199	 i:2 	 global-step:3982	 l-p:0.12080562114715576
epoch£º199	 i:3 	 global-step:3983	 l-p:0.1270836442708969
epoch£º199	 i:4 	 global-step:3984	 l-p:0.14026117324829102
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1737322211265564
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11406872421503067
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11667422950267792
epoch£º199	 i:8 	 global-step:3988	 l-p:0.1126544252038002
epoch£º199	 i:9 	 global-step:3989	 l-p:0.15627798438072205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0232, 5.0232, 5.0232],
        [5.0232, 5.0235, 5.0232],
        [5.0232, 5.0349, 5.0246],
        [5.0232, 5.3584, 5.3553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.1444816142320633 
model_pd.l_d.mean(): -20.52617645263672 
model_pd.lagr.mean(): -20.381694793701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4162], device='cuda:0')), ('power', tensor([-21.1756], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.1444816142320633
epoch£º200	 i:1 	 global-step:4001	 l-p:0.13946811854839325
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12589088082313538
epoch£º200	 i:3 	 global-step:4003	 l-p:0.19322837889194489
epoch£º200	 i:4 	 global-step:4004	 l-p:0.1326315999031067
epoch£º200	 i:5 	 global-step:4005	 l-p:0.24507567286491394
epoch£º200	 i:6 	 global-step:4006	 l-p:0.15810808539390564
epoch£º200	 i:7 	 global-step:4007	 l-p:0.1505333036184311
epoch£º200	 i:8 	 global-step:4008	 l-p:0.11471408605575562
epoch£º200	 i:9 	 global-step:4009	 l-p:0.7184508442878723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9198, 4.9198, 4.9198],
        [4.9198, 5.3880, 5.4722],
        [4.9198, 4.9198, 4.9198],
        [4.9198, 5.0090, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.36116451025009155 
model_pd.l_d.mean(): -18.556961059570312 
model_pd.lagr.mean(): -18.195796966552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5605], device='cuda:0')), ('power', tensor([-19.3323], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.36116451025009155
epoch£º201	 i:1 	 global-step:4021	 l-p:0.10966382920742035
epoch£º201	 i:2 	 global-step:4022	 l-p:0.08152461051940918
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13063693046569824
epoch£º201	 i:4 	 global-step:4024	 l-p:0.14911183714866638
epoch£º201	 i:5 	 global-step:4025	 l-p:0.1345100700855255
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12257126718759537
epoch£º201	 i:7 	 global-step:4027	 l-p:0.13519340753555298
epoch£º201	 i:8 	 global-step:4028	 l-p:0.12194161117076874
epoch£º201	 i:9 	 global-step:4029	 l-p:0.14951153099536896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0116, 5.0118, 5.0116],
        [5.0116, 5.0117, 5.0116],
        [5.0116, 5.0999, 5.0506],
        [5.0116, 5.0241, 5.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.1333932876586914 
model_pd.l_d.mean(): -20.06035614013672 
model_pd.lagr.mean(): -19.926963806152344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.7405], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.1333932876586914
epoch£º202	 i:1 	 global-step:4041	 l-p:0.14933185279369354
epoch£º202	 i:2 	 global-step:4042	 l-p:0.13128115236759186
epoch£º202	 i:3 	 global-step:4043	 l-p:0.150267094373703
epoch£º202	 i:4 	 global-step:4044	 l-p:0.19725219905376434
epoch£º202	 i:5 	 global-step:4045	 l-p:0.1459570825099945
epoch£º202	 i:6 	 global-step:4046	 l-p:0.13243140280246735
epoch£º202	 i:7 	 global-step:4047	 l-p:0.1337037831544876
epoch£º202	 i:8 	 global-step:4048	 l-p:0.0385635644197464
epoch£º202	 i:9 	 global-step:4049	 l-p:0.2104753851890564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.3476, 5.4806],
        [4.8243, 4.9077, 4.8599],
        [4.8243, 4.8753, 4.8393],
        [4.8243, 4.8244, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.0695255920290947 
model_pd.l_d.mean(): -18.929147720336914 
model_pd.lagr.mean(): -18.859622955322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-19.6922], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.0695255920290947
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10468463599681854
epoch£º203	 i:2 	 global-step:4062	 l-p:0.10909398645162582
epoch£º203	 i:3 	 global-step:4063	 l-p:0.13938407599925995
epoch£º203	 i:4 	 global-step:4064	 l-p:-0.014071797952055931
epoch£º203	 i:5 	 global-step:4065	 l-p:0.187998965382576
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13480104506015778
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13843999803066254
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12696512043476105
epoch£º203	 i:9 	 global-step:4069	 l-p:0.13477723300457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9546, 5.4648, 5.5770],
        [4.9546, 4.9546, 4.9546],
        [4.9546, 5.4553, 5.5605],
        [4.9546, 5.0616, 5.0084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.12281705439090729 
model_pd.l_d.mean(): -20.37240982055664 
model_pd.lagr.mean(): -20.249591827392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0676], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.12281705439090729
epoch£º204	 i:1 	 global-step:4081	 l-p:0.15242266654968262
epoch£º204	 i:2 	 global-step:4082	 l-p:0.30758044123649597
epoch£º204	 i:3 	 global-step:4083	 l-p:0.31291332840919495
epoch£º204	 i:4 	 global-step:4084	 l-p:0.14396913349628448
epoch£º204	 i:5 	 global-step:4085	 l-p:0.1036086231470108
epoch£º204	 i:6 	 global-step:4086	 l-p:0.1427825540304184
epoch£º204	 i:7 	 global-step:4087	 l-p:0.1495537906885147
epoch£º204	 i:8 	 global-step:4088	 l-p:0.14276456832885742
epoch£º204	 i:9 	 global-step:4089	 l-p:0.13359703123569489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9503, 5.0170, 4.9744],
        [4.9503, 4.9503, 4.9503],
        [4.9503, 4.9504, 4.9503],
        [4.9503, 4.9502, 4.9503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.18842318654060364 
model_pd.l_d.mean(): -19.691709518432617 
model_pd.lagr.mean(): -19.503286361694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-20.4236], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.18842318654060364
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13660219311714172
epoch£º205	 i:2 	 global-step:4102	 l-p:0.12505237758159637
epoch£º205	 i:3 	 global-step:4103	 l-p:0.15809878706932068
epoch£º205	 i:4 	 global-step:4104	 l-p:0.15348687767982483
epoch£º205	 i:5 	 global-step:4105	 l-p:0.07311389595270157
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14254175126552582
epoch£º205	 i:7 	 global-step:4107	 l-p:0.17050471901893616
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11225280165672302
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12313812226057053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 5.0670, 5.0377],
        [5.0280, 5.9752, 6.4747],
        [5.0280, 5.5954, 5.7456],
        [5.0280, 5.0280, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13938264548778534 
model_pd.l_d.mean(): -20.223575592041016 
model_pd.lagr.mean(): -20.084192276000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-20.9178], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13938264548778534
epoch£º206	 i:1 	 global-step:4121	 l-p:0.10292661190032959
epoch£º206	 i:2 	 global-step:4122	 l-p:0.121990866959095
epoch£º206	 i:3 	 global-step:4123	 l-p:0.15110355615615845
epoch£º206	 i:4 	 global-step:4124	 l-p:0.11495425552129745
epoch£º206	 i:5 	 global-step:4125	 l-p:0.1677977293729782
epoch£º206	 i:6 	 global-step:4126	 l-p:0.13609254360198975
epoch£º206	 i:7 	 global-step:4127	 l-p:0.11340614408254623
epoch£º206	 i:8 	 global-step:4128	 l-p:0.17041076719760895
epoch£º206	 i:9 	 global-step:4129	 l-p:0.12894950807094574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9937, 4.9758],
        [4.9725, 4.9725, 4.9725],
        [4.9725, 5.2571, 5.2334],
        [4.9725, 5.1429, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.1319868266582489 
model_pd.l_d.mean(): -19.8414306640625 
model_pd.lagr.mean(): -19.709444046020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5270], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.1319868266582489
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12597118318080902
epoch£º207	 i:2 	 global-step:4142	 l-p:0.2022983878850937
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16058866679668427
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16470664739608765
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1359075903892517
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12252843379974365
epoch£º207	 i:7 	 global-step:4147	 l-p:0.574617862701416
epoch£º207	 i:8 	 global-step:4148	 l-p:0.12738673388957977
epoch£º207	 i:9 	 global-step:4149	 l-p:0.13486701250076294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.8915, 4.8914],
        [4.8914, 5.3771, 5.4766],
        [4.8914, 5.0204, 4.9652],
        [4.8914, 4.8914, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.1326298862695694 
model_pd.l_d.mean(): -20.8674259185791 
model_pd.lagr.mean(): -20.73479652404785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.5234], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.1326298862695694
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.1531858593225479
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12531201541423798
epoch£º208	 i:3 	 global-step:4163	 l-p:0.1290951818227768
epoch£º208	 i:4 	 global-step:4164	 l-p:0.1365460604429245
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14292168617248535
epoch£º208	 i:6 	 global-step:4166	 l-p:0.1874127984046936
epoch£º208	 i:7 	 global-step:4167	 l-p:0.09756166487932205
epoch£º208	 i:8 	 global-step:4168	 l-p:0.21033141016960144
epoch£º208	 i:9 	 global-step:4169	 l-p:0.15457583963871002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8772, 5.2160, 5.2225],
        [4.8772, 5.8037, 6.3035],
        [4.8772, 4.9276, 4.8915],
        [4.8772, 5.1284, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.08282289654016495 
model_pd.l_d.mean(): -20.242412567138672 
model_pd.lagr.mean(): -20.32523536682129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.9817], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.08282289654016495
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13659356534481049
epoch£º209	 i:2 	 global-step:4182	 l-p:0.12859106063842773
epoch£º209	 i:3 	 global-step:4183	 l-p:0.14129552245140076
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1428033858537674
epoch£º209	 i:5 	 global-step:4185	 l-p:0.10170566290616989
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13482913374900818
epoch£º209	 i:7 	 global-step:4187	 l-p:0.2004922777414322
epoch£º209	 i:8 	 global-step:4188	 l-p:0.15681444108486176
epoch£º209	 i:9 	 global-step:4189	 l-p:0.12428855895996094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0026, 5.0026, 5.0026],
        [5.0026, 5.0026, 5.0026],
        [5.0026, 5.1573, 5.1009],
        [5.0026, 5.0026, 5.0026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.17078813910484314 
model_pd.l_d.mean(): -20.625289916992188 
model_pd.lagr.mean(): -20.45450210571289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.2785], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.17078813910484314
epoch£º210	 i:1 	 global-step:4201	 l-p:0.13863717019557953
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14798417687416077
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12522023916244507
epoch£º210	 i:4 	 global-step:4204	 l-p:0.13508866727352142
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12279637902975082
epoch£º210	 i:6 	 global-step:4206	 l-p:0.11828740686178207
epoch£º210	 i:7 	 global-step:4207	 l-p:0.12348642945289612
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.3294040858745575
epoch£º210	 i:9 	 global-step:4209	 l-p:0.126250222325325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0662, 5.4719, 5.5063],
        [5.0662, 5.0662, 5.0662],
        [5.0662, 5.2085, 5.1511],
        [5.0662, 5.0662, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.13839441537857056 
model_pd.l_d.mean(): -19.563989639282227 
model_pd.lagr.mean(): -19.425594329833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4470], device='cuda:0')), ('power', tensor([-20.2343], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.13839441537857056
epoch£º211	 i:1 	 global-step:4221	 l-p:0.1270965039730072
epoch£º211	 i:2 	 global-step:4222	 l-p:0.15052713453769684
epoch£º211	 i:3 	 global-step:4223	 l-p:0.1718132048845291
epoch£º211	 i:4 	 global-step:4224	 l-p:0.12507739663124084
epoch£º211	 i:5 	 global-step:4225	 l-p:0.15359805524349213
epoch£º211	 i:6 	 global-step:4226	 l-p:0.1060926616191864
epoch£º211	 i:7 	 global-step:4227	 l-p:0.11786523461341858
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12420202046632767
epoch£º211	 i:9 	 global-step:4229	 l-p:0.1325763463973999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9751, 5.4480, 5.5325],
        [4.9751, 4.9752, 4.9751],
        [4.9751, 4.9751, 4.9751],
        [4.9751, 5.4175, 5.4810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1375970095396042 
model_pd.l_d.mean(): -20.943195343017578 
model_pd.lagr.mean(): -20.805599212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3865], device='cuda:0')), ('power', tensor([-21.5668], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1375970095396042
epoch£º212	 i:1 	 global-step:4241	 l-p:0.11819646507501602
epoch£º212	 i:2 	 global-step:4242	 l-p:0.14406277239322662
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12057139724493027
epoch£º212	 i:4 	 global-step:4244	 l-p:0.24618405103683472
epoch£º212	 i:5 	 global-step:4245	 l-p:0.150636687874794
epoch£º212	 i:6 	 global-step:4246	 l-p:0.11993896216154099
epoch£º212	 i:7 	 global-step:4247	 l-p:0.18512950837612152
epoch£º212	 i:8 	 global-step:4248	 l-p:-1.002088189125061
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10819800943136215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7771, 4.7775, 4.7771],
        [4.7771, 4.7771, 4.7771],
        [4.7771, 4.9229, 4.8684],
        [4.7771, 4.7839, 4.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.1223149374127388 
model_pd.l_d.mean(): -19.526840209960938 
model_pd.lagr.mean(): -19.404525756835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5788], device='cuda:0')), ('power', tensor([-20.3314], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.1223149374127388
epoch£º213	 i:1 	 global-step:4261	 l-p:0.13258716464042664
epoch£º213	 i:2 	 global-step:4262	 l-p:0.13917942345142365
epoch£º213	 i:3 	 global-step:4263	 l-p:0.1341514140367508
epoch£º213	 i:4 	 global-step:4264	 l-p:0.02058812417089939
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14378251135349274
epoch£º213	 i:6 	 global-step:4266	 l-p:0.13463516533374786
epoch£º213	 i:7 	 global-step:4267	 l-p:0.11960296332836151
epoch£º213	 i:8 	 global-step:4268	 l-p:0.4489443898200989
epoch£º213	 i:9 	 global-step:4269	 l-p:0.0965387299656868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8271, 4.8272, 4.8271],
        [4.8271, 4.8527, 4.8307],
        [4.8271, 4.9692, 4.9139],
        [4.8271, 4.8271, 4.8271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.14025670289993286 
model_pd.l_d.mean(): -20.610736846923828 
model_pd.lagr.mean(): -20.47047996520996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-21.3137], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.14025670289993286
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12440107762813568
epoch£º214	 i:2 	 global-step:4282	 l-p:0.09175214916467667
epoch£º214	 i:3 	 global-step:4283	 l-p:0.186186283826828
epoch£º214	 i:4 	 global-step:4284	 l-p:0.13782799243927002
epoch£º214	 i:5 	 global-step:4285	 l-p:0.3250751495361328
epoch£º214	 i:6 	 global-step:4286	 l-p:0.17263665795326233
epoch£º214	 i:7 	 global-step:4287	 l-p:0.4614766538143158
epoch£º214	 i:8 	 global-step:4288	 l-p:0.1424938440322876
epoch£º214	 i:9 	 global-step:4289	 l-p:0.1854369193315506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9984, 5.1567, 5.1002],
        [4.9984, 4.9986, 4.9984],
        [4.9984, 5.0103, 4.9995],
        [4.9984, 4.9984, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.15980635583400726 
model_pd.l_d.mean(): -20.52351188659668 
model_pd.lagr.mean(): -20.363704681396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.1926], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.15980635583400726
epoch£º215	 i:1 	 global-step:4301	 l-p:0.12998102605342865
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14428989589214325
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11818587779998779
epoch£º215	 i:4 	 global-step:4304	 l-p:4.198372840881348
epoch£º215	 i:5 	 global-step:4305	 l-p:0.1255015730857849
epoch£º215	 i:6 	 global-step:4306	 l-p:0.13928644359111786
epoch£º215	 i:7 	 global-step:4307	 l-p:0.11865352094173431
epoch£º215	 i:8 	 global-step:4308	 l-p:0.12832893431186676
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11150715500116348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0822, 5.0830, 5.0822],
        [5.0822, 5.4354, 5.4393],
        [5.0822, 5.0987, 5.0843],
        [5.0822, 5.0822, 5.0822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11852449178695679 
model_pd.l_d.mean(): -19.11777114868164 
model_pd.lagr.mean(): -18.99924659729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-19.7967], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11852449178695679
epoch£º216	 i:1 	 global-step:4321	 l-p:0.1184166967868805
epoch£º216	 i:2 	 global-step:4322	 l-p:0.15726186335086823
epoch£º216	 i:3 	 global-step:4323	 l-p:0.150069922208786
epoch£º216	 i:4 	 global-step:4324	 l-p:0.15143783390522003
epoch£º216	 i:5 	 global-step:4325	 l-p:0.1887783706188202
epoch£º216	 i:6 	 global-step:4326	 l-p:0.08566505461931229
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12580369412899017
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12295937538146973
epoch£º216	 i:9 	 global-step:4329	 l-p:0.15654714405536652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9541, 4.9626, 4.9545],
        [4.9541, 5.3954, 5.4601],
        [4.9541, 5.1410, 5.0883],
        [4.9541, 4.9541, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.13429275155067444 
model_pd.l_d.mean(): -20.62839126586914 
model_pd.lagr.mean(): -20.494098663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.2842], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.13429275155067444
epoch£º217	 i:1 	 global-step:4341	 l-p:0.10389852523803711
epoch£º217	 i:2 	 global-step:4342	 l-p:0.24805009365081787
epoch£º217	 i:3 	 global-step:4343	 l-p:0.15983635187149048
epoch£º217	 i:4 	 global-step:4344	 l-p:0.17266637086868286
epoch£º217	 i:5 	 global-step:4345	 l-p:0.12978795170783997
epoch£º217	 i:6 	 global-step:4346	 l-p:0.13334868848323822
epoch£º217	 i:7 	 global-step:4347	 l-p:0.287324458360672
epoch£º217	 i:8 	 global-step:4348	 l-p:0.2138374298810959
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13903777301311493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9595, 4.9619, 4.9595],
        [4.9595, 4.9595, 4.9595],
        [4.9595, 4.9596, 4.9595],
        [4.9595, 4.9595, 4.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12264924496412277 
model_pd.l_d.mean(): -20.660490036010742 
model_pd.lagr.mean(): -20.537839889526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.3169], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12264924496412277
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13353928923606873
epoch£º218	 i:2 	 global-step:4362	 l-p:0.13659650087356567
epoch£º218	 i:3 	 global-step:4363	 l-p:0.17740033566951752
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12672999501228333
epoch£º218	 i:5 	 global-step:4365	 l-p:0.25689926743507385
epoch£º218	 i:6 	 global-step:4366	 l-p:0.6217960119247437
epoch£º218	 i:7 	 global-step:4367	 l-p:0.12597456574440002
epoch£º218	 i:8 	 global-step:4368	 l-p:0.1969660520553589
epoch£º218	 i:9 	 global-step:4369	 l-p:0.08986929059028625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0179, 5.0223, 5.0181],
        [5.0179, 5.0281, 5.0186],
        [5.0179, 5.1762, 5.1192],
        [5.0179, 6.0427, 6.6261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.119644895195961 
model_pd.l_d.mean(): -18.85625648498535 
model_pd.lagr.mean(): -18.73661231994629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-19.5765], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.119644895195961
epoch£º219	 i:1 	 global-step:4381	 l-p:0.042839135974645615
epoch£º219	 i:2 	 global-step:4382	 l-p:0.1376485973596573
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11594980210065842
epoch£º219	 i:4 	 global-step:4384	 l-p:0.15602195262908936
epoch£º219	 i:5 	 global-step:4385	 l-p:0.11611153930425644
epoch£º219	 i:6 	 global-step:4386	 l-p:0.14327876269817352
epoch£º219	 i:7 	 global-step:4387	 l-p:0.12767721712589264
epoch£º219	 i:8 	 global-step:4388	 l-p:0.11719165742397308
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13013790547847748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0530, 5.0532, 5.0530],
        [5.0530, 5.1161, 5.0739],
        [5.0530, 5.0530, 5.0530],
        [5.0530, 5.0650, 5.0540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.14565736055374146 
model_pd.l_d.mean(): -20.346193313598633 
model_pd.lagr.mean(): -20.200536727905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.0001], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.14565736055374146
epoch£º220	 i:1 	 global-step:4401	 l-p:0.12677864730358124
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10777498781681061
epoch£º220	 i:3 	 global-step:4403	 l-p:0.1395167112350464
epoch£º220	 i:4 	 global-step:4404	 l-p:0.14900276064872742
epoch£º220	 i:5 	 global-step:4405	 l-p:0.1521829217672348
epoch£º220	 i:6 	 global-step:4406	 l-p:0.05570348724722862
epoch£º220	 i:7 	 global-step:4407	 l-p:0.14175046980381012
epoch£º220	 i:8 	 global-step:4408	 l-p:0.15823209285736084
epoch£º220	 i:9 	 global-step:4409	 l-p:0.1670175939798355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7709, 5.1911, 5.2546],
        [4.7709, 5.0290, 5.0021],
        [4.7709, 4.8659, 4.8135],
        [4.7709, 4.7709, 4.7709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.16124564409255981 
model_pd.l_d.mean(): -20.192264556884766 
model_pd.lagr.mean(): -20.03101921081543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5210], device='cuda:0')), ('power', tensor([-20.9451], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.16124564409255981
epoch£º221	 i:1 	 global-step:4421	 l-p:0.1181596890091896
epoch£º221	 i:2 	 global-step:4422	 l-p:0.1422463357448578
epoch£º221	 i:3 	 global-step:4423	 l-p:0.1549845039844513
epoch£º221	 i:4 	 global-step:4424	 l-p:0.07717098295688629
epoch£º221	 i:5 	 global-step:4425	 l-p:0.06181303784251213
epoch£º221	 i:6 	 global-step:4426	 l-p:0.12762536108493805
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13835738599300385
epoch£º221	 i:8 	 global-step:4428	 l-p:0.17249545454978943
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13363121449947357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8986, 5.8886, 6.4529],
        [4.8986, 5.4194, 5.5461],
        [4.8986, 5.8448, 6.3632],
        [4.8986, 5.3769, 5.4718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.11896386742591858 
model_pd.l_d.mean(): -19.598281860351562 
model_pd.lagr.mean(): -19.479318618774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5572], device='cuda:0')), ('power', tensor([-20.3816], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.11896386742591858
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11687115579843521
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14638757705688477
epoch£º222	 i:3 	 global-step:4443	 l-p:0.1474517285823822
epoch£º222	 i:4 	 global-step:4444	 l-p:0.12878713011741638
epoch£º222	 i:5 	 global-step:4445	 l-p:0.13543131947517395
epoch£º222	 i:6 	 global-step:4446	 l-p:0.2072126716375351
epoch£º222	 i:7 	 global-step:4447	 l-p:0.0953366830945015
epoch£º222	 i:8 	 global-step:4448	 l-p:0.05854788050055504
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.10613049566745758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 4.8278, 4.8277],
        [4.8277, 4.9321, 4.8779],
        [4.8277, 5.0143, 4.9635],
        [4.8277, 4.8358, 4.8275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.25729215145111084 
model_pd.l_d.mean(): -20.74217414855957 
model_pd.lagr.mean(): -20.484882354736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.4427], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.25729215145111084
epoch£º223	 i:1 	 global-step:4461	 l-p:0.1392478197813034
epoch£º223	 i:2 	 global-step:4462	 l-p:0.13328200578689575
epoch£º223	 i:3 	 global-step:4463	 l-p:0.11807559430599213
epoch£º223	 i:4 	 global-step:4464	 l-p:0.1210925281047821
epoch£º223	 i:5 	 global-step:4465	 l-p:0.19608847796916962
epoch£º223	 i:6 	 global-step:4466	 l-p:0.28321146965026855
epoch£º223	 i:7 	 global-step:4467	 l-p:0.23467937111854553
epoch£º223	 i:8 	 global-step:4468	 l-p:0.1359187215566635
epoch£º223	 i:9 	 global-step:4469	 l-p:0.16691112518310547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0357, 5.0357],
        [5.0357, 5.0357, 5.0357],
        [5.0357, 5.0659, 5.0409],
        [5.0357, 5.0363, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12211979925632477 
model_pd.l_d.mean(): -20.573835372924805 
model_pd.lagr.mean(): -20.45171546936035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.2066], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12211979925632477
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12360110133886337
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13439425826072693
epoch£º224	 i:3 	 global-step:4483	 l-p:0.14684328436851501
epoch£º224	 i:4 	 global-step:4484	 l-p:0.1420174092054367
epoch£º224	 i:5 	 global-step:4485	 l-p:0.0754988044500351
epoch£º224	 i:6 	 global-step:4486	 l-p:0.15511009097099304
epoch£º224	 i:7 	 global-step:4487	 l-p:0.15428386628627777
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10857050865888596
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12164744734764099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.2218, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.1165766492486 
model_pd.l_d.mean(): -19.73858070373535 
model_pd.lagr.mean(): -19.62200355529785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400], device='cuda:0')), ('power', tensor([-20.4037], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.1165766492486
epoch£º225	 i:1 	 global-step:4501	 l-p:0.013092264533042908
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12361682951450348
epoch£º225	 i:3 	 global-step:4503	 l-p:0.17280267179012299
epoch£º225	 i:4 	 global-step:4504	 l-p:0.12017183005809784
epoch£º225	 i:5 	 global-step:4505	 l-p:0.17262393236160278
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11893907189369202
epoch£º225	 i:7 	 global-step:4507	 l-p:0.1253851056098938
epoch£º225	 i:8 	 global-step:4508	 l-p:0.1652192622423172
epoch£º225	 i:9 	 global-step:4509	 l-p:0.12730993330478668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9763, 5.1359, 5.0790],
        [4.9763, 5.5084, 5.6376],
        [4.9763, 5.0413, 4.9975],
        [4.9763, 4.9764, 4.9763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12490181624889374 
model_pd.l_d.mean(): -20.594322204589844 
model_pd.lagr.mean(): -20.469419479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12490181624889374
epoch£º226	 i:1 	 global-step:4521	 l-p:0.21628962457180023
epoch£º226	 i:2 	 global-step:4522	 l-p:0.09313720464706421
epoch£º226	 i:3 	 global-step:4523	 l-p:0.23873263597488403
epoch£º226	 i:4 	 global-step:4524	 l-p:0.14849358797073364
epoch£º226	 i:5 	 global-step:4525	 l-p:0.17835858464241028
epoch£º226	 i:6 	 global-step:4526	 l-p:0.12457742542028427
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11364511400461197
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14239205420017242
epoch£º226	 i:9 	 global-step:4529	 l-p:0.1936783790588379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 5.1062, 5.0497],
        [4.9905, 4.9915, 4.9905],
        [4.9905, 5.0125, 4.9930],
        [4.9905, 4.9906, 4.9905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1475016325712204 
model_pd.l_d.mean(): -19.997440338134766 
model_pd.lagr.mean(): -19.849939346313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-20.6807], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1475016325712204
epoch£º227	 i:1 	 global-step:4541	 l-p:0.11991046369075775
epoch£º227	 i:2 	 global-step:4542	 l-p:0.12921573221683502
epoch£º227	 i:3 	 global-step:4543	 l-p:0.1655161827802658
epoch£º227	 i:4 	 global-step:4544	 l-p:0.21127581596374512
epoch£º227	 i:5 	 global-step:4545	 l-p:0.20451577007770538
epoch£º227	 i:6 	 global-step:4546	 l-p:0.12588340044021606
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12425275892019272
epoch£º227	 i:8 	 global-step:4548	 l-p:0.12692123651504517
epoch£º227	 i:9 	 global-step:4549	 l-p:0.1702885925769806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0181, 5.0181, 5.0181],
        [5.0181, 5.0181, 5.0181],
        [5.0181, 5.0183, 5.0181],
        [5.0181, 5.0480, 5.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.13167747855186462 
model_pd.l_d.mean(): -20.522972106933594 
model_pd.lagr.mean(): -20.391294479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.1845], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.13167747855186462
epoch£º228	 i:1 	 global-step:4561	 l-p:0.1289374679327011
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13833247125148773
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1295648068189621
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09396234154701233
epoch£º228	 i:5 	 global-step:4565	 l-p:0.19357191026210785
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13425298035144806
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14209306240081787
epoch£º228	 i:8 	 global-step:4568	 l-p:0.17280547320842743
epoch£º228	 i:9 	 global-step:4569	 l-p:0.17881327867507935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9766, 4.9767, 4.9766],
        [4.9766, 5.5341, 5.6835],
        [4.9766, 4.9894, 4.9771],
        [4.9766, 4.9847, 4.9766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.13712650537490845 
model_pd.l_d.mean(): -18.64252471923828 
model_pd.lagr.mean(): -18.50539779663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5444], device='cuda:0')), ('power', tensor([-19.4024], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.13712650537490845
epoch£º229	 i:1 	 global-step:4581	 l-p:0.140464648604393
epoch£º229	 i:2 	 global-step:4582	 l-p:0.1595020890235901
epoch£º229	 i:3 	 global-step:4583	 l-p:0.17146345973014832
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11762578785419464
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13368377089500427
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.06002841889858246
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12178655713796616
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11237741261720657
epoch£º229	 i:9 	 global-step:4589	 l-p:0.12861771881580353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0868, 5.1082, 5.0893],
        [5.0868, 6.0698, 6.6006],
        [5.0868, 5.1167, 5.0917],
        [5.0868, 5.2927, 5.2404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.10759017616510391 
model_pd.l_d.mean(): -19.03960609436035 
model_pd.lagr.mean(): -18.932016372680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-19.7501], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.10759017616510391
epoch£º230	 i:1 	 global-step:4601	 l-p:0.13219846785068512
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12211953103542328
epoch£º230	 i:3 	 global-step:4603	 l-p:0.15398156642913818
epoch£º230	 i:4 	 global-step:4604	 l-p:0.16988860070705414
epoch£º230	 i:5 	 global-step:4605	 l-p:0.13059712946414948
epoch£º230	 i:6 	 global-step:4606	 l-p:0.23631122708320618
epoch£º230	 i:7 	 global-step:4607	 l-p:0.1523713767528534
epoch£º230	 i:8 	 global-step:4608	 l-p:0.14110572636127472
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.10143419355154037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8813, 4.8813, 4.8813],
        [4.8813, 4.8814, 4.8813],
        [4.8813, 5.7612, 6.2150],
        [4.8813, 4.9288, 4.8918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.1291504204273224 
model_pd.l_d.mean(): -20.14002227783203 
model_pd.lagr.mean(): -20.01087188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.8803], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.1291504204273224
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14236991107463837
epoch£º231	 i:2 	 global-step:4622	 l-p:0.12565530836582184
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13442808389663696
epoch£º231	 i:4 	 global-step:4624	 l-p:0.1384107619524002
epoch£º231	 i:5 	 global-step:4625	 l-p:0.2722987234592438
epoch£º231	 i:6 	 global-step:4626	 l-p:0.13639825582504272
epoch£º231	 i:7 	 global-step:4627	 l-p:0.06167778745293617
epoch£º231	 i:8 	 global-step:4628	 l-p:0.046748120337724686
epoch£º231	 i:9 	 global-step:4629	 l-p:0.16914209723472595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7763, 4.7763, 4.7763],
        [4.7763, 4.7983, 4.7767],
        [4.7763, 5.1426, 5.1713],
        [4.7763, 4.7768, 4.7761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12486601620912552 
model_pd.l_d.mean(): -20.680591583251953 
model_pd.lagr.mean(): -20.55572509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4879], device='cuda:0')), ('power', tensor([-21.4049], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12486601620912552
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18945933878421783
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05649740993976593
epoch£º232	 i:3 	 global-step:4643	 l-p:0.14071445167064667
epoch£º232	 i:4 	 global-step:4644	 l-p:0.1408204585313797
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13375656306743622
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14836947619915009
epoch£º232	 i:7 	 global-step:4647	 l-p:0.1239173486828804
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11937319487333298
epoch£º232	 i:9 	 global-step:4649	 l-p:0.1213810071349144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0946, 5.0614],
        [5.0523, 5.0704, 5.0538],
        [5.0523, 5.3566, 5.3386],
        [5.0523, 5.8850, 6.2702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.08485952019691467 
model_pd.l_d.mean(): -19.99919891357422 
model_pd.lagr.mean(): -19.914339065551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.7057], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.08485952019691467
epoch£º233	 i:1 	 global-step:4661	 l-p:0.13710099458694458
epoch£º233	 i:2 	 global-step:4662	 l-p:0.11049775034189224
epoch£º233	 i:3 	 global-step:4663	 l-p:0.1328967958688736
epoch£º233	 i:4 	 global-step:4664	 l-p:0.13812418282032013
epoch£º233	 i:5 	 global-step:4665	 l-p:0.15475541353225708
epoch£º233	 i:6 	 global-step:4666	 l-p:0.14768436551094055
epoch£º233	 i:7 	 global-step:4667	 l-p:0.13534024357795715
epoch£º233	 i:8 	 global-step:4668	 l-p:0.120579294860363
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12988720834255219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 4.9703, 4.9682],
        [4.9684, 4.9763, 4.9682],
        [4.9684, 4.9815, 4.9686],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12316101044416428 
model_pd.l_d.mean(): -20.02406120300293 
model_pd.lagr.mean(): -19.90089988708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.7472], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.12316101044416428
epoch£º234	 i:1 	 global-step:4681	 l-p:0.5396808981895447
epoch£º234	 i:2 	 global-step:4682	 l-p:0.1247696653008461
epoch£º234	 i:3 	 global-step:4683	 l-p:0.12900124490261078
epoch£º234	 i:4 	 global-step:4684	 l-p:0.19838836789131165
epoch£º234	 i:5 	 global-step:4685	 l-p:0.03690686076879501
epoch£º234	 i:6 	 global-step:4686	 l-p:0.13044263422489166
epoch£º234	 i:7 	 global-step:4687	 l-p:0.14489741623401642
epoch£º234	 i:8 	 global-step:4688	 l-p:0.1398475468158722
epoch£º234	 i:9 	 global-step:4689	 l-p:0.07255198806524277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7859, 5.4235, 5.6598],
        [4.7859, 4.7859, 4.7859],
        [4.7859, 4.7870, 4.7854],
        [4.7859, 4.7859, 4.7859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13790154457092285 
model_pd.l_d.mean(): -20.54977798461914 
model_pd.lagr.mean(): -20.411876678466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.2660], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13790154457092285
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.32346028089523315
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13479290902614594
epoch£º235	 i:3 	 global-step:4703	 l-p:0.04905625060200691
epoch£º235	 i:4 	 global-step:4704	 l-p:0.1635494977235794
epoch£º235	 i:5 	 global-step:4705	 l-p:0.1732666939496994
epoch£º235	 i:6 	 global-step:4706	 l-p:0.13064482808113098
epoch£º235	 i:7 	 global-step:4707	 l-p:0.1404723823070526
epoch£º235	 i:8 	 global-step:4708	 l-p:0.12356261909008026
epoch£º235	 i:9 	 global-step:4709	 l-p:0.12258122116327286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 5.1054, 5.0573],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 5.0316, 4.9734],
        [4.8974, 4.9192, 4.8983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13090723752975464 
model_pd.l_d.mean(): -20.494421005249023 
model_pd.lagr.mean(): -20.363513946533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13090723752975464
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13499096035957336
epoch£º236	 i:2 	 global-step:4722	 l-p:0.13498491048812866
epoch£º236	 i:3 	 global-step:4723	 l-p:0.1384635716676712
epoch£º236	 i:4 	 global-step:4724	 l-p:0.3356318473815918
epoch£º236	 i:5 	 global-step:4725	 l-p:0.11200204491615295
epoch£º236	 i:6 	 global-step:4726	 l-p:0.14923816919326782
epoch£º236	 i:7 	 global-step:4727	 l-p:0.11484336853027344
epoch£º236	 i:8 	 global-step:4728	 l-p:0.15211382508277893
epoch£º236	 i:9 	 global-step:4729	 l-p:0.1276562660932541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.5559, 5.8734],
        [4.8243, 4.9016, 4.8511],
        [4.8243, 4.8243, 4.8243],
        [4.8243, 4.9505, 4.8927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.0506710410118103 
model_pd.l_d.mean(): -19.58149528503418 
model_pd.lagr.mean(): -19.530824661254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5423], device='cuda:0')), ('power', tensor([-20.3494], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.0506710410118103
epoch£º237	 i:1 	 global-step:4741	 l-p:0.13289475440979004
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.03774227201938629
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14688017964363098
epoch£º237	 i:4 	 global-step:4744	 l-p:0.12754689157009125
epoch£º237	 i:5 	 global-step:4745	 l-p:0.18750905990600586
epoch£º237	 i:6 	 global-step:4746	 l-p:0.12392672896385193
epoch£º237	 i:7 	 global-step:4747	 l-p:0.14507350325584412
epoch£º237	 i:8 	 global-step:4748	 l-p:0.13124534487724304
epoch£º237	 i:9 	 global-step:4749	 l-p:0.12022145837545395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9807, 5.8701, 6.3205],
        [4.9807, 5.0297, 4.9917],
        [4.9807, 5.4909, 5.6044],
        [4.9807, 4.9807, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.1320255994796753 
model_pd.l_d.mean(): -17.28903579711914 
model_pd.lagr.mean(): -17.157011032104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6337], device='cuda:0')), ('power', tensor([-18.1253], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.1320255994796753
epoch£º238	 i:1 	 global-step:4761	 l-p:0.19713696837425232
epoch£º238	 i:2 	 global-step:4762	 l-p:0.126944437623024
epoch£º238	 i:3 	 global-step:4763	 l-p:0.1331428736448288
epoch£º238	 i:4 	 global-step:4764	 l-p:0.15771733224391937
epoch£º238	 i:5 	 global-step:4765	 l-p:0.19939784705638885
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09317587316036224
epoch£º238	 i:7 	 global-step:4767	 l-p:0.18406496942043304
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12464819848537445
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12719909846782684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9628, 5.4570, 5.5601],
        [4.9628, 5.7587, 6.1201],
        [4.9628, 5.0487, 4.9961],
        [4.9628, 5.3190, 5.3318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.1583385467529297 
model_pd.l_d.mean(): -20.348302841186523 
model_pd.lagr.mean(): -20.189964294433594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.0516], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.1583385467529297
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10930310934782028
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12825250625610352
epoch£º239	 i:3 	 global-step:4783	 l-p:0.1361604779958725
epoch£º239	 i:4 	 global-step:4784	 l-p:-0.09999551624059677
epoch£º239	 i:5 	 global-step:4785	 l-p:0.1320541352033615
epoch£º239	 i:6 	 global-step:4786	 l-p:0.12197931110858917
epoch£º239	 i:7 	 global-step:4787	 l-p:0.1579267829656601
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11743637174367905
epoch£º239	 i:9 	 global-step:4789	 l-p:0.10356569290161133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7702, 5.0885, 5.0902],
        [4.7702, 4.8018, 4.7717],
        [4.7702, 4.8309, 4.7852],
        [4.7702, 4.7700, 4.7702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.043878257274627686 
model_pd.l_d.mean(): -20.714492797851562 
model_pd.lagr.mean(): -20.758371353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4213], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.043878257274627686
epoch£º240	 i:1 	 global-step:4801	 l-p:0.16233287751674652
epoch£º240	 i:2 	 global-step:4802	 l-p:0.14044243097305298
epoch£º240	 i:3 	 global-step:4803	 l-p:0.06483619660139084
epoch£º240	 i:4 	 global-step:4804	 l-p:0.13033080101013184
epoch£º240	 i:5 	 global-step:4805	 l-p:0.12709687650203705
epoch£º240	 i:6 	 global-step:4806	 l-p:0.14522698521614075
epoch£º240	 i:7 	 global-step:4807	 l-p:0.15289242565631866
epoch£º240	 i:8 	 global-step:4808	 l-p:0.2011890709400177
epoch£º240	 i:9 	 global-step:4809	 l-p:0.008089723065495491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9235, 4.9732, 4.9339],
        [4.9235, 4.9235, 4.9235],
        [4.9235, 5.4266, 5.5395],
        [4.9235, 5.6785, 6.0061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.34435969591140747 
model_pd.l_d.mean(): -20.448394775390625 
model_pd.lagr.mean(): -20.104034423828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.1421], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.34435969591140747
epoch£º241	 i:1 	 global-step:4821	 l-p:0.357613742351532
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11997747421264648
epoch£º241	 i:3 	 global-step:4823	 l-p:0.155412957072258
epoch£º241	 i:4 	 global-step:4824	 l-p:0.1340673863887787
epoch£º241	 i:5 	 global-step:4825	 l-p:0.12761984765529633
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11834271252155304
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11930916458368301
epoch£º241	 i:8 	 global-step:4828	 l-p:0.13367924094200134
epoch£º241	 i:9 	 global-step:4829	 l-p:0.0709376409649849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0557, 5.3535, 5.3325],
        [5.0557, 5.0730, 5.0563],
        [5.0557, 5.0557, 5.0557],
        [5.0557, 5.7603, 6.0273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.14043207466602325 
model_pd.l_d.mean(): -20.276185989379883 
model_pd.lagr.mean(): -20.135753631591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-20.9462], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.14043207466602325
epoch£º242	 i:1 	 global-step:4841	 l-p:0.12477576732635498
epoch£º242	 i:2 	 global-step:4842	 l-p:0.1574760526418686
epoch£º242	 i:3 	 global-step:4843	 l-p:0.1265925019979477
epoch£º242	 i:4 	 global-step:4844	 l-p:0.1492302268743515
epoch£º242	 i:5 	 global-step:4845	 l-p:0.13248252868652344
epoch£º242	 i:6 	 global-step:4846	 l-p:0.09521365910768509
epoch£º242	 i:7 	 global-step:4847	 l-p:0.1341165453195572
epoch£º242	 i:8 	 global-step:4848	 l-p:0.1246325671672821
epoch£º242	 i:9 	 global-step:4849	 l-p:0.2675161361694336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9689, 4.9940, 4.9703],
        [4.9689, 4.9688, 4.9689],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 5.5872, 5.7882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.15441393852233887 
model_pd.l_d.mean(): -20.061826705932617 
model_pd.lagr.mean(): -19.907413482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.15441393852233887
epoch£º243	 i:1 	 global-step:4861	 l-p:0.35000187158584595
epoch£º243	 i:2 	 global-step:4862	 l-p:0.12944942712783813
epoch£º243	 i:3 	 global-step:4863	 l-p:0.13986194133758545
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12119434773921967
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11844716966152191
epoch£º243	 i:6 	 global-step:4866	 l-p:0.19587743282318115
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.09590064734220505
epoch£º243	 i:8 	 global-step:4868	 l-p:0.13141128420829773
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13166089355945587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8765, 4.8887, 4.8750],
        [4.8765, 5.1567, 5.1348],
        [4.8765, 4.9078, 4.8785],
        [4.8765, 5.4180, 5.5648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.05519275367259979 
model_pd.l_d.mean(): -20.495203018188477 
model_pd.lagr.mean(): -20.44001007080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.05519275367259979
epoch£º244	 i:1 	 global-step:4881	 l-p:0.13948319852352142
epoch£º244	 i:2 	 global-step:4882	 l-p:0.12296625226736069
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12927116453647614
epoch£º244	 i:4 	 global-step:4884	 l-p:0.12551866471767426
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.06093684211373329
epoch£º244	 i:6 	 global-step:4886	 l-p:0.1350993514060974
epoch£º244	 i:7 	 global-step:4887	 l-p:0.22194068133831024
epoch£º244	 i:8 	 global-step:4888	 l-p:0.08169914036989212
epoch£º244	 i:9 	 global-step:4889	 l-p:0.15736466646194458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.0081, 4.9491],
        [4.8648, 5.1494, 5.1300],
        [4.8648, 4.8648, 4.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.08372001349925995 
model_pd.l_d.mean(): -20.004383087158203 
model_pd.lagr.mean(): -19.920663833618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-20.7625], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.08372001349925995
epoch£º245	 i:1 	 global-step:4901	 l-p:0.21301613748073578
epoch£º245	 i:2 	 global-step:4902	 l-p:-0.30335181951522827
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14887544512748718
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11591038107872009
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13196298480033875
epoch£º245	 i:6 	 global-step:4906	 l-p:0.08011098951101303
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1683952659368515
epoch£º245	 i:8 	 global-step:4908	 l-p:0.12428325414657593
epoch£º245	 i:9 	 global-step:4909	 l-p:0.12341108173131943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 5.6221, 5.7100],
        [5.1325, 5.1326, 5.1325],
        [5.1325, 5.1848, 5.1450],
        [5.1325, 5.1325, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12045373767614365 
model_pd.l_d.mean(): -19.457027435302734 
model_pd.lagr.mean(): -19.33657455444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-20.1510], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12045373767614365
epoch£º246	 i:1 	 global-step:4921	 l-p:0.1298840492963791
epoch£º246	 i:2 	 global-step:4922	 l-p:0.12733466923236847
epoch£º246	 i:3 	 global-step:4923	 l-p:0.13676434755325317
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13089241087436676
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12898048758506775
epoch£º246	 i:6 	 global-step:4926	 l-p:0.0277854111045599
epoch£º246	 i:7 	 global-step:4927	 l-p:0.1389230489730835
epoch£º246	 i:8 	 global-step:4928	 l-p:0.15403792262077332
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12092166393995285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 5.0493, 5.0493],
        [5.0493, 5.0572, 5.0487],
        [5.0493, 5.1392, 5.0847],
        [5.0493, 5.0493, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.07591906189918518 
model_pd.l_d.mean(): -19.95372772216797 
model_pd.lagr.mean(): -19.877809524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-20.6225], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.07591906189918518
epoch£º247	 i:1 	 global-step:4941	 l-p:0.13495294749736786
epoch£º247	 i:2 	 global-step:4942	 l-p:0.1699838787317276
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13954563438892365
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14203213155269623
epoch£º247	 i:5 	 global-step:4945	 l-p:0.19284285604953766
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12907513976097107
epoch£º247	 i:7 	 global-step:4947	 l-p:0.13004077970981598
epoch£º247	 i:8 	 global-step:4948	 l-p:0.11993251740932465
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11529268324375153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0459, 5.0459, 5.0459],
        [5.0459, 5.0459, 5.0459],
        [5.0459, 5.0530, 5.0452],
        [5.0459, 5.0459, 5.0459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.10974408686161041 
model_pd.l_d.mean(): -20.614585876464844 
model_pd.lagr.mean(): -20.504840850830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133], device='cuda:0')), ('power', tensor([-21.2620], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.10974408686161041
epoch£º248	 i:1 	 global-step:4961	 l-p:0.1400810331106186
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12368979305028915
epoch£º248	 i:3 	 global-step:4963	 l-p:0.17744748294353485
epoch£º248	 i:4 	 global-step:4964	 l-p:0.12761981785297394
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13680577278137207
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13288091123104095
epoch£º248	 i:7 	 global-step:4967	 l-p:0.14310403168201447
epoch£º248	 i:8 	 global-step:4968	 l-p:0.12229941785335541
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.1590026319026947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9005, 5.2959, 5.3360],
        [4.9005, 5.3443, 5.4166],
        [4.9005, 4.9669, 4.9183],
        [4.9005, 5.6387, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.006082420237362385 
model_pd.l_d.mean(): -20.677946090698242 
model_pd.lagr.mean(): -20.671863555908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.3645], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.006082420237362385
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15246006846427917
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12304075062274933
epoch£º249	 i:3 	 global-step:4983	 l-p:0.18176645040512085
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21217229962348938
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13354545831680298
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10747230052947998
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12473603338003159
epoch£º249	 i:8 	 global-step:4988	 l-p:0.15054069459438324
epoch£º249	 i:9 	 global-step:4989	 l-p:0.49478593468666077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 5.1167, 5.0600],
        [4.9395, 4.9395, 4.9395],
        [4.9395, 4.9498, 4.9377],
        [4.9395, 4.9395, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16068591177463531 
model_pd.l_d.mean(): -19.522035598754883 
model_pd.lagr.mean(): -19.36134910583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.2634], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16068591177463531
epoch£º250	 i:1 	 global-step:5001	 l-p:0.1691584289073944
epoch£º250	 i:2 	 global-step:5002	 l-p:0.12794606387615204
epoch£º250	 i:3 	 global-step:5003	 l-p:0.14027269184589386
epoch£º250	 i:4 	 global-step:5004	 l-p:0.1282813996076584
epoch£º250	 i:5 	 global-step:5005	 l-p:0.1265292763710022
epoch£º250	 i:6 	 global-step:5006	 l-p:0.14377357065677643
epoch£º250	 i:7 	 global-step:5007	 l-p:0.12437330931425095
epoch£º250	 i:8 	 global-step:5008	 l-p:0.16294169425964355
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08773385733366013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.2101, 5.1496],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 5.1302, 5.0783],
        [5.0521, 5.0521, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18399040400981903 
model_pd.l_d.mean(): -20.15699005126953 
model_pd.lagr.mean(): -19.972999572753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.8441], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18399040400981903
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13463881611824036
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12298599630594254
epoch£º251	 i:3 	 global-step:5023	 l-p:0.1195945218205452
epoch£º251	 i:4 	 global-step:5024	 l-p:0.1421976387500763
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11592807620763779
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07899542897939682
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12587970495224
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12722148001194
epoch£º251	 i:9 	 global-step:5029	 l-p:0.13055460155010223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0204, 5.0203, 5.0204],
        [5.0204, 5.9337, 6.4042],
        [5.0204, 5.0946, 5.0436],
        [5.0204, 5.1837, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.1282462626695633 
model_pd.l_d.mean(): -20.0947265625 
model_pd.lagr.mean(): -19.966480255126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.1282462626695633
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21431605517864227
epoch£º252	 i:2 	 global-step:5042	 l-p:0.12720659375190735
epoch£º252	 i:3 	 global-step:5043	 l-p:0.1036955714225769
epoch£º252	 i:4 	 global-step:5044	 l-p:0.1775175780057907
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12636111676692963
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5341914892196655
epoch£º252	 i:7 	 global-step:5047	 l-p:0.1344223916530609
epoch£º252	 i:8 	 global-step:5048	 l-p:0.13038672506809235
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07688408344984055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8350, 5.6682, 6.0837],
        [4.8350, 4.9044, 4.8533],
        [4.8350, 4.8962, 4.8481],
        [4.8350, 5.2284, 5.2715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.13382481038570404 
model_pd.l_d.mean(): -20.601797103881836 
model_pd.lagr.mean(): -20.467971801757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.3105], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.13382481038570404
epoch£º253	 i:1 	 global-step:5061	 l-p:0.22805161774158478
epoch£º253	 i:2 	 global-step:5062	 l-p:0.11031186580657959
epoch£º253	 i:3 	 global-step:5063	 l-p:0.1383545845746994
epoch£º253	 i:4 	 global-step:5064	 l-p:0.07849016785621643
epoch£º253	 i:5 	 global-step:5065	 l-p:0.14165431261062622
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13150343298912048
epoch£º253	 i:7 	 global-step:5067	 l-p:0.23731376230716705
epoch£º253	 i:8 	 global-step:5068	 l-p:0.14333127439022064
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12494716048240662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0128, 5.0220, 5.0113],
        [5.0128, 5.0533, 5.0177],
        [5.0128, 5.0128, 5.0128],
        [5.0128, 5.0128, 5.0128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.1571912318468094 
model_pd.l_d.mean(): -20.200695037841797 
model_pd.lagr.mean(): -20.04350471496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4580], device='cuda:0')), ('power', tensor([-20.8892], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.1571912318468094
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09934968501329422
epoch£º254	 i:2 	 global-step:5082	 l-p:0.14270053803920746
epoch£º254	 i:3 	 global-step:5083	 l-p:0.13175180554389954
epoch£º254	 i:4 	 global-step:5084	 l-p:0.10440124571323395
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11703601479530334
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11476926505565643
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14848491549491882
epoch£º254	 i:8 	 global-step:5088	 l-p:0.15060895681381226
epoch£º254	 i:9 	 global-step:5089	 l-p:0.15368366241455078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0550, 5.9603, 6.4183],
        [5.0550, 5.1889, 5.1273],
        [5.0550, 5.4994, 5.5621],
        [5.0550, 6.1157, 6.7342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.1571519672870636 
model_pd.l_d.mean(): -18.83354949951172 
model_pd.lagr.mean(): -18.6763973236084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-19.5653], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.1571519672870636
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11566032469272614
epoch£º255	 i:2 	 global-step:5102	 l-p:0.09030486643314362
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12296579033136368
epoch£º255	 i:4 	 global-step:5104	 l-p:0.17287567257881165
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12643708288669586
epoch£º255	 i:6 	 global-step:5106	 l-p:0.11780302971601486
epoch£º255	 i:7 	 global-step:5107	 l-p:0.2258419692516327
epoch£º255	 i:8 	 global-step:5108	 l-p:0.1900680512189865
epoch£º255	 i:9 	 global-step:5109	 l-p:0.143213152885437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9523, 4.9802, 4.9521],
        [4.9523, 4.9530, 4.9516],
        [4.9523, 4.9523, 4.9523],
        [4.9523, 5.3125, 5.3285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.14429029822349548 
model_pd.l_d.mean(): -20.63076400756836 
model_pd.lagr.mean(): -20.486473083496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-21.3014], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.14429029822349548
epoch£º256	 i:1 	 global-step:5121	 l-p:-1.1144531965255737
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13404062390327454
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13178788125514984
epoch£º256	 i:4 	 global-step:5124	 l-p:0.08615988492965698
epoch£º256	 i:5 	 global-step:5125	 l-p:0.1331517994403839
epoch£º256	 i:6 	 global-step:5126	 l-p:0.13757146894931793
epoch£º256	 i:7 	 global-step:5127	 l-p:0.2609454393386841
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12209361046552658
epoch£º256	 i:9 	 global-step:5129	 l-p:0.4239569306373596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8820, 4.8820, 4.8820],
        [4.8820, 4.8820, 4.8820],
        [4.8820, 5.0531, 4.9953],
        [4.8820, 4.8845, 4.8800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12436868250370026 
model_pd.l_d.mean(): -19.610563278198242 
model_pd.lagr.mean(): -19.486194610595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-20.3304], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12436868250370026
epoch£º257	 i:1 	 global-step:5141	 l-p:0.1405891329050064
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12482401728630066
epoch£º257	 i:3 	 global-step:5143	 l-p:0.07230646163225174
epoch£º257	 i:4 	 global-step:5144	 l-p:0.17366541922092438
epoch£º257	 i:5 	 global-step:5145	 l-p:0.12843534350395203
epoch£º257	 i:6 	 global-step:5146	 l-p:0.13023331761360168
epoch£º257	 i:7 	 global-step:5147	 l-p:0.1706530749797821
epoch£º257	 i:8 	 global-step:5148	 l-p:0.28412094712257385
epoch£º257	 i:9 	 global-step:5149	 l-p:0.1318129003047943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0314, 5.0313, 5.0314],
        [5.0314, 5.0617, 5.0324],
        [5.0314, 5.0314, 5.0314],
        [5.0314, 5.4106, 5.4335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12389229238033295 
model_pd.l_d.mean(): -20.088342666625977 
model_pd.lagr.mean(): -19.96445083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-20.7781], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12389229238033295
epoch£º258	 i:1 	 global-step:5161	 l-p:0.12640558183193207
epoch£º258	 i:2 	 global-step:5162	 l-p:0.18260356783866882
epoch£º258	 i:3 	 global-step:5163	 l-p:0.10651660710573196
epoch£º258	 i:4 	 global-step:5164	 l-p:0.11889410018920898
epoch£º258	 i:5 	 global-step:5165	 l-p:0.1604616940021515
epoch£º258	 i:6 	 global-step:5166	 l-p:0.16654054820537567
epoch£º258	 i:7 	 global-step:5167	 l-p:0.1216881275177002
epoch£º258	 i:8 	 global-step:5168	 l-p:0.11846434324979782
epoch£º258	 i:9 	 global-step:5169	 l-p:0.1397002637386322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0140, 5.3885, 5.4095],
        [5.0140, 5.0140, 5.0140],
        [5.0140, 5.0140, 5.0140],
        [5.0140, 5.0819, 5.0318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11529970169067383 
model_pd.l_d.mean(): -18.97616195678711 
model_pd.lagr.mean(): -18.860862731933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4973], device='cuda:0')), ('power', tensor([-19.6915], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11529970169067383
epoch£º259	 i:1 	 global-step:5181	 l-p:0.1017417386174202
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15407653152942657
epoch£º259	 i:3 	 global-step:5183	 l-p:0.1472892016172409
epoch£º259	 i:4 	 global-step:5184	 l-p:-1.527649164199829
epoch£º259	 i:5 	 global-step:5185	 l-p:-0.006531924940645695
epoch£º259	 i:6 	 global-step:5186	 l-p:0.12284564971923828
epoch£º259	 i:7 	 global-step:5187	 l-p:0.12742482125759125
epoch£º259	 i:8 	 global-step:5188	 l-p:0.062501922249794
epoch£º259	 i:9 	 global-step:5189	 l-p:0.15135891735553741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8637, 5.3769, 5.5040],
        [4.8637, 5.1160, 5.0820],
        [4.8637, 4.8637, 4.8637],
        [4.8637, 4.9322, 4.8801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.19852150976657867 
model_pd.l_d.mean(): -20.154621124267578 
model_pd.lagr.mean(): -19.956100463867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5213], device='cuda:0')), ('power', tensor([-20.9073], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.19852150976657867
epoch£º260	 i:1 	 global-step:5201	 l-p:0.07469414174556732
epoch£º260	 i:2 	 global-step:5202	 l-p:0.1928083747625351
epoch£º260	 i:3 	 global-step:5203	 l-p:0.1109367311000824
epoch£º260	 i:4 	 global-step:5204	 l-p:0.025396503508090973
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13697195053100586
epoch£º260	 i:6 	 global-step:5206	 l-p:0.133844256401062
epoch£º260	 i:7 	 global-step:5207	 l-p:0.15530163049697876
epoch£º260	 i:8 	 global-step:5208	 l-p:0.3685714602470398
epoch£º260	 i:9 	 global-step:5209	 l-p:0.13023610413074493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9962, 4.9962, 4.9962],
        [4.9962, 5.3226, 5.3179],
        [4.9962, 5.3647, 5.3833],
        [4.9962, 5.0011, 4.9941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.18194767832756042 
model_pd.l_d.mean(): -20.06734275817871 
model_pd.lagr.mean(): -19.885395050048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-20.7443], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.18194767832756042
epoch£º261	 i:1 	 global-step:5221	 l-p:0.09599167853593826
epoch£º261	 i:2 	 global-step:5222	 l-p:0.15627343952655792
epoch£º261	 i:3 	 global-step:5223	 l-p:0.1169573962688446
epoch£º261	 i:4 	 global-step:5224	 l-p:0.1524825394153595
epoch£º261	 i:5 	 global-step:5225	 l-p:0.13656766712665558
epoch£º261	 i:6 	 global-step:5226	 l-p:0.12250274419784546
epoch£º261	 i:7 	 global-step:5227	 l-p:0.13417251408100128
epoch£º261	 i:8 	 global-step:5228	 l-p:0.1426502764225006
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13730381429195404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0409, 5.0411, 5.0405],
        [5.0409, 5.0510, 5.0387],
        [5.0409, 5.0407, 5.0409],
        [5.0409, 5.0407, 5.0409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.12419316172599792 
model_pd.l_d.mean(): -19.544960021972656 
model_pd.lagr.mean(): -19.420766830444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.2472], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.12419316172599792
epoch£º262	 i:1 	 global-step:5241	 l-p:0.1304539442062378
epoch£º262	 i:2 	 global-step:5242	 l-p:0.14176829159259796
epoch£º262	 i:3 	 global-step:5243	 l-p:-0.09313716739416122
epoch£º262	 i:4 	 global-step:5244	 l-p:0.07202691584825516
epoch£º262	 i:5 	 global-step:5245	 l-p:0.2841472625732422
epoch£º262	 i:6 	 global-step:5246	 l-p:0.186776801943779
epoch£º262	 i:7 	 global-step:5247	 l-p:0.14947669208049774
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.013661393895745277
epoch£º262	 i:9 	 global-step:5249	 l-p:0.13480611145496368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9484, 5.4100, 5.4921],
        [4.9484, 5.2272, 5.2015],
        [4.9484, 4.9498, 4.9467],
        [4.9484, 5.3541, 5.3983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1324475109577179 
model_pd.l_d.mean(): -19.34610366821289 
model_pd.lagr.mean(): -19.213655471801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-20.0728], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1324475109577179
epoch£º263	 i:1 	 global-step:5261	 l-p:0.335457444190979
epoch£º263	 i:2 	 global-step:5262	 l-p:0.1356286257505417
epoch£º263	 i:3 	 global-step:5263	 l-p:0.13561271131038666
epoch£º263	 i:4 	 global-step:5264	 l-p:0.1855790913105011
epoch£º263	 i:5 	 global-step:5265	 l-p:0.14000394940376282
epoch£º263	 i:6 	 global-step:5266	 l-p:0.1633867621421814
epoch£º263	 i:7 	 global-step:5267	 l-p:0.13209271430969238
epoch£º263	 i:8 	 global-step:5268	 l-p:0.08913145214319229
epoch£º263	 i:9 	 global-step:5269	 l-p:0.12508121132850647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0682, 5.0680],
        [5.0683, 5.5738, 5.6790],
        [5.0683, 5.8368, 6.1623],
        [5.0683, 5.4151, 5.4178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.15184153616428375 
model_pd.l_d.mean(): -20.497411727905273 
model_pd.lagr.mean(): -20.345569610595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.1517], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.15184153616428375
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1298167109489441
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10284630209207535
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12148132175207138
epoch£º264	 i:4 	 global-step:5284	 l-p:0.1427631378173828
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14112089574337006
epoch£º264	 i:6 	 global-step:5286	 l-p:0.17193542420864105
epoch£º264	 i:7 	 global-step:5287	 l-p:0.2111588567495346
epoch£º264	 i:8 	 global-step:5288	 l-p:0.14568595588207245
epoch£º264	 i:9 	 global-step:5289	 l-p:0.11967840045690536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.8771, 6.2761],
        [5.0351, 5.1887, 5.1264],
        [5.0351, 5.0351, 5.0351],
        [5.0351, 5.0351, 5.0351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11227467656135559 
model_pd.l_d.mean(): -19.7979736328125 
model_pd.lagr.mean(): -19.685699462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.5225], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11227467656135559
epoch£º265	 i:1 	 global-step:5301	 l-p:0.14329460263252258
epoch£º265	 i:2 	 global-step:5302	 l-p:0.13343201577663422
epoch£º265	 i:3 	 global-step:5303	 l-p:0.20620408654212952
epoch£º265	 i:4 	 global-step:5304	 l-p:0.0875764936208725
epoch£º265	 i:5 	 global-step:5305	 l-p:0.1542975753545761
epoch£º265	 i:6 	 global-step:5306	 l-p:0.1297081857919693
epoch£º265	 i:7 	 global-step:5307	 l-p:0.13245701789855957
epoch£º265	 i:8 	 global-step:5308	 l-p:0.14639060199260712
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12137749046087265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0445, 5.0441, 5.0444],
        [5.0445, 5.0444, 5.0445],
        [5.0445, 5.0459, 5.0433],
        [5.0445, 5.0444, 5.0445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.12450975179672241 
model_pd.l_d.mean(): -20.352909088134766 
model_pd.lagr.mean(): -20.2283992767334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.0259], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.12450975179672241
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13145428895950317
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12626104056835175
epoch£º266	 i:3 	 global-step:5323	 l-p:0.5615311861038208
epoch£º266	 i:4 	 global-step:5324	 l-p:0.15948937833309174
epoch£º266	 i:5 	 global-step:5325	 l-p:10.144381523132324
epoch£º266	 i:6 	 global-step:5326	 l-p:0.14159812033176422
epoch£º266	 i:7 	 global-step:5327	 l-p:0.21279925107955933
epoch£º266	 i:8 	 global-step:5328	 l-p:0.13084839284420013
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13453295826911926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0244, 5.0244, 5.0244],
        [5.0244, 5.1136, 5.0554],
        [5.0244, 5.0244, 5.0244],
        [5.0244, 5.0244, 5.0236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.1321459412574768 
model_pd.l_d.mean(): -19.963224411010742 
model_pd.lagr.mean(): -19.831077575683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4891], device='cuda:0')), ('power', tensor([-20.6810], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.1321459412574768
epoch£º267	 i:1 	 global-step:5341	 l-p:0.1468360722064972
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11768390238285065
epoch£º267	 i:3 	 global-step:5343	 l-p:0.1355074644088745
epoch£º267	 i:4 	 global-step:5344	 l-p:0.12092374265193939
epoch£º267	 i:5 	 global-step:5345	 l-p:0.21905916929244995
epoch£º267	 i:6 	 global-step:5346	 l-p:0.5527904033660889
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12545275688171387
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13591377437114716
epoch£º267	 i:9 	 global-step:5349	 l-p:0.18190482258796692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9703, 4.9702, 4.9703],
        [4.9703, 5.1782, 5.1258],
        [4.9703, 4.9699, 4.9693],
        [4.9703, 4.9703, 4.9703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.15084511041641235 
model_pd.l_d.mean(): -20.2397518157959 
model_pd.lagr.mean(): -20.08890724182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-20.9493], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.15084511041641235
epoch£º268	 i:1 	 global-step:5361	 l-p:0.13835982978343964
epoch£º268	 i:2 	 global-step:5362	 l-p:0.15181148052215576
epoch£º268	 i:3 	 global-step:5363	 l-p:0.1222202256321907
epoch£º268	 i:4 	 global-step:5364	 l-p:0.2443118542432785
epoch£º268	 i:5 	 global-step:5365	 l-p:0.26862677931785583
epoch£º268	 i:6 	 global-step:5366	 l-p:0.17841176688671112
epoch£º268	 i:7 	 global-step:5367	 l-p:0.13186384737491608
epoch£º268	 i:8 	 global-step:5368	 l-p:0.14728322625160217
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12736961245536804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0325, 5.0324, 5.0325],
        [5.0325, 5.0320, 5.0323],
        [5.0325, 5.0438, 5.0290],
        [5.0325, 5.0364, 5.0301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.18203307688236237 
model_pd.l_d.mean(): -20.420316696166992 
model_pd.lagr.mean(): -20.238283157348633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.1038], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.18203307688236237
epoch£º269	 i:1 	 global-step:5381	 l-p:0.21479356288909912
epoch£º269	 i:2 	 global-step:5382	 l-p:0.13989399373531342
epoch£º269	 i:3 	 global-step:5383	 l-p:0.11545661836862564
epoch£º269	 i:4 	 global-step:5384	 l-p:0.12399600446224213
epoch£º269	 i:5 	 global-step:5385	 l-p:0.12340652942657471
epoch£º269	 i:6 	 global-step:5386	 l-p:0.1081584244966507
epoch£º269	 i:7 	 global-step:5387	 l-p:0.1861184984445572
epoch£º269	 i:8 	 global-step:5388	 l-p:0.14221957325935364
epoch£º269	 i:9 	 global-step:5389	 l-p:0.10535752773284912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 4.9340, 4.9346],
        [4.9348, 5.1876, 5.1509],
        [4.9348, 4.9346, 4.9348],
        [4.9348, 5.0432, 4.9805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13600005209445953 
model_pd.l_d.mean(): -20.645370483398438 
model_pd.lagr.mean(): -20.509370803833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-21.3222], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.13600005209445953
epoch£º270	 i:1 	 global-step:5401	 l-p:0.018865542486310005
epoch£º270	 i:2 	 global-step:5402	 l-p:0.05067436397075653
epoch£º270	 i:3 	 global-step:5403	 l-p:0.15853287279605865
epoch£º270	 i:4 	 global-step:5404	 l-p:0.14541110396385193
epoch£º270	 i:5 	 global-step:5405	 l-p:0.02308046817779541
epoch£º270	 i:6 	 global-step:5406	 l-p:0.2564148008823395
epoch£º270	 i:7 	 global-step:5407	 l-p:0.1349005550146103
epoch£º270	 i:8 	 global-step:5408	 l-p:0.13464783132076263
epoch£º270	 i:9 	 global-step:5409	 l-p:0.18864372372627258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8825, 4.8825, 4.8825],
        [4.8825, 4.9691, 4.9093],
        [4.8825, 5.2627, 5.2948],
        [4.8825, 5.5864, 5.8745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.09902116656303406 
model_pd.l_d.mean(): -20.487171173095703 
model_pd.lagr.mean(): -20.38814926147461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4833], device='cuda:0')), ('power', tensor([-21.2047], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.09902116656303406
epoch£º271	 i:1 	 global-step:5421	 l-p:0.1266249716281891
epoch£º271	 i:2 	 global-step:5422	 l-p:0.16636797785758972
epoch£º271	 i:3 	 global-step:5423	 l-p:0.16887032985687256
epoch£º271	 i:4 	 global-step:5424	 l-p:0.13838353753089905
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12122775614261627
epoch£º271	 i:6 	 global-step:5426	 l-p:-0.11978849023580551
epoch£º271	 i:7 	 global-step:5427	 l-p:0.1288655698299408
epoch£º271	 i:8 	 global-step:5428	 l-p:0.234013170003891
epoch£º271	 i:9 	 global-step:5429	 l-p:0.13734035193920135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9952, 4.9944, 4.9943],
        [4.9952, 5.4821, 5.5795],
        [4.9952, 4.9964, 4.9929],
        [4.9952, 4.9943, 4.9948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.2938719689846039 
model_pd.l_d.mean(): -19.212461471557617 
model_pd.lagr.mean(): -18.918588638305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9314], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.2938719689846039
epoch£º272	 i:1 	 global-step:5441	 l-p:0.1365991234779358
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11329837888479233
epoch£º272	 i:3 	 global-step:5443	 l-p:0.08178596198558807
epoch£º272	 i:4 	 global-step:5444	 l-p:0.13547953963279724
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12132086604833603
epoch£º272	 i:6 	 global-step:5446	 l-p:0.16656136512756348
epoch£º272	 i:7 	 global-step:5447	 l-p:0.12424466758966446
epoch£º272	 i:8 	 global-step:5448	 l-p:0.1505809873342514
epoch£º272	 i:9 	 global-step:5449	 l-p:0.13227683305740356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0223, 5.0896, 5.0368],
        [5.0223, 5.0219, 5.0223],
        [5.0223, 5.0253, 5.0195],
        [5.0223, 5.0217, 5.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.14185570180416107 
model_pd.l_d.mean(): -18.519275665283203 
model_pd.lagr.mean(): -18.37742042541504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5394], device='cuda:0')), ('power', tensor([-19.2726], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.14185570180416107
epoch£º273	 i:1 	 global-step:5461	 l-p:0.1320277601480484
epoch£º273	 i:2 	 global-step:5462	 l-p:0.13623575866222382
epoch£º273	 i:3 	 global-step:5463	 l-p:0.3429632782936096
epoch£º273	 i:4 	 global-step:5464	 l-p:0.1195235624909401
epoch£º273	 i:5 	 global-step:5465	 l-p:0.08032267540693283
epoch£º273	 i:6 	 global-step:5466	 l-p:0.14396455883979797
epoch£º273	 i:7 	 global-step:5467	 l-p:0.135268434882164
epoch£º273	 i:8 	 global-step:5468	 l-p:0.1356997787952423
epoch£º273	 i:9 	 global-step:5469	 l-p:0.29274243116378784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8274, 5.0227, 4.9690],
        [4.8274, 5.5095, 5.7844],
        [4.8274, 4.8266, 4.8273],
        [4.8274, 5.5077, 5.7810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.054802168160676956 
model_pd.l_d.mean(): -20.69412612915039 
model_pd.lagr.mean(): -20.639324188232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4008], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.054802168160676956
epoch£º274	 i:1 	 global-step:5481	 l-p:0.13886068761348724
epoch£º274	 i:2 	 global-step:5482	 l-p:0.25189071893692017
epoch£º274	 i:3 	 global-step:5483	 l-p:-0.2432786524295807
epoch£º274	 i:4 	 global-step:5484	 l-p:0.15412960946559906
epoch£º274	 i:5 	 global-step:5485	 l-p:0.14366105198860168
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12708310782909393
epoch£º274	 i:7 	 global-step:5487	 l-p:0.12615133821964264
epoch£º274	 i:8 	 global-step:5488	 l-p:-0.6082085371017456
epoch£º274	 i:9 	 global-step:5489	 l-p:0.10900141298770905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 5.1849, 5.1569],
        [4.9125, 5.2080, 5.1908],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 4.9110, 4.9116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.16206717491149902 
model_pd.l_d.mean(): -20.77850914001465 
model_pd.lagr.mean(): -20.61644172668457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.16206717491149902
epoch£º275	 i:1 	 global-step:5501	 l-p:0.1341918408870697
epoch£º275	 i:2 	 global-step:5502	 l-p:0.12437798827886581
epoch£º275	 i:3 	 global-step:5503	 l-p:0.216753289103508
epoch£º275	 i:4 	 global-step:5504	 l-p:0.1866174340248108
epoch£º275	 i:5 	 global-step:5505	 l-p:0.1739410012960434
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11600824445486069
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10471250116825104
epoch£º275	 i:8 	 global-step:5508	 l-p:0.150125190615654
epoch£º275	 i:9 	 global-step:5509	 l-p:0.1275109052658081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.8176, 6.0676],
        [5.1271, 5.1271, 5.1263],
        [5.1271, 5.1271, 5.1271],
        [5.1271, 5.1269, 5.1271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12380992621183395 
model_pd.l_d.mean(): -20.774250030517578 
model_pd.lagr.mean(): -20.650440216064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3680], device='cuda:0')), ('power', tensor([-21.3771], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12380992621183395
epoch£º276	 i:1 	 global-step:5521	 l-p:0.14502371847629547
epoch£º276	 i:2 	 global-step:5522	 l-p:0.1440570205450058
epoch£º276	 i:3 	 global-step:5523	 l-p:0.1332983374595642
epoch£º276	 i:4 	 global-step:5524	 l-p:0.15606924891471863
epoch£º276	 i:5 	 global-step:5525	 l-p:0.17251712083816528
epoch£º276	 i:6 	 global-step:5526	 l-p:0.1378840208053589
epoch£º276	 i:7 	 global-step:5527	 l-p:0.11647133529186249
epoch£º276	 i:8 	 global-step:5528	 l-p:0.11094430834054947
epoch£º276	 i:9 	 global-step:5529	 l-p:0.15905451774597168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9983, 5.4807, 5.5747],
        [4.9983, 5.5675, 5.7278],
        [4.9983, 4.9983, 4.9983],
        [4.9983, 5.3711, 5.3922]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.14515872299671173 
model_pd.l_d.mean(): -20.201637268066406 
model_pd.lagr.mean(): -20.05647850036621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-20.9100], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.14515872299671173
epoch£º277	 i:1 	 global-step:5541	 l-p:0.12425204366445541
epoch£º277	 i:2 	 global-step:5542	 l-p:0.12559276819229126
epoch£º277	 i:3 	 global-step:5543	 l-p:0.14565825462341309
epoch£º277	 i:4 	 global-step:5544	 l-p:-0.2064816951751709
epoch£º277	 i:5 	 global-step:5545	 l-p:0.10350963473320007
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14581215381622314
epoch£º277	 i:7 	 global-step:5547	 l-p:0.035056222230196
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13850925862789154
epoch£º277	 i:9 	 global-step:5549	 l-p:0.14383183419704437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8265, 4.8249, 4.8262],
        [4.8265, 4.8242, 4.8237],
        [4.8265, 4.8369, 4.8173],
        [4.8265, 5.3123, 5.4225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.1357344090938568 
model_pd.l_d.mean(): -19.174793243408203 
model_pd.lagr.mean(): -19.039058685302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.9367], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.1357344090938568
epoch£º278	 i:1 	 global-step:5561	 l-p:0.17827850580215454
epoch£º278	 i:2 	 global-step:5562	 l-p:0.07298065721988678
epoch£º278	 i:3 	 global-step:5563	 l-p:0.12317584455013275
epoch£º278	 i:4 	 global-step:5564	 l-p:0.1778782606124878
epoch£º278	 i:5 	 global-step:5565	 l-p:0.12192607671022415
epoch£º278	 i:6 	 global-step:5566	 l-p:0.15087948739528656
epoch£º278	 i:7 	 global-step:5567	 l-p:0.12226232141256332
epoch£º278	 i:8 	 global-step:5568	 l-p:0.18435096740722656
epoch£º278	 i:9 	 global-step:5569	 l-p:0.1289307326078415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0364, 5.0798, 5.0374],
        [5.0364, 5.0378, 5.0335],
        [5.0364, 5.5247, 5.6203],
        [5.0364, 5.0704, 5.0341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.1345261186361313 
model_pd.l_d.mean(): -20.484111785888672 
model_pd.lagr.mean(): -20.349586486816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.1462], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.1345261186361313
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21252833306789398
epoch£º279	 i:2 	 global-step:5582	 l-p:0.10726537555456161
epoch£º279	 i:3 	 global-step:5583	 l-p:0.09274210035800934
epoch£º279	 i:4 	 global-step:5584	 l-p:0.13641737401485443
epoch£º279	 i:5 	 global-step:5585	 l-p:0.15300734341144562
epoch£º279	 i:6 	 global-step:5586	 l-p:0.15079191327095032
epoch£º279	 i:7 	 global-step:5587	 l-p:0.12356416136026382
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13840946555137634
epoch£º279	 i:9 	 global-step:5589	 l-p:0.1360638588666916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0596, 5.0598],
        [5.0598, 5.0612, 5.0571],
        [5.0598, 5.0715, 5.0546],
        [5.0598, 5.0969, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.1250229924917221 
model_pd.l_d.mean(): -20.04180335998535 
model_pd.lagr.mean(): -19.916780471801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-20.7016], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.1250229924917221
epoch£º280	 i:1 	 global-step:5601	 l-p:0.1488095372915268
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14775069057941437
epoch£º280	 i:3 	 global-step:5603	 l-p:0.13088008761405945
epoch£º280	 i:4 	 global-step:5604	 l-p:0.13280771672725677
epoch£º280	 i:5 	 global-step:5605	 l-p:0.10554162412881851
epoch£º280	 i:6 	 global-step:5606	 l-p:0.09655748307704926
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12395510077476501
epoch£º280	 i:8 	 global-step:5608	 l-p:0.13273684680461884
epoch£º280	 i:9 	 global-step:5609	 l-p:0.09748189896345139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8522, 4.9434, 4.8800],
        [4.8522, 4.8520, 4.8522],
        [4.8522, 4.8499, 4.8513],
        [4.8522, 4.8558, 4.8442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.09651296585798264 
model_pd.l_d.mean(): -20.61924171447754 
model_pd.lagr.mean(): -20.522727966308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.3306], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.09651296585798264
epoch£º281	 i:1 	 global-step:5621	 l-p:0.09925911575555801
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15712416172027588
epoch£º281	 i:3 	 global-step:5623	 l-p:0.3320666551589966
epoch£º281	 i:4 	 global-step:5624	 l-p:0.10363954305648804
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11883733421564102
epoch£º281	 i:6 	 global-step:5626	 l-p:0.1637696921825409
epoch£º281	 i:7 	 global-step:5627	 l-p:0.1379396766424179
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13264857232570648
epoch£º281	 i:9 	 global-step:5629	 l-p:0.12104999274015427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9773, 4.9855, 4.9704],
        [4.9773, 5.1079, 5.0417],
        [4.9773, 4.9895, 4.9699],
        [4.9773, 5.6460, 5.8926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.09968576580286026 
model_pd.l_d.mean(): -18.578153610229492 
model_pd.lagr.mean(): -18.47846794128418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5939], device='cuda:0')), ('power', tensor([-19.3878], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.09968576580286026
epoch£º282	 i:1 	 global-step:5641	 l-p:0.1355687826871872
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18353843688964844
epoch£º282	 i:3 	 global-step:5643	 l-p:0.2197140008211136
epoch£º282	 i:4 	 global-step:5644	 l-p:0.1769660860300064
epoch£º282	 i:5 	 global-step:5645	 l-p:0.14839611947536469
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11336778104305267
epoch£º282	 i:7 	 global-step:5647	 l-p:0.1495729386806488
epoch£º282	 i:8 	 global-step:5648	 l-p:0.13100749254226685
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12774108350276947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0740, 5.3293, 5.2884],
        [5.0740, 5.7697, 6.0299],
        [5.0740, 5.3293, 5.2883],
        [5.0740, 5.0733, 5.0739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.0844595730304718 
model_pd.l_d.mean(): -20.070396423339844 
model_pd.lagr.mean(): -19.985937118530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-20.7644], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.0844595730304718
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12172410637140274
epoch£º283	 i:2 	 global-step:5662	 l-p:0.12639610469341278
epoch£º283	 i:3 	 global-step:5663	 l-p:0.1427799016237259
epoch£º283	 i:4 	 global-step:5664	 l-p:0.19764328002929688
epoch£º283	 i:5 	 global-step:5665	 l-p:0.13367171585559845
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12401832640171051
epoch£º283	 i:7 	 global-step:5667	 l-p:0.3641042113304138
epoch£º283	 i:8 	 global-step:5668	 l-p:0.17825160920619965
epoch£º283	 i:9 	 global-step:5669	 l-p:0.34325942397117615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0046, 5.0029, 5.0038],
        [5.0046, 5.0034, 5.0043],
        [5.0046, 5.0045, 5.0046],
        [5.0046, 5.1477, 5.0814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.1825154572725296 
model_pd.l_d.mean(): -20.75795555114746 
model_pd.lagr.mean(): -20.575439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.4069], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.1825154572725296
epoch£º284	 i:1 	 global-step:5681	 l-p:0.1748896688222885
epoch£º284	 i:2 	 global-step:5682	 l-p:0.13659559190273285
epoch£º284	 i:3 	 global-step:5683	 l-p:0.15289218723773956
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12271939963102341
epoch£º284	 i:5 	 global-step:5685	 l-p:0.030006341636180878
epoch£º284	 i:6 	 global-step:5686	 l-p:0.1278688758611679
epoch£º284	 i:7 	 global-step:5687	 l-p:0.1385963261127472
epoch£º284	 i:8 	 global-step:5688	 l-p:0.11706012487411499
epoch£º284	 i:9 	 global-step:5689	 l-p:0.1015322208404541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 5.1175, 5.1175],
        [5.1175, 5.1165, 5.1170],
        [5.1175, 5.1295, 5.1122],
        [5.1175, 5.3606, 5.3139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1117585226893425 
model_pd.l_d.mean(): -20.456348419189453 
model_pd.lagr.mean(): -20.344589233398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4089], device='cuda:0')), ('power', tensor([-21.0975], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.1117585226893425
epoch£º285	 i:1 	 global-step:5701	 l-p:0.12621694803237915
epoch£º285	 i:2 	 global-step:5702	 l-p:0.1350867599248886
epoch£º285	 i:3 	 global-step:5703	 l-p:0.1220228374004364
epoch£º285	 i:4 	 global-step:5704	 l-p:0.1280943602323532
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.3993166387081146
epoch£º285	 i:6 	 global-step:5706	 l-p:0.03739537298679352
epoch£º285	 i:7 	 global-step:5707	 l-p:-0.0398980975151062
epoch£º285	 i:8 	 global-step:5708	 l-p:0.19223353266716003
epoch£º285	 i:9 	 global-step:5709	 l-p:0.12695787847042084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9446, 4.9422, 4.9430],
        [4.9446, 4.9446, 4.9446],
        [4.9446, 4.9446, 4.9446],
        [4.9446, 4.9440, 4.9446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.13077841699123383 
model_pd.l_d.mean(): -20.62534523010254 
model_pd.lagr.mean(): -20.494565963745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3010], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.13077841699123383
epoch£º286	 i:1 	 global-step:5721	 l-p:0.1318010538816452
epoch£º286	 i:2 	 global-step:5722	 l-p:0.2587124705314636
epoch£º286	 i:3 	 global-step:5723	 l-p:0.10383805632591248
epoch£º286	 i:4 	 global-step:5724	 l-p:0.07641248404979706
epoch£º286	 i:5 	 global-step:5725	 l-p:0.007955655455589294
epoch£º286	 i:6 	 global-step:5726	 l-p:0.13086733222007751
epoch£º286	 i:7 	 global-step:5727	 l-p:0.13495151698589325
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14067360758781433
epoch£º286	 i:9 	 global-step:5729	 l-p:0.13850149512290955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0042, 5.2002, 5.1418],
        [5.0042, 5.2785, 5.2473],
        [5.0042, 5.0297, 4.9968],
        [5.0042, 5.0030, 5.0011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.1399042308330536 
model_pd.l_d.mean(): -20.427669525146484 
model_pd.lagr.mean(): -20.287765502929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.1089], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.1399042308330536
epoch£º287	 i:1 	 global-step:5741	 l-p:0.12817712128162384
epoch£º287	 i:2 	 global-step:5742	 l-p:0.1828031837940216
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.025033101439476013
epoch£º287	 i:4 	 global-step:5744	 l-p:0.13319110870361328
epoch£º287	 i:5 	 global-step:5745	 l-p:0.14534516632556915
epoch£º287	 i:6 	 global-step:5746	 l-p:0.13910174369812012
epoch£º287	 i:7 	 global-step:5747	 l-p:0.14264735579490662
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14321079850196838
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.03931599110364914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9343, 4.9316, 4.9329],
        [4.9343, 4.9343, 4.9343],
        [4.9343, 4.9339, 4.9343],
        [4.9343, 5.0732, 5.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.19525215029716492 
model_pd.l_d.mean(): -20.571918487548828 
model_pd.lagr.mean(): -20.376667022705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.19525215029716492
epoch£º288	 i:1 	 global-step:5761	 l-p:0.14523424208164215
epoch£º288	 i:2 	 global-step:5762	 l-p:0.14765065908432007
epoch£º288	 i:3 	 global-step:5763	 l-p:0.5839135646820068
epoch£º288	 i:4 	 global-step:5764	 l-p:0.14082489907741547
epoch£º288	 i:5 	 global-step:5765	 l-p:0.1325242668390274
epoch£º288	 i:6 	 global-step:5766	 l-p:0.14808110892772675
epoch£º288	 i:7 	 global-step:5767	 l-p:0.12429959326982498
epoch£º288	 i:8 	 global-step:5768	 l-p:0.09988407045602798
epoch£º288	 i:9 	 global-step:5769	 l-p:0.4008820652961731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9869, 5.3255, 5.3270],
        [4.9869, 4.9866, 4.9869],
        [4.9869, 4.9867, 4.9869],
        [4.9869, 4.9846, 4.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11736751347780228 
model_pd.l_d.mean(): -19.02417755126953 
model_pd.lagr.mean(): -18.906810760498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5396], device='cuda:0')), ('power', tensor([-19.7833], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11736751347780228
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09225087612867355
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8263351917266846
epoch£º289	 i:3 	 global-step:5783	 l-p:0.13661032915115356
epoch£º289	 i:4 	 global-step:5784	 l-p:0.15133588016033173
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7828860878944397
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17849773168563843
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14371667802333832
epoch£º289	 i:8 	 global-step:5788	 l-p:0.12316887825727463
epoch£º289	 i:9 	 global-step:5789	 l-p:0.14203521609306335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 4.9905, 4.9905],
        [4.9905, 4.9894, 4.9903],
        [4.9905, 4.9905, 4.9905],
        [4.9905, 5.0120, 4.9811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.15303821861743927 
model_pd.l_d.mean(): -20.612634658813477 
model_pd.lagr.mean(): -20.459596633911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.2681], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.15303821861743927
epoch£º290	 i:1 	 global-step:5801	 l-p:0.12861035764217377
epoch£º290	 i:2 	 global-step:5802	 l-p:0.12778346240520477
epoch£º290	 i:3 	 global-step:5803	 l-p:0.9658832550048828
epoch£º290	 i:4 	 global-step:5804	 l-p:0.12943334877490997
epoch£º290	 i:5 	 global-step:5805	 l-p:0.12769176065921783
epoch£º290	 i:6 	 global-step:5806	 l-p:0.14098630845546722
epoch£º290	 i:7 	 global-step:5807	 l-p:0.2168552726507187
epoch£º290	 i:8 	 global-step:5808	 l-p:0.1510615348815918
epoch£º290	 i:9 	 global-step:5809	 l-p:0.14817187190055847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8546, 4.8820, 4.8421],
        [4.8546, 5.0853, 5.0404],
        [4.8546, 4.8509, 4.8529],
        [4.8546, 4.8545, 4.8546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.09717355668544769 
model_pd.l_d.mean(): -20.9140625 
model_pd.lagr.mean(): -20.8168888092041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.5845], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.09717355668544769
epoch£º291	 i:1 	 global-step:5821	 l-p:0.1170828565955162
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12082244455814362
epoch£º291	 i:3 	 global-step:5823	 l-p:0.16658052802085876
epoch£º291	 i:4 	 global-step:5824	 l-p:0.12981438636779785
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.13650116324424744
epoch£º291	 i:6 	 global-step:5826	 l-p:0.17807050049304962
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16347339749336243
epoch£º291	 i:8 	 global-step:5828	 l-p:0.09888023883104324
epoch£º291	 i:9 	 global-step:5829	 l-p:0.1325400471687317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1124, 5.1124, 5.1124],
        [5.1124, 5.1111, 5.1121],
        [5.1124, 5.1122, 5.1124],
        [5.1124, 5.1124, 5.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.12873521447181702 
model_pd.l_d.mean(): -20.044719696044922 
model_pd.lagr.mean(): -19.915985107421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-20.7065], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.12873521447181702
epoch£º292	 i:1 	 global-step:5841	 l-p:0.12402542680501938
epoch£º292	 i:2 	 global-step:5842	 l-p:0.1181144118309021
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11644270271062851
epoch£º292	 i:4 	 global-step:5844	 l-p:0.10087177902460098
epoch£º292	 i:5 	 global-step:5845	 l-p:0.1557733565568924
epoch£º292	 i:6 	 global-step:5846	 l-p:0.1152753010392189
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18496555089950562
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12896797060966492
epoch£º292	 i:9 	 global-step:5849	 l-p:0.1269388198852539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0554, 5.1135, 5.0594],
        [5.0554, 5.1108, 5.0579],
        [5.0554, 5.0622, 5.0473],
        [5.0554, 5.0580, 5.0489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12848801910877228 
model_pd.l_d.mean(): -20.611303329467773 
model_pd.lagr.mean(): -20.48281478881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4025], device='cuda:0')), ('power', tensor([-21.2477], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12848801910877228
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11645077168941498
epoch£º293	 i:2 	 global-step:5862	 l-p:0.2900146245956421
epoch£º293	 i:3 	 global-step:5863	 l-p:-7.063501834869385
epoch£º293	 i:4 	 global-step:5864	 l-p:0.1326766312122345
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12385362386703491
epoch£º293	 i:6 	 global-step:5866	 l-p:-0.1457495242357254
epoch£º293	 i:7 	 global-step:5867	 l-p:0.12844005227088928
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14894221723079681
epoch£º293	 i:9 	 global-step:5869	 l-p:0.2639259397983551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9377, 4.9451, 4.9260],
        [4.9377, 5.2143, 5.1858],
        [4.9377, 5.2856, 5.2947],
        [4.9377, 5.0054, 4.9449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.16290155053138733 
model_pd.l_d.mean(): -19.48529815673828 
model_pd.lagr.mean(): -19.322397232055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.2075], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.16290155053138733
epoch£º294	 i:1 	 global-step:5881	 l-p:0.1438758820295334
epoch£º294	 i:2 	 global-step:5882	 l-p:-4.355845928192139
epoch£º294	 i:3 	 global-step:5883	 l-p:0.12894901633262634
epoch£º294	 i:4 	 global-step:5884	 l-p:0.11628782749176025
epoch£º294	 i:5 	 global-step:5885	 l-p:0.5169561505317688
epoch£º294	 i:6 	 global-step:5886	 l-p:0.15592285990715027
epoch£º294	 i:7 	 global-step:5887	 l-p:0.11370448768138885
epoch£º294	 i:8 	 global-step:5888	 l-p:0.12960663437843323
epoch£º294	 i:9 	 global-step:5889	 l-p:-0.23723742365837097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9659, 4.9916, 4.9546],
        [4.9659, 4.9658, 4.9659],
        [4.9659, 4.9653, 4.9659],
        [4.9659, 4.9659, 4.9659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.140809565782547 
model_pd.l_d.mean(): -20.867055892944336 
model_pd.lagr.mean(): -20.726245880126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.5156], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.140809565782547
epoch£º295	 i:1 	 global-step:5901	 l-p:0.12568336725234985
epoch£º295	 i:2 	 global-step:5902	 l-p:0.027415666729211807
epoch£º295	 i:3 	 global-step:5903	 l-p:-0.026668790727853775
epoch£º295	 i:4 	 global-step:5904	 l-p:0.05216483026742935
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13871970772743225
epoch£º295	 i:6 	 global-step:5906	 l-p:0.1544971466064453
epoch£º295	 i:7 	 global-step:5907	 l-p:0.12524065375328064
epoch£º295	 i:8 	 global-step:5908	 l-p:0.1386476308107376
epoch£º295	 i:9 	 global-step:5909	 l-p:0.14043939113616943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0515, 5.0952, 5.0475],
        [5.0515, 5.0498, 5.0511],
        [5.0515, 5.0512, 5.0515],
        [5.0515, 5.0515, 5.0515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.16300903260707855 
model_pd.l_d.mean(): -20.47294807434082 
model_pd.lagr.mean(): -20.309938430786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4275], device='cuda:0')), ('power', tensor([-21.1333], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.16300903260707855
epoch£º296	 i:1 	 global-step:5921	 l-p:0.13326123356819153
epoch£º296	 i:2 	 global-step:5922	 l-p:0.13685882091522217
epoch£º296	 i:3 	 global-step:5923	 l-p:0.19994589686393738
epoch£º296	 i:4 	 global-step:5924	 l-p:0.1771273910999298
epoch£º296	 i:5 	 global-step:5925	 l-p:0.1065429076552391
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12545143067836761
epoch£º296	 i:7 	 global-step:5927	 l-p:0.1370769888162613
epoch£º296	 i:8 	 global-step:5928	 l-p:0.1336115598678589
epoch£º296	 i:9 	 global-step:5929	 l-p:0.10468777269124985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9698, 5.6610, 5.9290],
        [4.9698, 4.9874, 4.9569],
        [4.9698, 4.9664, 4.9682],
        [4.9698, 4.9695, 4.9698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.5473794937133789 
model_pd.l_d.mean(): -19.515121459960938 
model_pd.lagr.mean(): -18.967742919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.2616], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.5473794937133789
epoch£º297	 i:1 	 global-step:5941	 l-p:0.12502488493919373
epoch£º297	 i:2 	 global-step:5942	 l-p:0.13434557616710663
epoch£º297	 i:3 	 global-step:5943	 l-p:0.11104173213243484
epoch£º297	 i:4 	 global-step:5944	 l-p:0.13559068739414215
epoch£º297	 i:5 	 global-step:5945	 l-p:0.14317891001701355
epoch£º297	 i:6 	 global-step:5946	 l-p:-0.8860841989517212
epoch£º297	 i:7 	 global-step:5947	 l-p:0.09995981305837631
epoch£º297	 i:8 	 global-step:5948	 l-p:0.12287310510873795
epoch£º297	 i:9 	 global-step:5949	 l-p:0.15814842283725739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7991, 4.7991, 4.7991],
        [4.7991, 4.7934, 4.7899],
        [4.7991, 4.7990, 4.7991],
        [4.7991, 5.6078, 6.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.1404288411140442 
model_pd.l_d.mean(): -19.8391170501709 
model_pd.lagr.mean(): -19.698688507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5305], device='cuda:0')), ('power', tensor([-20.5978], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.1404288411140442
epoch£º298	 i:1 	 global-step:5961	 l-p:0.6375563144683838
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15388809144496918
epoch£º298	 i:3 	 global-step:5963	 l-p:0.12986387312412262
epoch£º298	 i:4 	 global-step:5964	 l-p:0.19711460173130035
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12739445269107819
epoch£º298	 i:6 	 global-step:5966	 l-p:-0.2473074346780777
epoch£º298	 i:7 	 global-step:5967	 l-p:0.15202845633029938
epoch£º298	 i:8 	 global-step:5968	 l-p:0.12999403476715088
epoch£º298	 i:9 	 global-step:5969	 l-p:0.29855087399482727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0139, 5.0110, 5.0129],
        [5.0139, 5.0220, 5.0022],
        [5.0139, 5.0642, 5.0108],
        [5.0139, 5.1473, 5.0771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12206844240427017 
model_pd.l_d.mean(): -20.472898483276367 
model_pd.lagr.mean(): -20.350830078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-21.1511], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12206844240427017
epoch£º299	 i:1 	 global-step:5981	 l-p:0.1743214726448059
epoch£º299	 i:2 	 global-step:5982	 l-p:0.24806128442287445
epoch£º299	 i:3 	 global-step:5983	 l-p:0.1285732090473175
epoch£º299	 i:4 	 global-step:5984	 l-p:0.132839173078537
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14669257402420044
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1329367458820343
epoch£º299	 i:7 	 global-step:5987	 l-p:0.2901473045349121
epoch£º299	 i:8 	 global-step:5988	 l-p:0.09505388140678406
epoch£º299	 i:9 	 global-step:5989	 l-p:0.20692020654678345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0229, 5.7162, 5.9805],
        [5.0229, 5.0198, 5.0181],
        [5.0229, 5.0229, 5.0229],
        [5.0229, 5.0196, 5.0184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.16598090529441833 
model_pd.l_d.mean(): -20.656347274780273 
model_pd.lagr.mean(): -20.490365982055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.3102], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.16598090529441833
epoch£º300	 i:1 	 global-step:6001	 l-p:0.21646791696548462
epoch£º300	 i:2 	 global-step:6002	 l-p:0.1334065943956375
epoch£º300	 i:3 	 global-step:6003	 l-p:0.13363300263881683
epoch£º300	 i:4 	 global-step:6004	 l-p:0.1341158002614975
epoch£º300	 i:5 	 global-step:6005	 l-p:0.12811483442783356
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11988542228937149
epoch£º300	 i:7 	 global-step:6007	 l-p:0.3095892369747162
epoch£º300	 i:8 	 global-step:6008	 l-p:0.12937031686306
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14313535392284393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9377, 4.9406, 4.9247],
        [4.9377, 4.9377, 4.9377],
        [4.9377, 5.0852, 5.0159],
        [4.9377, 4.9377, 4.9377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.058398324996232986 
model_pd.l_d.mean(): -19.231918334960938 
model_pd.lagr.mean(): -19.173519134521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5223], device='cuda:0')), ('power', tensor([-19.9756], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.058398324996232986
epoch£º301	 i:1 	 global-step:6021	 l-p:0.12727992236614227
epoch£º301	 i:2 	 global-step:6022	 l-p:0.1192379891872406
epoch£º301	 i:3 	 global-step:6023	 l-p:0.14934198558330536
epoch£º301	 i:4 	 global-step:6024	 l-p:0.10254012048244476
epoch£º301	 i:5 	 global-step:6025	 l-p:0.14144353568553925
epoch£º301	 i:6 	 global-step:6026	 l-p:0.1447882503271103
epoch£º301	 i:7 	 global-step:6027	 l-p:0.01756063476204872
epoch£º301	 i:8 	 global-step:6028	 l-p:0.7024155855178833
epoch£º301	 i:9 	 global-step:6029	 l-p:0.09091013669967651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8460, 4.8512, 4.8279],
        [4.8460, 4.8454, 4.8460],
        [4.8460, 5.4194, 5.5990],
        [4.8460, 4.8455, 4.8460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.14680954813957214 
model_pd.l_d.mean(): -20.742305755615234 
model_pd.lagr.mean(): -20.595497131347656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.4385], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.14680954813957214
epoch£º302	 i:1 	 global-step:6041	 l-p:0.1870872527360916
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12256300449371338
epoch£º302	 i:3 	 global-step:6043	 l-p:0.07790027558803558
epoch£º302	 i:4 	 global-step:6044	 l-p:0.14936919510364532
epoch£º302	 i:5 	 global-step:6045	 l-p:0.13531015813350677
epoch£º302	 i:6 	 global-step:6046	 l-p:0.24835240840911865
epoch£º302	 i:7 	 global-step:6047	 l-p:0.1464027613401413
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14350679516792297
epoch£º302	 i:9 	 global-step:6049	 l-p:-0.119735486805439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9055, 4.9055, 4.9055],
        [4.9055, 4.9093, 4.8898],
        [4.9055, 5.7123, 6.0948],
        [4.9055, 4.9384, 4.8908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.13937585055828094 
model_pd.l_d.mean(): -19.162534713745117 
model_pd.lagr.mean(): -19.02315902709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.13937585055828094
epoch£º303	 i:1 	 global-step:6061	 l-p:0.16718652844429016
epoch£º303	 i:2 	 global-step:6062	 l-p:0.138127863407135
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.6777768731117249
epoch£º303	 i:4 	 global-step:6064	 l-p:0.20238590240478516
epoch£º303	 i:5 	 global-step:6065	 l-p:0.0914691612124443
epoch£º303	 i:6 	 global-step:6066	 l-p:0.13700099289417267
epoch£º303	 i:7 	 global-step:6067	 l-p:0.13158181309700012
epoch£º303	 i:8 	 global-step:6068	 l-p:0.13905641436576843
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09800189733505249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1354, 5.1354, 5.1354],
        [5.1354, 5.1354, 5.1354],
        [5.1354, 5.1350, 5.1354],
        [5.1354, 5.1353, 5.1354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12617941200733185 
model_pd.l_d.mean(): -19.068735122680664 
model_pd.lagr.mean(): -18.942556381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-19.7427], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12617941200733185
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11136993020772934
epoch£º304	 i:2 	 global-step:6082	 l-p:0.12761838734149933
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14276795089244843
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15873108804225922
epoch£º304	 i:5 	 global-step:6085	 l-p:0.1686539351940155
epoch£º304	 i:6 	 global-step:6086	 l-p:0.07513013482093811
epoch£º304	 i:7 	 global-step:6087	 l-p:0.14923925697803497
epoch£º304	 i:8 	 global-step:6088	 l-p:0.12580639123916626
epoch£º304	 i:9 	 global-step:6089	 l-p:0.1314244568347931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1760, 5.1220],
        [5.1249, 5.1248, 5.1248],
        [5.1249, 5.8272, 6.0890],
        [5.1249, 5.1287, 5.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.1336079239845276 
model_pd.l_d.mean(): -20.151153564453125 
model_pd.lagr.mean(): -20.017545700073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-20.8263], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.1336079239845276
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12808473408222198
epoch£º305	 i:2 	 global-step:6102	 l-p:0.09282781183719635
epoch£º305	 i:3 	 global-step:6103	 l-p:0.18333034217357635
epoch£º305	 i:4 	 global-step:6104	 l-p:0.18401074409484863
epoch£º305	 i:5 	 global-step:6105	 l-p:0.1267383098602295
epoch£º305	 i:6 	 global-step:6106	 l-p:0.1318437159061432
epoch£º305	 i:7 	 global-step:6107	 l-p:0.17742255330085754
epoch£º305	 i:8 	 global-step:6108	 l-p:0.112718865275383
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11281263828277588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1129, 5.1129, 5.1129],
        [5.1129, 5.7846, 6.0210],
        [5.1129, 5.1129, 5.1129],
        [5.1129, 5.2188, 5.1479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.1572641134262085 
model_pd.l_d.mean(): -19.032516479492188 
model_pd.lagr.mean(): -18.87525177001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5016], device='cuda:0')), ('power', tensor([-19.7529], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.1572641134262085
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11841477453708649
epoch£º306	 i:2 	 global-step:6122	 l-p:0.12553861737251282
epoch£º306	 i:3 	 global-step:6123	 l-p:0.09412101656198502
epoch£º306	 i:4 	 global-step:6124	 l-p:0.13849814236164093
epoch£º306	 i:5 	 global-step:6125	 l-p:0.13392756879329681
epoch£º306	 i:6 	 global-step:6126	 l-p:0.14442110061645508
epoch£º306	 i:7 	 global-step:6127	 l-p:0.11816855520009995
epoch£º306	 i:8 	 global-step:6128	 l-p:-0.0905412808060646
epoch£º306	 i:9 	 global-step:6129	 l-p:0.03499206155538559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9537, 4.9537, 4.9537],
        [4.9537, 4.9480, 4.9459],
        [4.9537, 5.3714, 5.4232],
        [4.9537, 4.9477, 4.9471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14339935779571533 
model_pd.l_d.mean(): -20.598785400390625 
model_pd.lagr.mean(): -20.455385208129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4503], device='cuda:0')), ('power', tensor([-21.2838], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14339935779571533
epoch£º307	 i:1 	 global-step:6141	 l-p:0.1346116065979004
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12127894163131714
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14151228964328766
epoch£º307	 i:4 	 global-step:6144	 l-p:0.1410454511642456
epoch£º307	 i:5 	 global-step:6145	 l-p:0.11213008314371109
epoch£º307	 i:6 	 global-step:6146	 l-p:-0.05014796182513237
epoch£º307	 i:7 	 global-step:6147	 l-p:0.2741602957248688
epoch£º307	 i:8 	 global-step:6148	 l-p:0.15613195300102234
epoch£º307	 i:9 	 global-step:6149	 l-p:0.1296434998512268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8487, 4.8445, 4.8477],
        [4.8487, 4.8432, 4.8468],
        [4.8487, 4.8409, 4.8375],
        [4.8487, 4.9192, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.1322658807039261 
model_pd.l_d.mean(): -20.99280548095703 
model_pd.lagr.mean(): -20.86054039001465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.6594], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.1322658807039261
epoch£º308	 i:1 	 global-step:6161	 l-p:0.09652608633041382
epoch£º308	 i:2 	 global-step:6162	 l-p:0.14248648285865784
epoch£º308	 i:3 	 global-step:6163	 l-p:0.15824776887893677
epoch£º308	 i:4 	 global-step:6164	 l-p:0.15516285598278046
epoch£º308	 i:5 	 global-step:6165	 l-p:0.8286182284355164
epoch£º308	 i:6 	 global-step:6166	 l-p:0.06155000999569893
epoch£º308	 i:7 	 global-step:6167	 l-p:0.29276973009109497
epoch£º308	 i:8 	 global-step:6168	 l-p:0.1291363686323166
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13179351389408112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9778, 4.9487],
        [4.9677, 5.2055, 5.1576],
        [4.9677, 4.9676, 4.9677],
        [4.9677, 5.3406, 5.3624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.17572146654129028 
model_pd.l_d.mean(): -19.05775260925293 
model_pd.lagr.mean(): -18.882030487060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5267], device='cuda:0')), ('power', tensor([-19.8040], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.17572146654129028
epoch£º309	 i:1 	 global-step:6181	 l-p:0.3626810610294342
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14014627039432526
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12193924933671951
epoch£º309	 i:4 	 global-step:6184	 l-p:0.11997274309396744
epoch£º309	 i:5 	 global-step:6185	 l-p:0.1315300315618515
epoch£º309	 i:6 	 global-step:6186	 l-p:0.13492096960544586
epoch£º309	 i:7 	 global-step:6187	 l-p:0.16579888761043549
epoch£º309	 i:8 	 global-step:6188	 l-p:0.15374228358268738
epoch£º309	 i:9 	 global-step:6189	 l-p:0.1293954849243164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.1585, 5.1603],
        [5.1610, 5.1609, 5.1609],
        [5.1610, 5.4175, 5.3716],
        [5.1610, 5.7042, 5.8320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.15380732715129852 
model_pd.l_d.mean(): -20.70897102355957 
model_pd.lagr.mean(): -20.555164337158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3758], device='cuda:0')), ('power', tensor([-21.3191], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.15380732715129852
epoch£º310	 i:1 	 global-step:6201	 l-p:-0.006572661455720663
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12052857875823975
epoch£º310	 i:3 	 global-step:6203	 l-p:0.11358439922332764
epoch£º310	 i:4 	 global-step:6204	 l-p:0.14468954503536224
epoch£º310	 i:5 	 global-step:6205	 l-p:0.13638272881507874
epoch£º310	 i:6 	 global-step:6206	 l-p:0.1522899866104126
epoch£º310	 i:7 	 global-step:6207	 l-p:0.13299179077148438
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10509851574897766
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13693702220916748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0439, 5.0878, 5.0332],
        [5.0439, 5.0841, 5.0317],
        [5.0439, 5.0527, 5.0272],
        [5.0439, 5.1119, 5.0466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.1171499639749527 
model_pd.l_d.mean(): -19.068416595458984 
model_pd.lagr.mean(): -18.95126724243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4955], device='cuda:0')), ('power', tensor([-19.7829], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.1171499639749527
epoch£º311	 i:1 	 global-step:6221	 l-p:0.283936470746994
epoch£º311	 i:2 	 global-step:6222	 l-p:0.22381484508514404
epoch£º311	 i:3 	 global-step:6223	 l-p:0.13373717665672302
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12389300018548965
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11507884413003922
epoch£º311	 i:6 	 global-step:6226	 l-p:0.18339207768440247
epoch£º311	 i:7 	 global-step:6227	 l-p:-0.020576348528265953
epoch£º311	 i:8 	 global-step:6228	 l-p:0.1472877860069275
epoch£º311	 i:9 	 global-step:6229	 l-p:0.13762064278125763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9731, 5.0137, 4.9584],
        [4.9731, 4.9701, 4.9725],
        [4.9731, 4.9731, 4.9731],
        [4.9731, 5.4439, 5.5318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): -0.01914362795650959 
model_pd.l_d.mean(): -20.766109466552734 
model_pd.lagr.mean(): -20.785253524780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4259], device='cuda:0')), ('power', tensor([-21.4281], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:-0.01914362795650959
epoch£º312	 i:1 	 global-step:6241	 l-p:0.11115226149559021
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10846473276615143
epoch£º312	 i:3 	 global-step:6243	 l-p:0.27352771162986755
epoch£º312	 i:4 	 global-step:6244	 l-p:0.14767003059387207
epoch£º312	 i:5 	 global-step:6245	 l-p:0.08407413214445114
epoch£º312	 i:6 	 global-step:6246	 l-p:0.16262385249137878
epoch£º312	 i:7 	 global-step:6247	 l-p:0.13305732607841492
epoch£º312	 i:8 	 global-step:6248	 l-p:0.13755033910274506
epoch£º312	 i:9 	 global-step:6249	 l-p:-0.03854016587138176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9665, 4.9592, 4.9588],
        [4.9665, 5.5546, 5.7348],
        [4.9665, 5.0059, 4.9507],
        [4.9665, 4.9620, 4.9651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.15064089000225067 
model_pd.l_d.mean(): -20.67906951904297 
model_pd.lagr.mean(): -20.52842903137207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.3518], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.15064089000225067
epoch£º313	 i:1 	 global-step:6261	 l-p:0.13287830352783203
epoch£º313	 i:2 	 global-step:6262	 l-p:0.2559150159358978
epoch£º313	 i:3 	 global-step:6263	 l-p:0.19050876796245575
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14570747315883636
epoch£º313	 i:5 	 global-step:6265	 l-p:0.11845050752162933
epoch£º313	 i:6 	 global-step:6266	 l-p:0.1698959469795227
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13224588334560394
epoch£º313	 i:8 	 global-step:6268	 l-p:0.07409235090017319
epoch£º313	 i:9 	 global-step:6269	 l-p:0.018218763172626495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9392, 4.9392],
        [4.9392, 4.9335, 4.9371],
        [4.9392, 4.9334, 4.9249],
        [4.9392, 4.9398, 4.9194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.037653230130672455 
model_pd.l_d.mean(): -19.995229721069336 
model_pd.lagr.mean(): -19.957576751708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5268], device='cuda:0')), ('power', tensor([-20.7519], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.037653230130672455
epoch£º314	 i:1 	 global-step:6281	 l-p:0.1402258723974228
epoch£º314	 i:2 	 global-step:6282	 l-p:0.15122491121292114
epoch£º314	 i:3 	 global-step:6283	 l-p:0.12579424679279327
epoch£º314	 i:4 	 global-step:6284	 l-p:1.1160480976104736
epoch£º314	 i:5 	 global-step:6285	 l-p:0.2455480843782425
epoch£º314	 i:6 	 global-step:6286	 l-p:0.13788549602031708
epoch£º314	 i:7 	 global-step:6287	 l-p:0.1283726692199707
epoch£º314	 i:8 	 global-step:6288	 l-p:0.09822792559862137
epoch£º314	 i:9 	 global-step:6289	 l-p:0.13888847827911377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 5.3409, 5.2833],
        [5.1184, 5.1562, 5.1058],
        [5.1184, 5.1184, 5.1184],
        [5.1184, 5.5534, 5.6064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.12841519713401794 
model_pd.l_d.mean(): -20.20193099975586 
model_pd.lagr.mean(): -20.073514938354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4268], device='cuda:0')), ('power', tensor([-20.8586], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.12841519713401794
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11614522337913513
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11049440503120422
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12834109365940094
epoch£º315	 i:4 	 global-step:6304	 l-p:0.1318933069705963
epoch£º315	 i:5 	 global-step:6305	 l-p:0.1897697001695633
epoch£º315	 i:6 	 global-step:6306	 l-p:0.2027197778224945
epoch£º315	 i:7 	 global-step:6307	 l-p:0.1526680886745453
epoch£º315	 i:8 	 global-step:6308	 l-p:0.119810089468956
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13605345785617828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9858, 4.9817, 4.9847],
        [4.9858, 4.9850, 4.9857],
        [4.9858, 5.0405, 4.9771],
        [4.9858, 4.9784, 4.9771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.1389811635017395 
model_pd.l_d.mean(): -20.967649459838867 
model_pd.lagr.mean(): -20.82866859436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3907], device='cuda:0')), ('power', tensor([-21.5959], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.1389811635017395
epoch£º316	 i:1 	 global-step:6321	 l-p:0.07709809392690659
epoch£º316	 i:2 	 global-step:6322	 l-p:0.4807923436164856
epoch£º316	 i:3 	 global-step:6323	 l-p:0.14785030484199524
epoch£º316	 i:4 	 global-step:6324	 l-p:0.12107225507497787
epoch£º316	 i:5 	 global-step:6325	 l-p:0.1479174643754959
epoch£º316	 i:6 	 global-step:6326	 l-p:0.10582240670919418
epoch£º316	 i:7 	 global-step:6327	 l-p:0.04037993773818016
epoch£º316	 i:8 	 global-step:6328	 l-p:0.13349376618862152
epoch£º316	 i:9 	 global-step:6329	 l-p:0.11983293294906616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7130, 4.6993, 4.7003],
        [4.7130, 4.7123, 4.7129],
        [4.7130, 4.7062, 4.6832],
        [4.7130, 4.7025, 4.6873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.12889112532138824 
model_pd.l_d.mean(): -20.399307250976562 
model_pd.lagr.mean(): -20.270416259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5464], device='cuda:0')), ('power', tensor([-21.1803], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.12889112532138824
epoch£º317	 i:1 	 global-step:6341	 l-p:0.18019874393939972
epoch£º317	 i:2 	 global-step:6342	 l-p:0.19571982324123383
epoch£º317	 i:3 	 global-step:6343	 l-p:0.13777297735214233
epoch£º317	 i:4 	 global-step:6344	 l-p:0.2450740784406662
epoch£º317	 i:5 	 global-step:6345	 l-p:0.13611933588981628
epoch£º317	 i:6 	 global-step:6346	 l-p:0.1326497197151184
epoch£º317	 i:7 	 global-step:6347	 l-p:0.1307341754436493
epoch£º317	 i:8 	 global-step:6348	 l-p:0.15438930690288544
epoch£º317	 i:9 	 global-step:6349	 l-p:0.14083312451839447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8655, 4.8554, 4.8532],
        [4.8655, 4.8654, 4.8655],
        [4.8655, 4.8605, 4.8643],
        [4.8655, 4.8651, 4.8655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.0680805891752243 
model_pd.l_d.mean(): -20.253557205200195 
model_pd.lagr.mean(): -20.185476303100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5076], device='cuda:0')), ('power', tensor([-20.9933], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.0680805891752243
epoch£º318	 i:1 	 global-step:6361	 l-p:0.14379577338695526
epoch£º318	 i:2 	 global-step:6362	 l-p:0.1296747624874115
epoch£º318	 i:3 	 global-step:6363	 l-p:0.20032212138175964
epoch£º318	 i:4 	 global-step:6364	 l-p:0.13221585750579834
epoch£º318	 i:5 	 global-step:6365	 l-p:0.016869181767106056
epoch£º318	 i:6 	 global-step:6366	 l-p:-0.23753149807453156
epoch£º318	 i:7 	 global-step:6367	 l-p:0.13401278853416443
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12124985456466675
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12355611473321915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1095, 5.1155, 5.0925],
        [5.1095, 5.1405, 5.0932],
        [5.1095, 5.1095, 5.1095],
        [5.1095, 5.1095, 5.1095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.16215388476848602 
model_pd.l_d.mean(): -20.502559661865234 
model_pd.lagr.mean(): -20.34040641784668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4167], device='cuda:0')), ('power', tensor([-21.1522], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.16215388476848602
epoch£º319	 i:1 	 global-step:6381	 l-p:0.114603690803051
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12514743208885193
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13227394223213196
epoch£º319	 i:4 	 global-step:6384	 l-p:0.13703489303588867
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12372565269470215
epoch£º319	 i:6 	 global-step:6386	 l-p:0.5079277753829956
epoch£º319	 i:7 	 global-step:6387	 l-p:-0.04332447424530983
epoch£º319	 i:8 	 global-step:6388	 l-p:0.16664069890975952
epoch£º319	 i:9 	 global-step:6389	 l-p:0.18416206538677216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9959, 5.0162, 4.9728],
        [4.9959, 5.0646, 4.9948],
        [4.9959, 5.0511, 4.9861],
        [4.9959, 4.9937, 4.9956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.16122150421142578 
model_pd.l_d.mean(): -20.164134979248047 
model_pd.lagr.mean(): -20.002914428710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4424], device='cuda:0')), ('power', tensor([-20.8364], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.16122150421142578
epoch£º320	 i:1 	 global-step:6401	 l-p:0.14414554834365845
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10229602456092834
epoch£º320	 i:3 	 global-step:6403	 l-p:0.18258915841579437
epoch£º320	 i:4 	 global-step:6404	 l-p:0.11613249778747559
epoch£º320	 i:5 	 global-step:6405	 l-p:0.14323030412197113
epoch£º320	 i:6 	 global-step:6406	 l-p:0.12763652205467224
epoch£º320	 i:7 	 global-step:6407	 l-p:0.19647163152694702
epoch£º320	 i:8 	 global-step:6408	 l-p:0.20475663244724274
epoch£º320	 i:9 	 global-step:6409	 l-p:0.14190565049648285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0661, 5.0594, 5.0621],
        [5.0661, 5.4028, 5.3978],
        [5.0661, 5.0634, 5.0656],
        [5.0661, 5.0659, 5.0661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.1169271171092987 
model_pd.l_d.mean(): -19.574094772338867 
model_pd.lagr.mean(): -19.457168579101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.2767], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.1169271171092987
epoch£º321	 i:1 	 global-step:6421	 l-p:0.2740183174610138
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11344262957572937
epoch£º321	 i:3 	 global-step:6423	 l-p:0.12763574719429016
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12436970323324203
epoch£º321	 i:5 	 global-step:6425	 l-p:1.3150852918624878
epoch£º321	 i:6 	 global-step:6426	 l-p:0.12559832632541656
epoch£º321	 i:7 	 global-step:6427	 l-p:0.22487880289554596
epoch£º321	 i:8 	 global-step:6428	 l-p:0.14105257391929626
epoch£º321	 i:9 	 global-step:6429	 l-p:0.151620551943779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9837, 5.8173, 6.2156],
        [4.9837, 4.9836, 4.9837],
        [4.9837, 4.9834, 4.9837],
        [4.9837, 4.9837, 4.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.17557388544082642 
model_pd.l_d.mean(): -20.64219093322754 
model_pd.lagr.mean(): -20.466617584228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4331], device='cuda:0')), ('power', tensor([-21.3101], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.17557388544082642
epoch£º322	 i:1 	 global-step:6441	 l-p:0.1435038149356842
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11667425185441971
epoch£º322	 i:3 	 global-step:6443	 l-p:0.18841859698295593
epoch£º322	 i:4 	 global-step:6444	 l-p:0.10391154140233994
epoch£º322	 i:5 	 global-step:6445	 l-p:0.13545240461826324
epoch£º322	 i:6 	 global-step:6446	 l-p:0.12767252326011658
epoch£º322	 i:7 	 global-step:6447	 l-p:0.03926105052232742
epoch£º322	 i:8 	 global-step:6448	 l-p:-0.37892088294029236
epoch£º322	 i:9 	 global-step:6449	 l-p:-0.31883448362350464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0225, 5.0225, 5.0225],
        [5.0225, 5.0196, 5.0220],
        [5.0225, 5.1800, 5.1058],
        [5.0225, 5.0144, 5.0171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.5504828691482544 
model_pd.l_d.mean(): -20.37472152709961 
model_pd.lagr.mean(): -19.824237823486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4574], device='cuda:0')), ('power', tensor([-21.0646], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.5504828691482544
epoch£º323	 i:1 	 global-step:6461	 l-p:0.14295734465122223
epoch£º323	 i:2 	 global-step:6462	 l-p:0.10053400695323944
epoch£º323	 i:3 	 global-step:6463	 l-p:0.1653008759021759
epoch£º323	 i:4 	 global-step:6464	 l-p:0.128659188747406
epoch£º323	 i:5 	 global-step:6465	 l-p:0.12048917263746262
epoch£º323	 i:6 	 global-step:6466	 l-p:0.386065810918808
epoch£º323	 i:7 	 global-step:6467	 l-p:0.116605743765831
epoch£º323	 i:8 	 global-step:6468	 l-p:0.11369672417640686
epoch£º323	 i:9 	 global-step:6469	 l-p:0.126211479306221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2329, 5.2329, 5.2329],
        [5.2329, 5.2329, 5.2329],
        [5.2329, 5.2288, 5.2314],
        [5.2329, 5.2426, 5.2165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.1329965740442276 
model_pd.l_d.mean(): -19.66802978515625 
model_pd.lagr.mean(): -19.535032272338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.3710], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.1329965740442276
epoch£º324	 i:1 	 global-step:6481	 l-p:0.11956031620502472
epoch£º324	 i:2 	 global-step:6482	 l-p:0.14998260140419006
epoch£º324	 i:3 	 global-step:6483	 l-p:0.13087056577205658
epoch£º324	 i:4 	 global-step:6484	 l-p:0.09797313809394836
epoch£º324	 i:5 	 global-step:6485	 l-p:0.16938713192939758
epoch£º324	 i:6 	 global-step:6486	 l-p:0.1324610561132431
epoch£º324	 i:7 	 global-step:6487	 l-p:0.13429193198680878
epoch£º324	 i:8 	 global-step:6488	 l-p:0.10378364473581314
epoch£º324	 i:9 	 global-step:6489	 l-p:0.3083948791027069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8372, 4.8518, 4.8030],
        [4.8372, 4.8304, 4.8353],
        [4.8372, 4.8371, 4.8372],
        [4.8372, 4.9196, 4.8411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.13694186508655548 
model_pd.l_d.mean(): -19.286540985107422 
model_pd.lagr.mean(): -19.149599075317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5372], device='cuda:0')), ('power', tensor([-20.0460], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.13694186508655548
epoch£º325	 i:1 	 global-step:6501	 l-p:0.12121796607971191
epoch£º325	 i:2 	 global-step:6502	 l-p:0.013099097646772861
epoch£º325	 i:3 	 global-step:6503	 l-p:0.1428343802690506
epoch£º325	 i:4 	 global-step:6504	 l-p:0.13059073686599731
epoch£º325	 i:5 	 global-step:6505	 l-p:0.14194560050964355
epoch£º325	 i:6 	 global-step:6506	 l-p:0.15707789361476898
epoch£º325	 i:7 	 global-step:6507	 l-p:0.12427398562431335
epoch£º325	 i:8 	 global-step:6508	 l-p:0.11070557683706284
epoch£º325	 i:9 	 global-step:6509	 l-p:0.12979568541049957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8927, 4.9687, 4.8917],
        [4.8927, 4.8920, 4.8927],
        [4.8927, 5.6512, 5.9892],
        [4.8927, 4.9557, 4.8818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.1330525279045105 
model_pd.l_d.mean(): -20.32090950012207 
model_pd.lagr.mean(): -20.187856674194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4925], device='cuda:0')), ('power', tensor([-21.0460], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.1330525279045105
epoch£º326	 i:1 	 global-step:6521	 l-p:0.13146886229515076
epoch£º326	 i:2 	 global-step:6522	 l-p:0.16200217604637146
epoch£º326	 i:3 	 global-step:6523	 l-p:0.10397132486104965
epoch£º326	 i:4 	 global-step:6524	 l-p:0.16503484547138214
epoch£º326	 i:5 	 global-step:6525	 l-p:0.10309430956840515
epoch£º326	 i:6 	 global-step:6526	 l-p:0.1257156878709793
epoch£º326	 i:7 	 global-step:6527	 l-p:0.137406587600708
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11461741477251053
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13391990959644318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8891, 5.6614, 6.0126],
        [4.8891, 4.8889, 4.8890],
        [4.8891, 4.8767, 4.8708],
        [4.8891, 4.8882, 4.8890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.1335887312889099 
model_pd.l_d.mean(): -20.16053581237793 
model_pd.lagr.mean(): -20.026947021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.8912], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.1335887312889099
epoch£º327	 i:1 	 global-step:6541	 l-p:-0.12445425987243652
epoch£º327	 i:2 	 global-step:6542	 l-p:0.11732300370931625
epoch£º327	 i:3 	 global-step:6543	 l-p:0.1146736890077591
epoch£º327	 i:4 	 global-step:6544	 l-p:0.03652452304959297
epoch£º327	 i:5 	 global-step:6545	 l-p:0.15867716073989868
epoch£º327	 i:6 	 global-step:6546	 l-p:0.15495207905769348
epoch£º327	 i:7 	 global-step:6547	 l-p:0.09076020866632462
epoch£º327	 i:8 	 global-step:6548	 l-p:0.12974046170711517
epoch£º327	 i:9 	 global-step:6549	 l-p:0.1272977739572525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0242, 5.0199, 5.0233],
        [5.0242, 5.0224, 5.0240],
        [5.0242, 5.1036, 5.0277],
        [5.0242, 5.5230, 5.6273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.25856390595436096 
model_pd.l_d.mean(): -20.098087310791016 
model_pd.lagr.mean(): -19.839523315429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4824], device='cuda:0')), ('power', tensor([-20.8105], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.25856390595436096
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12415950745344162
epoch£º328	 i:2 	 global-step:6562	 l-p:0.10240951180458069
epoch£º328	 i:3 	 global-step:6563	 l-p:0.37041598558425903
epoch£º328	 i:4 	 global-step:6564	 l-p:0.16444788873195648
epoch£º328	 i:5 	 global-step:6565	 l-p:0.11170751601457596
epoch£º328	 i:6 	 global-step:6566	 l-p:0.2125226855278015
epoch£º328	 i:7 	 global-step:6567	 l-p:0.16764721274375916
epoch£º328	 i:8 	 global-step:6568	 l-p:0.12915809452533722
epoch£º328	 i:9 	 global-step:6569	 l-p:0.10590989887714386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0918, 5.0918, 5.0918],
        [5.0918, 5.3487, 5.3023],
        [5.0918, 5.0912, 5.0918],
        [5.0918, 5.0834, 5.0799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.1304122656583786 
model_pd.l_d.mean(): -19.03474235534668 
model_pd.lagr.mean(): -18.904329299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5191], device='cuda:0')), ('power', tensor([-19.7730], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.1304122656583786
epoch£º329	 i:1 	 global-step:6581	 l-p:0.1136685162782669
epoch£º329	 i:2 	 global-step:6582	 l-p:0.11671382933855057
epoch£º329	 i:3 	 global-step:6583	 l-p:0.14428089559078217
epoch£º329	 i:4 	 global-step:6584	 l-p:0.11810328811407089
epoch£º329	 i:5 	 global-step:6585	 l-p:0.16577906906604767
epoch£º329	 i:6 	 global-step:6586	 l-p:0.12643249332904816
epoch£º329	 i:7 	 global-step:6587	 l-p:0.7059066891670227
epoch£º329	 i:8 	 global-step:6588	 l-p:0.11245574802160263
epoch£º329	 i:9 	 global-step:6589	 l-p:-0.05427146703004837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9571, 4.9565, 4.9571],
        [4.9571, 4.9562, 4.9570],
        [4.9571, 4.9566, 4.9571],
        [4.9571, 4.9571, 4.9571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.12962326407432556 
model_pd.l_d.mean(): -20.536508560180664 
model_pd.lagr.mean(): -20.406885147094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4494], device='cuda:0')), ('power', tensor([-21.2199], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.12962326407432556
epoch£º330	 i:1 	 global-step:6601	 l-p:0.15901920199394226
epoch£º330	 i:2 	 global-step:6602	 l-p:0.14914393424987793
epoch£º330	 i:3 	 global-step:6603	 l-p:0.019660934805870056
epoch£º330	 i:4 	 global-step:6604	 l-p:0.1565050482749939
epoch£º330	 i:5 	 global-step:6605	 l-p:-0.09866312891244888
epoch£º330	 i:6 	 global-step:6606	 l-p:0.13588564097881317
epoch£º330	 i:7 	 global-step:6607	 l-p:0.12081523984670639
epoch£º330	 i:8 	 global-step:6608	 l-p:0.12588617205619812
epoch£º330	 i:9 	 global-step:6609	 l-p:0.14911746978759766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7951, 4.8319, 4.7616],
        [4.7951, 5.4723, 5.7469],
        [4.7951, 4.7922, 4.7947],
        [4.7951, 5.3696, 5.5553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.0970442146062851 
model_pd.l_d.mean(): -20.29233169555664 
model_pd.lagr.mean(): -20.195287704467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5339], device='cuda:0')), ('power', tensor([-21.0594], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.0970442146062851
epoch£º331	 i:1 	 global-step:6621	 l-p:0.1353578120470047
epoch£º331	 i:2 	 global-step:6622	 l-p:0.15625420212745667
epoch£º331	 i:3 	 global-step:6623	 l-p:0.12380500882863998
epoch£º331	 i:4 	 global-step:6624	 l-p:0.15801703929901123
epoch£º331	 i:5 	 global-step:6625	 l-p:0.176075741648674
epoch£º331	 i:6 	 global-step:6626	 l-p:0.13288258016109467
epoch£º331	 i:7 	 global-step:6627	 l-p:-0.162032812833786
epoch£º331	 i:8 	 global-step:6628	 l-p:0.12713728845119476
epoch£º331	 i:9 	 global-step:6629	 l-p:0.11893340945243835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0847, 5.1019, 5.0570],
        [5.0847, 5.2582, 5.1840],
        [5.0847, 5.1495, 5.0769],
        [5.0847, 5.0827, 5.0845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.12720336019992828 
model_pd.l_d.mean(): -20.289670944213867 
model_pd.lagr.mean(): -20.16246795654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-20.9761], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.12720336019992828
epoch£º332	 i:1 	 global-step:6641	 l-p:0.11339059472084045
epoch£º332	 i:2 	 global-step:6642	 l-p:0.21156102418899536
epoch£º332	 i:3 	 global-step:6643	 l-p:0.10809867829084396
epoch£º332	 i:4 	 global-step:6644	 l-p:0.199701726436615
epoch£º332	 i:5 	 global-step:6645	 l-p:0.1398448348045349
epoch£º332	 i:6 	 global-step:6646	 l-p:0.13083143532276154
epoch£º332	 i:7 	 global-step:6647	 l-p:0.1460409015417099
epoch£º332	 i:8 	 global-step:6648	 l-p:0.17532840371131897
epoch£º332	 i:9 	 global-step:6649	 l-p:0.11936347931623459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0702, 5.0702, 5.0702],
        [5.0702, 5.3128, 5.2602],
        [5.0702, 5.0702, 5.0702],
        [5.0702, 5.0702, 5.0702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.15237361192703247 
model_pd.l_d.mean(): -20.37112808227539 
model_pd.lagr.mean(): -20.218753814697266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.0377], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.15237361192703247
epoch£º333	 i:1 	 global-step:6661	 l-p:0.11904045939445496
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12116633355617523
epoch£º333	 i:3 	 global-step:6663	 l-p:0.32992854714393616
epoch£º333	 i:4 	 global-step:6664	 l-p:0.1751037985086441
epoch£º333	 i:5 	 global-step:6665	 l-p:0.14286074042320251
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12382562458515167
epoch£º333	 i:7 	 global-step:6667	 l-p:-0.05065144598484039
epoch£º333	 i:8 	 global-step:6668	 l-p:0.13908541202545166
epoch£º333	 i:9 	 global-step:6669	 l-p:0.015774577856063843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0001, 4.9998, 5.0001],
        [5.0001, 4.9980, 4.9999],
        [5.0001, 5.0316, 4.9705],
        [5.0001, 4.9968, 4.9705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.14902839064598083 
model_pd.l_d.mean(): -20.88802719116211 
model_pd.lagr.mean(): -20.738998413085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.5271], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.14902839064598083
epoch£º334	 i:1 	 global-step:6681	 l-p:0.19218482077121735
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1436711996793747
epoch£º334	 i:3 	 global-step:6683	 l-p:0.011888232082128525
epoch£º334	 i:4 	 global-step:6684	 l-p:0.1461806297302246
epoch£º334	 i:5 	 global-step:6685	 l-p:0.396838903427124
epoch£º334	 i:6 	 global-step:6686	 l-p:0.1343296468257904
epoch£º334	 i:7 	 global-step:6687	 l-p:0.23629148304462433
epoch£º334	 i:8 	 global-step:6688	 l-p:0.12672685086727142
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11220821738243103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.1054, 5.0977],
        [5.1144, 5.8745, 6.1886],
        [5.1144, 5.1144, 5.1144],
        [5.1144, 5.9553, 6.3442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10183247923851013 
model_pd.l_d.mean(): -19.247283935546875 
model_pd.lagr.mean(): -19.145450592041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5060], device='cuda:0')), ('power', tensor([-19.9744], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10183247923851013
epoch£º335	 i:1 	 global-step:6701	 l-p:0.13101257383823395
epoch£º335	 i:2 	 global-step:6702	 l-p:0.17222747206687927
epoch£º335	 i:3 	 global-step:6703	 l-p:0.16061584651470184
epoch£º335	 i:4 	 global-step:6704	 l-p:0.1352945864200592
epoch£º335	 i:5 	 global-step:6705	 l-p:0.14105306565761566
epoch£º335	 i:6 	 global-step:6706	 l-p:0.5790061950683594
epoch£º335	 i:7 	 global-step:6707	 l-p:0.12093465775251389
epoch£º335	 i:8 	 global-step:6708	 l-p:0.13791923224925995
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12453484535217285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0146, 5.0017, 4.9975],
        [5.0146, 5.0132, 4.9831],
        [5.0146, 5.2490, 5.1938],
        [5.0146, 5.0028, 4.9939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11912105232477188 
model_pd.l_d.mean(): -19.382762908935547 
model_pd.lagr.mean(): -19.263641357421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-20.0716], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11912105232477188
epoch£º336	 i:1 	 global-step:6721	 l-p:0.05999043956398964
epoch£º336	 i:2 	 global-step:6722	 l-p:0.13146941363811493
epoch£º336	 i:3 	 global-step:6723	 l-p:0.08916889131069183
epoch£º336	 i:4 	 global-step:6724	 l-p:0.14244870841503143
epoch£º336	 i:5 	 global-step:6725	 l-p:0.134068101644516
epoch£º336	 i:6 	 global-step:6726	 l-p:4.832336902618408
epoch£º336	 i:7 	 global-step:6727	 l-p:0.12133719027042389
epoch£º336	 i:8 	 global-step:6728	 l-p:0.14317168295383453
epoch£º336	 i:9 	 global-step:6729	 l-p:-0.2153918743133545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8704, 5.7362, 6.1778],
        [4.8704, 4.8536, 4.8567],
        [4.8704, 4.9451, 4.8617],
        [4.8704, 5.2962, 5.3585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13655374944210052 
model_pd.l_d.mean(): -18.462188720703125 
model_pd.lagr.mean(): -18.32563591003418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5703], device='cuda:0')), ('power', tensor([-19.2464], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13655374944210052
epoch£º337	 i:1 	 global-step:6741	 l-p:0.315009206533432
epoch£º337	 i:2 	 global-step:6742	 l-p:0.0395478755235672
epoch£º337	 i:3 	 global-step:6743	 l-p:0.13173866271972656
epoch£º337	 i:4 	 global-step:6744	 l-p:0.10648541897535324
epoch£º337	 i:5 	 global-step:6745	 l-p:0.13109572231769562
epoch£º337	 i:6 	 global-step:6746	 l-p:0.13009271025657654
epoch£º337	 i:7 	 global-step:6747	 l-p:0.14945808053016663
epoch£º337	 i:8 	 global-step:6748	 l-p:0.14724959433078766
epoch£º337	 i:9 	 global-step:6749	 l-p:0.1942823976278305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9228, 4.9076, 4.8965],
        [4.9228, 4.9090, 4.9148],
        [4.9228, 4.9191, 4.9223],
        [4.9228, 4.9228, 4.9228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.1352623999118805 
model_pd.l_d.mean(): -20.28205108642578 
model_pd.lagr.mean(): -20.14678955078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4941], device='cuda:0')), ('power', tensor([-21.0084], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.1352623999118805
epoch£º338	 i:1 	 global-step:6761	 l-p:0.12306863814592361
epoch£º338	 i:2 	 global-step:6762	 l-p:0.09766027331352234
epoch£º338	 i:3 	 global-step:6763	 l-p:0.1315968632698059
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.13718563318252563
epoch£º338	 i:5 	 global-step:6765	 l-p:0.15567108988761902
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10427296161651611
epoch£º338	 i:7 	 global-step:6767	 l-p:0.174364373087883
epoch£º338	 i:8 	 global-step:6768	 l-p:0.1095634251832962
epoch£º338	 i:9 	 global-step:6769	 l-p:0.14984135329723358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.6033, 5.7588],
        [5.0369, 5.0230, 5.0217],
        [5.0369, 5.0245, 5.0289],
        [5.0369, 5.0369, 5.0369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.12556228041648865 
model_pd.l_d.mean(): -20.413944244384766 
model_pd.lagr.mean(): -20.288381576538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.0979], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.12556228041648865
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13719789683818817
epoch£º339	 i:2 	 global-step:6782	 l-p:0.13021665811538696
epoch£º339	 i:3 	 global-step:6783	 l-p:2.605499505996704
epoch£º339	 i:4 	 global-step:6784	 l-p:0.15750350058078766
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.041141144931316376
epoch£º339	 i:6 	 global-step:6786	 l-p:0.14586351811885834
epoch£º339	 i:7 	 global-step:6787	 l-p:0.12610740959644318
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15346860885620117
epoch£º339	 i:9 	 global-step:6789	 l-p:-0.6829670667648315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0608, 5.4448, 5.4663],
        [5.0608, 5.0591, 5.0606],
        [5.0608, 5.0606, 5.0608],
        [5.0608, 5.0489, 5.0379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.12189044058322906 
model_pd.l_d.mean(): -19.938488006591797 
model_pd.lagr.mean(): -19.81659698486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.6483], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.12189044058322906
epoch£º340	 i:1 	 global-step:6801	 l-p:0.13456867635250092
epoch£º340	 i:2 	 global-step:6802	 l-p:0.2667161822319031
epoch£º340	 i:3 	 global-step:6803	 l-p:0.14003998041152954
epoch£º340	 i:4 	 global-step:6804	 l-p:0.12685626745224
epoch£º340	 i:5 	 global-step:6805	 l-p:0.11116737872362137
epoch£º340	 i:6 	 global-step:6806	 l-p:0.21036213636398315
epoch£º340	 i:7 	 global-step:6807	 l-p:0.12348626554012299
epoch£º340	 i:8 	 global-step:6808	 l-p:0.1772124022245407
epoch£º340	 i:9 	 global-step:6809	 l-p:0.11709514260292053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1213, 5.1207, 5.1213],
        [5.1213, 5.1205, 5.0906],
        [5.1213, 5.6040, 5.6887],
        [5.1213, 5.6217, 5.7193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.12915635108947754 
model_pd.l_d.mean(): -20.100048065185547 
model_pd.lagr.mean(): -19.97089195251465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4617], device='cuda:0')), ('power', tensor([-20.7913], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.12915635108947754
epoch£º341	 i:1 	 global-step:6821	 l-p:0.14438797533512115
epoch£º341	 i:2 	 global-step:6822	 l-p:0.13545887172222137
epoch£º341	 i:3 	 global-step:6823	 l-p:0.11935155838727951
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12974047660827637
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12102573364973068
epoch£º341	 i:6 	 global-step:6826	 l-p:-0.006438340991735458
epoch£º341	 i:7 	 global-step:6827	 l-p:0.1472901701927185
epoch£º341	 i:8 	 global-step:6828	 l-p:0.09337180852890015
epoch£º341	 i:9 	 global-step:6829	 l-p:0.3096613883972168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9301, 4.9301, 4.9301],
        [4.9301, 5.4593, 5.5945],
        [4.9301, 5.6545, 5.9559],
        [4.9301, 5.4602, 5.5961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.1482037454843521 
model_pd.l_d.mean(): -20.724027633666992 
model_pd.lagr.mean(): -20.575824737548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.4089], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.1482037454843521
epoch£º342	 i:1 	 global-step:6841	 l-p:0.3681659698486328
epoch£º342	 i:2 	 global-step:6842	 l-p:0.1425003707408905
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12593406438827515
epoch£º342	 i:4 	 global-step:6844	 l-p:0.1330079436302185
epoch£º342	 i:5 	 global-step:6845	 l-p:0.29672175645828247
epoch£º342	 i:6 	 global-step:6846	 l-p:0.0405091866850853
epoch£º342	 i:7 	 global-step:6847	 l-p:0.08171357959508896
epoch£º342	 i:8 	 global-step:6848	 l-p:0.06842990219593048
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13764457404613495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0444, 5.6057, 5.7562],
        [5.0444, 5.0444, 5.0444],
        [5.0444, 5.7955, 6.1084],
        [5.0444, 5.0327, 5.0386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13664868474006653 
model_pd.l_d.mean(): -20.753604888916016 
model_pd.lagr.mean(): -20.61695671081543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4062], device='cuda:0')), ('power', tensor([-21.3952], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13664868474006653
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13255847990512848
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13344815373420715
epoch£º343	 i:3 	 global-step:6863	 l-p:0.12363169342279434
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13292530179023743
epoch£º343	 i:5 	 global-step:6865	 l-p:-0.06472849100828171
epoch£º343	 i:6 	 global-step:6866	 l-p:0.13096104562282562
epoch£º343	 i:7 	 global-step:6867	 l-p:-0.7896410822868347
epoch£º343	 i:8 	 global-step:6868	 l-p:0.1279238760471344
epoch£º343	 i:9 	 global-step:6869	 l-p:0.13060830533504486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[4.9872, 5.7180, 6.0188],
        [4.9872, 5.5500, 5.7069],
        [4.9872, 4.9756, 4.9527],
        [4.9872, 4.9741, 4.9546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14548198878765106 
model_pd.l_d.mean(): -20.780500411987305 
model_pd.lagr.mean(): -20.635019302368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-21.4367], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14548198878765106
epoch£º344	 i:1 	 global-step:6881	 l-p:0.11093837767839432
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13273203372955322
epoch£º344	 i:3 	 global-step:6883	 l-p:0.1280149519443512
epoch£º344	 i:4 	 global-step:6884	 l-p:0.10200387984514236
epoch£º344	 i:5 	 global-step:6885	 l-p:0.1508641391992569
epoch£º344	 i:6 	 global-step:6886	 l-p:0.13333654403686523
epoch£º344	 i:7 	 global-step:6887	 l-p:0.0891733318567276
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14824658632278442
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.22170022130012512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8921, 4.8817, 4.8887],
        [4.8921, 5.6002, 5.8908],
        [4.8921, 4.8795, 4.8870],
        [4.8921, 4.9555, 4.8702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12389993667602539 
model_pd.l_d.mean(): -19.50411033630371 
model_pd.lagr.mean(): -19.380210876464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4985], device='cuda:0')), ('power', tensor([-20.2265], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12389993667602539
epoch£º345	 i:1 	 global-step:6901	 l-p:0.13993960618972778
epoch£º345	 i:2 	 global-step:6902	 l-p:0.11953147500753403
epoch£º345	 i:3 	 global-step:6903	 l-p:0.1458413153886795
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11829017102718353
epoch£º345	 i:5 	 global-step:6905	 l-p:-47.59688949584961
epoch£º345	 i:6 	 global-step:6906	 l-p:0.0840844139456749
epoch£º345	 i:7 	 global-step:6907	 l-p:0.18365351855754852
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13269925117492676
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12384680658578873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0423, 5.0362, 5.0409],
        [5.0423, 5.0422, 5.0423],
        [5.0423, 5.0285, 5.0344],
        [5.0423, 5.1642, 5.0773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.14533355832099915 
model_pd.l_d.mean(): -19.896690368652344 
model_pd.lagr.mean(): -19.75135612487793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5092], device='cuda:0')), ('power', tensor([-20.6342], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.14533355832099915
epoch£º346	 i:1 	 global-step:6921	 l-p:0.1311853528022766
epoch£º346	 i:2 	 global-step:6922	 l-p:0.1408340334892273
epoch£º346	 i:3 	 global-step:6923	 l-p:0.2104940563440323
epoch£º346	 i:4 	 global-step:6924	 l-p:0.13010522723197937
epoch£º346	 i:5 	 global-step:6925	 l-p:0.10457336902618408
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12169591337442398
epoch£º346	 i:7 	 global-step:6927	 l-p:0.1288100928068161
epoch£º346	 i:8 	 global-step:6928	 l-p:0.13515549898147583
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.20680789649486542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0047, 5.0047, 5.0047],
        [5.0047, 5.0827, 4.9962],
        [5.0047, 5.7987, 6.1549],
        [5.0047, 5.0018, 5.0043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.12074877321720123 
model_pd.l_d.mean(): -19.966609954833984 
model_pd.lagr.mean(): -19.845861434936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.7023], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.12074877321720123
epoch£º347	 i:1 	 global-step:6941	 l-p:0.07752783596515656
epoch£º347	 i:2 	 global-step:6942	 l-p:0.08441241830587387
epoch£º347	 i:3 	 global-step:6943	 l-p:0.07732512056827545
epoch£º347	 i:4 	 global-step:6944	 l-p:0.12715812027454376
epoch£º347	 i:5 	 global-step:6945	 l-p:0.16943325102329254
epoch£º347	 i:6 	 global-step:6946	 l-p:0.25379157066345215
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12547573447227478
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11916042864322662
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13623306155204773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0395, 5.0220, 5.0210],
        [5.0395, 5.0258, 5.0322],
        [5.0395, 5.0222, 5.0234],
        [5.0395, 5.0395, 5.0395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.1362108737230301 
model_pd.l_d.mean(): -20.422521591186523 
model_pd.lagr.mean(): -20.28631019592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.0903], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.1362108737230301
epoch£º348	 i:1 	 global-step:6961	 l-p:-0.001091814017854631
epoch£º348	 i:2 	 global-step:6962	 l-p:0.161231130361557
epoch£º348	 i:3 	 global-step:6963	 l-p:0.19374796748161316
epoch£º348	 i:4 	 global-step:6964	 l-p:0.12871000170707703
epoch£º348	 i:5 	 global-step:6965	 l-p:0.11992993950843811
epoch£º348	 i:6 	 global-step:6966	 l-p:0.1488959640264511
epoch£º348	 i:7 	 global-step:6967	 l-p:-0.02207031287252903
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13957162201404572
epoch£º348	 i:9 	 global-step:6969	 l-p:0.05708536505699158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9722, 4.9722, 4.9722],
        [4.9722, 4.9712, 4.9721],
        [4.9722, 4.9710, 4.9721],
        [4.9722, 4.9574, 4.9647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.13934524357318878 
model_pd.l_d.mean(): -18.90041732788086 
model_pd.lagr.mean(): -18.761072158813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5466], device='cuda:0')), ('power', tensor([-19.6652], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.13934524357318878
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12776370346546173
epoch£º349	 i:2 	 global-step:6982	 l-p:0.13445672392845154
epoch£º349	 i:3 	 global-step:6983	 l-p:0.15716251730918884
epoch£º349	 i:4 	 global-step:6984	 l-p:0.12429229170084
epoch£º349	 i:5 	 global-step:6985	 l-p:0.14517879486083984
epoch£º349	 i:6 	 global-step:6986	 l-p:0.1580149084329605
epoch£º349	 i:7 	 global-step:6987	 l-p:-0.011940696276724339
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13827218115329742
epoch£º349	 i:9 	 global-step:6989	 l-p:0.08197877556085587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8142, 4.8129, 4.8142],
        [4.8142, 5.2589, 5.3384],
        [4.8142, 5.1198, 5.1051],
        [4.8142, 5.2676, 5.3535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.2172839343547821 
model_pd.l_d.mean(): -20.494461059570312 
model_pd.lagr.mean(): -20.277177810668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5135], device='cuda:0')), ('power', tensor([-21.2429], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.2172839343547821
epoch£º350	 i:1 	 global-step:7001	 l-p:0.12997303903102875
epoch£º350	 i:2 	 global-step:7002	 l-p:0.06702005118131638
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11903066188097
epoch£º350	 i:4 	 global-step:7004	 l-p:0.2739531695842743
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13373063504695892
epoch£º350	 i:6 	 global-step:7006	 l-p:0.12178593128919601
epoch£º350	 i:7 	 global-step:7007	 l-p:0.052322596311569214
epoch£º350	 i:8 	 global-step:7008	 l-p:0.1362629234790802
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12332405894994736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0261, 5.0077, 5.0105],
        [5.0261, 5.0071, 5.0012],
        [5.0261, 5.1834, 5.0997],
        [5.0261, 5.2023, 5.1232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): -0.032130394130945206 
model_pd.l_d.mean(): -20.173349380493164 
model_pd.lagr.mean(): -20.205480575561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.8904], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:-0.032130394130945206
epoch£º351	 i:1 	 global-step:7021	 l-p:0.8160321116447449
epoch£º351	 i:2 	 global-step:7022	 l-p:0.10933393239974976
epoch£º351	 i:3 	 global-step:7023	 l-p:0.13918425142765045
epoch£º351	 i:4 	 global-step:7024	 l-p:0.12207794189453125
epoch£º351	 i:5 	 global-step:7025	 l-p:0.13246870040893555
epoch£º351	 i:6 	 global-step:7026	 l-p:0.15278784930706024
epoch£º351	 i:7 	 global-step:7027	 l-p:0.11992302536964417
epoch£º351	 i:8 	 global-step:7028	 l-p:0.06599386036396027
epoch£º351	 i:9 	 global-step:7029	 l-p:0.13380035758018494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9633, 4.9440, 4.9496],
        [4.9633, 5.0427, 4.9526],
        [4.9633, 4.9428, 4.9297],
        [4.9633, 5.1682, 5.0988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.09187434613704681 
model_pd.l_d.mean(): -20.98409080505371 
model_pd.lagr.mean(): -20.892215728759766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4020], device='cuda:0')), ('power', tensor([-21.6240], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.09187434613704681
epoch£º352	 i:1 	 global-step:7041	 l-p:0.18780513107776642
epoch£º352	 i:2 	 global-step:7042	 l-p:0.141738623380661
epoch£º352	 i:3 	 global-step:7043	 l-p:0.19142860174179077
epoch£º352	 i:4 	 global-step:7044	 l-p:0.14550481736660004
epoch£º352	 i:5 	 global-step:7045	 l-p:0.7342195510864258
epoch£º352	 i:6 	 global-step:7046	 l-p:0.49410438537597656
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11343176662921906
epoch£º352	 i:8 	 global-step:7048	 l-p:0.13722528517246246
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11855906993150711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1263, 5.1215, 5.1255],
        [5.1263, 5.1245, 5.1262],
        [5.1263, 5.1246, 5.1262],
        [5.1263, 5.1263, 5.1263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12594468891620636 
model_pd.l_d.mean(): -20.453210830688477 
model_pd.lagr.mean(): -20.327266693115234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12594468891620636
epoch£º353	 i:1 	 global-step:7061	 l-p:0.13182543218135834
epoch£º353	 i:2 	 global-step:7062	 l-p:0.17463740706443787
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13927395641803741
epoch£º353	 i:4 	 global-step:7064	 l-p:0.11260256171226501
epoch£º353	 i:5 	 global-step:7065	 l-p:0.1660926640033722
epoch£º353	 i:6 	 global-step:7066	 l-p:0.14801621437072754
epoch£º353	 i:7 	 global-step:7067	 l-p:0.14122939109802246
epoch£º353	 i:8 	 global-step:7068	 l-p:-0.20762023329734802
epoch£º353	 i:9 	 global-step:7069	 l-p:0.1186075285077095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0090, 4.9917, 4.9985],
        [5.0090, 5.2762, 5.2319],
        [5.0090, 5.8306, 6.2114],
        [5.0090, 5.0038, 5.0081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12725324928760529 
model_pd.l_d.mean(): -20.74989128112793 
model_pd.lagr.mean(): -20.622638702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4037], device='cuda:0')), ('power', tensor([-21.3890], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12725324928760529
epoch£º354	 i:1 	 global-step:7081	 l-p:0.2792724668979645
epoch£º354	 i:2 	 global-step:7082	 l-p:0.08077628910541534
epoch£º354	 i:3 	 global-step:7083	 l-p:0.1829802244901657
epoch£º354	 i:4 	 global-step:7084	 l-p:0.14134171605110168
epoch£º354	 i:5 	 global-step:7085	 l-p:0.13539695739746094
epoch£º354	 i:6 	 global-step:7086	 l-p:0.1435963362455368
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13872326910495758
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12464971095323563
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.035146020352840424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9854, 5.8056, 6.1875],
        [4.9854, 4.9727, 4.9806],
        [4.9854, 4.9824, 4.9851],
        [4.9854, 4.9843, 4.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13102109730243683 
model_pd.l_d.mean(): -20.144336700439453 
model_pd.lagr.mean(): -20.013315200805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4747], device='cuda:0')), ('power', tensor([-20.8493], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13102109730243683
epoch£º355	 i:1 	 global-step:7101	 l-p:0.12831005454063416
epoch£º355	 i:2 	 global-step:7102	 l-p:0.17015349864959717
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12439920008182526
epoch£º355	 i:4 	 global-step:7104	 l-p:0.147760808467865
epoch£º355	 i:5 	 global-step:7105	 l-p:0.16785666346549988
epoch£º355	 i:6 	 global-step:7106	 l-p:0.15003302693367004
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08292829245328903
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14035315811634064
epoch£º355	 i:9 	 global-step:7109	 l-p:0.08337182551622391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8217, 4.7932, 4.7916],
        [4.8217, 5.2307, 5.2830],
        [4.8217, 4.8217, 4.8217],
        [4.8217, 4.7929, 4.7893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16294622421264648 
model_pd.l_d.mean(): -19.89017105102539 
model_pd.lagr.mean(): -19.727224349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-20.6370], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16294622421264648
epoch£º356	 i:1 	 global-step:7121	 l-p:0.15072353184223175
epoch£º356	 i:2 	 global-step:7122	 l-p:0.1478242427110672
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19764363765716553
epoch£º356	 i:4 	 global-step:7124	 l-p:0.07928063720464706
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10907882452011108
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18216586112976074
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06906752288341522
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08818753808736801
epoch£º356	 i:9 	 global-step:7129	 l-p:0.1481580138206482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9219, 5.6699, 5.9917],
        [4.9219, 4.9201, 4.9218],
        [4.9219, 4.9140, 4.9202],
        [4.9219, 4.9004, 4.8745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 3.2004318237304688 
model_pd.l_d.mean(): -20.009998321533203 
model_pd.lagr.mean(): -16.809566497802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5413], device='cuda:0')), ('power', tensor([-20.7815], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:3.2004318237304688
epoch£º357	 i:1 	 global-step:7141	 l-p:0.15443024039268494
epoch£º357	 i:2 	 global-step:7142	 l-p:0.13715627789497375
epoch£º357	 i:3 	 global-step:7143	 l-p:0.9710903763771057
epoch£º357	 i:4 	 global-step:7144	 l-p:0.0664530023932457
epoch£º357	 i:5 	 global-step:7145	 l-p:0.1377786099910736
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13199791312217712
epoch£º357	 i:7 	 global-step:7147	 l-p:0.12690268456935883
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11657474935054779
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12825725972652435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9398, 4.9390, 4.9398],
        [4.9398, 4.9398, 4.9398],
        [4.9398, 4.9422, 4.8807],
        [4.9398, 5.5223, 5.6982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.07931170612573624 
model_pd.l_d.mean(): -20.620925903320312 
model_pd.lagr.mean(): -20.541614532470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.3118], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.07931170612573624
epoch£º358	 i:1 	 global-step:7161	 l-p:0.03231014311313629
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15524697303771973
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13316653668880463
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09350904077291489
epoch£º358	 i:5 	 global-step:7165	 l-p:0.1295919269323349
epoch£º358	 i:6 	 global-step:7166	 l-p:0.14746636152267456
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13389301300048828
epoch£º358	 i:8 	 global-step:7168	 l-p:0.2729966342449188
epoch£º358	 i:9 	 global-step:7169	 l-p:0.19049394130706787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9710, 4.9538, 4.9627],
        [4.9710, 4.9710, 4.9710],
        [4.9710, 4.9600, 4.9155],
        [4.9710, 4.9710, 4.9710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1410091072320938 
model_pd.l_d.mean(): -20.287614822387695 
model_pd.lagr.mean(): -20.1466064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-20.9962], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1410091072320938
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12735560536384583
epoch£º359	 i:2 	 global-step:7182	 l-p:0.13194768130779266
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12425590306520462
epoch£º359	 i:4 	 global-step:7184	 l-p:0.5070695281028748
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14466914534568787
epoch£º359	 i:6 	 global-step:7186	 l-p:0.24115225672721863
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09542840719223022
epoch£º359	 i:8 	 global-step:7188	 l-p:0.1705498844385147
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14186912775039673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9015, 4.8757, 4.8527],
        [4.9015, 4.8764, 4.8511],
        [4.9015, 4.8732, 4.8684],
        [4.9015, 4.9014, 4.9015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.1312170922756195 
model_pd.l_d.mean(): -20.13117218017578 
model_pd.lagr.mean(): -19.999954223632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5239], device='cuda:0')), ('power', tensor([-20.8863], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.1312170922756195
epoch£º360	 i:1 	 global-step:7201	 l-p:0.11325158923864365
epoch£º360	 i:2 	 global-step:7202	 l-p:0.1870877742767334
epoch£º360	 i:3 	 global-step:7203	 l-p:0.017950797453522682
epoch£º360	 i:4 	 global-step:7204	 l-p:0.16240160167217255
epoch£º360	 i:5 	 global-step:7205	 l-p:0.15006610751152039
epoch£º360	 i:6 	 global-step:7206	 l-p:0.146671861410141
epoch£º360	 i:7 	 global-step:7207	 l-p:0.08173656463623047
epoch£º360	 i:8 	 global-step:7208	 l-p:0.13108061254024506
epoch£º360	 i:9 	 global-step:7209	 l-p:0.14103513956069946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8488, 4.8488],
        [4.8488, 4.8487, 4.8488],
        [4.8488, 4.8486, 4.8488],
        [4.8488, 4.8472, 4.8487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.1771317571401596 
model_pd.l_d.mean(): -20.642587661743164 
model_pd.lagr.mean(): -20.465456008911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.3706], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.1771317571401596
epoch£º361	 i:1 	 global-step:7221	 l-p:0.19016927480697632
epoch£º361	 i:2 	 global-step:7222	 l-p:-0.14917710423469543
epoch£º361	 i:3 	 global-step:7223	 l-p:0.09282370656728745
epoch£º361	 i:4 	 global-step:7224	 l-p:0.1124541386961937
epoch£º361	 i:5 	 global-step:7225	 l-p:0.11554528772830963
epoch£º361	 i:6 	 global-step:7226	 l-p:0.1288319230079651
epoch£º361	 i:7 	 global-step:7227	 l-p:0.13299359381198883
epoch£º361	 i:8 	 global-step:7228	 l-p:-0.22809995710849762
epoch£º361	 i:9 	 global-step:7229	 l-p:0.3443868160247803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0869, 5.0733, 5.0388],
        [5.0869, 5.0869, 5.0869],
        [5.0869, 5.0869, 5.0869],
        [5.0869, 5.5298, 5.5864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.13100871443748474 
model_pd.l_d.mean(): -20.359050750732422 
model_pd.lagr.mean(): -20.228042602539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-21.0342], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.13100871443748474
epoch£º362	 i:1 	 global-step:7241	 l-p:0.19551964104175568
epoch£º362	 i:2 	 global-step:7242	 l-p:0.1358708292245865
epoch£º362	 i:3 	 global-step:7243	 l-p:0.137879878282547
epoch£º362	 i:4 	 global-step:7244	 l-p:0.17939317226409912
epoch£º362	 i:5 	 global-step:7245	 l-p:0.13186223804950714
epoch£º362	 i:6 	 global-step:7246	 l-p:0.14044469594955444
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1336047202348709
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12310714274644852
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13752381503582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1222, 5.1002, 5.0977],
        [5.1222, 5.1217, 5.1222],
        [5.1222, 5.1209, 5.1222],
        [5.1222, 5.1224, 5.0698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.19498692452907562 
model_pd.l_d.mean(): -20.6950626373291 
model_pd.lagr.mean(): -20.500076293945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3926], device='cuda:0')), ('power', tensor([-21.3222], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.19498692452907562
epoch£º363	 i:1 	 global-step:7261	 l-p:0.1503041386604309
epoch£º363	 i:2 	 global-step:7262	 l-p:0.08308009803295135
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11465740203857422
epoch£º363	 i:4 	 global-step:7264	 l-p:0.12735451757907867
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14189893007278442
epoch£º363	 i:6 	 global-step:7266	 l-p:0.11656955629587173
epoch£º363	 i:7 	 global-step:7267	 l-p:0.15544366836547852
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12031371891498566
epoch£º363	 i:9 	 global-step:7269	 l-p:0.12348587065935135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1268, 6.0416, 6.4960],
        [5.1268, 5.1044, 5.1019],
        [5.1268, 5.1267, 5.1268],
        [5.1268, 5.1374, 5.0736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14770397543907166 
model_pd.l_d.mean(): -19.33277702331543 
model_pd.lagr.mean(): -19.185073852539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.0204], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14770397543907166
epoch£º364	 i:1 	 global-step:7281	 l-p:0.12148921936750412
epoch£º364	 i:2 	 global-step:7282	 l-p:-1.0271304845809937
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11937540024518967
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13848596811294556
epoch£º364	 i:5 	 global-step:7285	 l-p:0.08951814472675323
epoch£º364	 i:6 	 global-step:7286	 l-p:0.0549326129257679
epoch£º364	 i:7 	 global-step:7287	 l-p:0.1763787418603897
epoch£º364	 i:8 	 global-step:7288	 l-p:0.1329720914363861
epoch£º364	 i:9 	 global-step:7289	 l-p:0.044765353202819824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8606, 4.8606, 4.8606],
        [4.8606, 4.8396, 4.7913],
        [4.8606, 4.8294, 4.8058],
        [4.8606, 4.8545, 4.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.0982549861073494 
model_pd.l_d.mean(): -20.77568817138672 
model_pd.lagr.mean(): -20.677433013916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.4775], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.0982549861073494
epoch£º365	 i:1 	 global-step:7301	 l-p:0.17632174491882324
epoch£º365	 i:2 	 global-step:7302	 l-p:0.1501721441745758
epoch£º365	 i:3 	 global-step:7303	 l-p:0.11083896458148956
epoch£º365	 i:4 	 global-step:7304	 l-p:0.14577952027320862
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13734735548496246
epoch£º365	 i:6 	 global-step:7306	 l-p:-0.4153642952442169
epoch£º365	 i:7 	 global-step:7307	 l-p:0.11338979750871658
epoch£º365	 i:8 	 global-step:7308	 l-p:0.12614883482456207
epoch£º365	 i:9 	 global-step:7309	 l-p:0.13455677032470703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9393, 4.9356, 4.9389],
        [4.9393, 5.6249, 5.8877],
        [4.9393, 5.7130, 6.0548],
        [4.9393, 4.9363, 4.9390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.1164741963148117 
model_pd.l_d.mean(): -18.692171096801758 
model_pd.lagr.mean(): -18.57569694519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6124], device='cuda:0')), ('power', tensor([-19.5220], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.1164741963148117
epoch£º366	 i:1 	 global-step:7321	 l-p:0.13697971403598785
epoch£º366	 i:2 	 global-step:7322	 l-p:0.1369888037443161
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13767200708389282
epoch£º366	 i:4 	 global-step:7324	 l-p:-0.7428145408630371
epoch£º366	 i:5 	 global-step:7325	 l-p:0.1326862871646881
epoch£º366	 i:6 	 global-step:7326	 l-p:0.09360837936401367
epoch£º366	 i:7 	 global-step:7327	 l-p:0.14085441827774048
epoch£º366	 i:8 	 global-step:7328	 l-p:0.06688384711742401
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12536990642547607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.0808, 5.8287, 6.1319],
        [5.0808, 5.0680, 5.0243],
        [5.0808, 5.0564, 5.0396],
        [5.0808, 5.1694, 5.0720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.22823669016361237 
model_pd.l_d.mean(): -19.335235595703125 
model_pd.lagr.mean(): -19.106998443603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.0591], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.22823669016361237
epoch£º367	 i:1 	 global-step:7341	 l-p:0.14305973052978516
epoch£º367	 i:2 	 global-step:7342	 l-p:0.14232495427131653
epoch£º367	 i:3 	 global-step:7343	 l-p:0.14377723634243011
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13072943687438965
epoch£º367	 i:5 	 global-step:7345	 l-p:0.1300479620695114
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14306314289569855
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12054944038391113
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10063178092241287
epoch£º367	 i:9 	 global-step:7349	 l-p:0.12953409552574158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2093, 5.2093, 5.2093],
        [5.2093, 5.2093, 5.2093],
        [5.2093, 5.2071, 5.2091],
        [5.2093, 5.2608, 5.1733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.1563127487897873 
model_pd.l_d.mean(): -20.297801971435547 
model_pd.lagr.mean(): -20.141489028930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-20.9504], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.1563127487897873
epoch£º368	 i:1 	 global-step:7361	 l-p:0.08503026515245438
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12415637075901031
epoch£º368	 i:3 	 global-step:7363	 l-p:0.20331023633480072
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11638108640909195
epoch£º368	 i:5 	 global-step:7365	 l-p:0.1357046514749527
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15769869089126587
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11758392304182053
epoch£º368	 i:8 	 global-step:7368	 l-p:0.10905170440673828
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18318407237529755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 5.1303, 5.0309],
        [5.0183, 5.0165, 5.0182],
        [5.0183, 5.0183, 5.0183],
        [5.0183, 5.7466, 6.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.14611241221427917 
model_pd.l_d.mean(): -19.73404312133789 
model_pd.lagr.mean(): -19.58793067932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-20.5017], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.14611241221427917
epoch£º369	 i:1 	 global-step:7381	 l-p:0.12363149225711823
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08349857479333878
epoch£º369	 i:3 	 global-step:7383	 l-p:0.1748940795660019
epoch£º369	 i:4 	 global-step:7384	 l-p:0.12767456471920013
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12350982427597046
epoch£º369	 i:6 	 global-step:7386	 l-p:0.16883790493011475
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09319991618394852
epoch£º369	 i:8 	 global-step:7388	 l-p:-2.8585901260375977
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12788887321949005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 5.0603, 4.9666],
        [5.0183, 5.0159, 5.0182],
        [5.0183, 5.5356, 5.6510],
        [5.0183, 4.9884, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.13192762434482574 
model_pd.l_d.mean(): -17.95648193359375 
model_pd.lagr.mean(): -17.824554443359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5624], device='cuda:0')), ('power', tensor([-18.7271], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.13192762434482574
epoch£º370	 i:1 	 global-step:7401	 l-p:0.13666744530200958
epoch£º370	 i:2 	 global-step:7402	 l-p:0.07132534682750702
epoch£º370	 i:3 	 global-step:7403	 l-p:0.1378811150789261
epoch£º370	 i:4 	 global-step:7404	 l-p:0.13357606530189514
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13490167260169983
epoch£º370	 i:6 	 global-step:7406	 l-p:0.15223172307014465
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11684295535087585
epoch£º370	 i:8 	 global-step:7408	 l-p:-5.6915154457092285
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12220318615436554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0345, 5.0345, 5.0345],
        [5.0345, 5.0333, 5.0344],
        [5.0345, 5.7565, 6.0408],
        [5.0345, 5.0235, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.09795329719781876 
model_pd.l_d.mean(): -20.284225463867188 
model_pd.lagr.mean(): -20.382179260253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.9851], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.09795329719781876
epoch£º371	 i:1 	 global-step:7421	 l-p:0.12921224534511566
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12137125432491302
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14552688598632812
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11169308423995972
epoch£º371	 i:5 	 global-step:7425	 l-p:-0.0910550206899643
epoch£º371	 i:6 	 global-step:7426	 l-p:0.11763022094964981
epoch£º371	 i:7 	 global-step:7427	 l-p:0.19881656765937805
epoch£º371	 i:8 	 global-step:7428	 l-p:0.12984681129455566
epoch£º371	 i:9 	 global-step:7429	 l-p:0.12190856039524078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 5.6346, 5.9971],
        [4.8474, 4.8474, 4.8474],
        [4.8474, 4.8460, 4.8473],
        [4.8474, 4.8539, 4.7659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.13016481697559357 
model_pd.l_d.mean(): -19.42595100402832 
model_pd.lagr.mean(): -19.295785903930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.2043], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.13016481697559357
epoch£º372	 i:1 	 global-step:7441	 l-p:0.06888876110315323
epoch£º372	 i:2 	 global-step:7442	 l-p:0.13453984260559082
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15795418620109558
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18107831478118896
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13206355273723602
epoch£º372	 i:6 	 global-step:7446	 l-p:0.20492485165596008
epoch£º372	 i:7 	 global-step:7447	 l-p:0.16459102928638458
epoch£º372	 i:8 	 global-step:7448	 l-p:0.1418556421995163
epoch£º372	 i:9 	 global-step:7449	 l-p:0.08554687350988388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8498, 5.5047, 5.7482],
        [4.8498, 4.8310, 4.8430],
        [4.8498, 4.8498, 4.8498],
        [4.8498, 4.8896, 4.7866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14398185908794403 
model_pd.l_d.mean(): -19.023681640625 
model_pd.lagr.mean(): -18.87969970703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.8190], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14398185908794403
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.04290914535522461
epoch£º373	 i:2 	 global-step:7462	 l-p:0.0807681456208229
epoch£º373	 i:3 	 global-step:7463	 l-p:0.15703366696834564
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11837472021579742
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13326892256736755
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14463968575000763
epoch£º373	 i:7 	 global-step:7467	 l-p:0.15959708392620087
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14679381251335144
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10243576765060425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9573, 4.9543, 4.8782],
        [4.9573, 4.9326, 4.9441],
        [4.9573, 4.9380, 4.9496],
        [4.9573, 5.7145, 6.0373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09905348718166351 
model_pd.l_d.mean(): -20.610036849975586 
model_pd.lagr.mean(): -20.510982513427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.2972], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09905348718166351
epoch£º374	 i:1 	 global-step:7481	 l-p:0.2339189052581787
epoch£º374	 i:2 	 global-step:7482	 l-p:0.06190194934606552
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12594948709011078
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13861659169197083
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1352631002664566
epoch£º374	 i:6 	 global-step:7486	 l-p:0.09785690158605576
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10802839696407318
epoch£º374	 i:8 	 global-step:7488	 l-p:0.1408647894859314
epoch£º374	 i:9 	 global-step:7489	 l-p:0.06713659316301346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9056, 4.9016, 4.9052],
        [4.9056, 4.9056, 4.9056],
        [4.9056, 4.9009, 4.9050],
        [4.9056, 4.9054, 4.9056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.0700472816824913 
model_pd.l_d.mean(): -20.469987869262695 
model_pd.lagr.mean(): -20.399940490722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.1667], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.0700472816824913
epoch£º375	 i:1 	 global-step:7501	 l-p:0.37040790915489197
epoch£º375	 i:2 	 global-step:7502	 l-p:0.1486731618642807
epoch£º375	 i:3 	 global-step:7503	 l-p:0.12172450125217438
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12726829946041107
epoch£º375	 i:5 	 global-step:7505	 l-p:0.13325433433055878
epoch£º375	 i:6 	 global-step:7506	 l-p:0.18703904747962952
epoch£º375	 i:7 	 global-step:7507	 l-p:0.13227073848247528
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14610444009304047
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11689028143882751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0618, 5.0332, 5.0401],
        [5.0618, 5.1067, 5.0088],
        [5.0618, 5.0617, 5.0618],
        [5.0618, 5.0404, 4.9941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11038938164710999 
model_pd.l_d.mean(): -19.676023483276367 
model_pd.lagr.mean(): -19.56563377380371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.3996], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11038938164710999
epoch£º376	 i:1 	 global-step:7521	 l-p:0.16779226064682007
epoch£º376	 i:2 	 global-step:7522	 l-p:0.16697798669338226
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13313241302967072
epoch£º376	 i:4 	 global-step:7524	 l-p:0.3268144130706787
epoch£º376	 i:5 	 global-step:7525	 l-p:0.13619357347488403
epoch£º376	 i:6 	 global-step:7526	 l-p:0.13498687744140625
epoch£º376	 i:7 	 global-step:7527	 l-p:0.11021130532026291
epoch£º376	 i:8 	 global-step:7528	 l-p:0.1325487345457077
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.09170849621295929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 5.0372, 4.9815],
        [5.0543, 5.0255, 5.0333],
        [5.0543, 5.0287, 5.0390],
        [5.0543, 5.7431, 5.9951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.016835197806358337 
model_pd.l_d.mean(): -20.28012466430664 
model_pd.lagr.mean(): -20.296958923339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.9575], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.016835197806358337
epoch£º377	 i:1 	 global-step:7541	 l-p:0.02534649707376957
epoch£º377	 i:2 	 global-step:7542	 l-p:0.16616882383823395
epoch£º377	 i:3 	 global-step:7543	 l-p:0.23497720062732697
epoch£º377	 i:4 	 global-step:7544	 l-p:0.11495573818683624
epoch£º377	 i:5 	 global-step:7545	 l-p:0.1291220486164093
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14148463308811188
epoch£º377	 i:7 	 global-step:7547	 l-p:0.13147695362567902
epoch£º377	 i:8 	 global-step:7548	 l-p:0.07333055138587952
epoch£º377	 i:9 	 global-step:7549	 l-p:0.12133582681417465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2202, 5.2534, 5.1656],
        [5.2202, 5.2202, 5.2202],
        [5.2202, 5.1928, 5.1843],
        [5.2202, 5.1983, 5.2070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.1363203525543213 
model_pd.l_d.mean(): -19.896944046020508 
model_pd.lagr.mean(): -19.760623931884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5831], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.1363203525543213
epoch£º378	 i:1 	 global-step:7561	 l-p:0.11166828125715256
epoch£º378	 i:2 	 global-step:7562	 l-p:0.13183987140655518
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13730739057064056
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10580717772245407
epoch£º378	 i:5 	 global-step:7565	 l-p:0.1556074619293213
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14392238855361938
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12419592589139938
epoch£º378	 i:8 	 global-step:7568	 l-p:0.12448830157518387
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13486020267009735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 4.9663, 4.9322],
        [4.9992, 4.9992, 4.9992],
        [4.9992, 4.9899, 4.9974],
        [4.9992, 4.9761, 4.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.146581768989563 
model_pd.l_d.mean(): -19.108745574951172 
model_pd.lagr.mean(): -18.9621639251709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.8638], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.146581768989563
epoch£º379	 i:1 	 global-step:7581	 l-p:0.09324248135089874
epoch£º379	 i:2 	 global-step:7582	 l-p:0.12446729093790054
epoch£º379	 i:3 	 global-step:7583	 l-p:0.09126622974872589
epoch£º379	 i:4 	 global-step:7584	 l-p:0.14159180223941803
epoch£º379	 i:5 	 global-step:7585	 l-p:0.1403525322675705
epoch£º379	 i:6 	 global-step:7586	 l-p:0.07638982683420181
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13409143686294556
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14801464974880219
epoch£º379	 i:9 	 global-step:7589	 l-p:0.12384337186813354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7712, 4.9185, 4.8204],
        [4.7712, 4.7572, 4.7678],
        [4.7712, 4.7709, 4.7712],
        [4.7712, 4.7682, 4.7710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.15763752162456512 
model_pd.l_d.mean(): -20.415163040161133 
model_pd.lagr.mean(): -20.257526397705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5207], device='cuda:0')), ('power', tensor([-21.1701], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.15763752162456512
epoch£º380	 i:1 	 global-step:7601	 l-p:0.1859702616930008
epoch£º380	 i:2 	 global-step:7602	 l-p:0.12983344495296478
epoch£º380	 i:3 	 global-step:7603	 l-p:0.12952113151550293
epoch£º380	 i:4 	 global-step:7604	 l-p:0.12224613130092621
epoch£º380	 i:5 	 global-step:7605	 l-p:0.16148467361927032
epoch£º380	 i:6 	 global-step:7606	 l-p:0.10794170945882797
epoch£º380	 i:7 	 global-step:7607	 l-p:0.028377117589116096
epoch£º380	 i:8 	 global-step:7608	 l-p:-0.06298236548900604
epoch£º380	 i:9 	 global-step:7609	 l-p:0.7171287536621094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7286, 4.7286, 4.7286],
        [4.7286, 5.1255, 5.1716],
        [4.7286, 4.6917, 4.7061],
        [4.7286, 4.6760, 4.6575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.3549639880657196 
model_pd.l_d.mean(): -20.108739852905273 
model_pd.lagr.mean(): -19.75377655029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5972], device='cuda:0')), ('power', tensor([-20.9385], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.3549639880657196
epoch£º381	 i:1 	 global-step:7621	 l-p:0.21399055421352386
epoch£º381	 i:2 	 global-step:7622	 l-p:0.10452689975500107
epoch£º381	 i:3 	 global-step:7623	 l-p:0.08688059449195862
epoch£º381	 i:4 	 global-step:7624	 l-p:0.3170757293701172
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12677046656608582
epoch£º381	 i:6 	 global-step:7626	 l-p:0.12902632355690002
epoch£º381	 i:7 	 global-step:7627	 l-p:0.17972993850708008
epoch£º381	 i:8 	 global-step:7628	 l-p:0.1108781024813652
epoch£º381	 i:9 	 global-step:7629	 l-p:0.13887998461723328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1343, 5.1006, 5.0968],
        [5.1343, 5.1341, 5.1343],
        [5.1343, 5.1336, 5.1343],
        [5.1343, 5.1022, 5.0787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.12881429493427277 
model_pd.l_d.mean(): -20.783681869506836 
model_pd.lagr.mean(): -20.65486717224121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3700], device='cuda:0')), ('power', tensor([-21.3887], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.12881429493427277
epoch£º382	 i:1 	 global-step:7641	 l-p:0.14454683661460876
epoch£º382	 i:2 	 global-step:7642	 l-p:0.1521768420934677
epoch£º382	 i:3 	 global-step:7643	 l-p:0.134989395737648
epoch£º382	 i:4 	 global-step:7644	 l-p:-0.0814518928527832
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11541914939880371
epoch£º382	 i:6 	 global-step:7646	 l-p:0.06348732113838196
epoch£º382	 i:7 	 global-step:7647	 l-p:0.18111295998096466
epoch£º382	 i:8 	 global-step:7648	 l-p:0.004199206829071045
epoch£º382	 i:9 	 global-step:7649	 l-p:0.1093364804983139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1032, 5.1005, 5.1030],
        [5.1032, 5.1028, 5.0238],
        [5.1032, 5.1032, 5.1032],
        [5.1032, 5.0740, 5.0835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.12752556800842285 
model_pd.l_d.mean(): -20.191389083862305 
model_pd.lagr.mean(): -20.06386375427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4504], device='cuda:0')), ('power', tensor([-20.8721], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.12752556800842285
epoch£º383	 i:1 	 global-step:7661	 l-p:0.31699007749557495
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12983423471450806
epoch£º383	 i:3 	 global-step:7663	 l-p:0.1216849610209465
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08499601483345032
epoch£º383	 i:5 	 global-step:7665	 l-p:0.1320868879556656
epoch£º383	 i:6 	 global-step:7666	 l-p:0.1430794596672058
epoch£º383	 i:7 	 global-step:7667	 l-p:0.1353113353252411
epoch£º383	 i:8 	 global-step:7668	 l-p:0.1735827475786209
epoch£º383	 i:9 	 global-step:7669	 l-p:-0.17546142637729645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9801, 5.5025, 5.6224],
        [4.9801, 4.9434, 4.9026],
        [4.9801, 5.2278, 5.1661],
        [4.9801, 4.9563, 4.9696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.3535443842411041 
model_pd.l_d.mean(): -20.768465042114258 
model_pd.lagr.mean(): -20.414920806884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4404], device='cuda:0')), ('power', tensor([-21.4453], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.3535443842411041
epoch£º384	 i:1 	 global-step:7681	 l-p:0.1582493633031845
epoch£º384	 i:2 	 global-step:7682	 l-p:0.1339360922574997
epoch£º384	 i:3 	 global-step:7683	 l-p:0.17421779036521912
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13931505382061005
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10903220623731613
epoch£º384	 i:6 	 global-step:7686	 l-p:0.10961055755615234
epoch£º384	 i:7 	 global-step:7687	 l-p:0.022256111726164818
epoch£º384	 i:8 	 global-step:7688	 l-p:0.11038968712091446
epoch£º384	 i:9 	 global-step:7689	 l-p:0.13219793140888214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9993, 4.9918, 4.9087],
        [4.9993, 4.9973, 4.9992],
        [4.9993, 4.9993, 4.9993],
        [4.9993, 5.4873, 5.5791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.13833190500736237 
model_pd.l_d.mean(): -20.233320236206055 
model_pd.lagr.mean(): -20.094987869262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-20.9509], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.13833190500736237
epoch£º385	 i:1 	 global-step:7701	 l-p:0.12392646819353104
epoch£º385	 i:2 	 global-step:7702	 l-p:0.14555028080940247
epoch£º385	 i:3 	 global-step:7703	 l-p:0.11721650511026382
epoch£º385	 i:4 	 global-step:7704	 l-p:0.12209644168615341
epoch£º385	 i:5 	 global-step:7705	 l-p:0.14839614927768707
epoch£º385	 i:6 	 global-step:7706	 l-p:0.10703346133232117
epoch£º385	 i:7 	 global-step:7707	 l-p:0.1300530731678009
epoch£º385	 i:8 	 global-step:7708	 l-p:0.2588004767894745
epoch£º385	 i:9 	 global-step:7709	 l-p:0.3702284097671509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8089, 4.7692, 4.7824],
        [4.8089, 4.9912, 4.9028],
        [4.8089, 4.8089, 4.8089],
        [4.8089, 4.7774, 4.7931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.17112097144126892 
model_pd.l_d.mean(): -20.180103302001953 
model_pd.lagr.mean(): -20.008981704711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5625], device='cuda:0')), ('power', tensor([-20.9752], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.17112097144126892
epoch£º386	 i:1 	 global-step:7721	 l-p:0.1598527729511261
epoch£º386	 i:2 	 global-step:7722	 l-p:0.16099734604358673
epoch£º386	 i:3 	 global-step:7723	 l-p:0.8601126670837402
epoch£º386	 i:4 	 global-step:7724	 l-p:0.045747436583042145
epoch£º386	 i:5 	 global-step:7725	 l-p:0.0853591337800026
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11207503825426102
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12181650102138519
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16283637285232544
epoch£º386	 i:9 	 global-step:7729	 l-p:0.12685465812683105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2632, 5.2436, 5.2547],
        [5.2632, 5.7627, 5.8443],
        [5.2632, 5.2627, 5.2632],
        [5.2632, 5.2339, 5.2398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.12548623979091644 
model_pd.l_d.mean(): -20.59626007080078 
model_pd.lagr.mean(): -20.470773696899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3674], device='cuda:0')), ('power', tensor([-21.1966], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.12548623979091644
epoch£º387	 i:1 	 global-step:7741	 l-p:0.12672673165798187
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12480545043945312
epoch£º387	 i:3 	 global-step:7743	 l-p:0.003424215130507946
epoch£º387	 i:4 	 global-step:7744	 l-p:0.11801362782716751
epoch£º387	 i:5 	 global-step:7745	 l-p:0.13423238694667816
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10919439047574997
epoch£º387	 i:7 	 global-step:7747	 l-p:0.14862379431724548
epoch£º387	 i:8 	 global-step:7748	 l-p:0.17216303944587708
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11776939779520035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1216, 5.1187, 5.1214],
        [5.1216, 5.0994, 5.1121],
        [5.1216, 5.1181, 5.1213],
        [5.1216, 5.1099, 5.0355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.11388852447271347 
model_pd.l_d.mean(): -19.43585968017578 
model_pd.lagr.mean(): -19.321971893310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5480], device='cuda:0')), ('power', tensor([-20.2080], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.11388852447271347
epoch£º388	 i:1 	 global-step:7761	 l-p:-0.03631485998630524
epoch£º388	 i:2 	 global-step:7762	 l-p:0.016480883583426476
epoch£º388	 i:3 	 global-step:7763	 l-p:0.18533611297607422
epoch£º388	 i:4 	 global-step:7764	 l-p:0.13344548642635345
epoch£º388	 i:5 	 global-step:7765	 l-p:0.15530596673488617
epoch£º388	 i:6 	 global-step:7766	 l-p:0.4163709878921509
epoch£º388	 i:7 	 global-step:7767	 l-p:0.15242838859558105
epoch£º388	 i:8 	 global-step:7768	 l-p:0.13112908601760864
epoch£º388	 i:9 	 global-step:7769	 l-p:0.12939928472042084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9318, 4.9862, 4.8696],
        [4.9318, 4.8954, 4.8324],
        [4.9318, 4.9316, 4.9318],
        [4.9318, 4.8943, 4.9068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.12375655025243759 
model_pd.l_d.mean(): -19.401655197143555 
model_pd.lagr.mean(): -19.27789878845215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5457], device='cuda:0')), ('power', tensor([-20.1711], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.12375655025243759
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11561857908964157
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15355321764945984
epoch£º389	 i:3 	 global-step:7783	 l-p:0.14448562264442444
epoch£º389	 i:4 	 global-step:7784	 l-p:0.052054475992918015
epoch£º389	 i:5 	 global-step:7785	 l-p:0.14746205508708954
epoch£º389	 i:6 	 global-step:7786	 l-p:0.1494196355342865
epoch£º389	 i:7 	 global-step:7787	 l-p:0.14190229773521423
epoch£º389	 i:8 	 global-step:7788	 l-p:0.09033475816249847
epoch£º389	 i:9 	 global-step:7789	 l-p:0.08332663029432297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0017, 5.0014, 5.0017],
        [5.0017, 4.9552, 4.9355],
        [5.0017, 5.5328, 5.6563],
        [5.0017, 5.0014, 5.0017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.0705280750989914 
model_pd.l_d.mean(): -20.67472267150879 
model_pd.lagr.mean(): -20.60419464111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.3512], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.0705280750989914
epoch£º390	 i:1 	 global-step:7801	 l-p:0.11922992765903473
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12510734796524048
epoch£º390	 i:3 	 global-step:7803	 l-p:0.15344028174877167
epoch£º390	 i:4 	 global-step:7804	 l-p:0.12350188940763474
epoch£º390	 i:5 	 global-step:7805	 l-p:0.16459056735038757
epoch£º390	 i:6 	 global-step:7806	 l-p:0.1190674677491188
epoch£º390	 i:7 	 global-step:7807	 l-p:0.13035781681537628
epoch£º390	 i:8 	 global-step:7808	 l-p:0.14475767314434052
epoch£º390	 i:9 	 global-step:7809	 l-p:0.13734422624111176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1400, 5.1323, 5.1388],
        [5.1400, 5.1371, 5.0515],
        [5.1400, 5.1185, 5.0527],
        [5.1400, 5.1364, 5.1397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.15321704745292664 
model_pd.l_d.mean(): -20.669870376586914 
model_pd.lagr.mean(): -20.516653060913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3956], device='cuda:0')), ('power', tensor([-21.2998], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.15321704745292664
epoch£º391	 i:1 	 global-step:7821	 l-p:0.4191804826259613
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13126178085803986
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12024904042482376
epoch£º391	 i:4 	 global-step:7824	 l-p:0.11072663962841034
epoch£º391	 i:5 	 global-step:7825	 l-p:0.12107880413532257
epoch£º391	 i:6 	 global-step:7826	 l-p:-0.1929868906736374
epoch£º391	 i:7 	 global-step:7827	 l-p:0.11878655850887299
epoch£º391	 i:8 	 global-step:7828	 l-p:0.12361880391836166
epoch£º391	 i:9 	 global-step:7829	 l-p:0.1687747985124588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8992, 4.8624, 4.8773],
        [4.8992, 4.8990, 4.8992],
        [4.8992, 5.6566, 5.9801],
        [4.8992, 4.8992, 4.8992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.11917974054813385 
model_pd.l_d.mean(): -19.911958694458008 
model_pd.lagr.mean(): -19.79277801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.6374], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.11917974054813385
epoch£º392	 i:1 	 global-step:7841	 l-p:0.11747479438781738
epoch£º392	 i:2 	 global-step:7842	 l-p:0.1467532366514206
epoch£º392	 i:3 	 global-step:7843	 l-p:0.14842794835567474
epoch£º392	 i:4 	 global-step:7844	 l-p:0.16313037276268005
epoch£º392	 i:5 	 global-step:7845	 l-p:0.21057327091693878
epoch£º392	 i:6 	 global-step:7846	 l-p:0.14514502882957458
epoch£º392	 i:7 	 global-step:7847	 l-p:0.3305489718914032
epoch£º392	 i:8 	 global-step:7848	 l-p:0.12530460953712463
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11513493955135345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9962, 5.6170, 5.8126],
        [4.9962, 4.9962, 4.9962],
        [4.9962, 5.2048, 5.1209],
        [4.9962, 4.9962, 4.9962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.2999788522720337 
model_pd.l_d.mean(): -20.328535079956055 
model_pd.lagr.mean(): -20.02855682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-21.0528], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.2999788522720337
epoch£º393	 i:1 	 global-step:7861	 l-p:0.15002988278865814
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15930607914924622
epoch£º393	 i:3 	 global-step:7863	 l-p:0.11972764879465103
epoch£º393	 i:4 	 global-step:7864	 l-p:0.38015297055244446
epoch£º393	 i:5 	 global-step:7865	 l-p:0.12671637535095215
epoch£º393	 i:6 	 global-step:7866	 l-p:-0.22197765111923218
epoch£º393	 i:7 	 global-step:7867	 l-p:0.14210322499275208
epoch£º393	 i:8 	 global-step:7868	 l-p:0.13427014648914337
epoch£º393	 i:9 	 global-step:7869	 l-p:0.12425313889980316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0851, 5.0496, 5.0614],
        [5.0851, 5.0847, 5.0851],
        [5.0851, 5.0515, 4.9933],
        [5.0851, 5.8134, 6.0924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.17642001807689667 
model_pd.l_d.mean(): -20.55608367919922 
model_pd.lagr.mean(): -20.379663467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4240], device='cuda:0')), ('power', tensor([-21.2138], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.17642001807689667
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11753474920988083
epoch£º394	 i:2 	 global-step:7882	 l-p:0.11135396361351013
epoch£º394	 i:3 	 global-step:7883	 l-p:0.09845006465911865
epoch£º394	 i:4 	 global-step:7884	 l-p:0.09909392893314362
epoch£º394	 i:5 	 global-step:7885	 l-p:0.12047065794467926
epoch£º394	 i:6 	 global-step:7886	 l-p:0.15069574117660522
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13498039543628693
epoch£º394	 i:8 	 global-step:7888	 l-p:0.18278783559799194
epoch£º394	 i:9 	 global-step:7889	 l-p:0.14677807688713074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.0354, 5.0369],
        [5.0369, 4.9924, 4.9963],
        [5.0369, 5.0369, 5.0369],
        [5.0369, 5.0004, 5.0133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.11706263571977615 
model_pd.l_d.mean(): -20.103776931762695 
model_pd.lagr.mean(): -19.986713409423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4933], device='cuda:0')), ('power', tensor([-20.8274], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.11706263571977615
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12162475287914276
epoch£º395	 i:2 	 global-step:7902	 l-p:0.1377226561307907
epoch£º395	 i:3 	 global-step:7903	 l-p:0.15367279946804047
epoch£º395	 i:4 	 global-step:7904	 l-p:0.07146667689085007
epoch£º395	 i:5 	 global-step:7905	 l-p:0.058955900371074677
epoch£º395	 i:6 	 global-step:7906	 l-p:0.1438322216272354
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11772920191287994
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1705506443977356
epoch£º395	 i:9 	 global-step:7909	 l-p:0.1658114641904831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9028, 5.0852, 4.9907],
        [4.9028, 5.1444, 5.0775],
        [4.9028, 4.8841, 4.8975],
        [4.9028, 4.8913, 4.9006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.13952216506004333 
model_pd.l_d.mean(): -21.063751220703125 
model_pd.lagr.mean(): -20.92422866821289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4174], device='cuda:0')), ('power', tensor([-21.7203], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.13952216506004333
epoch£º396	 i:1 	 global-step:7921	 l-p:0.04906729608774185
epoch£º396	 i:2 	 global-step:7922	 l-p:0.12576541304588318
epoch£º396	 i:3 	 global-step:7923	 l-p:0.14212879538536072
epoch£º396	 i:4 	 global-step:7924	 l-p:0.0937480702996254
epoch£º396	 i:5 	 global-step:7925	 l-p:0.16669483482837677
epoch£º396	 i:6 	 global-step:7926	 l-p:0.12212740629911423
epoch£º396	 i:7 	 global-step:7927	 l-p:0.05225633457303047
epoch£º396	 i:8 	 global-step:7928	 l-p:0.15644462406635284
epoch£º396	 i:9 	 global-step:7929	 l-p:0.10922098904848099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9445, 5.6174, 5.8605],
        [4.9445, 4.9391, 4.9439],
        [4.9445, 4.8976, 4.9063],
        [4.9445, 4.9005, 4.9123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11591672897338867 
model_pd.l_d.mean(): -19.302337646484375 
model_pd.lagr.mean(): -19.186420440673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5974], device='cuda:0')), ('power', tensor([-20.1235], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11591672897338867
epoch£º397	 i:1 	 global-step:7941	 l-p:0.045283135026693344
epoch£º397	 i:2 	 global-step:7942	 l-p:0.17020869255065918
epoch£º397	 i:3 	 global-step:7943	 l-p:0.13204778730869293
epoch£º397	 i:4 	 global-step:7944	 l-p:0.0805339515209198
epoch£º397	 i:5 	 global-step:7945	 l-p:0.24953524768352509
epoch£º397	 i:6 	 global-step:7946	 l-p:0.06638393551111221
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13999402523040771
epoch£º397	 i:8 	 global-step:7948	 l-p:0.13166899979114532
epoch£º397	 i:9 	 global-step:7949	 l-p:0.05177650228142738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0926, 5.3408, 5.2711],
        [5.0926, 5.0926, 5.0926],
        [5.0926, 5.0911, 5.0925],
        [5.0926, 5.2713, 5.1722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): -0.043189745396375656 
model_pd.l_d.mean(): -19.31651496887207 
model_pd.lagr.mean(): -19.359704971313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-20.0513], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:-0.043189745396375656
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10885155200958252
epoch£º398	 i:2 	 global-step:7962	 l-p:0.24220211803913116
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14516820013523102
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14283980429172516
epoch£º398	 i:5 	 global-step:7965	 l-p:0.11750146746635437
epoch£º398	 i:6 	 global-step:7966	 l-p:0.13457347452640533
epoch£º398	 i:7 	 global-step:7967	 l-p:0.12544259428977966
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12450062483549118
epoch£º398	 i:9 	 global-step:7969	 l-p:0.114249587059021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0876, 5.0871, 5.0876],
        [5.0876, 5.0456, 5.0550],
        [5.0876, 5.1806, 5.0609],
        [5.0876, 5.0596, 4.9829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11482688784599304 
model_pd.l_d.mean(): -20.03469467163086 
model_pd.lagr.mean(): -19.91986846923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4723], device='cuda:0')), ('power', tensor([-20.7360], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11482688784599304
epoch£º399	 i:1 	 global-step:7981	 l-p:0.08505281060934067
epoch£º399	 i:2 	 global-step:7982	 l-p:0.12240120023488998
epoch£º399	 i:3 	 global-step:7983	 l-p:-3.222867965698242
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13170526921749115
epoch£º399	 i:5 	 global-step:7985	 l-p:0.1436942219734192
epoch£º399	 i:6 	 global-step:7986	 l-p:0.13697123527526855
epoch£º399	 i:7 	 global-step:7987	 l-p:0.14881786704063416
epoch£º399	 i:8 	 global-step:7988	 l-p:0.16935037076473236
epoch£º399	 i:9 	 global-step:7989	 l-p:-0.06784234195947647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9459, 4.9337, 4.8273],
        [4.9459, 5.0588, 4.9395],
        [4.9459, 4.9295, 4.9419],
        [4.9459, 4.9242, 4.8249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.144120454788208 
model_pd.l_d.mean(): -19.655073165893555 
model_pd.lagr.mean(): -19.51095199584961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5051], device='cuda:0')), ('power', tensor([-20.3857], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.144120454788208
epoch£º400	 i:1 	 global-step:8001	 l-p:0.10587829351425171
epoch£º400	 i:2 	 global-step:8002	 l-p:0.10917013138532639
epoch£º400	 i:3 	 global-step:8003	 l-p:0.1410006433725357
epoch£º400	 i:4 	 global-step:8004	 l-p:0.14285047352313995
epoch£º400	 i:5 	 global-step:8005	 l-p:0.45679014921188354
epoch£º400	 i:6 	 global-step:8006	 l-p:0.142963707447052
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.067522332072258
epoch£º400	 i:8 	 global-step:8008	 l-p:0.10932646691799164
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13163356482982635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9858, 5.1508, 5.0464],
        [4.9858, 4.9855, 4.9858],
        [4.9858, 5.0980, 4.9785],
        [4.9858, 5.2918, 5.2560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14798736572265625 
model_pd.l_d.mean(): -19.490751266479492 
model_pd.lagr.mean(): -19.342763900756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5826], device='cuda:0')), ('power', tensor([-20.2988], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14798736572265625
epoch£º401	 i:1 	 global-step:8021	 l-p:0.15165109932422638
epoch£º401	 i:2 	 global-step:8022	 l-p:0.14773543179035187
epoch£º401	 i:3 	 global-step:8023	 l-p:0.390331894159317
epoch£º401	 i:4 	 global-step:8024	 l-p:0.12012454867362976
epoch£º401	 i:5 	 global-step:8025	 l-p:0.02462198957800865
epoch£º401	 i:6 	 global-step:8026	 l-p:0.1484324187040329
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11629287153482437
epoch£º401	 i:8 	 global-step:8028	 l-p:0.0946240946650505
epoch£º401	 i:9 	 global-step:8029	 l-p:0.1502772867679596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9772, 4.9772, 4.9772],
        [4.9772, 4.9767, 4.9772],
        [4.9772, 4.9305, 4.9418],
        [4.9772, 4.9772, 4.9772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13737882673740387 
model_pd.l_d.mean(): -20.754749298095703 
model_pd.lagr.mean(): -20.61737060546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4269], device='cuda:0')), ('power', tensor([-21.4175], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13737882673740387
epoch£º402	 i:1 	 global-step:8041	 l-p:0.14003047347068787
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13712596893310547
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09805575758218765
epoch£º402	 i:4 	 global-step:8044	 l-p:0.12099608778953552
epoch£º402	 i:5 	 global-step:8045	 l-p:0.07322913408279419
epoch£º402	 i:6 	 global-step:8046	 l-p:0.14327360689640045
epoch£º402	 i:7 	 global-step:8047	 l-p:0.19306081533432007
epoch£º402	 i:8 	 global-step:8048	 l-p:0.48696181178092957
epoch£º402	 i:9 	 global-step:8049	 l-p:0.16090959310531616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0147, 4.9746, 4.9895],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 4.9774, 4.9934],
        [5.0147, 4.9992, 5.0110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.17629194259643555 
model_pd.l_d.mean(): -20.52300262451172 
model_pd.lagr.mean(): -20.346710205078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.1962], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.17629194259643555
epoch£º403	 i:1 	 global-step:8061	 l-p:0.12880778312683105
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13692855834960938
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12853185832500458
epoch£º403	 i:4 	 global-step:8064	 l-p:0.07832001149654388
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13894318044185638
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12439725548028946
epoch£º403	 i:7 	 global-step:8067	 l-p:0.28334277868270874
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12406442314386368
epoch£º403	 i:9 	 global-step:8069	 l-p:0.07918716967105865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0714, 5.1961, 5.0790],
        [5.0714, 5.5454, 5.6170],
        [5.0714, 5.0435, 5.0595],
        [5.0714, 5.0666, 5.0709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.15471385419368744 
model_pd.l_d.mean(): -19.174297332763672 
model_pd.lagr.mean(): -19.019582748413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-19.9314], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.15471385419368744
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11422362923622131
epoch£º404	 i:2 	 global-step:8082	 l-p:0.12361261993646622
epoch£º404	 i:3 	 global-step:8083	 l-p:0.12914147973060608
epoch£º404	 i:4 	 global-step:8084	 l-p:0.1210266500711441
epoch£º404	 i:5 	 global-step:8085	 l-p:0.03894360736012459
epoch£º404	 i:6 	 global-step:8086	 l-p:0.16127975285053253
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12118585407733917
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.1551089882850647
epoch£º404	 i:9 	 global-step:8089	 l-p:0.14839422702789307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1014, 5.0898, 4.9916],
        [5.1014, 5.1014, 5.1014],
        [5.1014, 5.1014, 5.1014],
        [5.1014, 5.0518, 5.0125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10045552253723145 
model_pd.l_d.mean(): -20.365962982177734 
model_pd.lagr.mean(): -20.265506744384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.0492], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10045552253723145
epoch£º405	 i:1 	 global-step:8101	 l-p:0.16892968118190765
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10518676787614822
epoch£º405	 i:3 	 global-step:8103	 l-p:0.008700513280928135
epoch£º405	 i:4 	 global-step:8104	 l-p:-0.05345886945724487
epoch£º405	 i:5 	 global-step:8105	 l-p:0.03287704288959503
epoch£º405	 i:6 	 global-step:8106	 l-p:0.1570291817188263
epoch£º405	 i:7 	 global-step:8107	 l-p:0.15010999143123627
epoch£º405	 i:8 	 global-step:8108	 l-p:0.12559038400650024
epoch£º405	 i:9 	 global-step:8109	 l-p:0.1329478770494461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0978, 5.0533, 5.0628],
        [5.0978, 5.0795, 5.0926],
        [5.0978, 5.0473, 5.0094],
        [5.0978, 5.2671, 5.1621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.004810590762645006 
model_pd.l_d.mean(): -19.683773040771484 
model_pd.lagr.mean(): -19.67896270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4897], device='cuda:0')), ('power', tensor([-20.3990], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.004810590762645006
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09869426488876343
epoch£º406	 i:2 	 global-step:8122	 l-p:0.1300114542245865
epoch£º406	 i:3 	 global-step:8123	 l-p:0.1669372320175171
epoch£º406	 i:4 	 global-step:8124	 l-p:0.1505887359380722
epoch£º406	 i:5 	 global-step:8125	 l-p:0.42420142889022827
epoch£º406	 i:6 	 global-step:8126	 l-p:0.10511190444231033
epoch£º406	 i:7 	 global-step:8127	 l-p:0.07395116239786148
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12695717811584473
epoch£º406	 i:9 	 global-step:8129	 l-p:0.12874799966812134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0611, 5.0551, 5.0604],
        [5.0611, 5.0557, 5.0605],
        [5.0611, 5.1502, 5.0263],
        [5.0611, 5.0502, 4.9471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.1640244424343109 
model_pd.l_d.mean(): -20.644001007080078 
model_pd.lagr.mean(): -20.479976654052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.3017], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.1640244424343109
epoch£º407	 i:1 	 global-step:8141	 l-p:0.12643279135227203
epoch£º407	 i:2 	 global-step:8142	 l-p:0.15208087861537933
epoch£º407	 i:3 	 global-step:8143	 l-p:0.11851032078266144
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.30589738488197327
epoch£º407	 i:5 	 global-step:8145	 l-p:0.1463782638311386
epoch£º407	 i:6 	 global-step:8146	 l-p:0.1406048685312271
epoch£º407	 i:7 	 global-step:8147	 l-p:0.37535983324050903
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11569612473249435
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12227410823106766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1184, 5.1340],
        [5.1473, 5.1217, 5.1370],
        [5.1473, 5.0994, 5.0594],
        [5.1473, 5.2292, 5.1063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.3765721023082733 
model_pd.l_d.mean(): -20.58094596862793 
model_pd.lagr.mean(): -20.204374313354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4183], device='cuda:0')), ('power', tensor([-21.2331], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.3765721023082733
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13967202603816986
epoch£º408	 i:2 	 global-step:8162	 l-p:0.14620254933834076
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13925419747829437
epoch£º408	 i:4 	 global-step:8164	 l-p:0.2698366045951843
epoch£º408	 i:5 	 global-step:8165	 l-p:0.20604559779167175
epoch£º408	 i:6 	 global-step:8166	 l-p:0.12017711997032166
epoch£º408	 i:7 	 global-step:8167	 l-p:0.1217786967754364
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12908154726028442
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09388425946235657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 5.1365, 5.1500],
        [5.1559, 5.1346, 5.1488],
        [5.1559, 5.1543, 5.1558],
        [5.1559, 5.1548, 5.1559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.12832306325435638 
model_pd.l_d.mean(): -20.601760864257812 
model_pd.lagr.mean(): -20.473438262939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3869], device='cuda:0')), ('power', tensor([-21.2221], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.12832306325435638
epoch£º409	 i:1 	 global-step:8181	 l-p:0.16529586911201477
epoch£º409	 i:2 	 global-step:8182	 l-p:0.3117589056491852
epoch£º409	 i:3 	 global-step:8183	 l-p:0.1439478099346161
epoch£º409	 i:4 	 global-step:8184	 l-p:0.14055459201335907
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12384410947561264
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.02452675811946392
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.15423133969306946
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14307771623134613
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11667556315660477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1136, 5.0835, 5.0998],
        [5.1136, 5.0651, 5.0697],
        [5.1136, 5.1079, 5.1129],
        [5.1136, 5.1136, 5.1136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15392492711544037 
model_pd.l_d.mean(): -20.46546173095703 
model_pd.lagr.mean(): -20.31153678894043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4363], device='cuda:0')), ('power', tensor([-21.1348], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15392492711544037
epoch£º410	 i:1 	 global-step:8201	 l-p:0.1330236941576004
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09660182893276215
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12880884110927582
epoch£º410	 i:4 	 global-step:8204	 l-p:0.3283258080482483
epoch£º410	 i:5 	 global-step:8205	 l-p:0.1316029578447342
epoch£º410	 i:6 	 global-step:8206	 l-p:0.08867836743593216
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12982557713985443
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14572428166866302
epoch£º410	 i:9 	 global-step:8209	 l-p:0.08284906297922134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0055, 5.0055, 5.0055],
        [5.0055, 5.0048, 5.0055],
        [5.0055, 5.0053, 5.0055],
        [5.0055, 5.0043, 5.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.4922642111778259 
model_pd.l_d.mean(): -20.367488861083984 
model_pd.lagr.mean(): -19.875225067138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-21.0791], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.4922642111778259
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13549385964870453
epoch£º411	 i:2 	 global-step:8222	 l-p:0.1512942910194397
epoch£º411	 i:3 	 global-step:8223	 l-p:0.17312607169151306
epoch£º411	 i:4 	 global-step:8224	 l-p:0.08825834095478058
epoch£º411	 i:5 	 global-step:8225	 l-p:0.36304911971092224
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07587624341249466
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12248747795820236
epoch£º411	 i:8 	 global-step:8228	 l-p:0.13533709943294525
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14213480055332184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9785, 4.9726, 4.9778],
        [4.9785, 5.7479, 6.0695],
        [4.9785, 4.9785, 4.9785],
        [4.9785, 4.9254, 4.9333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.28530752658843994 
model_pd.l_d.mean(): -19.183740615844727 
model_pd.lagr.mean(): -18.898433685302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5364], device='cuda:0')), ('power', tensor([-19.9412], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.28530752658843994
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10840526223182678
epoch£º412	 i:2 	 global-step:8242	 l-p:0.13690805435180664
epoch£º412	 i:3 	 global-step:8243	 l-p:0.14638113975524902
epoch£º412	 i:4 	 global-step:8244	 l-p:0.1228187084197998
epoch£º412	 i:5 	 global-step:8245	 l-p:0.15709826350212097
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09288495779037476
epoch£º412	 i:7 	 global-step:8247	 l-p:0.07813264429569244
epoch£º412	 i:8 	 global-step:8248	 l-p:0.16440166532993317
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11918371170759201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9202, 4.8746, 4.8914],
        [4.9202, 4.9202, 4.9202],
        [4.9202, 5.6087, 5.8644],
        [4.9202, 4.9202, 4.9202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1500740796327591 
model_pd.l_d.mean(): -19.941410064697266 
model_pd.lagr.mean(): -19.791336059570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5257], device='cuda:0')), ('power', tensor([-20.6963], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.1500740796327591
epoch£º413	 i:1 	 global-step:8261	 l-p:0.1477936953306198
epoch£º413	 i:2 	 global-step:8262	 l-p:0.1354639232158661
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07719632238149643
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11970971524715424
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18296881020069122
epoch£º413	 i:6 	 global-step:8266	 l-p:0.17006616294384003
epoch£º413	 i:7 	 global-step:8267	 l-p:0.1554315835237503
epoch£º413	 i:8 	 global-step:8268	 l-p:0.026837332174181938
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13065114617347717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8873, 4.8836, 4.8870],
        [4.8873, 4.9998, 4.8759],
        [4.8873, 4.8873, 4.8873],
        [4.8873, 4.8629, 4.8795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08890830725431442 
model_pd.l_d.mean(): -19.56110382080078 
model_pd.lagr.mean(): -19.47219467163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5489], device='cuda:0')), ('power', tensor([-20.3356], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.08890830725431442
epoch£º414	 i:1 	 global-step:8281	 l-p:0.14454568922519684
epoch£º414	 i:2 	 global-step:8282	 l-p:0.17269332706928253
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12554730474948883
epoch£º414	 i:4 	 global-step:8284	 l-p:0.15087270736694336
epoch£º414	 i:5 	 global-step:8285	 l-p:0.17389129102230072
epoch£º414	 i:6 	 global-step:8286	 l-p:0.08931399136781693
epoch£º414	 i:7 	 global-step:8287	 l-p:0.10468912124633789
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13419674336910248
epoch£º414	 i:9 	 global-step:8289	 l-p:0.16007529199123383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9336, 5.0213, 4.8924],
        [4.9336, 5.3976, 5.4703],
        [4.9336, 4.9702, 4.8390],
        [4.9336, 4.9197, 4.9308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10670863837003708 
model_pd.l_d.mean(): -19.901390075683594 
model_pd.lagr.mean(): -19.794681549072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5674], device='cuda:0')), ('power', tensor([-20.6984], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.10670863837003708
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12467721849679947
epoch£º415	 i:2 	 global-step:8302	 l-p:0.15854868292808533
epoch£º415	 i:3 	 global-step:8303	 l-p:0.1297820657491684
epoch£º415	 i:4 	 global-step:8304	 l-p:0.14163275063037872
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.135952889919281
epoch£º415	 i:6 	 global-step:8306	 l-p:0.1343488246202469
epoch£º415	 i:7 	 global-step:8307	 l-p:0.133292093873024
epoch£º415	 i:8 	 global-step:8308	 l-p:0.0532858744263649
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12813785672187805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9581, 4.9272, 4.9453],
        [4.9581, 4.9581, 4.9581],
        [4.9581, 5.0333, 4.9028],
        [4.9581, 4.9581, 4.9581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.16988365352153778 
model_pd.l_d.mean(): -20.660343170166016 
model_pd.lagr.mean(): -20.490459442138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-21.3619], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.16988365352153778
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.4915291368961334
epoch£º416	 i:2 	 global-step:8322	 l-p:0.07465578615665436
epoch£º416	 i:3 	 global-step:8323	 l-p:0.11425073444843292
epoch£º416	 i:4 	 global-step:8324	 l-p:0.12423799932003021
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08341222256422043
epoch£º416	 i:6 	 global-step:8326	 l-p:0.11063811928033829
epoch£º416	 i:7 	 global-step:8327	 l-p:0.12571987509727478
epoch£º416	 i:8 	 global-step:8328	 l-p:0.1194005012512207
epoch£º416	 i:9 	 global-step:8329	 l-p:0.127730593085289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0229, 5.0147, 5.0217],
        [5.0229, 5.0059, 5.0187],
        [5.0229, 4.9789, 4.9946],
        [5.0229, 5.0229, 5.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.3483411967754364 
model_pd.l_d.mean(): -20.429136276245117 
model_pd.lagr.mean(): -20.080795288085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.1246], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.3483411967754364
epoch£º417	 i:1 	 global-step:8341	 l-p:0.14090891182422638
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12917733192443848
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12080106139183044
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12457917630672455
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14458060264587402
epoch£º417	 i:6 	 global-step:8346	 l-p:0.047878969460725784
epoch£º417	 i:7 	 global-step:8347	 l-p:0.10591327399015427
epoch£º417	 i:8 	 global-step:8348	 l-p:0.14273768663406372
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14636695384979248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9365, 4.9364, 4.9365],
        [4.9365, 4.9364, 4.9365],
        [4.9365, 5.1249, 5.0265],
        [4.9365, 4.8730, 4.8673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.08322759717702866 
model_pd.l_d.mean(): -19.642789840698242 
model_pd.lagr.mean(): -19.726016998291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5215], device='cuda:0')), ('power', tensor([-20.3901], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.08322759717702866
epoch£º418	 i:1 	 global-step:8361	 l-p:0.15134941041469574
epoch£º418	 i:2 	 global-step:8362	 l-p:0.1097770631313324
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13780704140663147
epoch£º418	 i:4 	 global-step:8364	 l-p:0.14891283214092255
epoch£º418	 i:5 	 global-step:8365	 l-p:0.11990153789520264
epoch£º418	 i:6 	 global-step:8366	 l-p:0.1207948848605156
epoch£º418	 i:7 	 global-step:8367	 l-p:0.182358980178833
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11714721471071243
epoch£º418	 i:9 	 global-step:8369	 l-p:0.13263097405433655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9461, 4.8828, 4.8774],
        [4.9461, 4.9775, 4.8457],
        [4.9461, 4.9145, 4.8095],
        [4.9461, 4.9371, 4.9448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.15546804666519165 
model_pd.l_d.mean(): -20.31970977783203 
model_pd.lagr.mean(): -20.164241790771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-21.0609], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.15546804666519165
epoch£º419	 i:1 	 global-step:8381	 l-p:0.13357210159301758
epoch£º419	 i:2 	 global-step:8382	 l-p:0.1164209172129631
epoch£º419	 i:3 	 global-step:8383	 l-p:0.13319095969200134
epoch£º419	 i:4 	 global-step:8384	 l-p:0.14062993228435516
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08391723781824112
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.025618037208914757
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12788140773773193
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13942252099514008
epoch£º419	 i:9 	 global-step:8389	 l-p:0.2879520058631897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9825, 5.1299, 5.0148],
        [4.9825, 4.9240, 4.9274],
        [4.9825, 4.9185, 4.8819],
        [4.9825, 4.9821, 4.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.07012531906366348 
model_pd.l_d.mean(): -19.63835334777832 
model_pd.lagr.mean(): -19.568227767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5047], device='cuda:0')), ('power', tensor([-20.3685], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.07012531906366348
epoch£º420	 i:1 	 global-step:8401	 l-p:0.12930820882320404
epoch£º420	 i:2 	 global-step:8402	 l-p:0.13456742465496063
epoch£º420	 i:3 	 global-step:8403	 l-p:0.13209453225135803
epoch£º420	 i:4 	 global-step:8404	 l-p:0.1305827796459198
epoch£º420	 i:5 	 global-step:8405	 l-p:0.24238616228103638
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07489163428544998
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12492995709180832
epoch£º420	 i:8 	 global-step:8408	 l-p:0.10993380099534988
epoch£º420	 i:9 	 global-step:8409	 l-p:0.22990135848522186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0570, 5.0007, 4.9477],
        [5.0570, 5.6755, 5.8590],
        [5.0570, 5.0227, 5.0406],
        [5.0570, 5.0510, 5.0563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09883154928684235 
model_pd.l_d.mean(): -20.313613891601562 
model_pd.lagr.mean(): -20.21478271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-21.0084], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09883154928684235
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12569551169872284
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11559426784515381
epoch£º421	 i:3 	 global-step:8423	 l-p:0.13706468045711517
epoch£º421	 i:4 	 global-step:8424	 l-p:0.17564530670642853
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06599002331495285
epoch£º421	 i:6 	 global-step:8426	 l-p:0.9569184184074402
epoch£º421	 i:7 	 global-step:8427	 l-p:0.14213575422763824
epoch£º421	 i:8 	 global-step:8428	 l-p:0.13493624329566956
epoch£º421	 i:9 	 global-step:8429	 l-p:0.12724870443344116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0733, 5.0587, 5.0701],
        [5.0733, 5.0733, 5.0733],
        [5.0733, 5.0726, 5.0733],
        [5.0733, 5.0975, 4.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.2659284770488739 
model_pd.l_d.mean(): -20.276098251342773 
model_pd.lagr.mean(): -20.010169982910156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.9773], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.2659284770488739
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12950538098812103
epoch£º422	 i:2 	 global-step:8442	 l-p:0.1179918646812439
epoch£º422	 i:3 	 global-step:8443	 l-p:0.12316201627254486
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16174516081809998
epoch£º422	 i:5 	 global-step:8445	 l-p:0.09192309528589249
epoch£º422	 i:6 	 global-step:8446	 l-p:0.08559119701385498
epoch£º422	 i:7 	 global-step:8447	 l-p:0.010759527795016766
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15884947776794434
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12416991591453552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1365, 5.1365, 5.1365],
        [5.1365, 5.2183, 5.0900],
        [5.1365, 5.1198, 5.1323],
        [5.1365, 5.1177, 5.0164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10818616300821304 
model_pd.l_d.mean(): -20.10442352294922 
model_pd.lagr.mean(): -19.99623680114746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-20.8017], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10818616300821304
epoch£º423	 i:1 	 global-step:8461	 l-p:-1.8509827852249146
epoch£º423	 i:2 	 global-step:8462	 l-p:0.32171371579170227
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1413220763206482
epoch£º423	 i:4 	 global-step:8464	 l-p:0.19338126480579376
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11404075473546982
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13491013646125793
epoch£º423	 i:7 	 global-step:8467	 l-p:0.13224229216575623
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12404154241085052
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12108027935028076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1199, 5.1144],
        [5.1739, 5.1718, 5.1738],
        [5.1739, 5.4554, 5.3955],
        [5.1739, 5.1739, 5.1739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.12905974686145782 
model_pd.l_d.mean(): -20.150043487548828 
model_pd.lagr.mean(): -20.020984649658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-20.8435], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.12905974686145782
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13376933336257935
epoch£º424	 i:2 	 global-step:8482	 l-p:-0.5401515960693359
epoch£º424	 i:3 	 global-step:8483	 l-p:0.10334911197423935
epoch£º424	 i:4 	 global-step:8484	 l-p:0.04981868341565132
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14419427514076233
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13359932601451874
epoch£º424	 i:7 	 global-step:8487	 l-p:0.09087638556957245
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.012072992511093616
epoch£º424	 i:9 	 global-step:8489	 l-p:0.2920684516429901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0279, 4.9711, 4.9770],
        [5.0279, 4.9691, 4.9112],
        [5.0279, 5.1799, 5.0650],
        [5.0279, 5.0279, 5.0279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1693100482225418 
model_pd.l_d.mean(): -19.516862869262695 
model_pd.lagr.mean(): -19.347553253173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-20.2263], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1693100482225418
epoch£º425	 i:1 	 global-step:8501	 l-p:0.14283518493175507
epoch£º425	 i:2 	 global-step:8502	 l-p:0.2550394833087921
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.2927619218826294
epoch£º425	 i:4 	 global-step:8504	 l-p:0.0909310132265091
epoch£º425	 i:5 	 global-step:8505	 l-p:0.11864259093999863
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11912361532449722
epoch£º425	 i:7 	 global-step:8507	 l-p:0.014695650897920132
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07553580403327942
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12304842472076416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1143, 5.1063, 5.1132],
        [5.1143, 5.1137, 5.1143],
        [5.1143, 5.1119, 5.1142],
        [5.1143, 5.4757, 5.4644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.6613050103187561 
model_pd.l_d.mean(): -19.044958114624023 
model_pd.lagr.mean(): -19.706262588500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-19.7945], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.6613050103187561
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1866564154624939
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13610133528709412
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14553984999656677
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12301184982061386
epoch£º426	 i:5 	 global-step:8525	 l-p:0.12663501501083374
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12463259696960449
epoch£º426	 i:7 	 global-step:8527	 l-p:0.0994543731212616
epoch£º426	 i:8 	 global-step:8528	 l-p:0.15275420248508453
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11399274319410324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 5.0530, 5.0582],
        [5.0588, 5.0562, 5.0586],
        [5.0588, 5.0137, 5.0299],
        [5.0588, 5.0623, 4.9397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1618494987487793 
model_pd.l_d.mean(): -19.81324005126953 
model_pd.lagr.mean(): -19.651390075683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-20.4881], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1618494987487793
epoch£º427	 i:1 	 global-step:8541	 l-p:0.10332973301410675
epoch£º427	 i:2 	 global-step:8542	 l-p:0.2645569443702698
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11587313562631607
epoch£º427	 i:4 	 global-step:8544	 l-p:0.12024811655282974
epoch£º427	 i:5 	 global-step:8545	 l-p:0.005321245174854994
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17394396662712097
epoch£º427	 i:7 	 global-step:8547	 l-p:0.07568065822124481
epoch£º427	 i:8 	 global-step:8548	 l-p:0.1545541137456894
epoch£º427	 i:9 	 global-step:8549	 l-p:0.14120706915855408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[4.9935, 5.4506, 5.5121],
        [4.9935, 4.9304, 4.8731],
        [4.9935, 5.3168, 5.2871],
        [4.9935, 4.9387, 4.8609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.25898170471191406 
model_pd.l_d.mean(): -20.606470108032227 
model_pd.lagr.mean(): -20.347488403320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-21.2964], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.25898170471191406
epoch£º428	 i:1 	 global-step:8561	 l-p:0.1197006031870842
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10669715702533722
epoch£º428	 i:3 	 global-step:8563	 l-p:0.13411787152290344
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12288042902946472
epoch£º428	 i:5 	 global-step:8565	 l-p:0.007093085907399654
epoch£º428	 i:6 	 global-step:8566	 l-p:0.16051077842712402
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1319269984960556
epoch£º428	 i:8 	 global-step:8568	 l-p:0.17215050756931305
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11811305582523346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9044, 4.8036],
        [4.9485, 4.9484, 4.9485],
        [4.9485, 5.1253, 5.0192],
        [4.9485, 5.6233, 5.8617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11054813116788864 
model_pd.l_d.mean(): -19.406150817871094 
model_pd.lagr.mean(): -19.295602798461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5921], device='cuda:0')), ('power', tensor([-20.2230], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11054813116788864
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10607732832431793
epoch£º429	 i:2 	 global-step:8582	 l-p:0.15056787431240082
epoch£º429	 i:3 	 global-step:8583	 l-p:0.15707255899906158
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16860127449035645
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.003382124938070774
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11074229329824448
epoch£º429	 i:7 	 global-step:8587	 l-p:0.11249712854623795
epoch£º429	 i:8 	 global-step:8588	 l-p:0.07944297045469284
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11280275881290436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9808, 4.9504, 4.9691],
        [4.9808, 5.6613, 5.9016],
        [4.9808, 4.9223, 4.9319],
        [4.9808, 4.9795, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.31371715664863586 
model_pd.l_d.mean(): -19.34921646118164 
model_pd.lagr.mean(): -19.035499572753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5953], device='cuda:0')), ('power', tensor([-20.1687], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.31371715664863586
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14896687865257263
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13904303312301636
epoch£º430	 i:3 	 global-step:8603	 l-p:0.149549201130867
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13181038200855255
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12201718986034393
epoch£º430	 i:6 	 global-step:8606	 l-p:0.12015285342931747
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12325289100408554
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11300381273031235
epoch£º430	 i:9 	 global-step:8609	 l-p:0.148874893784523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9229, 5.0312, 4.9013],
        [4.9229, 5.5934, 5.8300],
        [4.9229, 4.9108, 4.9208],
        [4.9229, 4.8661, 4.7764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.03984377905726433 
model_pd.l_d.mean(): -20.11870765686035 
model_pd.lagr.mean(): -20.0788631439209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5264], device='cuda:0')), ('power', tensor([-20.8763], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.03984377905726433
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10813751816749573
epoch£º431	 i:2 	 global-step:8622	 l-p:0.13771262764930725
epoch£º431	 i:3 	 global-step:8623	 l-p:0.13268537819385529
epoch£º431	 i:4 	 global-step:8624	 l-p:0.16534820199012756
epoch£º431	 i:5 	 global-step:8625	 l-p:0.0848669782280922
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12768323719501495
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13922953605651855
epoch£º431	 i:8 	 global-step:8628	 l-p:0.1414257436990738
epoch£º431	 i:9 	 global-step:8629	 l-p:0.17456626892089844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9281, 5.5272, 5.7036],
        [4.9281, 4.9281, 4.9281],
        [4.9281, 4.9101, 4.9239],
        [4.9281, 4.8954, 4.9151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.14536836743354797 
model_pd.l_d.mean(): -20.183759689331055 
model_pd.lagr.mean(): -20.03839111328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5215], device='cuda:0')), ('power', tensor([-20.9370], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.14536836743354797
epoch£º432	 i:1 	 global-step:8641	 l-p:0.1542971134185791
epoch£º432	 i:2 	 global-step:8642	 l-p:0.09835847467184067
epoch£º432	 i:3 	 global-step:8643	 l-p:0.12304037809371948
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11284001916646957
epoch£º432	 i:5 	 global-step:8645	 l-p:0.13262484967708588
epoch£º432	 i:6 	 global-step:8646	 l-p:0.137478768825531
epoch£º432	 i:7 	 global-step:8647	 l-p:0.0478251576423645
epoch£º432	 i:8 	 global-step:8648	 l-p:0.2894519865512848
epoch£º432	 i:9 	 global-step:8649	 l-p:0.14621540904045105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0194, 4.9538, 4.9012],
        [5.0194, 5.2105, 5.1084],
        [5.0194, 5.0194, 5.0194],
        [5.0194, 4.9944, 5.0113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.14007912576198578 
model_pd.l_d.mean(): -20.10106658935547 
model_pd.lagr.mean(): -19.960987091064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.8324], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.14007912576198578
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1453026980161667
epoch£º433	 i:2 	 global-step:8662	 l-p:0.1320330649614334
epoch£º433	 i:3 	 global-step:8663	 l-p:0.38871386647224426
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11383809894323349
epoch£º433	 i:5 	 global-step:8665	 l-p:0.05069553107023239
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.024762067943811417
epoch£º433	 i:7 	 global-step:8667	 l-p:0.18875572085380554
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14636015892028809
epoch£º433	 i:9 	 global-step:8669	 l-p:0.1392069160938263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0584, 5.0328, 4.9216],
        [5.0584, 5.3094, 5.2347],
        [5.0584, 4.9930, 4.9806],
        [5.0584, 4.9945, 4.9443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14564377069473267 
model_pd.l_d.mean(): -20.890472412109375 
model_pd.lagr.mean(): -20.744829177856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4040], device='cuda:0')), ('power', tensor([-21.5314], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14564377069473267
epoch£º434	 i:1 	 global-step:8681	 l-p:0.1467507779598236
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11819932609796524
epoch£º434	 i:3 	 global-step:8683	 l-p:0.5651878118515015
epoch£º434	 i:4 	 global-step:8684	 l-p:0.04735688492655754
epoch£º434	 i:5 	 global-step:8685	 l-p:0.19866099953651428
epoch£º434	 i:6 	 global-step:8686	 l-p:0.12220903486013412
epoch£º434	 i:7 	 global-step:8687	 l-p:0.1514805108308792
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14047111570835114
epoch£º434	 i:9 	 global-step:8689	 l-p:0.09900407493114471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9969, 4.9967, 4.9969],
        [4.9969, 4.9969, 4.9969],
        [4.9969, 4.9968, 4.9969],
        [4.9969, 5.5927, 5.7603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.1541573703289032 
model_pd.l_d.mean(): -20.823307037353516 
model_pd.lagr.mean(): -20.66914939880371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4220], device='cuda:0')), ('power', tensor([-21.4818], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.1541573703289032
epoch£º435	 i:1 	 global-step:8701	 l-p:0.09412255883216858
epoch£º435	 i:2 	 global-step:8702	 l-p:0.10899773985147476
epoch£º435	 i:3 	 global-step:8703	 l-p:0.1262579709291458
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13680770993232727
epoch£º435	 i:5 	 global-step:8705	 l-p:0.10105179250240326
epoch£º435	 i:6 	 global-step:8706	 l-p:0.08225149661302567
epoch£º435	 i:7 	 global-step:8707	 l-p:0.18015284836292267
epoch£º435	 i:8 	 global-step:8708	 l-p:0.17118555307388306
epoch£º435	 i:9 	 global-step:8709	 l-p:0.13285009562969208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9148, 5.4608, 5.5948],
        [4.9148, 5.2133, 5.1705],
        [4.9148, 4.9068, 4.9138],
        [4.9148, 4.9898, 4.8511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10706588625907898 
model_pd.l_d.mean(): -20.48853874206543 
model_pd.lagr.mean(): -20.381473541259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-21.2369], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10706588625907898
epoch£º436	 i:1 	 global-step:8721	 l-p:0.10461031645536423
epoch£º436	 i:2 	 global-step:8722	 l-p:0.09773038327693939
epoch£º436	 i:3 	 global-step:8723	 l-p:-0.11623794585466385
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12231385707855225
epoch£º436	 i:5 	 global-step:8725	 l-p:0.13956452906131744
epoch£º436	 i:6 	 global-step:8726	 l-p:0.14960166811943054
epoch£º436	 i:7 	 global-step:8727	 l-p:0.16389043629169464
epoch£º436	 i:8 	 global-step:8728	 l-p:0.1448744237422943
epoch£º436	 i:9 	 global-step:8729	 l-p:0.11639382690191269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0042, 5.0003, 5.0039],
        [5.0042, 5.7585, 6.0595],
        [5.0042, 5.1926, 5.0887],
        [5.0042, 5.2240, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.07362119853496552 
model_pd.l_d.mean(): -19.47456169128418 
model_pd.lagr.mean(): -19.40093994140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-20.1884], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.07362119853496552
epoch£º437	 i:1 	 global-step:8741	 l-p:-11.963396072387695
epoch£º437	 i:2 	 global-step:8742	 l-p:0.18729408085346222
epoch£º437	 i:3 	 global-step:8743	 l-p:0.08962778002023697
epoch£º437	 i:4 	 global-step:8744	 l-p:0.08320307731628418
epoch£º437	 i:5 	 global-step:8745	 l-p:0.11090261489152908
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13994604349136353
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12743018567562103
epoch£º437	 i:8 	 global-step:8748	 l-p:0.18897850811481476
epoch£º437	 i:9 	 global-step:8749	 l-p:0.1114453673362732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2169, 5.1626, 5.1098],
        [5.2169, 5.2852, 5.1530],
        [5.2169, 5.1934, 5.2092],
        [5.2169, 5.2169, 5.2169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12479004263877869 
model_pd.l_d.mean(): -19.382476806640625 
model_pd.lagr.mean(): -19.257686614990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-20.0315], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12479004263877869
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13374321162700653
epoch£º438	 i:2 	 global-step:8762	 l-p:0.20010234415531158
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10667595267295837
epoch£º438	 i:4 	 global-step:8764	 l-p:0.08998841792345047
epoch£º438	 i:5 	 global-step:8765	 l-p:0.13514886796474457
epoch£º438	 i:6 	 global-step:8766	 l-p:0.2426578402519226
epoch£º438	 i:7 	 global-step:8767	 l-p:0.12198503315448761
epoch£º438	 i:8 	 global-step:8768	 l-p:0.15077310800552368
epoch£º438	 i:9 	 global-step:8769	 l-p:0.12076260149478912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1714, 5.1410, 5.0414],
        [5.1714, 5.1611, 5.0457],
        [5.1714, 5.1648, 5.0472],
        [5.1714, 5.1711, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.10238288342952728 
model_pd.l_d.mean(): -20.77895736694336 
model_pd.lagr.mean(): -20.67657470703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3750], device='cuda:0')), ('power', tensor([-21.3891], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.10238288342952728
epoch£º439	 i:1 	 global-step:8781	 l-p:0.12095007300376892
epoch£º439	 i:2 	 global-step:8782	 l-p:-0.1589217334985733
epoch£º439	 i:3 	 global-step:8783	 l-p:0.12002763152122498
epoch£º439	 i:4 	 global-step:8784	 l-p:0.13257186114788055
epoch£º439	 i:5 	 global-step:8785	 l-p:-0.08404649794101715
epoch£º439	 i:6 	 global-step:8786	 l-p:0.13696779310703278
epoch£º439	 i:7 	 global-step:8787	 l-p:0.28791746497154236
epoch£º439	 i:8 	 global-step:8788	 l-p:0.15589040517807007
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12705445289611816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0736, 5.0486, 5.0656],
        [5.0736, 5.0535, 5.0683],
        [5.0736, 5.0062, 4.9904],
        [5.0736, 5.2464, 5.1348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.10307815670967102 
model_pd.l_d.mean(): -19.380725860595703 
model_pd.lagr.mean(): -19.277647018432617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4852], device='cuda:0')), ('power', tensor([-20.0881], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.10307815670967102
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12917374074459076
epoch£º440	 i:2 	 global-step:8802	 l-p:0.11356599628925323
epoch£º440	 i:3 	 global-step:8803	 l-p:0.19615955650806427
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14511872828006744
epoch£º440	 i:5 	 global-step:8805	 l-p:0.1365496665239334
epoch£º440	 i:6 	 global-step:8806	 l-p:0.010158872231841087
epoch£º440	 i:7 	 global-step:8807	 l-p:0.10438434034585953
epoch£º440	 i:8 	 global-step:8808	 l-p:0.15042778849601746
epoch£º440	 i:9 	 global-step:8809	 l-p:0.18409545719623566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0181, 5.0102, 5.0171],
        [5.0181, 5.1776, 5.0613],
        [5.0181, 5.0172, 5.0180],
        [5.0181, 4.9592, 4.8769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.0636649876832962 
model_pd.l_d.mean(): -19.164306640625 
model_pd.lagr.mean(): -19.10064125061035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-19.9350], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.0636649876832962
epoch£º441	 i:1 	 global-step:8821	 l-p:0.14391128718852997
epoch£º441	 i:2 	 global-step:8822	 l-p:-0.016882427036762238
epoch£º441	 i:3 	 global-step:8823	 l-p:0.15927760303020477
epoch£º441	 i:4 	 global-step:8824	 l-p:0.06945226341485977
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13492608070373535
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12708325684070587
epoch£º441	 i:7 	 global-step:8827	 l-p:0.17012439668178558
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11512303352355957
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12256579846143723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.1595, 5.1609],
        [5.1610, 5.5876, 5.6150],
        [5.1610, 5.2107, 5.0759],
        [5.1610, 5.1422, 5.1561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12398659437894821 
model_pd.l_d.mean(): -20.28153419494629 
model_pd.lagr.mean(): -20.157546997070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-20.9291], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12398659437894821
epoch£º442	 i:1 	 global-step:8841	 l-p:0.13075950741767883
epoch£º442	 i:2 	 global-step:8842	 l-p:0.5463070869445801
epoch£º442	 i:3 	 global-step:8843	 l-p:0.15028290450572968
epoch£º442	 i:4 	 global-step:8844	 l-p:0.1490553766489029
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.20259793102741241
epoch£º442	 i:6 	 global-step:8846	 l-p:0.13538551330566406
epoch£º442	 i:7 	 global-step:8847	 l-p:0.13768719136714935
epoch£º442	 i:8 	 global-step:8848	 l-p:0.1169937327504158
epoch£º442	 i:9 	 global-step:8849	 l-p:5.953850746154785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 5.1325, 5.1512],
        [5.1671, 5.1664, 5.1671],
        [5.1671, 5.1661, 5.1670],
        [5.1671, 5.1225, 5.1395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.11284974962472916 
model_pd.l_d.mean(): -20.045089721679688 
model_pd.lagr.mean(): -19.932239532470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4228], device='cuda:0')), ('power', tensor([-20.6960], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.11284974962472916
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13276517391204834
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12777671217918396
epoch£º443	 i:3 	 global-step:8863	 l-p:1.0809210538864136
epoch£º443	 i:4 	 global-step:8864	 l-p:0.15636862814426422
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13694341480731964
epoch£º443	 i:6 	 global-step:8866	 l-p:0.1163792833685875
epoch£º443	 i:7 	 global-step:8867	 l-p:0.01574520952999592
epoch£º443	 i:8 	 global-step:8868	 l-p:0.127630814909935
epoch£º443	 i:9 	 global-step:8869	 l-p:0.4267628788948059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0659, 5.2193, 5.0999],
        [5.0659, 5.0196, 5.0385],
        [5.0659, 4.9964, 4.9540],
        [5.0659, 5.1622, 5.0273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 3.074408769607544 
model_pd.l_d.mean(): -18.972192764282227 
model_pd.lagr.mean(): -15.897784233093262 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5350], device='cuda:0')), ('power', tensor([-19.7260], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:3.074408769607544
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14182981848716736
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13396814465522766
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12738774716854095
epoch£º444	 i:4 	 global-step:8884	 l-p:1.099678874015808
epoch£º444	 i:5 	 global-step:8885	 l-p:0.12866288423538208
epoch£º444	 i:6 	 global-step:8886	 l-p:0.14783082902431488
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15803100168704987
epoch£º444	 i:8 	 global-step:8888	 l-p:0.16685505211353302
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.11562415957450867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9639, 4.9635, 4.9639],
        [4.9639, 4.8940, 4.8211],
        [4.9639, 4.9536, 4.9624],
        [4.9639, 4.8944, 4.8964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.1344296932220459 
model_pd.l_d.mean(): -20.641366958618164 
model_pd.lagr.mean(): -20.77579689025879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-21.3428], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.1344296932220459
epoch£º445	 i:1 	 global-step:8901	 l-p:0.14126627147197723
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10361045598983765
epoch£º445	 i:3 	 global-step:8903	 l-p:0.02169163152575493
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13691119849681854
epoch£º445	 i:5 	 global-step:8905	 l-p:0.16820697486400604
epoch£º445	 i:6 	 global-step:8906	 l-p:0.11055324226617813
epoch£º445	 i:7 	 global-step:8907	 l-p:0.11804952472448349
epoch£º445	 i:8 	 global-step:8908	 l-p:0.13401421904563904
epoch£º445	 i:9 	 global-step:8909	 l-p:0.1294286847114563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.9855, 4.9167, 4.8442],
        [4.9855, 5.4444, 5.5050],
        [4.9855, 4.9486, 4.8311],
        [4.9855, 5.4702, 5.5502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.1151072308421135 
model_pd.l_d.mean(): -20.20532989501953 
model_pd.lagr.mean(): -20.09022331237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5030], device='cuda:0')), ('power', tensor([-20.9399], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.1151072308421135
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14256003499031067
epoch£º446	 i:2 	 global-step:8922	 l-p:0.13230226933956146
epoch£º446	 i:3 	 global-step:8923	 l-p:0.10332394391298294
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14215676486492157
epoch£º446	 i:5 	 global-step:8925	 l-p:0.11988052725791931
epoch£º446	 i:6 	 global-step:8926	 l-p:0.14260318875312805
epoch£º446	 i:7 	 global-step:8927	 l-p:0.1499301642179489
epoch£º446	 i:8 	 global-step:8928	 l-p:0.057297926396131516
epoch£º446	 i:9 	 global-step:8929	 l-p:0.0793527290225029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9600, 4.9551, 4.9596],
        [4.9600, 4.9219, 4.9432],
        [4.9600, 4.8971, 4.8055],
        [4.9600, 4.8995, 4.8040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10417339205741882 
model_pd.l_d.mean(): -20.392742156982422 
model_pd.lagr.mean(): -20.2885684967041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-21.1195], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10417339205741882
epoch£º447	 i:1 	 global-step:8941	 l-p:0.14486806094646454
epoch£º447	 i:2 	 global-step:8942	 l-p:0.15028445422649384
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13703279197216034
epoch£º447	 i:4 	 global-step:8944	 l-p:0.13435673713684082
epoch£º447	 i:5 	 global-step:8945	 l-p:-0.015020051039755344
epoch£º447	 i:6 	 global-step:8946	 l-p:0.14778338372707367
epoch£º447	 i:7 	 global-step:8947	 l-p:0.0992576852440834
epoch£º447	 i:8 	 global-step:8948	 l-p:0.09834928065538406
epoch£º447	 i:9 	 global-step:8949	 l-p:0.1189686581492424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9727, 5.5481, 5.6992],
        [4.9727, 5.3656, 5.3796],
        [4.9727, 4.9442, 4.9632],
        [4.9727, 4.9726, 4.9727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13683386147022247 
model_pd.l_d.mean(): -19.821138381958008 
model_pd.lagr.mean(): -19.68430519104004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.5830], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13683386147022247
epoch£º448	 i:1 	 global-step:8961	 l-p:0.09951674193143845
epoch£º448	 i:2 	 global-step:8962	 l-p:0.14783793687820435
epoch£º448	 i:3 	 global-step:8963	 l-p:0.010810756124556065
epoch£º448	 i:4 	 global-step:8964	 l-p:0.09661412984132767
epoch£º448	 i:5 	 global-step:8965	 l-p:0.08903378248214722
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13677850365638733
epoch£º448	 i:7 	 global-step:8967	 l-p:0.13351531326770782
epoch£º448	 i:8 	 global-step:8968	 l-p:0.22004462778568268
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13766902685165405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9249, 4.9227, 4.9248],
        [4.9249, 4.8451, 4.8296],
        [4.9249, 4.8852, 4.9073],
        [4.9249, 4.8767, 4.8987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.11544331163167953 
model_pd.l_d.mean(): -20.773401260375977 
model_pd.lagr.mean(): -20.657957077026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-21.4762], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.11544331163167953
epoch£º449	 i:1 	 global-step:8981	 l-p:0.10929582267999649
epoch£º449	 i:2 	 global-step:8982	 l-p:0.17044833302497864
epoch£º449	 i:3 	 global-step:8983	 l-p:0.1739397794008255
epoch£º449	 i:4 	 global-step:8984	 l-p:0.16031381487846375
epoch£º449	 i:5 	 global-step:8985	 l-p:0.0647825375199318
epoch£º449	 i:6 	 global-step:8986	 l-p:0.14182975888252258
epoch£º449	 i:7 	 global-step:8987	 l-p:0.1227070763707161
epoch£º449	 i:8 	 global-step:8988	 l-p:0.1787031888961792
epoch£º449	 i:9 	 global-step:8989	 l-p:0.11115739494562149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9592, 4.9587, 4.9592],
        [4.9592, 4.9590, 4.9592],
        [4.9592, 4.9560, 4.9590],
        [4.9592, 5.2109, 5.1363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09339653700590134 
model_pd.l_d.mean(): -20.68358039855957 
model_pd.lagr.mean(): -20.59018325805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-21.3871], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.09339653700590134
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15773823857307434
epoch£º450	 i:2 	 global-step:9002	 l-p:0.12044845521450043
epoch£º450	 i:3 	 global-step:9003	 l-p:0.1280045509338379
epoch£º450	 i:4 	 global-step:9004	 l-p:0.08017592132091522
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.16626735031604767
epoch£º450	 i:6 	 global-step:9006	 l-p:0.17736652493476868
epoch£º450	 i:7 	 global-step:9007	 l-p:0.1493941694498062
epoch£º450	 i:8 	 global-step:9008	 l-p:0.09578527510166168
epoch£º450	 i:9 	 global-step:9009	 l-p:0.12144852429628372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0037, 4.9813, 4.9976],
        [5.0037, 4.9517, 4.9716],
        [5.0037, 5.0036, 5.0037],
        [5.0037, 5.0034, 5.0037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13510291278362274 
model_pd.l_d.mean(): -18.772518157958984 
model_pd.lagr.mean(): -18.637414932250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5872], device='cuda:0')), ('power', tensor([-19.5775], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13510291278362274
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10830478370189667
epoch£º451	 i:2 	 global-step:9022	 l-p:0.14219771325588226
epoch£º451	 i:3 	 global-step:9023	 l-p:0.15878444910049438
epoch£º451	 i:4 	 global-step:9024	 l-p:0.07744260877370834
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11993282288312912
epoch£º451	 i:6 	 global-step:9026	 l-p:-0.10255937278270721
epoch£º451	 i:7 	 global-step:9027	 l-p:0.15062785148620605
epoch£º451	 i:8 	 global-step:9028	 l-p:0.1355471909046173
epoch£º451	 i:9 	 global-step:9029	 l-p:0.05686914175748825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0102, 4.9480, 4.9618],
        [5.0102, 5.0089, 5.0102],
        [5.0102, 5.0075, 5.0101],
        [5.0102, 5.0102, 5.0102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.27165505290031433 
model_pd.l_d.mean(): -20.097105026245117 
model_pd.lagr.mean(): -19.825450897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5063], device='cuda:0')), ('power', tensor([-20.8338], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.27165505290031433
epoch£º452	 i:1 	 global-step:9041	 l-p:-0.1572769582271576
epoch£º452	 i:2 	 global-step:9042	 l-p:0.1090882197022438
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12661071121692657
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12854062020778656
epoch£º452	 i:5 	 global-step:9045	 l-p:0.07589492201805115
epoch£º452	 i:6 	 global-step:9046	 l-p:0.12236111611127853
epoch£º452	 i:7 	 global-step:9047	 l-p:0.0595342256128788
epoch£º452	 i:8 	 global-step:9048	 l-p:0.13448631763458252
epoch£º452	 i:9 	 global-step:9049	 l-p:0.1371109038591385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0577, 5.0563, 5.0576],
        [5.0577, 5.0577, 5.0577],
        [5.0577, 5.0556, 5.0576],
        [5.0577, 5.0208, 5.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.26720744371414185 
model_pd.l_d.mean(): -20.694942474365234 
model_pd.lagr.mean(): -20.427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.3628], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.26720744371414185
epoch£º453	 i:1 	 global-step:9061	 l-p:0.14376506209373474
epoch£º453	 i:2 	 global-step:9062	 l-p:0.06971926242113113
epoch£º453	 i:3 	 global-step:9063	 l-p:0.08083934336900711
epoch£º453	 i:4 	 global-step:9064	 l-p:0.12693557143211365
epoch£º453	 i:5 	 global-step:9065	 l-p:0.13015347719192505
epoch£º453	 i:6 	 global-step:9066	 l-p:0.2787291407585144
epoch£º453	 i:7 	 global-step:9067	 l-p:0.0787532702088356
epoch£º453	 i:8 	 global-step:9068	 l-p:0.11067851632833481
epoch£º453	 i:9 	 global-step:9069	 l-p:0.13958264887332916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1220, 5.0547, 5.0544],
        [5.1220, 5.2641, 5.1375],
        [5.1220, 5.1193, 5.1218],
        [5.1220, 5.1104, 5.1201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.14351379871368408 
model_pd.l_d.mean(): -20.399883270263672 
model_pd.lagr.mean(): -20.25636863708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4157], device='cuda:0')), ('power', tensor([-21.0474], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.14351379871368408
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.13395772874355316
epoch£º454	 i:2 	 global-step:9082	 l-p:0.1710163801908493
epoch£º454	 i:3 	 global-step:9083	 l-p:0.1471637338399887
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15702414512634277
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12284842878580093
epoch£º454	 i:6 	 global-step:9086	 l-p:0.1166607066988945
epoch£º454	 i:7 	 global-step:9087	 l-p:0.0990997925400734
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.0208598505705595
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13340327143669128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0203, 4.9484, 4.8724],
        [5.0203, 5.0198, 5.0203],
        [5.0203, 5.0202, 5.0203],
        [5.0203, 5.0203, 5.0203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.13030505180358887 
model_pd.l_d.mean(): -20.21296501159668 
model_pd.lagr.mean(): -20.082660675048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.9364], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.13030505180358887
epoch£º455	 i:1 	 global-step:9101	 l-p:0.1209716796875
epoch£º455	 i:2 	 global-step:9102	 l-p:0.1518792062997818
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12655435502529144
epoch£º455	 i:4 	 global-step:9104	 l-p:0.0979396253824234
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11204750090837479
epoch£º455	 i:6 	 global-step:9106	 l-p:0.12750332057476044
epoch£º455	 i:7 	 global-step:9107	 l-p:0.26928243041038513
epoch£º455	 i:8 	 global-step:9108	 l-p:0.17043346166610718
epoch£º455	 i:9 	 global-step:9109	 l-p:0.12988372147083282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8421, 4.8421, 4.8421],
        [4.8421, 4.7739, 4.7916],
        [4.8421, 4.8415, 4.8421],
        [4.8421, 4.8391, 4.8419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11533021926879883 
model_pd.l_d.mean(): -20.447315216064453 
model_pd.lagr.mean(): -20.331985473632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5097], device='cuda:0')), ('power', tensor([-21.1913], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11533021926879883
epoch£º456	 i:1 	 global-step:9121	 l-p:0.16401313245296478
epoch£º456	 i:2 	 global-step:9122	 l-p:0.1533477008342743
epoch£º456	 i:3 	 global-step:9123	 l-p:0.2717740535736084
epoch£º456	 i:4 	 global-step:9124	 l-p:0.13570764660835266
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18313533067703247
epoch£º456	 i:6 	 global-step:9126	 l-p:0.14186987280845642
epoch£º456	 i:7 	 global-step:9127	 l-p:0.2156776636838913
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14659574627876282
epoch£º456	 i:9 	 global-step:9129	 l-p:0.09572645276784897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9106, 5.0829, 4.9680],
        [4.9106, 4.8390, 4.8509],
        [4.9106, 4.9150, 4.7652],
        [4.9106, 4.9106, 4.9106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.20014755427837372 
model_pd.l_d.mean(): -20.884355545043945 
model_pd.lagr.mean(): -20.684207916259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.5782], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.20014755427837372
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08358219265937805
epoch£º457	 i:2 	 global-step:9142	 l-p:0.14565566182136536
epoch£º457	 i:3 	 global-step:9143	 l-p:0.14013560116291046
epoch£º457	 i:4 	 global-step:9144	 l-p:0.10616355389356613
epoch£º457	 i:5 	 global-step:9145	 l-p:0.15337379276752472
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12337787449359894
epoch£º457	 i:7 	 global-step:9147	 l-p:0.11775419116020203
epoch£º457	 i:8 	 global-step:9148	 l-p:0.02903876267373562
epoch£º457	 i:9 	 global-step:9149	 l-p:0.1235373392701149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0681, 5.0680, 5.0681],
        [5.0681, 5.0268, 5.0484],
        [5.0681, 5.0681, 5.0681],
        [5.0681, 5.0680, 5.0681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.1452244371175766 
model_pd.l_d.mean(): -20.724647521972656 
model_pd.lagr.mean(): -20.579423904418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4256], device='cuda:0')), ('power', tensor([-21.3858], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.1452244371175766
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11859913915395737
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13252216577529907
epoch£º458	 i:3 	 global-step:9163	 l-p:0.1872621476650238
epoch£º458	 i:4 	 global-step:9164	 l-p:0.1150253638625145
epoch£º458	 i:5 	 global-step:9165	 l-p:0.12608803808689117
epoch£º458	 i:6 	 global-step:9166	 l-p:-0.011921496130526066
epoch£º458	 i:7 	 global-step:9167	 l-p:0.13277778029441833
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12315366417169571
epoch£º458	 i:9 	 global-step:9169	 l-p:0.23948459327220917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1087, 5.1087, 5.1087],
        [5.1087, 5.2453, 5.1155],
        [5.1087, 5.8618, 6.1480],
        [5.1087, 5.1087, 5.1087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.13953320682048798 
model_pd.l_d.mean(): -20.2210750579834 
model_pd.lagr.mean(): -20.081541061401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-20.8733], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.13953320682048798
epoch£º459	 i:1 	 global-step:9181	 l-p:0.14394807815551758
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11593516916036606
epoch£º459	 i:3 	 global-step:9183	 l-p:0.25336888432502747
epoch£º459	 i:4 	 global-step:9184	 l-p:0.07460910826921463
epoch£º459	 i:5 	 global-step:9185	 l-p:0.059478651732206345
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13631314039230347
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11335611343383789
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15185390412807465
epoch£º459	 i:9 	 global-step:9189	 l-p:0.04665474221110344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0354, 5.0260, 5.0342],
        [5.0354, 4.9613, 4.8867],
        [5.0354, 5.7044, 5.9246],
        [5.0354, 5.0677, 4.9199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.1235395222902298 
model_pd.l_d.mean(): -18.784982681274414 
model_pd.lagr.mean(): -18.66144371032715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5695], device='cuda:0')), ('power', tensor([-19.5720], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.1235395222902298
epoch£º460	 i:1 	 global-step:9201	 l-p:0.13084758818149567
epoch£º460	 i:2 	 global-step:9202	 l-p:0.126237154006958
epoch£º460	 i:3 	 global-step:9203	 l-p:0.25601398944854736
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10883516073226929
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14567765593528748
epoch£º460	 i:6 	 global-step:9206	 l-p:0.14438121020793915
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13837333023548126
epoch£º460	 i:8 	 global-step:9208	 l-p:0.10168737173080444
epoch£º460	 i:9 	 global-step:9209	 l-p:0.11140262335538864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9415, 4.9394, 4.9414],
        [4.9415, 4.8565, 4.7905],
        [4.9415, 4.8954, 4.9189],
        [4.9415, 5.0245, 4.8783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.16889458894729614 
model_pd.l_d.mean(): -18.578784942626953 
model_pd.lagr.mean(): -18.40989112854004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5822], device='cuda:0')), ('power', tensor([-19.3765], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.16889458894729614
epoch£º461	 i:1 	 global-step:9221	 l-p:0.04017726331949234
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11197759211063385
epoch£º461	 i:3 	 global-step:9223	 l-p:0.15990079939365387
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13906148076057434
epoch£º461	 i:5 	 global-step:9225	 l-p:0.1302538365125656
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15980549156665802
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07882864028215408
epoch£º461	 i:8 	 global-step:9228	 l-p:0.09789305180311203
epoch£º461	 i:9 	 global-step:9229	 l-p:0.11721494048833847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0067, 4.9664, 4.8384],
        [5.0067, 4.9404, 4.8405],
        [5.0067, 4.9661, 4.9887],
        [5.0067, 4.9667, 4.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09251917898654938 
model_pd.l_d.mean(): -20.467676162719727 
model_pd.lagr.mean(): -20.37515640258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.1748], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09251917898654938
epoch£º462	 i:1 	 global-step:9241	 l-p:0.3066271245479584
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14608286321163177
epoch£º462	 i:3 	 global-step:9243	 l-p:0.09732789546251297
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.1228671744465828
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11980132013559341
epoch£º462	 i:6 	 global-step:9246	 l-p:0.15201768279075623
epoch£º462	 i:7 	 global-step:9247	 l-p:0.0642663761973381
epoch£º462	 i:8 	 global-step:9248	 l-p:0.093787781894207
epoch£º462	 i:9 	 global-step:9249	 l-p:0.14139842987060547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0642, 5.0627, 5.0642],
        [5.0642, 5.0535, 5.0626],
        [5.0642, 5.0546, 5.0629],
        [5.0642, 5.0641, 5.0642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.21861028671264648 
model_pd.l_d.mean(): -18.225576400756836 
model_pd.lagr.mean(): -18.00696563720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5989], device='cuda:0')), ('power', tensor([-19.0365], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.21861028671264648
epoch£º463	 i:1 	 global-step:9261	 l-p:-1.0330456495285034
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14639976620674133
epoch£º463	 i:3 	 global-step:9263	 l-p:0.061307139694690704
epoch£º463	 i:4 	 global-step:9264	 l-p:0.12678490579128265
epoch£º463	 i:5 	 global-step:9265	 l-p:0.11840774863958359
epoch£º463	 i:6 	 global-step:9266	 l-p:0.1235797181725502
epoch£º463	 i:7 	 global-step:9267	 l-p:0.130476713180542
epoch£º463	 i:8 	 global-step:9268	 l-p:0.0814303606748581
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11849942803382874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0996, 5.0960, 5.0994],
        [5.0996, 5.1871, 5.0435],
        [5.0996, 5.3016, 5.1955],
        [5.0996, 5.0996, 5.0996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.0016234016511589289 
model_pd.l_d.mean(): -20.58664894104004 
model_pd.lagr.mean(): -20.585025787353516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.2497], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.0016234016511589289
epoch£º464	 i:1 	 global-step:9281	 l-p:0.1913354992866516
epoch£º464	 i:2 	 global-step:9282	 l-p:0.07221483439207077
epoch£º464	 i:3 	 global-step:9283	 l-p:0.13356620073318481
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11371388286352158
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12203231453895569
epoch£º464	 i:6 	 global-step:9286	 l-p:0.31909388303756714
epoch£º464	 i:7 	 global-step:9287	 l-p:0.14463277161121368
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17794948816299438
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13325972855091095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0927, 5.0927, 5.0927],
        [5.0927, 5.0913, 5.0927],
        [5.0927, 5.0727, 5.0879],
        [5.0927, 5.0134, 4.9691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12989360094070435 
model_pd.l_d.mean(): -20.126853942871094 
model_pd.lagr.mean(): -19.996959686279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4906], device='cuda:0')), ('power', tensor([-20.8479], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12989360094070435
epoch£º465	 i:1 	 global-step:9301	 l-p:0.5430336594581604
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12352105230093002
epoch£º465	 i:3 	 global-step:9303	 l-p:0.13436837494373322
epoch£º465	 i:4 	 global-step:9304	 l-p:0.12039835005998611
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12991775572299957
epoch£º465	 i:6 	 global-step:9306	 l-p:0.1523999720811844
epoch£º465	 i:7 	 global-step:9307	 l-p:0.1577710062265396
epoch£º465	 i:8 	 global-step:9308	 l-p:0.1429138332605362
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09698060154914856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9993, 4.9169, 4.9064],
        [4.9993, 4.9130, 4.8702],
        [4.9993, 4.9837, 4.9963],
        [4.9993, 4.9944, 4.8468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.1394818127155304 
model_pd.l_d.mean(): -18.97753143310547 
model_pd.lagr.mean(): -18.838048934936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-19.7446], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.1394818127155304
epoch£º466	 i:1 	 global-step:9321	 l-p:0.12682649493217468
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13773338496685028
epoch£º466	 i:3 	 global-step:9323	 l-p:0.12992824614048004
epoch£º466	 i:4 	 global-step:9324	 l-p:3.2757673263549805
epoch£º466	 i:5 	 global-step:9325	 l-p:0.06545824557542801
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11473124474287033
epoch£º466	 i:7 	 global-step:9327	 l-p:0.0831947848200798
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12758854031562805
epoch£º466	 i:9 	 global-step:9329	 l-p:0.11132373660802841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0480, 5.0479, 5.0480],
        [5.0480, 5.1804, 5.0467],
        [5.0480, 5.0247, 5.0418],
        [5.0480, 4.9645, 4.9359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12553252279758453 
model_pd.l_d.mean(): -20.329025268554688 
model_pd.lagr.mean(): -20.203493118286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-21.0291], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12553252279758453
epoch£º467	 i:1 	 global-step:9341	 l-p:0.1949518471956253
epoch£º467	 i:2 	 global-step:9342	 l-p:0.3755895793437958
epoch£º467	 i:3 	 global-step:9343	 l-p:0.1219639927148819
epoch£º467	 i:4 	 global-step:9344	 l-p:0.1444510668516159
epoch£º467	 i:5 	 global-step:9345	 l-p:1.3176679611206055
epoch£º467	 i:6 	 global-step:9346	 l-p:0.056466441601514816
epoch£º467	 i:7 	 global-step:9347	 l-p:0.0568864569067955
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11678849160671234
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11389250308275223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1819, 5.1491, 5.1693],
        [5.1819, 5.3476, 5.2251],
        [5.1819, 5.1303, 5.1497],
        [5.1819, 5.8645, 6.0829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.4469025731086731 
model_pd.l_d.mean(): -20.519508361816406 
model_pd.lagr.mean(): -20.07260513305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-21.1702], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.4469025731086731
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10659968852996826
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12661299109458923
epoch£º468	 i:3 	 global-step:9363	 l-p:0.20706889033317566
epoch£º468	 i:4 	 global-step:9364	 l-p:0.14535796642303467
epoch£º468	 i:5 	 global-step:9365	 l-p:0.1440655142068863
epoch£º468	 i:6 	 global-step:9366	 l-p:0.1296544373035431
epoch£º468	 i:7 	 global-step:9367	 l-p:0.11134147644042969
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.1858304888010025
epoch£º468	 i:9 	 global-step:9369	 l-p:0.1133415699005127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1404, 5.0985, 5.1204],
        [5.1404, 5.1404, 5.1404],
        [5.1404, 5.1375, 5.1402],
        [5.1404, 5.1404, 5.1404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09365399926900864 
model_pd.l_d.mean(): -20.47344398498535 
model_pd.lagr.mean(): -20.379789352416992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.1179], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09365399926900864
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08345045149326324
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1290750354528427
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13531532883644104
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.029921064153313637
epoch£º469	 i:5 	 global-step:9385	 l-p:0.5023150444030762
epoch£º469	 i:6 	 global-step:9386	 l-p:0.17943355441093445
epoch£º469	 i:7 	 global-step:9387	 l-p:0.1386413425207138
epoch£º469	 i:8 	 global-step:9388	 l-p:0.10629410296678543
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11702773720026016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0400, 5.0398, 5.0400],
        [5.0400, 4.9569, 4.9421],
        [5.0400, 4.9862, 5.0084],
        [5.0400, 5.0399, 5.0400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12224505096673965 
model_pd.l_d.mean(): -20.591405868530273 
model_pd.lagr.mean(): -20.469160079956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12224505096673965
epoch£º470	 i:1 	 global-step:9401	 l-p:0.1037975549697876
epoch£º470	 i:2 	 global-step:9402	 l-p:0.13229556381702423
epoch£º470	 i:3 	 global-step:9403	 l-p:0.05561598017811775
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15254667401313782
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09326966851949692
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16476909816265106
epoch£º470	 i:7 	 global-step:9407	 l-p:0.15056903660297394
epoch£º470	 i:8 	 global-step:9408	 l-p:0.08934441208839417
epoch£º470	 i:9 	 global-step:9409	 l-p:-6.2246222496032715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0127, 5.0127, 5.0127],
        [5.0127, 5.0127, 5.0127],
        [5.0127, 5.0127, 5.0127],
        [5.0127, 5.0127, 5.0127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.38473638892173767 
model_pd.l_d.mean(): -20.064273834228516 
model_pd.lagr.mean(): -19.679536819458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5128], device='cuda:0')), ('power', tensor([-20.8073], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.38473638892173767
epoch£º471	 i:1 	 global-step:9421	 l-p:0.14253133535385132
epoch£º471	 i:2 	 global-step:9422	 l-p:-44.42096710205078
epoch£º471	 i:3 	 global-step:9423	 l-p:0.13910022377967834
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.09878987818956375
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13832174241542816
epoch£º471	 i:6 	 global-step:9426	 l-p:0.12702232599258423
epoch£º471	 i:7 	 global-step:9427	 l-p:0.13306379318237305
epoch£º471	 i:8 	 global-step:9428	 l-p:0.06737568974494934
epoch£º471	 i:9 	 global-step:9429	 l-p:0.137456014752388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0743, 5.0104, 4.9064],
        [5.0743, 5.0537, 5.0694],
        [5.0743, 5.4237, 5.3986],
        [5.0743, 5.0743, 5.0743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12851709127426147 
model_pd.l_d.mean(): -20.618276596069336 
model_pd.lagr.mean(): -20.48975944519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4314], device='cuda:0')), ('power', tensor([-21.2842], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12851709127426147
epoch£º472	 i:1 	 global-step:9441	 l-p:0.27736082673072815
epoch£º472	 i:2 	 global-step:9442	 l-p:0.1267084926366806
epoch£º472	 i:3 	 global-step:9443	 l-p:0.1369638442993164
epoch£º472	 i:4 	 global-step:9444	 l-p:0.14302657544612885
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12298497557640076
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08206745982170105
epoch£º472	 i:7 	 global-step:9447	 l-p:0.11387574672698975
epoch£º472	 i:8 	 global-step:9448	 l-p:0.1934146136045456
epoch£º472	 i:9 	 global-step:9449	 l-p:0.11958981305360794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9258, 4.8313, 4.7802],
        [4.9258, 4.9258, 4.9258],
        [4.9258, 4.9258, 4.9258],
        [4.9258, 4.9030, 4.9202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.12427287548780441 
model_pd.l_d.mean(): -19.676807403564453 
model_pd.lagr.mean(): -19.552534103393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5985], device='cuda:0')), ('power', tensor([-20.5032], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.12427287548780441
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12684382498264313
epoch£º473	 i:2 	 global-step:9462	 l-p:0.18377470970153809
epoch£º473	 i:3 	 global-step:9463	 l-p:0.11705640703439713
epoch£º473	 i:4 	 global-step:9464	 l-p:0.1338348388671875
epoch£º473	 i:5 	 global-step:9465	 l-p:0.14628753066062927
epoch£º473	 i:6 	 global-step:9466	 l-p:0.17345260083675385
epoch£º473	 i:7 	 global-step:9467	 l-p:0.14801448583602905
epoch£º473	 i:8 	 global-step:9468	 l-p:0.116402767598629
epoch£º473	 i:9 	 global-step:9469	 l-p:0.15620703995227814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9010, 4.8989, 4.9009],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.8377, 4.8606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.1273406445980072 
model_pd.l_d.mean(): -20.272228240966797 
model_pd.lagr.mean(): -20.144887924194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-21.0323], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.1273406445980072
epoch£º474	 i:1 	 global-step:9481	 l-p:0.15032196044921875
epoch£º474	 i:2 	 global-step:9482	 l-p:0.15229782462120056
epoch£º474	 i:3 	 global-step:9483	 l-p:0.1656147688627243
epoch£º474	 i:4 	 global-step:9484	 l-p:0.14959383010864258
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12212046980857849
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12890106439590454
epoch£º474	 i:7 	 global-step:9487	 l-p:0.17398248612880707
epoch£º474	 i:8 	 global-step:9488	 l-p:0.1126977801322937
epoch£º474	 i:9 	 global-step:9489	 l-p:0.027396664023399353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9931, 4.9190, 4.9315],
        [4.9931, 4.9052, 4.8906],
        [4.9931, 4.9931, 4.9931],
        [4.9931, 4.9931, 4.9931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.14262507855892181 
model_pd.l_d.mean(): -20.048311233520508 
model_pd.lagr.mean(): -19.905685424804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5259], device='cuda:0')), ('power', tensor([-20.8046], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.14262507855892181
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15202583372592926
epoch£º475	 i:2 	 global-step:9502	 l-p:0.1372099071741104
epoch£º475	 i:3 	 global-step:9503	 l-p:0.0931270569562912
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10519092530012131
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18530866503715515
epoch£º475	 i:6 	 global-step:9506	 l-p:0.14978337287902832
epoch£º475	 i:7 	 global-step:9507	 l-p:0.19514569640159607
epoch£º475	 i:8 	 global-step:9508	 l-p:0.3011842370033264
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09889906644821167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 5.5382, 5.5330],
        [5.1513, 5.1513, 5.1513],
        [5.1513, 5.4700, 5.4218],
        [5.1513, 5.1023, 5.1244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12023450434207916 
model_pd.l_d.mean(): -20.456926345825195 
model_pd.lagr.mean(): -20.33669090270996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4250], device='cuda:0')), ('power', tensor([-21.1146], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12023450434207916
epoch£º476	 i:1 	 global-step:9521	 l-p:0.12864850461483002
epoch£º476	 i:2 	 global-step:9522	 l-p:0.2122471183538437
epoch£º476	 i:3 	 global-step:9523	 l-p:0.1339777112007141
epoch£º476	 i:4 	 global-step:9524	 l-p:0.14812080562114716
epoch£º476	 i:5 	 global-step:9525	 l-p:0.1373755931854248
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.1727602779865265
epoch£º476	 i:7 	 global-step:9527	 l-p:0.1224343478679657
epoch£º476	 i:8 	 global-step:9528	 l-p:0.06830298900604248
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12444663792848587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.1135, 5.1144],
        [5.1144, 5.2202, 5.0762],
        [5.1144, 5.0710, 5.0940],
        [5.1144, 5.0307, 4.9798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12912628054618835 
model_pd.l_d.mean(): -19.334667205810547 
model_pd.lagr.mean(): -19.205541610717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.0603], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12912628054618835
epoch£º477	 i:1 	 global-step:9541	 l-p:0.13489970564842224
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11050818115472794
epoch£º477	 i:3 	 global-step:9543	 l-p:0.13537226617336273
epoch£º477	 i:4 	 global-step:9544	 l-p:0.1119799017906189
epoch£º477	 i:5 	 global-step:9545	 l-p:0.03378625959157944
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.055837906897068024
epoch£º477	 i:7 	 global-step:9547	 l-p:0.1310662180185318
epoch£º477	 i:8 	 global-step:9548	 l-p:0.1599946916103363
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16208042204380035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9411, 4.9411, 4.9411],
        [4.9411, 5.3503, 5.3717],
        [4.9411, 4.9411, 4.9411],
        [4.9411, 4.8652, 4.7491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.0937018170952797 
model_pd.l_d.mean(): -18.751972198486328 
model_pd.lagr.mean(): -18.65826988220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5953], device='cuda:0')), ('power', tensor([-19.5650], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.0937018170952797
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16553683578968048
epoch£º478	 i:2 	 global-step:9562	 l-p:0.11535660177469254
epoch£º478	 i:3 	 global-step:9563	 l-p:0.18759681284427643
epoch£º478	 i:4 	 global-step:9564	 l-p:0.13445615768432617
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13975673913955688
epoch£º478	 i:6 	 global-step:9566	 l-p:0.13191059231758118
epoch£º478	 i:7 	 global-step:9567	 l-p:0.1604158580303192
epoch£º478	 i:8 	 global-step:9568	 l-p:0.1292620599269867
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15451309084892273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8936, 5.2779, 5.2846],
        [4.8936, 4.8767, 4.8904],
        [4.8936, 4.8109, 4.6952],
        [4.8936, 4.8216, 4.8422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.12589393556118011 
model_pd.l_d.mean(): -19.8925724029541 
model_pd.lagr.mean(): -19.766677856445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5562], device='cuda:0')), ('power', tensor([-20.6781], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.12589393556118011
epoch£º479	 i:1 	 global-step:9581	 l-p:0.12331242114305496
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1557430922985077
epoch£º479	 i:3 	 global-step:9583	 l-p:0.10198257118463516
epoch£º479	 i:4 	 global-step:9584	 l-p:0.1567971259355545
epoch£º479	 i:5 	 global-step:9585	 l-p:0.19740821421146393
epoch£º479	 i:6 	 global-step:9586	 l-p:0.17527097463607788
epoch£º479	 i:7 	 global-step:9587	 l-p:0.20798282325267792
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.015274567529559135
epoch£º479	 i:9 	 global-step:9589	 l-p:0.24286775290966034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8578, 4.8577, 4.8578],
        [4.8578, 4.7614, 4.7538],
        [4.8578, 4.8577, 4.8578],
        [4.8578, 4.7940, 4.8193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12723413109779358 
model_pd.l_d.mean(): -19.347145080566406 
model_pd.lagr.mean(): -19.219911575317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5474], device='cuda:0')), ('power', tensor([-20.1177], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12723413109779358
epoch£º480	 i:1 	 global-step:9601	 l-p:0.1359543353319168
epoch£º480	 i:2 	 global-step:9602	 l-p:0.17951546609401703
epoch£º480	 i:3 	 global-step:9603	 l-p:0.15455159544944763
epoch£º480	 i:4 	 global-step:9604	 l-p:0.0915641263127327
epoch£º480	 i:5 	 global-step:9605	 l-p:0.17635589838027954
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15450166165828705
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14530020952224731
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10975402593612671
epoch£º480	 i:9 	 global-step:9609	 l-p:0.09917569905519485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0322, 4.9976, 5.0199],
        [5.0322, 5.0322, 5.0322],
        [5.0322, 5.0279, 5.0319],
        [5.0322, 5.0321, 5.0322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.1365683376789093 
model_pd.l_d.mean(): -20.824792861938477 
model_pd.lagr.mean(): -20.68822479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4285], device='cuda:0')), ('power', tensor([-21.4901], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.1365683376789093
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13668587803840637
epoch£º481	 i:2 	 global-step:9622	 l-p:0.14124560356140137
epoch£º481	 i:3 	 global-step:9623	 l-p:0.16112349927425385
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.10080379247665405
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14086899161338806
epoch£º481	 i:6 	 global-step:9626	 l-p:0.14012423157691956
epoch£º481	 i:7 	 global-step:9627	 l-p:0.13500024378299713
epoch£º481	 i:8 	 global-step:9628	 l-p:0.06874902546405792
epoch£º481	 i:9 	 global-step:9629	 l-p:0.13913680613040924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9653, 4.9425, 4.9598],
        [4.9653, 4.9158, 4.9414],
        [4.9653, 4.8769, 4.7821],
        [4.9653, 5.0322, 4.8751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.07811729609966278 
model_pd.l_d.mean(): -20.909366607666016 
model_pd.lagr.mean(): -20.831249237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.5779], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.07811729609966278
epoch£º482	 i:1 	 global-step:9641	 l-p:0.1136665865778923
epoch£º482	 i:2 	 global-step:9642	 l-p:0.10108857601881027
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08966007828712463
epoch£º482	 i:4 	 global-step:9644	 l-p:0.17872187495231628
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11747390776872635
epoch£º482	 i:6 	 global-step:9646	 l-p:0.14209453761577606
epoch£º482	 i:7 	 global-step:9647	 l-p:0.1290239840745926
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12237090617418289
epoch£º482	 i:9 	 global-step:9649	 l-p:0.09171608090400696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0726, 5.0726, 5.0726],
        [5.0726, 5.1598, 5.0085],
        [5.0726, 5.0722, 5.0726],
        [5.0726, 5.0726, 5.0726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.027363872155547142 
model_pd.l_d.mean(): -18.7219181060791 
model_pd.lagr.mean(): -18.749282836914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5601], device='cuda:0')), ('power', tensor([-19.4987], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.027363872155547142
epoch£º483	 i:1 	 global-step:9661	 l-p:0.23117850720882416
epoch£º483	 i:2 	 global-step:9662	 l-p:0.15078289806842804
epoch£º483	 i:3 	 global-step:9663	 l-p:0.12123473733663559
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13649286329746246
epoch£º483	 i:5 	 global-step:9665	 l-p:0.014301356859505177
epoch£º483	 i:6 	 global-step:9666	 l-p:0.1222897619009018
epoch£º483	 i:7 	 global-step:9667	 l-p:0.11001118272542953
epoch£º483	 i:8 	 global-step:9668	 l-p:0.15135541558265686
epoch£º483	 i:9 	 global-step:9669	 l-p:0.13534113764762878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.0796, 5.3037, 5.2035],
        [5.0796, 4.9987, 4.9090],
        [5.0796, 5.0393, 4.9006],
        [5.0796, 5.3290, 5.2419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.13146863877773285 
model_pd.l_d.mean(): -19.638174057006836 
model_pd.lagr.mean(): -19.50670623779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5477], device='cuda:0')), ('power', tensor([-20.4122], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.13146863877773285
epoch£º484	 i:1 	 global-step:9681	 l-p:0.14079749584197998
epoch£º484	 i:2 	 global-step:9682	 l-p:0.23859988152980804
epoch£º484	 i:3 	 global-step:9683	 l-p:0.05477001890540123
epoch£º484	 i:4 	 global-step:9684	 l-p:0.07758894562721252
epoch£º484	 i:5 	 global-step:9685	 l-p:-0.09840372949838638
epoch£º484	 i:6 	 global-step:9686	 l-p:0.1339411586523056
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12886817753314972
epoch£º484	 i:8 	 global-step:9688	 l-p:0.15025869011878967
epoch£º484	 i:9 	 global-step:9689	 l-p:0.12501607835292816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0214, 4.9892, 5.0108],
        [5.0214, 4.9427, 4.9532],
        [5.0214, 5.0214, 5.0214],
        [5.0214, 5.0214, 5.0214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.1305469125509262 
model_pd.l_d.mean(): -21.121870040893555 
model_pd.lagr.mean(): -20.991323471069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3741], device='cuda:0')), ('power', tensor([-21.7347], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.1305469125509262
epoch£º485	 i:1 	 global-step:9701	 l-p:0.09121350198984146
epoch£º485	 i:2 	 global-step:9702	 l-p:0.12023892998695374
epoch£º485	 i:3 	 global-step:9703	 l-p:0.1288166642189026
epoch£º485	 i:4 	 global-step:9704	 l-p:0.06238080933690071
epoch£º485	 i:5 	 global-step:9705	 l-p:0.1273394525051117
epoch£º485	 i:6 	 global-step:9706	 l-p:0.16528292000293732
epoch£º485	 i:7 	 global-step:9707	 l-p:0.24394157528877258
epoch£º485	 i:8 	 global-step:9708	 l-p:0.16892211139202118
epoch£º485	 i:9 	 global-step:9709	 l-p:0.49044448137283325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8660, 4.8078, 4.6582],
        [4.8660, 4.7610, 4.7293],
        [4.8660, 5.0196, 4.8896],
        [4.8660, 4.8056, 4.8322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.16844157874584198 
model_pd.l_d.mean(): -20.03697967529297 
model_pd.lagr.mean(): -19.86853790283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5685], device='cuda:0')), ('power', tensor([-20.8366], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.16844157874584198
epoch£º486	 i:1 	 global-step:9721	 l-p:0.1254623979330063
epoch£º486	 i:2 	 global-step:9722	 l-p:0.1456902027130127
epoch£º486	 i:3 	 global-step:9723	 l-p:0.22414541244506836
epoch£º486	 i:4 	 global-step:9724	 l-p:0.1309836357831955
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12331637740135193
epoch£º486	 i:6 	 global-step:9726	 l-p:0.16955211758613586
epoch£º486	 i:7 	 global-step:9727	 l-p:0.2494422048330307
epoch£º486	 i:8 	 global-step:9728	 l-p:0.09964039921760559
epoch£º486	 i:9 	 global-step:9729	 l-p:0.13475419580936432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9106, 4.8082, 4.7750],
        [4.9106, 4.9105, 4.9106],
        [4.9106, 4.9106, 4.9106],
        [4.9106, 4.9286, 4.7630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.12066333740949631 
model_pd.l_d.mean(): -20.162216186523438 
model_pd.lagr.mean(): -20.041553497314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5223], device='cuda:0')), ('power', tensor([-20.9160], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.12066333740949631
epoch£º487	 i:1 	 global-step:9741	 l-p:0.19125263392925262
epoch£º487	 i:2 	 global-step:9742	 l-p:0.1305343061685562
epoch£º487	 i:3 	 global-step:9743	 l-p:0.028145041316747665
epoch£º487	 i:4 	 global-step:9744	 l-p:0.16934920847415924
epoch£º487	 i:5 	 global-step:9745	 l-p:0.11423913389444351
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13936847448349
epoch£º487	 i:7 	 global-step:9747	 l-p:0.14691755175590515
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13295263051986694
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13261795043945312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.0230, 5.0241],
        [5.0241, 4.9618, 4.8319],
        [5.0241, 5.0020, 5.0189],
        [5.0241, 5.3749, 5.3500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13752122223377228 
model_pd.l_d.mean(): -20.403928756713867 
model_pd.lagr.mean(): -20.266407012939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-21.1175], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13752122223377228
epoch£º488	 i:1 	 global-step:9761	 l-p:0.09189137071371078
epoch£º488	 i:2 	 global-step:9762	 l-p:0.1606345921754837
epoch£º488	 i:3 	 global-step:9763	 l-p:0.11615848541259766
epoch£º488	 i:4 	 global-step:9764	 l-p:0.15381433069705963
epoch£º488	 i:5 	 global-step:9765	 l-p:0.04315894842147827
epoch£º488	 i:6 	 global-step:9766	 l-p:0.10457349568605423
epoch£º488	 i:7 	 global-step:9767	 l-p:0.10222312808036804
epoch£º488	 i:8 	 global-step:9768	 l-p:0.1250268965959549
epoch£º488	 i:9 	 global-step:9769	 l-p:0.16909213364124298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9662, 4.9330, 4.9554],
        [4.9662, 4.9659, 4.9662],
        [4.9662, 4.9300, 4.9535],
        [4.9662, 4.9662, 4.9662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.11879181861877441 
model_pd.l_d.mean(): -20.744354248046875 
model_pd.lagr.mean(): -20.62556266784668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4434], device='cuda:0')), ('power', tensor([-21.4239], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.11879181861877441
epoch£º489	 i:1 	 global-step:9781	 l-p:0.12327078729867935
epoch£º489	 i:2 	 global-step:9782	 l-p:0.04326716810464859
epoch£º489	 i:3 	 global-step:9783	 l-p:0.12818385660648346
epoch£º489	 i:4 	 global-step:9784	 l-p:0.17978517711162567
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1756163388490677
epoch£º489	 i:6 	 global-step:9786	 l-p:0.140173077583313
epoch£º489	 i:7 	 global-step:9787	 l-p:0.14138951897621155
epoch£º489	 i:8 	 global-step:9788	 l-p:0.11243079602718353
epoch£º489	 i:9 	 global-step:9789	 l-p:0.13408887386322021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0090, 5.0090, 5.0090],
        [5.0090, 5.0086, 5.0090],
        [5.0090, 5.0083, 5.0090],
        [5.0090, 5.0090, 5.0090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.13926629722118378 
model_pd.l_d.mean(): -20.839675903320312 
model_pd.lagr.mean(): -20.700408935546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4252], device='cuda:0')), ('power', tensor([-21.5017], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.13926629722118378
epoch£º490	 i:1 	 global-step:9801	 l-p:0.11358892172574997
epoch£º490	 i:2 	 global-step:9802	 l-p:0.16627810895442963
epoch£º490	 i:3 	 global-step:9803	 l-p:0.110089011490345
epoch£º490	 i:4 	 global-step:9804	 l-p:0.09132487326860428
epoch£º490	 i:5 	 global-step:9805	 l-p:0.10915501415729523
epoch£º490	 i:6 	 global-step:9806	 l-p:0.10152444243431091
epoch£º490	 i:7 	 global-step:9807	 l-p:0.11796852201223373
epoch£º490	 i:8 	 global-step:9808	 l-p:0.30160003900527954
epoch£º490	 i:9 	 global-step:9809	 l-p:0.1304353028535843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0705, 5.4907, 5.5097],
        [5.0705, 4.9776, 4.9536],
        [5.0705, 5.0327, 5.0561],
        [5.0705, 5.2510, 5.1289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.32083436846733093 
model_pd.l_d.mean(): -19.425521850585938 
model_pd.lagr.mean(): -19.104686737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.1403], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.32083436846733093
epoch£º491	 i:1 	 global-step:9821	 l-p:0.1274137645959854
epoch£º491	 i:2 	 global-step:9822	 l-p:0.13353507220745087
epoch£º491	 i:3 	 global-step:9823	 l-p:0.1384509801864624
epoch£º491	 i:4 	 global-step:9824	 l-p:0.0842605009675026
epoch£º491	 i:5 	 global-step:9825	 l-p:0.1061926931142807
epoch£º491	 i:6 	 global-step:9826	 l-p:0.1360790878534317
epoch£º491	 i:7 	 global-step:9827	 l-p:-0.0035934827756136656
epoch£º491	 i:8 	 global-step:9828	 l-p:0.12182623893022537
epoch£º491	 i:9 	 global-step:9829	 l-p:0.12371475249528885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1497, 5.1497, 5.1497],
        [5.1497, 5.0913, 4.9686],
        [5.1497, 5.1424, 5.1490],
        [5.1497, 5.1443, 5.1493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.07277901470661163 
model_pd.l_d.mean(): -20.665985107421875 
model_pd.lagr.mean(): -20.59320640563965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.3198], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.07277901470661163
epoch£º492	 i:1 	 global-step:9841	 l-p:0.13063481450080872
epoch£º492	 i:2 	 global-step:9842	 l-p:0.09700074791908264
epoch£º492	 i:3 	 global-step:9843	 l-p:0.07127348333597183
epoch£º492	 i:4 	 global-step:9844	 l-p:0.1319090574979782
epoch£º492	 i:5 	 global-step:9845	 l-p:0.03186438977718353
epoch£º492	 i:6 	 global-step:9846	 l-p:0.1442752331495285
epoch£º492	 i:7 	 global-step:9847	 l-p:0.12923461198806763
epoch£º492	 i:8 	 global-step:9848	 l-p:0.4467514455318451
epoch£º492	 i:9 	 global-step:9849	 l-p:0.19148914515972137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1135, 5.8462, 6.1070],
        [5.1135, 5.1132, 5.1135],
        [5.1135, 5.0249, 5.0127],
        [5.1135, 5.6759, 5.7974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.10530408471822739 
model_pd.l_d.mean(): -17.596635818481445 
model_pd.lagr.mean(): -17.491331100463867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6302], device='cuda:0')), ('power', tensor([-18.4326], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.10530408471822739
epoch£º493	 i:1 	 global-step:9861	 l-p:0.11618129163980484
epoch£º493	 i:2 	 global-step:9862	 l-p:0.13449791073799133
epoch£º493	 i:3 	 global-step:9863	 l-p:0.18940532207489014
epoch£º493	 i:4 	 global-step:9864	 l-p:0.13286307454109192
epoch£º493	 i:5 	 global-step:9865	 l-p:-0.0033293829765170813
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11604387313127518
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13458149135112762
epoch£º493	 i:8 	 global-step:9868	 l-p:-3.3792834281921387
epoch£º493	 i:9 	 global-step:9869	 l-p:0.17376703023910522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1051, 5.0400, 4.9169],
        [5.1051, 5.7641, 5.9634],
        [5.1051, 5.0266, 5.0366],
        [5.1051, 5.0879, 5.1017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.11738235503435135 
model_pd.l_d.mean(): -18.22039794921875 
model_pd.lagr.mean(): -18.103015899658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5984], device='cuda:0')), ('power', tensor([-19.0308], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.11738235503435135
epoch£º494	 i:1 	 global-step:9881	 l-p:0.14554210007190704
epoch£º494	 i:2 	 global-step:9882	 l-p:0.39027512073516846
epoch£º494	 i:3 	 global-step:9883	 l-p:0.04822101816534996
epoch£º494	 i:4 	 global-step:9884	 l-p:0.13738389313220978
epoch£º494	 i:5 	 global-step:9885	 l-p:0.1602458953857422
epoch£º494	 i:6 	 global-step:9886	 l-p:-0.003027434227988124
epoch£º494	 i:7 	 global-step:9887	 l-p:0.13313248753547668
epoch£º494	 i:8 	 global-step:9888	 l-p:0.6886727213859558
epoch£º494	 i:9 	 global-step:9889	 l-p:0.1374179571866989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2203, 5.1383, 5.1303],
        [5.2203, 5.2197, 5.2203],
        [5.2203, 5.5778, 5.5471],
        [5.2203, 5.2096, 5.2188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.12503714859485626 
model_pd.l_d.mean(): -20.40045738220215 
model_pd.lagr.mean(): -20.275421142578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4083], device='cuda:0')), ('power', tensor([-21.0404], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.12503714859485626
epoch£º495	 i:1 	 global-step:9901	 l-p:0.12091578543186188
epoch£º495	 i:2 	 global-step:9902	 l-p:0.12935736775398254
epoch£º495	 i:3 	 global-step:9903	 l-p:0.12136340886354446
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.3521709740161896
epoch£º495	 i:5 	 global-step:9905	 l-p:-0.5149320960044861
epoch£º495	 i:6 	 global-step:9906	 l-p:0.1569332331418991
epoch£º495	 i:7 	 global-step:9907	 l-p:0.11889993399381638
epoch£º495	 i:8 	 global-step:9908	 l-p:0.13385576009750366
epoch£º495	 i:9 	 global-step:9909	 l-p:0.103761687874794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1136, 5.1136, 5.1136],
        [5.1136, 5.0551, 5.0790],
        [5.1136, 5.1136, 5.1136],
        [5.1136, 5.4414, 5.3962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.13179978728294373 
model_pd.l_d.mean(): -20.56955337524414 
model_pd.lagr.mean(): -20.437753677368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4265], device='cuda:0')), ('power', tensor([-21.2299], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.13179978728294373
epoch£º496	 i:1 	 global-step:9921	 l-p:0.12302932143211365
epoch£º496	 i:2 	 global-step:9922	 l-p:0.08530843257904053
epoch£º496	 i:3 	 global-step:9923	 l-p:0.22758623957633972
epoch£º496	 i:4 	 global-step:9924	 l-p:-0.051642220467329025
epoch£º496	 i:5 	 global-step:9925	 l-p:0.11637406796216965
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13013659417629242
epoch£º496	 i:7 	 global-step:9927	 l-p:0.07049098610877991
epoch£º496	 i:8 	 global-step:9928	 l-p:0.18012288212776184
epoch£º496	 i:9 	 global-step:9929	 l-p:0.12679770588874817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0242, 4.9373, 4.8299],
        [5.0242, 5.0242, 5.0242],
        [5.0242, 5.0241, 5.0242],
        [5.0242, 5.0225, 5.0241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.14198768138885498 
model_pd.l_d.mean(): -20.311901092529297 
model_pd.lagr.mean(): -20.16991424560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4939], device='cuda:0')), ('power', tensor([-21.0384], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.14198768138885498
epoch£º497	 i:1 	 global-step:9941	 l-p:-0.5074160099029541
epoch£º497	 i:2 	 global-step:9942	 l-p:0.09966211020946503
epoch£º497	 i:3 	 global-step:9943	 l-p:0.10829544067382812
epoch£º497	 i:4 	 global-step:9944	 l-p:0.12645618617534637
epoch£º497	 i:5 	 global-step:9945	 l-p:0.1489972621202469
epoch£º497	 i:6 	 global-step:9946	 l-p:0.1264742761850357
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14283834397792816
epoch£º497	 i:8 	 global-step:9948	 l-p:0.10416822880506516
epoch£º497	 i:9 	 global-step:9949	 l-p:0.2215621918439865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9005, 4.8538, 4.8809],
        [4.9005, 5.2631, 5.2508],
        [4.9005, 5.0247, 4.8785],
        [4.9005, 4.8929, 4.8998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.16580063104629517 
model_pd.l_d.mean(): -20.402103424072266 
model_pd.lagr.mean(): -20.236303329467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5142], device='cuda:0')), ('power', tensor([-21.1502], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.16580063104629517
epoch£º498	 i:1 	 global-step:9961	 l-p:0.1589042693376541
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12526030838489532
epoch£º498	 i:3 	 global-step:9963	 l-p:0.17778083682060242
epoch£º498	 i:4 	 global-step:9964	 l-p:0.2040393054485321
epoch£º498	 i:5 	 global-step:9965	 l-p:0.13985608518123627
epoch£º498	 i:6 	 global-step:9966	 l-p:0.17749102413654327
epoch£º498	 i:7 	 global-step:9967	 l-p:0.15128649771213531
epoch£º498	 i:8 	 global-step:9968	 l-p:0.09891878813505173
epoch£º498	 i:9 	 global-step:9969	 l-p:0.11481237411499023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9972, 4.9965, 4.9972],
        [4.9972, 5.3573, 5.3378],
        [4.9972, 4.9972, 4.9972],
        [4.9972, 4.9972, 4.9972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.1650630384683609 
model_pd.l_d.mean(): -21.005781173706055 
model_pd.lagr.mean(): -20.840717315673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4018], device='cuda:0')), ('power', tensor([-21.6458], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.1650630384683609
epoch£º499	 i:1 	 global-step:9981	 l-p:0.10911176353693008
epoch£º499	 i:2 	 global-step:9982	 l-p:0.10916860401630402
epoch£º499	 i:3 	 global-step:9983	 l-p:0.14498957991600037
epoch£º499	 i:4 	 global-step:9984	 l-p:-0.276723712682724
epoch£º499	 i:5 	 global-step:9985	 l-p:0.1358291357755661
epoch£º499	 i:6 	 global-step:9986	 l-p:0.12281446158885956
epoch£º499	 i:7 	 global-step:9987	 l-p:0.08865077793598175
epoch£º499	 i:8 	 global-step:9988	 l-p:0.13131004571914673
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12373916059732437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 5.0714, 5.0869],
        [5.1451, 5.0672, 4.9617],
        [5.1451, 5.1018, 5.1265],
        [5.1451, 5.2594, 5.1104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.07630930840969086 
model_pd.l_d.mean(): -20.789445877075195 
model_pd.lagr.mean(): -20.713136672973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.4257], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.07630930840969086
epoch£º500	 i:1 	 global-step:10001	 l-p:0.12652526795864105
epoch£º500	 i:2 	 global-step:10002	 l-p:0.06643987447023392
epoch£º500	 i:3 	 global-step:10003	 l-p:0.11773931980133057
epoch£º500	 i:4 	 global-step:10004	 l-p:0.1136629581451416
epoch£º500	 i:5 	 global-step:10005	 l-p:0.12028590589761734
epoch£º500	 i:6 	 global-step:10006	 l-p:0.11543849855661392
epoch£º500	 i:7 	 global-step:10007	 l-p:0.13681212067604065
epoch£º500	 i:8 	 global-step:10008	 l-p:-0.012851753272116184
epoch£º500	 i:9 	 global-step:10009	 l-p:0.1299687922000885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 5.0667, 5.0659],
        [5.1522, 5.0784, 4.9657],
        [5.1522, 5.0951, 4.9629],
        [5.1522, 5.1267, 5.1454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.07330130785703659 
model_pd.l_d.mean(): -19.95610809326172 
model_pd.lagr.mean(): -19.8828067779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4739], device='cuda:0')), ('power', tensor([-20.6582], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.07330130785703659
epoch£º501	 i:1 	 global-step:10021	 l-p:0.14972704648971558
epoch£º501	 i:2 	 global-step:10022	 l-p:0.09369316697120667
epoch£º501	 i:3 	 global-step:10023	 l-p:0.1069539412856102
epoch£º501	 i:4 	 global-step:10024	 l-p:0.13314205408096313
epoch£º501	 i:5 	 global-step:10025	 l-p:0.10575386881828308
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11250399053096771
epoch£º501	 i:7 	 global-step:10027	 l-p:0.12990856170654297
epoch£º501	 i:8 	 global-step:10028	 l-p:-0.02031758241355419
epoch£º501	 i:9 	 global-step:10029	 l-p:0.12974165380001068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0630, 4.9634, 4.8944],
        [5.0630, 5.0630, 5.0630],
        [5.0630, 5.4285, 5.4086],
        [5.0630, 5.0627, 5.0630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.13764315843582153 
model_pd.l_d.mean(): -19.59109878540039 
model_pd.lagr.mean(): -19.453454971313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5104], device='cuda:0')), ('power', tensor([-20.3266], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.13764315843582153
epoch£º502	 i:1 	 global-step:10041	 l-p:-0.09267399460077286
epoch£º502	 i:2 	 global-step:10042	 l-p:0.16719211637973785
epoch£º502	 i:3 	 global-step:10043	 l-p:0.1477171927690506
epoch£º502	 i:4 	 global-step:10044	 l-p:0.12217971682548523
epoch£º502	 i:5 	 global-step:10045	 l-p:0.15473830699920654
epoch£º502	 i:6 	 global-step:10046	 l-p:0.04443589225411415
epoch£º502	 i:7 	 global-step:10047	 l-p:0.13648822903633118
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13549892604351044
epoch£º502	 i:9 	 global-step:10049	 l-p:0.12103582918643951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9475, 4.9474, 4.9475],
        [4.9475, 5.2600, 5.2109],
        [4.9475, 4.9475, 4.9475],
        [4.9475, 4.9236, 4.9419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.16022029519081116 
model_pd.l_d.mean(): -19.816787719726562 
model_pd.lagr.mean(): -19.656566619873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5138], device='cuda:0')), ('power', tensor([-20.5581], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.16022029519081116
epoch£º503	 i:1 	 global-step:10061	 l-p:0.11957931518554688
epoch£º503	 i:2 	 global-step:10062	 l-p:0.12165442854166031
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13738904893398285
epoch£º503	 i:4 	 global-step:10064	 l-p:0.10007112473249435
epoch£º503	 i:5 	 global-step:10065	 l-p:0.21174487471580505
epoch£º503	 i:6 	 global-step:10066	 l-p:0.21182966232299805
epoch£º503	 i:7 	 global-step:10067	 l-p:0.4358138144016266
epoch£º503	 i:8 	 global-step:10068	 l-p:0.1321890950202942
epoch£º503	 i:9 	 global-step:10069	 l-p:0.09777948260307312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 4.8780, 4.8879],
        [4.8894, 4.7846, 4.6718],
        [4.8894, 4.8587, 4.8807],
        [4.8894, 4.7915, 4.7978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.10240379720926285 
model_pd.l_d.mean(): -20.45244026184082 
model_pd.lagr.mean(): -20.35003662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5233], device='cuda:0')), ('power', tensor([-21.2104], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.10240379720926285
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11603764444589615
epoch£º504	 i:2 	 global-step:10082	 l-p:0.19554199278354645
epoch£º504	 i:3 	 global-step:10083	 l-p:0.40237680077552795
epoch£º504	 i:4 	 global-step:10084	 l-p:0.16533544659614563
epoch£º504	 i:5 	 global-step:10085	 l-p:0.13189685344696045
epoch£º504	 i:6 	 global-step:10086	 l-p:0.1328044980764389
epoch£º504	 i:7 	 global-step:10087	 l-p:0.15555398166179657
epoch£º504	 i:8 	 global-step:10088	 l-p:0.1638428270816803
epoch£º504	 i:9 	 global-step:10089	 l-p:0.1592095047235489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9500, 4.8446, 4.8292],
        [4.9500, 4.9000, 4.9278],
        [4.9500, 4.9455, 4.9497],
        [4.9500, 4.8825, 4.9093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.19222602248191833 
model_pd.l_d.mean(): -20.643386840820312 
model_pd.lagr.mean(): -20.451160430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4739], device='cuda:0')), ('power', tensor([-21.3530], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.19222602248191833
epoch£º505	 i:1 	 global-step:10101	 l-p:0.1583138108253479
epoch£º505	 i:2 	 global-step:10102	 l-p:0.10747931152582169
epoch£º505	 i:3 	 global-step:10103	 l-p:0.14928418397903442
epoch£º505	 i:4 	 global-step:10104	 l-p:0.08237612247467041
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13266858458518982
epoch£º505	 i:6 	 global-step:10106	 l-p:-0.15458618104457855
epoch£º505	 i:7 	 global-step:10107	 l-p:0.14201843738555908
epoch£º505	 i:8 	 global-step:10108	 l-p:0.11195558309555054
epoch£º505	 i:9 	 global-step:10109	 l-p:0.07903511077165604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1245, 5.1137, 5.1230],
        [5.1245, 5.8141, 6.0346],
        [5.1245, 5.6665, 5.7687],
        [5.1245, 5.0566, 4.9260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.07395243644714355 
model_pd.l_d.mean(): -19.712520599365234 
model_pd.lagr.mean(): -19.638568878173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-20.4473], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.07395243644714355
epoch£º506	 i:1 	 global-step:10121	 l-p:0.06431114673614502
epoch£º506	 i:2 	 global-step:10122	 l-p:0.17608867585659027
epoch£º506	 i:3 	 global-step:10123	 l-p:0.1097327172756195
epoch£º506	 i:4 	 global-step:10124	 l-p:0.22287940979003906
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11234314739704132
epoch£º506	 i:6 	 global-step:10126	 l-p:0.14197953045368195
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10380765795707703
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11621209233999252
epoch£º506	 i:9 	 global-step:10129	 l-p:0.13008512556552887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2609, 5.2194, 5.2434],
        [5.2609, 5.1709, 5.1242],
        [5.2609, 5.1787, 5.0939],
        [5.2609, 5.9313, 6.1259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.1362406462430954 
model_pd.l_d.mean(): -20.341943740844727 
model_pd.lagr.mean(): -20.205703735351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-20.9935], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.1362406462430954
epoch£º507	 i:1 	 global-step:10141	 l-p:0.12346143275499344
epoch£º507	 i:2 	 global-step:10142	 l-p:0.11965394765138626
epoch£º507	 i:3 	 global-step:10143	 l-p:0.17078761756420135
epoch£º507	 i:4 	 global-step:10144	 l-p:0.121792733669281
epoch£º507	 i:5 	 global-step:10145	 l-p:0.06439981609582901
epoch£º507	 i:6 	 global-step:10146	 l-p:0.05618245154619217
epoch£º507	 i:7 	 global-step:10147	 l-p:0.17734767496585846
epoch£º507	 i:8 	 global-step:10148	 l-p:0.007914847694337368
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12123727798461914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1752, 5.1525, 5.1698],
        [5.1752, 5.1730, 5.1751],
        [5.1752, 5.1747, 5.1752],
        [5.1752, 5.1936, 5.0289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09799868613481522 
model_pd.l_d.mean(): -19.463951110839844 
model_pd.lagr.mean(): -19.365951538085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-20.1815], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09799868613481522
epoch£º508	 i:1 	 global-step:10161	 l-p:0.13282421231269836
epoch£º508	 i:2 	 global-step:10162	 l-p:0.20433282852172852
epoch£º508	 i:3 	 global-step:10163	 l-p:-0.024425458163022995
epoch£º508	 i:4 	 global-step:10164	 l-p:0.009762587025761604
epoch£º508	 i:5 	 global-step:10165	 l-p:0.1454533338546753
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.040751803666353226
epoch£º508	 i:7 	 global-step:10167	 l-p:0.11003310978412628
epoch£º508	 i:8 	 global-step:10168	 l-p:0.1465490162372589
epoch£º508	 i:9 	 global-step:10169	 l-p:0.1384454369544983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2029, 5.4540, 5.3592],
        [5.2029, 5.2025, 5.2029],
        [5.2029, 5.9402, 6.1945],
        [5.2029, 5.1651, 5.1888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.3772403597831726 
model_pd.l_d.mean(): -19.882225036621094 
model_pd.lagr.mean(): -19.50498390197754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518], device='cuda:0')), ('power', tensor([-20.5610], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.3772403597831726
epoch£º509	 i:1 	 global-step:10181	 l-p:0.44737938046455383
epoch£º509	 i:2 	 global-step:10182	 l-p:0.11883744597434998
epoch£º509	 i:3 	 global-step:10183	 l-p:0.13882438838481903
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09536392241716385
epoch£º509	 i:5 	 global-step:10185	 l-p:0.1237492561340332
epoch£º509	 i:6 	 global-step:10186	 l-p:0.11495281010866165
epoch£º509	 i:7 	 global-step:10187	 l-p:-0.02870466187596321
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10842270404100418
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14116084575653076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 5.1404, 5.1417],
        [5.1417, 5.2800, 5.1353],
        [5.1417, 5.3169, 5.1867],
        [5.1417, 5.1607, 4.9936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.1167219802737236 
model_pd.l_d.mean(): -18.697830200195312 
model_pd.lagr.mean(): -18.58110809326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-19.3831], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.1167219802737236
epoch£º510	 i:1 	 global-step:10201	 l-p:0.10653723776340485
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13062675297260284
epoch£º510	 i:3 	 global-step:10203	 l-p:-0.01687208190560341
epoch£º510	 i:4 	 global-step:10204	 l-p:0.12681876122951508
epoch£º510	 i:5 	 global-step:10205	 l-p:0.13512523472309113
epoch£º510	 i:6 	 global-step:10206	 l-p:0.1451103836297989
epoch£º510	 i:7 	 global-step:10207	 l-p:0.12865500152111053
epoch£º510	 i:8 	 global-step:10208	 l-p:0.15562111139297485
epoch£º510	 i:9 	 global-step:10209	 l-p:0.12265369296073914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9588, 4.8870, 4.9134],
        [4.9588, 5.0153, 4.8448],
        [4.9588, 4.9251, 4.9485],
        [4.9588, 4.9588, 4.9588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.16299289464950562 
model_pd.l_d.mean(): -20.089752197265625 
model_pd.lagr.mean(): -19.926759719848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5410], device='cuda:0')), ('power', tensor([-20.8619], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.16299289464950562
epoch£º511	 i:1 	 global-step:10221	 l-p:0.13301166892051697
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12464217841625214
epoch£º511	 i:3 	 global-step:10223	 l-p:0.16096115112304688
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13322830200195312
epoch£º511	 i:5 	 global-step:10225	 l-p:0.17113849520683289
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12704236805438995
epoch£º511	 i:7 	 global-step:10227	 l-p:0.1576516181230545
epoch£º511	 i:8 	 global-step:10228	 l-p:0.06371007114648819
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14784160256385803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9436, 4.9320, 4.9421],
        [4.9436, 4.9021, 4.9285],
        [4.9436, 4.9789, 4.8040],
        [4.9436, 4.8447, 4.8496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.16377124190330505 
model_pd.l_d.mean(): -20.654815673828125 
model_pd.lagr.mean(): -20.491044998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4622], device='cuda:0')), ('power', tensor([-21.3526], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.16377124190330505
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10198754072189331
epoch£º512	 i:2 	 global-step:10242	 l-p:0.20649473369121552
epoch£º512	 i:3 	 global-step:10243	 l-p:0.1410520225763321
epoch£º512	 i:4 	 global-step:10244	 l-p:0.11419379711151123
epoch£º512	 i:5 	 global-step:10245	 l-p:0.11383714526891708
epoch£º512	 i:6 	 global-step:10246	 l-p:0.130683034658432
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14572444558143616
epoch£º512	 i:8 	 global-step:10248	 l-p:0.1619139462709427
epoch£º512	 i:9 	 global-step:10249	 l-p:0.23435118794441223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9205, 4.8045, 4.7698],
        [4.9205, 5.2354, 5.1871],
        [4.9205, 4.8100, 4.7957],
        [4.9205, 4.9183, 4.9204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.15217719972133636 
model_pd.l_d.mean(): -20.486318588256836 
model_pd.lagr.mean(): -20.33414077758789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5074], device='cuda:0')), ('power', tensor([-21.2284], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.15217719972133636
epoch£º513	 i:1 	 global-step:10261	 l-p:0.16529376804828644
epoch£º513	 i:2 	 global-step:10262	 l-p:0.13813748955726624
epoch£º513	 i:3 	 global-step:10263	 l-p:0.13999632000923157
epoch£º513	 i:4 	 global-step:10264	 l-p:0.17835931479930878
epoch£º513	 i:5 	 global-step:10265	 l-p:0.12711936235427856
epoch£º513	 i:6 	 global-step:10266	 l-p:0.1245989128947258
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12727640569210052
epoch£º513	 i:8 	 global-step:10268	 l-p:0.049349453300237656
epoch£º513	 i:9 	 global-step:10269	 l-p:0.08957619220018387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0564, 5.6087, 5.7219],
        [5.0564, 5.3885, 5.3445],
        [5.0564, 4.9584, 4.9554],
        [5.0564, 5.0562, 5.0564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.14307057857513428 
model_pd.l_d.mean(): -20.446596145629883 
model_pd.lagr.mean(): -20.303525924682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-21.1343], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.14307057857513428
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14514754712581635
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15912288427352905
epoch£º514	 i:3 	 global-step:10283	 l-p:0.024197502061724663
epoch£º514	 i:4 	 global-step:10284	 l-p:0.06604694575071335
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13012292981147766
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11482814699411392
epoch£º514	 i:7 	 global-step:10287	 l-p:0.13952772319316864
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12057275325059891
epoch£º514	 i:9 	 global-step:10289	 l-p:0.13346128165721893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0388, 5.3332, 5.2662],
        [5.0388, 5.2836, 5.1879],
        [5.0388, 4.9877, 5.0156],
        [5.0388, 5.0129, 4.8431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.004333969205617905 
model_pd.l_d.mean(): -19.058605194091797 
model_pd.lagr.mean(): -19.054271697998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5590], device='cuda:0')), ('power', tensor([-19.8379], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.004333969205617905
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10324881970882416
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11577821522951126
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15648697316646576
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13283558189868927
epoch£º515	 i:5 	 global-step:10305	 l-p:0.1335226446390152
epoch£º515	 i:6 	 global-step:10306	 l-p:0.09269081056118011
epoch£º515	 i:7 	 global-step:10307	 l-p:0.2313171774148941
epoch£º515	 i:8 	 global-step:10308	 l-p:0.23264376819133759
epoch£º515	 i:9 	 global-step:10309	 l-p:0.14661188423633575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9115, 4.9020, 4.9104],
        [4.9115, 4.8006, 4.7909],
        [4.9115, 5.1667, 5.0812],
        [4.9115, 4.8940, 4.9084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.28404751420021057 
model_pd.l_d.mean(): -20.006284713745117 
model_pd.lagr.mean(): -19.72223663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5610], device='cuda:0')), ('power', tensor([-20.7979], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.28404751420021057
epoch£º516	 i:1 	 global-step:10321	 l-p:0.19639049470424652
epoch£º516	 i:2 	 global-step:10322	 l-p:0.13993746042251587
epoch£º516	 i:3 	 global-step:10323	 l-p:0.1300686001777649
epoch£º516	 i:4 	 global-step:10324	 l-p:0.0974758192896843
epoch£º516	 i:5 	 global-step:10325	 l-p:0.17924317717552185
epoch£º516	 i:6 	 global-step:10326	 l-p:0.10996691137552261
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15584158897399902
epoch£º516	 i:8 	 global-step:10328	 l-p:0.13280652463436127
epoch£º516	 i:9 	 global-step:10329	 l-p:0.1419859081506729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9423, 4.9349, 4.9416],
        [4.9423, 4.8359, 4.7166],
        [4.9423, 4.8285, 4.7287],
        [4.9423, 4.9422, 4.9423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.10881904512643814 
model_pd.l_d.mean(): -20.055570602416992 
model_pd.lagr.mean(): -19.94675064086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5054], device='cuda:0')), ('power', tensor([-20.7910], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.10881904512643814
epoch£º517	 i:1 	 global-step:10341	 l-p:0.13177159428596497
epoch£º517	 i:2 	 global-step:10342	 l-p:0.1844858080148697
epoch£º517	 i:3 	 global-step:10343	 l-p:0.15452200174331665
epoch£º517	 i:4 	 global-step:10344	 l-p:0.12868155539035797
epoch£º517	 i:5 	 global-step:10345	 l-p:0.130701944231987
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10417893528938293
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.10093511641025543
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08446774631738663
epoch£º517	 i:9 	 global-step:10349	 l-p:0.1398003101348877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1451, 5.3122, 5.1761],
        [5.1451, 5.0996, 4.9434],
        [5.1451, 5.0697, 5.0905],
        [5.1451, 5.0409, 4.9970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11580397933721542 
model_pd.l_d.mean(): -20.300600051879883 
model_pd.lagr.mean(): -20.184795379638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4630], device='cuda:0')), ('power', tensor([-20.9953], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11580397933721542
epoch£º518	 i:1 	 global-step:10361	 l-p:0.04755121469497681
epoch£º518	 i:2 	 global-step:10362	 l-p:0.03582020848989487
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12211958318948746
epoch£º518	 i:4 	 global-step:10364	 l-p:0.19288142025470734
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13581159710884094
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09169654548168182
epoch£º518	 i:7 	 global-step:10367	 l-p:0.12513025104999542
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12046080082654953
epoch£º518	 i:9 	 global-step:10369	 l-p:0.23279786109924316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2327, 5.2300, 5.2325],
        [5.2327, 5.9159, 6.1204],
        [5.2327, 5.2297, 5.2325],
        [5.2327, 5.1706, 5.1954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.12655070424079895 
model_pd.l_d.mean(): -20.41286849975586 
model_pd.lagr.mean(): -20.286317825317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4062], device='cuda:0')), ('power', tensor([-21.0508], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.12655070424079895
epoch£º519	 i:1 	 global-step:10381	 l-p:0.1716209352016449
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11056150496006012
epoch£º519	 i:3 	 global-step:10383	 l-p:0.14893898367881775
epoch£º519	 i:4 	 global-step:10384	 l-p:-0.07950332760810852
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.24024951457977295
epoch£º519	 i:6 	 global-step:10386	 l-p:0.11983532458543777
epoch£º519	 i:7 	 global-step:10387	 l-p:0.0641603097319603
epoch£º519	 i:8 	 global-step:10388	 l-p:0.1134730875492096
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11599615961313248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1616, 5.1616, 5.1616],
        [5.1616, 5.6980, 5.7908],
        [5.1616, 5.1616, 5.1616],
        [5.1616, 5.1546, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13382753729820251 
model_pd.l_d.mean(): -19.565114974975586 
model_pd.lagr.mean(): -19.43128776550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-20.2332], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13382753729820251
epoch£º520	 i:1 	 global-step:10401	 l-p:0.06773220747709274
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12105955183506012
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.3081783354282379
epoch£º520	 i:4 	 global-step:10404	 l-p:0.15415477752685547
epoch£º520	 i:5 	 global-step:10405	 l-p:0.16928762197494507
epoch£º520	 i:6 	 global-step:10406	 l-p:0.1324906349182129
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12899620831012726
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13932786881923676
epoch£º520	 i:9 	 global-step:10409	 l-p:0.16552917659282684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0794, 4.9700, 4.8992],
        [5.0794, 5.0526, 5.0726],
        [5.0794, 5.6718, 5.8134],
        [5.0794, 4.9707, 4.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.1346680074930191 
model_pd.l_d.mean(): -20.728057861328125 
model_pd.lagr.mean(): -20.5933895111084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.3803], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.1346680074930191
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12422119081020355
epoch£º521	 i:2 	 global-step:10422	 l-p:-0.14214348793029785
epoch£º521	 i:3 	 global-step:10423	 l-p:0.15300355851650238
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11301364004611969
epoch£º521	 i:5 	 global-step:10425	 l-p:0.14839589595794678
epoch£º521	 i:6 	 global-step:10426	 l-p:0.14299459755420685
epoch£º521	 i:7 	 global-step:10427	 l-p:0.1449349820613861
epoch£º521	 i:8 	 global-step:10428	 l-p:0.11490939557552338
epoch£º521	 i:9 	 global-step:10429	 l-p:0.23154501616954803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9215, 4.9178, 4.9212],
        [4.9215, 4.9214, 4.9215],
        [4.9215, 4.9129, 4.9206],
        [4.9215, 4.9119, 4.9204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.11325762420892715 
model_pd.l_d.mean(): -20.751737594604492 
model_pd.lagr.mean(): -20.638479232788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4779], device='cuda:0')), ('power', tensor([-21.4667], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.11325762420892715
epoch£º522	 i:1 	 global-step:10441	 l-p:0.19705446064472198
epoch£º522	 i:2 	 global-step:10442	 l-p:0.214288130402565
epoch£º522	 i:3 	 global-step:10443	 l-p:0.14741456508636475
epoch£º522	 i:4 	 global-step:10444	 l-p:0.17646105587482452
epoch£º522	 i:5 	 global-step:10445	 l-p:0.16640697419643402
epoch£º522	 i:6 	 global-step:10446	 l-p:0.07350785285234451
epoch£º522	 i:7 	 global-step:10447	 l-p:0.10975272953510284
epoch£º522	 i:8 	 global-step:10448	 l-p:0.13297928869724274
epoch£º522	 i:9 	 global-step:10449	 l-p:0.11124248057603836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0347, 5.0171, 5.0315],
        [5.0347, 5.0347, 5.0347],
        [5.0347, 5.6091, 5.7388],
        [5.0347, 5.0345, 5.0347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.10559199750423431 
model_pd.l_d.mean(): -18.284894943237305 
model_pd.lagr.mean(): -18.179302215576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6166], device='cuda:0')), ('power', tensor([-19.1146], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.10559199750423431
epoch£º523	 i:1 	 global-step:10461	 l-p:0.0935845673084259
epoch£º523	 i:2 	 global-step:10462	 l-p:1.8136849403381348
epoch£º523	 i:3 	 global-step:10463	 l-p:0.15028659999370575
epoch£º523	 i:4 	 global-step:10464	 l-p:0.13572829961776733
epoch£º523	 i:5 	 global-step:10465	 l-p:0.012473654933273792
epoch£º523	 i:6 	 global-step:10466	 l-p:0.16373038291931152
epoch£º523	 i:7 	 global-step:10467	 l-p:0.15009991824626923
epoch£º523	 i:8 	 global-step:10468	 l-p:0.1430487185716629
epoch£º523	 i:9 	 global-step:10469	 l-p:0.14223690330982208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9894, 4.9785, 4.9880],
        [4.9894, 5.2579, 5.1754],
        [4.9894, 4.9891, 4.9894],
        [4.9894, 4.9790, 4.9882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.19979484379291534 
model_pd.l_d.mean(): -20.326597213745117 
model_pd.lagr.mean(): -20.126802444458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4882], device='cuda:0')), ('power', tensor([-21.0473], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.19979484379291534
epoch£º524	 i:1 	 global-step:10481	 l-p:0.13123424351215363
epoch£º524	 i:2 	 global-step:10482	 l-p:0.04917871952056885
epoch£º524	 i:3 	 global-step:10483	 l-p:0.12294436246156693
epoch£º524	 i:4 	 global-step:10484	 l-p:0.13617940247058868
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13403096795082092
epoch£º524	 i:6 	 global-step:10486	 l-p:0.1360175907611847
epoch£º524	 i:7 	 global-step:10487	 l-p:0.1256222277879715
epoch£º524	 i:8 	 global-step:10488	 l-p:0.1540946364402771
epoch£º524	 i:9 	 global-step:10489	 l-p:0.1151910349726677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9799, 4.9776, 4.9798],
        [4.9799, 4.9799, 4.9799],
        [4.9799, 4.9768, 4.9797],
        [4.9799, 4.8596, 4.8112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16573575139045715 
model_pd.l_d.mean(): -19.725223541259766 
model_pd.lagr.mean(): -19.55948829650879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5199], device='cuda:0')), ('power', tensor([-20.4718], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16573575139045715
epoch£º525	 i:1 	 global-step:10501	 l-p:0.07853366434574127
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12334384769201279
epoch£º525	 i:3 	 global-step:10503	 l-p:0.11845508217811584
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12307381629943848
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12376367300748825
epoch£º525	 i:6 	 global-step:10506	 l-p:0.179155632853508
epoch£º525	 i:7 	 global-step:10507	 l-p:0.16497032344341278
epoch£º525	 i:8 	 global-step:10508	 l-p:0.13018390536308289
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10001703351736069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0077, 4.9970, 5.0064],
        [5.0077, 4.8979, 4.8882],
        [5.0077, 5.0804, 4.9083],
        [5.0077, 5.1527, 5.0052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.14731577038764954 
model_pd.l_d.mean(): -20.656049728393555 
model_pd.lagr.mean(): -20.50873374938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-21.3598], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.14731577038764954
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14395539462566376
epoch£º526	 i:2 	 global-step:10522	 l-p:0.06135876849293709
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12709708511829376
epoch£º526	 i:4 	 global-step:10524	 l-p:0.12880510091781616
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13858528435230255
epoch£º526	 i:6 	 global-step:10526	 l-p:0.21812987327575684
epoch£º526	 i:7 	 global-step:10527	 l-p:0.1406785398721695
epoch£º526	 i:8 	 global-step:10528	 l-p:-0.1016325131058693
epoch£º526	 i:9 	 global-step:10529	 l-p:0.1367664486169815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1307, 5.1307, 5.1307],
        [5.1307, 5.0326, 4.9200],
        [5.1307, 5.1307, 5.1307],
        [5.1307, 5.1301, 5.1307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12963321805000305 
model_pd.l_d.mean(): -20.513399124145508 
model_pd.lagr.mean(): -20.383766174316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249], device='cuda:0')), ('power', tensor([-21.1715], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12963321805000305
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13664939999580383
epoch£º527	 i:2 	 global-step:10542	 l-p:0.1703958809375763
epoch£º527	 i:3 	 global-step:10543	 l-p:0.14373719692230225
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.036207180470228195
epoch£º527	 i:5 	 global-step:10545	 l-p:0.1332516372203827
epoch£º527	 i:6 	 global-step:10546	 l-p:0.13469941914081573
epoch£º527	 i:7 	 global-step:10547	 l-p:2.754765272140503
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12542127072811127
epoch£º527	 i:9 	 global-step:10549	 l-p:0.13709260523319244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0600, 5.0166, 5.0438],
        [5.0600, 5.0536, 5.0595],
        [5.0600, 5.3772, 5.3203],
        [5.0600, 5.3774, 5.3206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.019596993923187256 
model_pd.l_d.mean(): -19.77804946899414 
model_pd.lagr.mean(): -19.797645568847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.5251], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.019596993923187256
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12161251157522202
epoch£º528	 i:2 	 global-step:10562	 l-p:0.12963683903217316
epoch£º528	 i:3 	 global-step:10563	 l-p:0.14215035736560822
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12266518920660019
epoch£º528	 i:5 	 global-step:10565	 l-p:0.08541017025709152
epoch£º528	 i:6 	 global-step:10566	 l-p:0.1520284116268158
epoch£º528	 i:7 	 global-step:10567	 l-p:0.1404353380203247
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13262717425823212
epoch£º528	 i:9 	 global-step:10569	 l-p:0.1559574455022812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9517, 4.8385, 4.7128],
        [4.9517, 4.9070, 4.7252],
        [4.9517, 4.8756, 4.9047],
        [4.9517, 5.1619, 5.0461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.10521041601896286 
model_pd.l_d.mean(): -20.69211196899414 
model_pd.lagr.mean(): -20.58690071105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-21.4081], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.10521041601896286
epoch£º529	 i:1 	 global-step:10581	 l-p:0.1011139526963234
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14544428884983063
epoch£º529	 i:3 	 global-step:10583	 l-p:0.14436714351177216
epoch£º529	 i:4 	 global-step:10584	 l-p:0.2192801982164383
epoch£º529	 i:5 	 global-step:10585	 l-p:0.092060886323452
epoch£º529	 i:6 	 global-step:10586	 l-p:0.16968995332717896
epoch£º529	 i:7 	 global-step:10587	 l-p:0.16543667018413544
epoch£º529	 i:8 	 global-step:10588	 l-p:0.17391948401927948
epoch£º529	 i:9 	 global-step:10589	 l-p:0.1501200795173645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9918, 4.9918, 4.9918],
        [4.9918, 4.8681, 4.7979],
        [4.9918, 4.9918, 4.9918],
        [4.9918, 5.1035, 4.9414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13644778728485107 
model_pd.l_d.mean(): -20.338563919067383 
model_pd.lagr.mean(): -20.202116012573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-21.0696], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13644778728485107
epoch£º530	 i:1 	 global-step:10601	 l-p:0.13281528651714325
epoch£º530	 i:2 	 global-step:10602	 l-p:0.1241617351770401
epoch£º530	 i:3 	 global-step:10603	 l-p:0.09341780096292496
epoch£º530	 i:4 	 global-step:10604	 l-p:-1.4748187065124512
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13212667405605316
epoch£º530	 i:6 	 global-step:10606	 l-p:0.16118434071540833
epoch£º530	 i:7 	 global-step:10607	 l-p:0.10426490753889084
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13223543763160706
epoch£º530	 i:9 	 global-step:10609	 l-p:0.02402111515402794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1196, 5.1147, 5.1193],
        [5.1196, 5.1196, 5.1196],
        [5.1196, 5.1196, 5.1196],
        [5.1196, 5.1196, 5.1196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14034898579120636 
model_pd.l_d.mean(): -19.41496467590332 
model_pd.lagr.mean(): -19.274616241455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-20.1071], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14034898579120636
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12479784339666367
epoch£º531	 i:2 	 global-step:10622	 l-p:0.11736692488193512
epoch£º531	 i:3 	 global-step:10623	 l-p:0.13291925191879272
epoch£º531	 i:4 	 global-step:10624	 l-p:0.06650922447443008
epoch£º531	 i:5 	 global-step:10625	 l-p:0.12348732352256775
epoch£º531	 i:6 	 global-step:10626	 l-p:0.2647848129272461
epoch£º531	 i:7 	 global-step:10627	 l-p:0.1325121819972992
epoch£º531	 i:8 	 global-step:10628	 l-p:0.13285066187381744
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13166624307632446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1918, 5.2180, 5.0406],
        [5.1918, 5.1479, 5.1747],
        [5.1918, 5.2879, 5.1220],
        [5.1918, 5.1891, 5.1917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): -0.10189011693000793 
model_pd.l_d.mean(): -19.41156578063965 
model_pd.lagr.mean(): -19.513456344604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-20.1022], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:-0.10189011693000793
epoch£º532	 i:1 	 global-step:10641	 l-p:0.12780408561229706
epoch£º532	 i:2 	 global-step:10642	 l-p:0.19618047773838043
epoch£º532	 i:3 	 global-step:10643	 l-p:0.12407869100570679
epoch£º532	 i:4 	 global-step:10644	 l-p:-2.2641499042510986
epoch£º532	 i:5 	 global-step:10645	 l-p:0.14131082594394684
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11590416729450226
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11385136097669601
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10194827616214752
epoch£º532	 i:9 	 global-step:10649	 l-p:0.19091299176216125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2752, 5.6096, 5.5536],
        [5.2752, 5.2722, 5.2750],
        [5.2752, 5.2522, 5.2699],
        [5.2752, 5.1707, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12249188870191574 
model_pd.l_d.mean(): -19.708581924438477 
model_pd.lagr.mean(): -19.586090087890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-20.3552], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12249188870191574
epoch£º533	 i:1 	 global-step:10661	 l-p:0.11449383199214935
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11542617529630661
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14052990078926086
epoch£º533	 i:4 	 global-step:10664	 l-p:0.5685058236122131
epoch£º533	 i:5 	 global-step:10665	 l-p:0.13482317328453064
epoch£º533	 i:6 	 global-step:10666	 l-p:0.03750293701887131
epoch£º533	 i:7 	 global-step:10667	 l-p:0.07475576549768448
epoch£º533	 i:8 	 global-step:10668	 l-p:0.13634657859802246
epoch£º533	 i:9 	 global-step:10669	 l-p:0.28650256991386414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 5.0476, 5.0128],
        [5.1595, 5.0933, 4.9363],
        [5.1595, 5.1518, 5.1587],
        [5.1595, 5.0479, 4.9683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.07063250243663788 
model_pd.l_d.mean(): -19.95317268371582 
model_pd.lagr.mean(): -19.882539749145508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4953], device='cuda:0')), ('power', tensor([-20.6772], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.07063250243663788
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13559590280056
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12500175833702087
epoch£º534	 i:3 	 global-step:10683	 l-p:0.12729007005691528
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11127857118844986
epoch£º534	 i:5 	 global-step:10685	 l-p:0.23248660564422607
epoch£º534	 i:6 	 global-step:10686	 l-p:0.05525178462266922
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11553199589252472
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14414452016353607
epoch£º534	 i:9 	 global-step:10689	 l-p:0.15168209373950958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0615, 5.0613, 5.0615],
        [5.0615, 5.0414, 5.0576],
        [5.0615, 5.0615, 5.0615],
        [5.0615, 5.0182, 5.0457]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.4641551971435547 
model_pd.l_d.mean(): -20.323631286621094 
model_pd.lagr.mean(): -19.85947608947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.0136], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.4641551971435547
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14217975735664368
epoch£º535	 i:2 	 global-step:10702	 l-p:0.15187130868434906
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16067920625209808
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12417369335889816
epoch£º535	 i:5 	 global-step:10705	 l-p:0.15220589935779572
epoch£º535	 i:6 	 global-step:10706	 l-p:0.13037355244159698
epoch£º535	 i:7 	 global-step:10707	 l-p:0.11413545161485672
epoch£º535	 i:8 	 global-step:10708	 l-p:0.10850408673286438
epoch£º535	 i:9 	 global-step:10709	 l-p:0.1504509001970291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9545, 4.9436, 4.9532],
        [4.9545, 4.9491, 4.9541],
        [4.9545, 4.9445, 4.9533],
        [4.9545, 5.0492, 4.8787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.16653972864151 
model_pd.l_d.mean(): -19.57486915588379 
model_pd.lagr.mean(): -19.408329010009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-20.3431], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.16653972864151
epoch£º536	 i:1 	 global-step:10721	 l-p:0.08943586051464081
epoch£º536	 i:2 	 global-step:10722	 l-p:0.09359204024076462
epoch£º536	 i:3 	 global-step:10723	 l-p:0.11282023042440414
epoch£º536	 i:4 	 global-step:10724	 l-p:0.12709136307239532
epoch£º536	 i:5 	 global-step:10725	 l-p:0.135489359498024
epoch£º536	 i:6 	 global-step:10726	 l-p:0.15311068296432495
epoch£º536	 i:7 	 global-step:10727	 l-p:0.16571946442127228
epoch£º536	 i:8 	 global-step:10728	 l-p:0.1338212490081787
epoch£º536	 i:9 	 global-step:10729	 l-p:0.11860891431570053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0907, 5.0906, 5.0907],
        [5.0907, 4.9701, 4.9167],
        [5.0907, 4.9710, 4.9279],
        [5.0907, 5.0907, 5.0907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.11236928403377533 
model_pd.l_d.mean(): -20.84003448486328 
model_pd.lagr.mean(): -20.727664947509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3940], device='cuda:0')), ('power', tensor([-21.4702], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.11236928403377533
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12541554868221283
epoch£º537	 i:2 	 global-step:10742	 l-p:0.20300136506557465
epoch£º537	 i:3 	 global-step:10743	 l-p:0.07368914783000946
epoch£º537	 i:4 	 global-step:10744	 l-p:0.13450278341770172
epoch£º537	 i:5 	 global-step:10745	 l-p:0.04470042511820793
epoch£º537	 i:6 	 global-step:10746	 l-p:0.1244044080376625
epoch£º537	 i:7 	 global-step:10747	 l-p:0.1011379063129425
epoch£º537	 i:8 	 global-step:10748	 l-p:0.12820769846439362
epoch£º537	 i:9 	 global-step:10749	 l-p:0.15481355786323547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2311, 5.2002, 5.2223],
        [5.2311, 5.2306, 5.2311],
        [5.2311, 5.2311, 5.2311],
        [5.2311, 5.1819, 5.2099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.12767010927200317 
model_pd.l_d.mean(): -20.611175537109375 
model_pd.lagr.mean(): -20.483505249023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3913], device='cuda:0')), ('power', tensor([-21.2361], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.12767010927200317
epoch£º538	 i:1 	 global-step:10761	 l-p:-0.3196912109851837
epoch£º538	 i:2 	 global-step:10762	 l-p:0.13197557628154755
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11044904589653015
epoch£º538	 i:4 	 global-step:10764	 l-p:1.0809111595153809
epoch£º538	 i:5 	 global-step:10765	 l-p:0.15321341156959534
epoch£º538	 i:6 	 global-step:10766	 l-p:0.12949536740779877
epoch£º538	 i:7 	 global-step:10767	 l-p:0.2031814157962799
epoch£º538	 i:8 	 global-step:10768	 l-p:0.13257139921188354
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.03114817477762699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2144, 5.1350, 5.1577],
        [5.2144, 5.2144, 5.2144],
        [5.2144, 5.2020, 5.2127],
        [5.2144, 5.2143, 5.2144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11492516100406647 
model_pd.l_d.mean(): -20.48436164855957 
model_pd.lagr.mean(): -20.369436264038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4193], device='cuda:0')), ('power', tensor([-21.1364], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11492516100406647
epoch£º539	 i:1 	 global-step:10781	 l-p:-0.4879637658596039
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12858276069164276
epoch£º539	 i:3 	 global-step:10783	 l-p:0.1571853756904602
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12448649108409882
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12367597967386246
epoch£º539	 i:6 	 global-step:10786	 l-p:0.1151006743311882
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11659807711839676
epoch£º539	 i:8 	 global-step:10788	 l-p:0.17270149290561676
epoch£º539	 i:9 	 global-step:10789	 l-p:-1.4766731262207031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2119, 5.1285, 4.9881],
        [5.2119, 5.2119, 5.2119],
        [5.2119, 5.1128, 5.1153],
        [5.2119, 5.3629, 5.2127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.14786921441555023 
model_pd.l_d.mean(): -19.282548904418945 
model_pd.lagr.mean(): -19.134679794311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-20.0370], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.14786921441555023
epoch£º540	 i:1 	 global-step:10801	 l-p:0.662156879901886
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13041558861732483
epoch£º540	 i:3 	 global-step:10803	 l-p:0.12121396511793137
epoch£º540	 i:4 	 global-step:10804	 l-p:2.5697176456451416
epoch£º540	 i:5 	 global-step:10805	 l-p:0.11867282539606094
epoch£º540	 i:6 	 global-step:10806	 l-p:0.121762715280056
epoch£º540	 i:7 	 global-step:10807	 l-p:0.12552553415298462
epoch£º540	 i:8 	 global-step:10808	 l-p:0.12909595668315887
epoch£º540	 i:9 	 global-step:10809	 l-p:0.061273686587810516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1807, 5.1458, 5.1700],
        [5.1807, 5.1528, 5.1736],
        [5.1807, 5.7024, 5.7778],
        [5.1807, 5.1807, 5.1807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.1118040606379509 
model_pd.l_d.mean(): -19.210111618041992 
model_pd.lagr.mean(): -19.09830665588379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4689], device='cuda:0')), ('power', tensor([-19.8990], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.1118040606379509
epoch£º541	 i:1 	 global-step:10821	 l-p:0.1300312876701355
epoch£º541	 i:2 	 global-step:10822	 l-p:0.19244427978992462
epoch£º541	 i:3 	 global-step:10823	 l-p:0.272642582654953
epoch£º541	 i:4 	 global-step:10824	 l-p:0.1403190940618515
epoch£º541	 i:5 	 global-step:10825	 l-p:0.1119203269481659
epoch£º541	 i:6 	 global-step:10826	 l-p:0.09750502556562424
epoch£º541	 i:7 	 global-step:10827	 l-p:0.030487950891256332
epoch£º541	 i:8 	 global-step:10828	 l-p:0.1108463853597641
epoch£º541	 i:9 	 global-step:10829	 l-p:0.10823982208967209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1287, 5.1287, 5.1287],
        [5.1287, 5.0206, 4.9007],
        [5.1287, 5.8190, 6.0306],
        [5.1287, 5.0516, 4.8917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.12285864353179932 
model_pd.l_d.mean(): -18.577375411987305 
model_pd.lagr.mean(): -18.454517364501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-19.3298], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.12285864353179932
epoch£º542	 i:1 	 global-step:10841	 l-p:0.1394331306219101
epoch£º542	 i:2 	 global-step:10842	 l-p:0.12339095026254654
epoch£º542	 i:3 	 global-step:10843	 l-p:0.13589365780353546
epoch£º542	 i:4 	 global-step:10844	 l-p:0.1612185686826706
epoch£º542	 i:5 	 global-step:10845	 l-p:0.12628480792045593
epoch£º542	 i:6 	 global-step:10846	 l-p:0.10123557597398758
epoch£º542	 i:7 	 global-step:10847	 l-p:0.5716621279716492
epoch£º542	 i:8 	 global-step:10848	 l-p:0.0474066324532032
epoch£º542	 i:9 	 global-step:10849	 l-p:0.12225740402936935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1059, 5.0193, 4.8635],
        [5.1059, 5.7238, 5.8778],
        [5.1059, 5.0328, 5.0621],
        [5.1059, 5.1046, 5.1059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.12512750923633575 
model_pd.l_d.mean(): -20.053585052490234 
model_pd.lagr.mean(): -19.928457260131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-20.7558], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.12512750923633575
epoch£º543	 i:1 	 global-step:10861	 l-p:-0.4054245352745056
epoch£º543	 i:2 	 global-step:10862	 l-p:0.0746263861656189
epoch£º543	 i:3 	 global-step:10863	 l-p:0.0760788768529892
epoch£º543	 i:4 	 global-step:10864	 l-p:0.11909957975149155
epoch£º543	 i:5 	 global-step:10865	 l-p:0.11348240822553635
epoch£º543	 i:6 	 global-step:10866	 l-p:0.19999130070209503
epoch£º543	 i:7 	 global-step:10867	 l-p:0.16365815699100494
epoch£º543	 i:8 	 global-step:10868	 l-p:0.1313941478729248
epoch£º543	 i:9 	 global-step:10869	 l-p:0.14128349721431732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1116, 5.1105, 5.1116],
        [5.1116, 5.0495, 4.8763],
        [5.1116, 5.1116, 5.1116],
        [5.1116, 5.5033, 5.4888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.08869603276252747 
model_pd.l_d.mean(): -20.54959487915039 
model_pd.lagr.mean(): -20.460899353027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4239], device='cuda:0')), ('power', tensor([-21.2071], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.08869603276252747
epoch£º544	 i:1 	 global-step:10881	 l-p:0.12946054339408875
epoch£º544	 i:2 	 global-step:10882	 l-p:0.06310310959815979
epoch£º544	 i:3 	 global-step:10883	 l-p:0.12035922706127167
epoch£º544	 i:4 	 global-step:10884	 l-p:0.133987158536911
epoch£º544	 i:5 	 global-step:10885	 l-p:0.13133880496025085
epoch£º544	 i:6 	 global-step:10886	 l-p:0.09101337194442749
epoch£º544	 i:7 	 global-step:10887	 l-p:0.14320488274097443
epoch£º544	 i:8 	 global-step:10888	 l-p:0.15610143542289734
epoch£º544	 i:9 	 global-step:10889	 l-p:0.1359850913286209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 4.9787, 4.9890],
        [4.9905, 4.9447, 4.9740],
        [4.9905, 4.8698, 4.8611],
        [4.9905, 4.8720, 4.8679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.12180892378091812 
model_pd.l_d.mean(): -20.780038833618164 
model_pd.lagr.mean(): -20.65822982788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4544], device='cuda:0')), ('power', tensor([-21.4712], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.12180892378091812
epoch£º545	 i:1 	 global-step:10901	 l-p:0.15787819027900696
epoch£º545	 i:2 	 global-step:10902	 l-p:0.14084932208061218
epoch£º545	 i:3 	 global-step:10903	 l-p:0.12763553857803345
epoch£º545	 i:4 	 global-step:10904	 l-p:0.10566932708024979
epoch£º545	 i:5 	 global-step:10905	 l-p:0.17226611077785492
epoch£º545	 i:6 	 global-step:10906	 l-p:0.2099875658750534
epoch£º545	 i:7 	 global-step:10907	 l-p:0.14523160457611084
epoch£º545	 i:8 	 global-step:10908	 l-p:0.10349372774362564
epoch£º545	 i:9 	 global-step:10909	 l-p:0.12377535551786423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0055, 4.9786, 4.9993],
        [5.0055, 4.9798, 4.9997],
        [5.0055, 5.0054, 5.0055],
        [5.0055, 4.8782, 4.7664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13630236685276031 
model_pd.l_d.mean(): -19.83889389038086 
model_pd.lagr.mean(): -19.702590942382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-20.5141], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13630236685276031
epoch£º546	 i:1 	 global-step:10921	 l-p:0.16309905052185059
epoch£º546	 i:2 	 global-step:10922	 l-p:0.14601539075374603
epoch£º546	 i:3 	 global-step:10923	 l-p:0.13113927841186523
epoch£º546	 i:4 	 global-step:10924	 l-p:0.08608260750770569
epoch£º546	 i:5 	 global-step:10925	 l-p:0.017476985231041908
epoch£º546	 i:6 	 global-step:10926	 l-p:0.15083491802215576
epoch£º546	 i:7 	 global-step:10927	 l-p:0.1564805507659912
epoch£º546	 i:8 	 global-step:10928	 l-p:0.13016539812088013
epoch£º546	 i:9 	 global-step:10929	 l-p:0.44634220004081726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0734, 4.9777, 4.9978],
        [5.0734, 5.5730, 5.6367],
        [5.0734, 5.0724, 5.0734],
        [5.0734, 5.0734, 5.0734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.1380242258310318 
model_pd.l_d.mean(): -20.52510643005371 
model_pd.lagr.mean(): -20.387083053588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4526], device='cuda:0')), ('power', tensor([-21.2117], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.1380242258310318
epoch£º547	 i:1 	 global-step:10941	 l-p:0.02543024532496929
epoch£º547	 i:2 	 global-step:10942	 l-p:0.1183016374707222
epoch£º547	 i:3 	 global-step:10943	 l-p:0.11562396585941315
epoch£º547	 i:4 	 global-step:10944	 l-p:1.9766411781311035
epoch£º547	 i:5 	 global-step:10945	 l-p:0.16164630651474
epoch£º547	 i:6 	 global-step:10946	 l-p:0.14446206390857697
epoch£º547	 i:7 	 global-step:10947	 l-p:0.08275813609361649
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13783910870552063
epoch£º547	 i:9 	 global-step:10949	 l-p:0.12208627164363861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0528, 5.0526, 5.0528],
        [5.0528, 4.9648, 4.9914],
        [5.0528, 5.0216, 5.0446],
        [5.0528, 5.0073, 5.0363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.13248397409915924 
model_pd.l_d.mean(): -19.435714721679688 
model_pd.lagr.mean(): -19.30323028564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5245], device='cuda:0')), ('power', tensor([-20.1838], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.13248397409915924
epoch£º548	 i:1 	 global-step:10961	 l-p:0.1286163628101349
epoch£º548	 i:2 	 global-step:10962	 l-p:0.15677189826965332
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12595286965370178
epoch£º548	 i:4 	 global-step:10964	 l-p:0.18540960550308228
epoch£º548	 i:5 	 global-step:10965	 l-p:0.12977835536003113
epoch£º548	 i:6 	 global-step:10966	 l-p:0.13536085188388824
epoch£º548	 i:7 	 global-step:10967	 l-p:0.09891749918460846
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12091754376888275
epoch£º548	 i:9 	 global-step:10969	 l-p:0.09625805914402008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9961, 5.3278, 5.2774],
        [4.9961, 4.9910, 4.9958],
        [4.9961, 4.9882, 4.9954],
        [4.9961, 4.8628, 4.8249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.11131235212087631 
model_pd.l_d.mean(): -20.582950592041016 
model_pd.lagr.mean(): -20.471637725830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4561], device='cuda:0')), ('power', tensor([-21.2737], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.11131235212087631
epoch£º549	 i:1 	 global-step:10981	 l-p:0.14398644864559174
epoch£º549	 i:2 	 global-step:10982	 l-p:0.15421028435230255
epoch£º549	 i:3 	 global-step:10983	 l-p:0.1314811259508133
epoch£º549	 i:4 	 global-step:10984	 l-p:0.09829162061214447
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11429354548454285
epoch£º549	 i:6 	 global-step:10986	 l-p:0.1411948800086975
epoch£º549	 i:7 	 global-step:10987	 l-p:0.14243921637535095
epoch£º549	 i:8 	 global-step:10988	 l-p:0.13409897685050964
epoch£º549	 i:9 	 global-step:10989	 l-p:0.1015879437327385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9896, 4.9560, 4.9804],
        [4.9896, 4.8591, 4.7363],
        [4.9896, 4.8512, 4.7811],
        [4.9896, 4.9348, 4.9669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.1398625522851944 
model_pd.l_d.mean(): -18.537616729736328 
model_pd.lagr.mean(): -18.397754669189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5871], device='cuda:0')), ('power', tensor([-19.3399], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.1398625522851944
epoch£º550	 i:1 	 global-step:11001	 l-p:0.07979128509759903
epoch£º550	 i:2 	 global-step:11002	 l-p:0.1924031525850296
epoch£º550	 i:3 	 global-step:11003	 l-p:0.07254194468259811
epoch£º550	 i:4 	 global-step:11004	 l-p:0.11962653696537018
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13506808876991272
epoch£º550	 i:6 	 global-step:11006	 l-p:0.14983582496643066
epoch£º550	 i:7 	 global-step:11007	 l-p:0.13192683458328247
epoch£º550	 i:8 	 global-step:11008	 l-p:0.13400676846504211
epoch£º550	 i:9 	 global-step:11009	 l-p:0.11675357818603516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0433, 4.9351, 4.9475],
        [5.0433, 5.1415, 4.9662],
        [5.0433, 5.6194, 5.7417],
        [5.0433, 5.0433, 5.0433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.13049323856830597 
model_pd.l_d.mean(): -18.183427810668945 
model_pd.lagr.mean(): -18.052934646606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5859], device='cuda:0')), ('power', tensor([-18.9806], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.13049323856830597
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14092504978179932
epoch£º551	 i:2 	 global-step:11022	 l-p:0.09610268473625183
epoch£º551	 i:3 	 global-step:11023	 l-p:0.15742099285125732
epoch£º551	 i:4 	 global-step:11024	 l-p:0.10199650377035141
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15146468579769135
epoch£º551	 i:6 	 global-step:11026	 l-p:0.13861852884292603
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12772029638290405
epoch£º551	 i:8 	 global-step:11028	 l-p:0.128135085105896
epoch£º551	 i:9 	 global-step:11029	 l-p:0.1395546793937683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9404, 4.8039, 4.6773],
        [4.9404, 4.9403, 4.9404],
        [4.9404, 4.9403, 4.9404],
        [4.9404, 4.9404, 4.9404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11887533962726593 
model_pd.l_d.mean(): -19.95850372314453 
model_pd.lagr.mean(): -19.839628219604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.7228], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11887533962726593
epoch£º552	 i:1 	 global-step:11041	 l-p:0.21582838892936707
epoch£º552	 i:2 	 global-step:11042	 l-p:0.5145000219345093
epoch£º552	 i:3 	 global-step:11043	 l-p:0.16730046272277832
epoch£º552	 i:4 	 global-step:11044	 l-p:0.14029443264007568
epoch£º552	 i:5 	 global-step:11045	 l-p:0.12393895536661148
epoch£º552	 i:6 	 global-step:11046	 l-p:0.1316673755645752
epoch£º552	 i:7 	 global-step:11047	 l-p:0.18957507610321045
epoch£º552	 i:8 	 global-step:11048	 l-p:0.1857331097126007
epoch£º552	 i:9 	 global-step:11049	 l-p:0.20871266722679138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9324, 4.9324, 4.9324],
        [4.9324, 4.7955, 4.6651],
        [4.9324, 5.4296, 5.4976],
        [4.9324, 4.8871, 4.9169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.3238937556743622 
model_pd.l_d.mean(): -20.274320602416992 
model_pd.lagr.mean(): -19.95042610168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5434], device='cuda:0')), ('power', tensor([-21.0509], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.3238937556743622
epoch£º553	 i:1 	 global-step:11061	 l-p:0.11901025474071503
epoch£º553	 i:2 	 global-step:11062	 l-p:0.1135348230600357
epoch£º553	 i:3 	 global-step:11063	 l-p:0.17758528888225555
epoch£º553	 i:4 	 global-step:11064	 l-p:0.14127026498317719
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08236062526702881
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13419951498508453
epoch£º553	 i:7 	 global-step:11067	 l-p:0.1333719938993454
epoch£º553	 i:8 	 global-step:11068	 l-p:0.1048000305891037
epoch£º553	 i:9 	 global-step:11069	 l-p:0.1796247661113739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0416, 5.0203, 5.0375],
        [5.0416, 5.0407, 5.0416],
        [5.0416, 5.0152, 5.0356],
        [5.0416, 4.9847, 4.7928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.07809291034936905 
model_pd.l_d.mean(): -20.02381134033203 
model_pd.lagr.mean(): -19.94571876525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4608], device='cuda:0')), ('power', tensor([-20.7133], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.07809291034936905
epoch£º554	 i:1 	 global-step:11081	 l-p:0.12998466193675995
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09793535619974136
epoch£º554	 i:3 	 global-step:11083	 l-p:0.11253456026315689
epoch£º554	 i:4 	 global-step:11084	 l-p:0.14898748695850372
epoch£º554	 i:5 	 global-step:11085	 l-p:-0.39506056904792786
epoch£º554	 i:6 	 global-step:11086	 l-p:0.1244502142071724
epoch£º554	 i:7 	 global-step:11087	 l-p:0.14468418061733246
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12506747245788574
epoch£º554	 i:9 	 global-step:11089	 l-p:0.14163246750831604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1027, 5.1026, 5.1027],
        [5.1027, 5.0276, 5.0593],
        [5.1027, 5.0899, 5.1010],
        [5.1027, 5.0131, 5.0396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.14473555982112885 
model_pd.l_d.mean(): -20.43477439880371 
model_pd.lagr.mean(): -20.2900390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4433], device='cuda:0')), ('power', tensor([-21.1109], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.14473555982112885
epoch£º555	 i:1 	 global-step:11101	 l-p:0.1088595911860466
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10880780220031738
epoch£º555	 i:3 	 global-step:11103	 l-p:0.38597625494003296
epoch£º555	 i:4 	 global-step:11104	 l-p:-0.2488228678703308
epoch£º555	 i:5 	 global-step:11105	 l-p:0.07464776933193207
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14445257186889648
epoch£º555	 i:7 	 global-step:11107	 l-p:0.12315379828214645
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11304891109466553
epoch£º555	 i:9 	 global-step:11109	 l-p:0.13106407225131989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2569, 5.1394, 5.0443],
        [5.2569, 5.2569, 5.2569],
        [5.2569, 5.3315, 5.1504],
        [5.2569, 5.2551, 5.2569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.16596892476081848 
model_pd.l_d.mean(): -20.567317962646484 
model_pd.lagr.mean(): -20.401348114013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.2029], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.16596892476081848
epoch£º556	 i:1 	 global-step:11121	 l-p:0.769171953201294
epoch£º556	 i:2 	 global-step:11122	 l-p:0.2845134735107422
epoch£º556	 i:3 	 global-step:11123	 l-p:0.11959169059991837
epoch£º556	 i:4 	 global-step:11124	 l-p:0.135139599442482
epoch£º556	 i:5 	 global-step:11125	 l-p:0.1473325937986374
epoch£º556	 i:6 	 global-step:11126	 l-p:-0.08044036477804184
epoch£º556	 i:7 	 global-step:11127	 l-p:0.12554018199443817
epoch£º556	 i:8 	 global-step:11128	 l-p:0.1137552335858345
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13426710665225983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1751, 5.1650, 5.1740],
        [5.1751, 5.0882, 4.9230],
        [5.1751, 5.1175, 5.1491],
        [5.1751, 5.2421, 5.0570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.11445925384759903 
model_pd.l_d.mean(): -19.956825256347656 
model_pd.lagr.mean(): -19.842365264892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4936], device='cuda:0')), ('power', tensor([-20.6791], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.11445925384759903
epoch£º557	 i:1 	 global-step:11141	 l-p:0.1288696825504303
epoch£º557	 i:2 	 global-step:11142	 l-p:-0.030836420133709908
epoch£º557	 i:3 	 global-step:11143	 l-p:0.1293090432882309
epoch£º557	 i:4 	 global-step:11144	 l-p:0.12912873923778534
epoch£º557	 i:5 	 global-step:11145	 l-p:0.13725732266902924
epoch£º557	 i:6 	 global-step:11146	 l-p:1.2512949705123901
epoch£º557	 i:7 	 global-step:11147	 l-p:0.13941195607185364
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11694734543561935
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12099199742078781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0622, 4.9239, 4.8428],
        [5.0622, 5.0703, 4.8713],
        [5.0622, 5.0331, 5.0552],
        [5.0622, 5.0554, 5.0616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.0837329626083374 
model_pd.l_d.mean(): -19.915498733520508 
model_pd.lagr.mean(): -19.83176612854004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-20.6249], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.0837329626083374
epoch£º558	 i:1 	 global-step:11161	 l-p:0.04129582643508911
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12614530324935913
epoch£º558	 i:3 	 global-step:11163	 l-p:0.1330510526895523
epoch£º558	 i:4 	 global-step:11164	 l-p:0.11330783367156982
epoch£º558	 i:5 	 global-step:11165	 l-p:0.1369764655828476
epoch£º558	 i:6 	 global-step:11166	 l-p:0.12455562502145767
epoch£º558	 i:7 	 global-step:11167	 l-p:0.15811337530612946
epoch£º558	 i:8 	 global-step:11168	 l-p:0.12695544958114624
epoch£º558	 i:9 	 global-step:11169	 l-p:0.1517898291349411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0040, 4.9327, 4.7369],
        [5.0040, 4.9947, 5.0030],
        [5.0040, 5.0040, 5.0040],
        [5.0040, 5.0033, 5.0040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.14346854388713837 
model_pd.l_d.mean(): -19.890300750732422 
model_pd.lagr.mean(): -19.7468318939209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5633], device='cuda:0')), ('power', tensor([-20.6830], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.14346854388713837
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15099425613880157
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10803693532943726
epoch£º559	 i:3 	 global-step:11183	 l-p:0.16408954560756683
epoch£º559	 i:4 	 global-step:11184	 l-p:0.17751744389533997
epoch£º559	 i:5 	 global-step:11185	 l-p:0.14606545865535736
epoch£º559	 i:6 	 global-step:11186	 l-p:0.1771879345178604
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15929745137691498
epoch£º559	 i:8 	 global-step:11188	 l-p:0.0627732053399086
epoch£º559	 i:9 	 global-step:11189	 l-p:0.1200396865606308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0596, 5.6869, 5.8452],
        [5.0596, 4.9323, 4.7936],
        [5.0596, 5.6935, 5.8571],
        [5.0596, 5.0871, 4.8887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.12105710059404373 
model_pd.l_d.mean(): -18.7049617767334 
model_pd.lagr.mean(): -18.583904266357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5510], device='cuda:0')), ('power', tensor([-19.4722], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.12105710059404373
epoch£º560	 i:1 	 global-step:11201	 l-p:0.7439238429069519
epoch£º560	 i:2 	 global-step:11202	 l-p:0.08702386915683746
epoch£º560	 i:3 	 global-step:11203	 l-p:0.014419583603739738
epoch£º560	 i:4 	 global-step:11204	 l-p:0.22326909005641937
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11436503380537033
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13662582635879517
epoch£º560	 i:7 	 global-step:11207	 l-p:0.004969911649823189
epoch£º560	 i:8 	 global-step:11208	 l-p:0.1517283171415329
epoch£º560	 i:9 	 global-step:11209	 l-p:0.12344280630350113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2474, 5.2474, 5.2474],
        [5.2474, 5.1329, 5.0100],
        [5.2474, 5.2384, 5.2465],
        [5.2474, 5.3265, 5.1435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.4253293573856354 
model_pd.l_d.mean(): -19.6937313079834 
model_pd.lagr.mean(): -19.268402099609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4305], device='cuda:0')), ('power', tensor([-20.3486], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.4253293573856354
epoch£º561	 i:1 	 global-step:11221	 l-p:-3.1496574878692627
epoch£º561	 i:2 	 global-step:11222	 l-p:0.14452184736728668
epoch£º561	 i:3 	 global-step:11223	 l-p:0.11595581471920013
epoch£º561	 i:4 	 global-step:11224	 l-p:0.1309579759836197
epoch£º561	 i:5 	 global-step:11225	 l-p:0.12260838598012924
epoch£º561	 i:6 	 global-step:11226	 l-p:0.09921753406524658
epoch£º561	 i:7 	 global-step:11227	 l-p:0.13707208633422852
epoch£º561	 i:8 	 global-step:11228	 l-p:0.052077967673540115
epoch£º561	 i:9 	 global-step:11229	 l-p:0.27179840207099915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1831, 5.1452, 5.1715],
        [5.1831, 5.1825, 5.1831],
        [5.1831, 5.1830, 5.1831],
        [5.1831, 5.1182, 4.9335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.14002929627895355 
model_pd.l_d.mean(): -19.935842514038086 
model_pd.lagr.mean(): -19.795812606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4323], device='cuda:0')), ('power', tensor([-20.5952], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.14002929627895355
epoch£º562	 i:1 	 global-step:11241	 l-p:0.1293666660785675
epoch£º562	 i:2 	 global-step:11242	 l-p:0.17595437169075012
epoch£º562	 i:3 	 global-step:11243	 l-p:0.08683944493532181
epoch£º562	 i:4 	 global-step:11244	 l-p:0.11734704673290253
epoch£º562	 i:5 	 global-step:11245	 l-p:0.1379590630531311
epoch£º562	 i:6 	 global-step:11246	 l-p:0.14478878676891327
epoch£º562	 i:7 	 global-step:11247	 l-p:0.14053143560886383
epoch£º562	 i:8 	 global-step:11248	 l-p:0.05377126485109329
epoch£º562	 i:9 	 global-step:11249	 l-p:0.0897558405995369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0365, 5.0292, 5.0358],
        [5.0365, 4.9360, 4.9619],
        [5.0365, 4.9652, 4.9999],
        [5.0365, 5.0032, 5.0277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.06174566596746445 
model_pd.l_d.mean(): -20.59140968322754 
model_pd.lagr.mean(): -20.5296630859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4654], device='cuda:0')), ('power', tensor([-21.2918], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.06174566596746445
epoch£º563	 i:1 	 global-step:11261	 l-p:0.14590907096862793
epoch£º563	 i:2 	 global-step:11262	 l-p:0.1178000420331955
epoch£º563	 i:3 	 global-step:11263	 l-p:0.16534732282161713
epoch£º563	 i:4 	 global-step:11264	 l-p:0.0674198642373085
epoch£º563	 i:5 	 global-step:11265	 l-p:0.14825257658958435
epoch£º563	 i:6 	 global-step:11266	 l-p:0.13888975977897644
epoch£º563	 i:7 	 global-step:11267	 l-p:0.14739570021629333
epoch£º563	 i:8 	 global-step:11268	 l-p:0.11110406368970871
epoch£º563	 i:9 	 global-step:11269	 l-p:0.1264180988073349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0311, 4.9224, 4.7469],
        [5.0311, 5.0486, 4.8457],
        [5.0311, 4.8850, 4.8038],
        [5.0311, 4.9185, 4.9355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.057392992079257965 
model_pd.l_d.mean(): -19.69375991821289 
model_pd.lagr.mean(): -19.636367797851562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4857], device='cuda:0')), ('power', tensor([-20.4051], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.057392992079257965
epoch£º564	 i:1 	 global-step:11281	 l-p:0.21047723293304443
epoch£º564	 i:2 	 global-step:11282	 l-p:0.13393956422805786
epoch£º564	 i:3 	 global-step:11283	 l-p:0.1393834501504898
epoch£º564	 i:4 	 global-step:11284	 l-p:0.18501338362693787
epoch£º564	 i:5 	 global-step:11285	 l-p:0.10362035781145096
epoch£º564	 i:6 	 global-step:11286	 l-p:0.12921415269374847
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12707622349262238
epoch£º564	 i:8 	 global-step:11288	 l-p:0.12687808275222778
epoch£º564	 i:9 	 global-step:11289	 l-p:0.11428334563970566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9315, 4.9264, 4.9312],
        [4.9315, 4.8271, 4.8553],
        [4.9315, 5.4393, 5.5111],
        [4.9315, 5.4445, 5.5202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.6383285522460938 
model_pd.l_d.mean(): -20.16082763671875 
model_pd.lagr.mean(): -19.522499084472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5372], device='cuda:0')), ('power', tensor([-20.9299], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.6383285522460938
epoch£º565	 i:1 	 global-step:11301	 l-p:0.15594922006130219
epoch£º565	 i:2 	 global-step:11302	 l-p:0.23679406940937042
epoch£º565	 i:3 	 global-step:11303	 l-p:0.14731594920158386
epoch£º565	 i:4 	 global-step:11304	 l-p:0.17018914222717285
epoch£º565	 i:5 	 global-step:11305	 l-p:0.16705600917339325
epoch£º565	 i:6 	 global-step:11306	 l-p:0.15307246148586273
epoch£º565	 i:7 	 global-step:11307	 l-p:0.15965482592582703
epoch£º565	 i:8 	 global-step:11308	 l-p:0.07520104944705963
epoch£º565	 i:9 	 global-step:11309	 l-p:0.095241479575634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0559, 5.0426, 5.0542],
        [5.0559, 5.0542, 5.0558],
        [5.0559, 4.9589, 4.9871],
        [5.0559, 5.0065, 5.0379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.049067214131355286 
model_pd.l_d.mean(): -20.34526252746582 
model_pd.lagr.mean(): -20.29619598388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.0544], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.049067214131355286
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11773335933685303
epoch£º566	 i:2 	 global-step:11322	 l-p:0.0971071794629097
epoch£º566	 i:3 	 global-step:11323	 l-p:0.15354496240615845
epoch£º566	 i:4 	 global-step:11324	 l-p:0.15643203258514404
epoch£º566	 i:5 	 global-step:11325	 l-p:0.11598188430070877
epoch£º566	 i:6 	 global-step:11326	 l-p:0.20732466876506805
epoch£º566	 i:7 	 global-step:11327	 l-p:0.0636041909456253
epoch£º566	 i:8 	 global-step:11328	 l-p:0.09710080921649933
epoch£º566	 i:9 	 global-step:11329	 l-p:0.13173045217990875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.1403, 5.7389, 5.8676],
        [5.1403, 5.6724, 5.7501],
        [5.1403, 5.0241, 5.0309],
        [5.1403, 5.4850, 5.4310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.17585647106170654 
model_pd.l_d.mean(): -18.529760360717773 
model_pd.lagr.mean(): -18.353904724121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5618], device='cuda:0')), ('power', tensor([-19.3061], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.17585647106170654
epoch£º567	 i:1 	 global-step:11341	 l-p:0.14178898930549622
epoch£º567	 i:2 	 global-step:11342	 l-p:0.35752663016319275
epoch£º567	 i:3 	 global-step:11343	 l-p:0.13699178397655487
epoch£º567	 i:4 	 global-step:11344	 l-p:0.08005595952272415
epoch£º567	 i:5 	 global-step:11345	 l-p:0.13173964619636536
epoch£º567	 i:6 	 global-step:11346	 l-p:-0.038930319249629974
epoch£º567	 i:7 	 global-step:11347	 l-p:0.14929547905921936
epoch£º567	 i:8 	 global-step:11348	 l-p:0.13738906383514404
epoch£º567	 i:9 	 global-step:11349	 l-p:0.08331307023763657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 4.9997, 4.8651],
        [5.1281, 5.1190, 5.1272],
        [5.1281, 5.3198, 5.1774],
        [5.1281, 5.0082, 4.8576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.12088944762945175 
model_pd.l_d.mean(): -20.072792053222656 
model_pd.lagr.mean(): -19.951902389526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4894], device='cuda:0')), ('power', tensor([-20.7920], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.12088944762945175
epoch£º568	 i:1 	 global-step:11361	 l-p:0.15794368088245392
epoch£º568	 i:2 	 global-step:11362	 l-p:0.1172296479344368
epoch£º568	 i:3 	 global-step:11363	 l-p:0.11759687215089798
epoch£º568	 i:4 	 global-step:11364	 l-p:0.11696323752403259
epoch£º568	 i:5 	 global-step:11365	 l-p:0.12421628832817078
epoch£º568	 i:6 	 global-step:11366	 l-p:0.13631223142147064
epoch£º568	 i:7 	 global-step:11367	 l-p:0.044434402137994766
epoch£º568	 i:8 	 global-step:11368	 l-p:0.12450938671827316
epoch£º568	 i:9 	 global-step:11369	 l-p:0.10749687254428864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0065, 5.0065, 5.0065],
        [5.0065, 4.9740, 4.9984],
        [5.0065, 5.5366, 5.6198],
        [5.0065, 5.0065, 5.0065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.1295493245124817 
model_pd.l_d.mean(): -19.591079711914062 
model_pd.lagr.mean(): -19.461530685424805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5211], device='cuda:0')), ('power', tensor([-20.3374], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.1295493245124817
epoch£º569	 i:1 	 global-step:11381	 l-p:0.14525233209133148
epoch£º569	 i:2 	 global-step:11382	 l-p:0.1454320251941681
epoch£º569	 i:3 	 global-step:11383	 l-p:0.14175929129123688
epoch£º569	 i:4 	 global-step:11384	 l-p:0.08774550259113312
epoch£º569	 i:5 	 global-step:11385	 l-p:0.1557183712720871
epoch£º569	 i:6 	 global-step:11386	 l-p:0.19644246995449066
epoch£º569	 i:7 	 global-step:11387	 l-p:0.004313926678150892
epoch£º569	 i:8 	 global-step:11388	 l-p:0.4873659908771515
epoch£º569	 i:9 	 global-step:11389	 l-p:0.19296333193778992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9200, 4.9093, 4.9188],
        [4.9200, 4.9200, 4.9200],
        [4.9200, 4.7626, 4.6431],
        [4.9200, 5.1597, 5.0487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.18442153930664062 
model_pd.l_d.mean(): -20.13555335998535 
model_pd.lagr.mean(): -19.95113182067871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5166], device='cuda:0')), ('power', tensor([-20.8833], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.18442153930664062
epoch£º570	 i:1 	 global-step:11401	 l-p:0.1695757806301117
epoch£º570	 i:2 	 global-step:11402	 l-p:0.12926891446113586
epoch£º570	 i:3 	 global-step:11403	 l-p:0.08102316409349442
epoch£º570	 i:4 	 global-step:11404	 l-p:0.1359347552061081
epoch£º570	 i:5 	 global-step:11405	 l-p:0.15760500729084015
epoch£º570	 i:6 	 global-step:11406	 l-p:0.13650862872600555
epoch£º570	 i:7 	 global-step:11407	 l-p:0.13151773810386658
epoch£º570	 i:8 	 global-step:11408	 l-p:0.08997104316949844
epoch£º570	 i:9 	 global-step:11409	 l-p:0.14679773151874542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1038, 4.9614, 4.9117],
        [5.1038, 5.1038, 5.1038],
        [5.1038, 5.0738, 5.0966],
        [5.1038, 5.0985, 5.1035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.1616247147321701 
model_pd.l_d.mean(): -19.36032485961914 
model_pd.lagr.mean(): -19.198699951171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4909], device='cuda:0')), ('power', tensor([-20.0733], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.1616247147321701
epoch£º571	 i:1 	 global-step:11421	 l-p:-0.10399843007326126
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11143767833709717
epoch£º571	 i:3 	 global-step:11423	 l-p:0.20316779613494873
epoch£º571	 i:4 	 global-step:11424	 l-p:0.07844837754964828
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12052363902330399
epoch£º571	 i:6 	 global-step:11426	 l-p:0.14156126976013184
epoch£º571	 i:7 	 global-step:11427	 l-p:0.8040727972984314
epoch£º571	 i:8 	 global-step:11428	 l-p:0.13357193768024445
epoch£º571	 i:9 	 global-step:11429	 l-p:0.11610477417707443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1822, 5.1812, 5.1821],
        [5.1822, 5.8413, 6.0139],
        [5.1822, 5.7423, 5.8372],
        [5.1822, 5.1699, 5.1806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.11957957595586777 
model_pd.l_d.mean(): -19.550006866455078 
model_pd.lagr.mean(): -19.43042755126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.2665], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.11957957595586777
epoch£º572	 i:1 	 global-step:11441	 l-p:0.07857871055603027
epoch£º572	 i:2 	 global-step:11442	 l-p:0.2402365803718567
epoch£º572	 i:3 	 global-step:11443	 l-p:0.08306081593036652
epoch£º572	 i:4 	 global-step:11444	 l-p:0.12008140236139297
epoch£º572	 i:5 	 global-step:11445	 l-p:0.12233234196901321
epoch£º572	 i:6 	 global-step:11446	 l-p:0.14018520712852478
epoch£º572	 i:7 	 global-step:11447	 l-p:0.15410903096199036
epoch£º572	 i:8 	 global-step:11448	 l-p:-0.034604139626026154
epoch£º572	 i:9 	 global-step:11449	 l-p:0.1428707242012024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 5.1002, 5.1329],
        [5.1550, 5.0233, 5.0062],
        [5.1550, 5.0797, 5.1139],
        [5.1550, 5.1547, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.13834325969219208 
model_pd.l_d.mean(): -19.58272361755371 
model_pd.lagr.mean(): -19.444379806518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4493], device='cuda:0')), ('power', tensor([-20.2557], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.13834325969219208
epoch£º573	 i:1 	 global-step:11461	 l-p:0.1368628442287445
epoch£º573	 i:2 	 global-step:11462	 l-p:0.0511125847697258
epoch£º573	 i:3 	 global-step:11463	 l-p:0.07311072945594788
epoch£º573	 i:4 	 global-step:11464	 l-p:0.10211192816495895
epoch£º573	 i:5 	 global-step:11465	 l-p:-0.033848103135824203
epoch£º573	 i:6 	 global-step:11466	 l-p:0.3046693801879883
epoch£º573	 i:7 	 global-step:11467	 l-p:0.12847131490707397
epoch£º573	 i:8 	 global-step:11468	 l-p:0.16963008046150208
epoch£º573	 i:9 	 global-step:11469	 l-p:0.10688003152608871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1127, 4.9688, 4.8613],
        [5.1127, 5.1765, 4.9809],
        [5.1127, 5.1127, 5.1127],
        [5.1127, 5.1088, 5.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.1344064623117447 
model_pd.l_d.mean(): -20.327348709106445 
model_pd.lagr.mean(): -20.192941665649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-21.0074], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.1344064623117447
epoch£º574	 i:1 	 global-step:11481	 l-p:0.13637468218803406
epoch£º574	 i:2 	 global-step:11482	 l-p:0.05852460861206055
epoch£º574	 i:3 	 global-step:11483	 l-p:0.10418201982975006
epoch£º574	 i:4 	 global-step:11484	 l-p:0.10137174278497696
epoch£º574	 i:5 	 global-step:11485	 l-p:0.1554790884256363
epoch£º574	 i:6 	 global-step:11486	 l-p:0.1402326226234436
epoch£º574	 i:7 	 global-step:11487	 l-p:-0.047293584793806076
epoch£º574	 i:8 	 global-step:11488	 l-p:0.15549488365650177
epoch£º574	 i:9 	 global-step:11489	 l-p:0.11695519089698792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0839, 5.0719, 5.0825],
        [5.0839, 4.9513, 4.9444],
        [5.0839, 5.0839, 5.0839],
        [5.0839, 5.0704, 5.0821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.12848719954490662 
model_pd.l_d.mean(): -20.362382888793945 
model_pd.lagr.mean(): -20.233896255493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-21.0870], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.12848719954490662
epoch£º575	 i:1 	 global-step:11501	 l-p:0.11956170946359634
epoch£º575	 i:2 	 global-step:11502	 l-p:0.11834582686424255
epoch£º575	 i:3 	 global-step:11503	 l-p:0.1248660758137703
epoch£º575	 i:4 	 global-step:11504	 l-p:0.01919538527727127
epoch£º575	 i:5 	 global-step:11505	 l-p:0.1304512768983841
epoch£º575	 i:6 	 global-step:11506	 l-p:0.13116693496704102
epoch£º575	 i:7 	 global-step:11507	 l-p:0.10157664865255356
epoch£º575	 i:8 	 global-step:11508	 l-p:0.12212610989809036
epoch£º575	 i:9 	 global-step:11509	 l-p:0.16704677045345306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0218, 5.0215, 5.0218],
        [5.0218, 5.0183, 5.0216],
        [5.0218, 5.4857, 5.5167],
        [5.0218, 4.9419, 4.7353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.12486527860164642 
model_pd.l_d.mean(): -20.337718963623047 
model_pd.lagr.mean(): -20.212854385375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4954], device='cuda:0')), ('power', tensor([-21.0660], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.12486527860164642
epoch£º576	 i:1 	 global-step:11521	 l-p:0.14952605962753296
epoch£º576	 i:2 	 global-step:11522	 l-p:0.14089123904705048
epoch£º576	 i:3 	 global-step:11523	 l-p:0.2001015990972519
epoch£º576	 i:4 	 global-step:11524	 l-p:0.1348084658384323
epoch£º576	 i:5 	 global-step:11525	 l-p:0.15826694667339325
epoch£º576	 i:6 	 global-step:11526	 l-p:0.1723944991827011
epoch£º576	 i:7 	 global-step:11527	 l-p:0.10773123800754547
epoch£º576	 i:8 	 global-step:11528	 l-p:0.1113617867231369
epoch£º576	 i:9 	 global-step:11529	 l-p:0.1884046196937561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0250, 4.9244, 4.9554],
        [5.0250, 5.0250, 5.0250],
        [5.0250, 5.0871, 4.8875],
        [5.0250, 5.0250, 5.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.10360978543758392 
model_pd.l_d.mean(): -20.7099552154541 
model_pd.lagr.mean(): -20.606346130371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.4038], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.10360978543758392
epoch£º577	 i:1 	 global-step:11541	 l-p:0.1872236579656601
epoch£º577	 i:2 	 global-step:11542	 l-p:0.154926598072052
epoch£º577	 i:3 	 global-step:11543	 l-p:0.12049420922994614
epoch£º577	 i:4 	 global-step:11544	 l-p:0.09330520778894424
epoch£º577	 i:5 	 global-step:11545	 l-p:0.14300458133220673
epoch£º577	 i:6 	 global-step:11546	 l-p:0.13545535504817963
epoch£º577	 i:7 	 global-step:11547	 l-p:0.13069477677345276
epoch£º577	 i:8 	 global-step:11548	 l-p:0.08966225385665894
epoch£º577	 i:9 	 global-step:11549	 l-p:0.35311582684516907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9489, 4.9489, 4.9489],
        [4.9489, 4.9722, 4.7599],
        [4.9489, 4.7950, 4.7654],
        [4.9489, 4.7945, 4.6458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.13282421231269836 
model_pd.l_d.mean(): -20.576923370361328 
model_pd.lagr.mean(): -20.44409942626953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-21.2960], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.13282421231269836
epoch£º578	 i:1 	 global-step:11561	 l-p:0.16550585627555847
epoch£º578	 i:2 	 global-step:11562	 l-p:0.09612138569355011
epoch£º578	 i:3 	 global-step:11563	 l-p:0.11953672021627426
epoch£º578	 i:4 	 global-step:11564	 l-p:0.007415938191115856
epoch£º578	 i:5 	 global-step:11565	 l-p:0.2359393686056137
epoch£º578	 i:6 	 global-step:11566	 l-p:0.25251123309135437
epoch£º578	 i:7 	 global-step:11567	 l-p:0.15335138142108917
epoch£º578	 i:8 	 global-step:11568	 l-p:0.3249586522579193
epoch£º578	 i:9 	 global-step:11569	 l-p:0.16921819746494293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9942, 5.5252, 5.6061],
        [4.9942, 4.9181, 4.9555],
        [4.9942, 4.8402, 4.7029],
        [4.9942, 4.9387, 4.9731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.14941488206386566 
model_pd.l_d.mean(): -20.482351303100586 
model_pd.lagr.mean(): -20.332937240600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4800], device='cuda:0')), ('power', tensor([-21.1964], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.14941488206386566
epoch£º579	 i:1 	 global-step:11581	 l-p:0.07372807711362839
epoch£º579	 i:2 	 global-step:11582	 l-p:0.1490923911333084
epoch£º579	 i:3 	 global-step:11583	 l-p:0.10329612344503403
epoch£º579	 i:4 	 global-step:11584	 l-p:0.12086565047502518
epoch£º579	 i:5 	 global-step:11585	 l-p:0.1562061905860901
epoch£º579	 i:6 	 global-step:11586	 l-p:0.09060201793909073
epoch£º579	 i:7 	 global-step:11587	 l-p:0.11437048017978668
epoch£º579	 i:8 	 global-step:11588	 l-p:0.1532764732837677
epoch£º579	 i:9 	 global-step:11589	 l-p:0.052757006138563156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1039, 5.0835, 5.1003],
        [5.1039, 5.0981, 5.1034],
        [5.1039, 5.0423, 4.8336],
        [5.1039, 5.0779, 5.0984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.11052050441503525 
model_pd.l_d.mean(): -20.943925857543945 
model_pd.lagr.mean(): -20.833404541015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3916], device='cuda:0')), ('power', tensor([-21.5728], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.11052050441503525
epoch£º580	 i:1 	 global-step:11601	 l-p:0.14240328967571259
epoch£º580	 i:2 	 global-step:11602	 l-p:0.13059528172016144
epoch£º580	 i:3 	 global-step:11603	 l-p:0.11462141573429108
epoch£º580	 i:4 	 global-step:11604	 l-p:0.07681482285261154
epoch£º580	 i:5 	 global-step:11605	 l-p:0.19700177013874054
epoch£º580	 i:6 	 global-step:11606	 l-p:0.19157828390598297
epoch£º580	 i:7 	 global-step:11607	 l-p:0.07404685765504837
epoch£º580	 i:8 	 global-step:11608	 l-p:0.14277803897857666
epoch£º580	 i:9 	 global-step:11609	 l-p:0.12340491265058517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0550, 5.0550, 5.0550],
        [5.0550, 5.2488, 5.1041],
        [5.0550, 4.9032, 4.7754],
        [5.0550, 4.9593, 4.9927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.17044128477573395 
model_pd.l_d.mean(): -20.288028717041016 
model_pd.lagr.mean(): -20.11758804321289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4959], device='cuda:0')), ('power', tensor([-21.0163], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.17044128477573395
epoch£º581	 i:1 	 global-step:11621	 l-p:0.10335763543844223
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12477600574493408
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13233888149261475
epoch£º581	 i:4 	 global-step:11624	 l-p:0.1188305988907814
epoch£º581	 i:5 	 global-step:11625	 l-p:0.06783680617809296
epoch£º581	 i:6 	 global-step:11626	 l-p:0.08751332014799118
epoch£º581	 i:7 	 global-step:11627	 l-p:0.15783187747001648
epoch£º581	 i:8 	 global-step:11628	 l-p:0.13472214341163635
epoch£º581	 i:9 	 global-step:11629	 l-p:0.13891297578811646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0612, 5.0107, 5.0434],
        [5.0612, 5.5695, 5.6290],
        [5.0612, 5.0608, 5.0612],
        [5.0612, 5.0503, 5.0600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.14577993750572205 
model_pd.l_d.mean(): -18.50235366821289 
model_pd.lagr.mean(): -18.3565731048584 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5640], device='cuda:0')), ('power', tensor([-19.2806], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.14577993750572205
epoch£º582	 i:1 	 global-step:11641	 l-p:0.13971686363220215
epoch£º582	 i:2 	 global-step:11642	 l-p:0.12160764634609222
epoch£º582	 i:3 	 global-step:11643	 l-p:0.13552238047122955
epoch£º582	 i:4 	 global-step:11644	 l-p:0.14022530615329742
epoch£º582	 i:5 	 global-step:11645	 l-p:0.07323244214057922
epoch£º582	 i:6 	 global-step:11646	 l-p:0.09450653195381165
epoch£º582	 i:7 	 global-step:11647	 l-p:0.11849355697631836
epoch£º582	 i:8 	 global-step:11648	 l-p:0.1144479438662529
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13872955739498138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0337, 4.9993, 5.0250],
        [5.0337, 5.1310, 4.9410],
        [5.0337, 5.1527, 4.9716],
        [5.0337, 4.8904, 4.7292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.1364239603281021 
model_pd.l_d.mean(): -20.459827423095703 
model_pd.lagr.mean(): -20.32340431213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-21.1866], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.1364239603281021
epoch£º583	 i:1 	 global-step:11661	 l-p:0.14643055200576782
epoch£º583	 i:2 	 global-step:11662	 l-p:0.1357658952474594
epoch£º583	 i:3 	 global-step:11663	 l-p:0.11050481349229813
epoch£º583	 i:4 	 global-step:11664	 l-p:0.22789151966571808
epoch£º583	 i:5 	 global-step:11665	 l-p:0.18535283207893372
epoch£º583	 i:6 	 global-step:11666	 l-p:0.1004636138677597
epoch£º583	 i:7 	 global-step:11667	 l-p:0.13259223103523254
epoch£º583	 i:8 	 global-step:11668	 l-p:0.1321333348751068
epoch£º583	 i:9 	 global-step:11669	 l-p:0.26061350107192993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9704, 4.9492, 4.9667],
        [4.9704, 4.9704, 4.9704],
        [4.9704, 4.9144, 4.9495],
        [4.9704, 4.9035, 4.9410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.27463996410369873 
model_pd.l_d.mean(): -20.709583282470703 
model_pd.lagr.mean(): -20.43494415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.4197], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.27463996410369873
epoch£º584	 i:1 	 global-step:11681	 l-p:0.13156938552856445
epoch£º584	 i:2 	 global-step:11682	 l-p:0.16550809144973755
epoch£º584	 i:3 	 global-step:11683	 l-p:0.15321466326713562
epoch£º584	 i:4 	 global-step:11684	 l-p:0.11745995283126831
epoch£º584	 i:5 	 global-step:11685	 l-p:0.10393238812685013
epoch£º584	 i:6 	 global-step:11686	 l-p:0.14963361620903015
epoch£º584	 i:7 	 global-step:11687	 l-p:0.652800977230072
epoch£º584	 i:8 	 global-step:11688	 l-p:0.09537948668003082
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10901478677988052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1544, 5.0814, 5.1175],
        [5.1544, 5.1472, 5.1538],
        [5.1544, 5.1529, 5.1544],
        [5.1544, 5.1449, 5.1535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.017550434917211533 
model_pd.l_d.mean(): -20.444515228271484 
model_pd.lagr.mean(): -20.426965713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4231], device='cuda:0')), ('power', tensor([-21.1000], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.017550434917211533
epoch£º585	 i:1 	 global-step:11701	 l-p:0.1275707632303238
epoch£º585	 i:2 	 global-step:11702	 l-p:0.30285635590553284
epoch£º585	 i:3 	 global-step:11703	 l-p:0.03808482736349106
epoch£º585	 i:4 	 global-step:11704	 l-p:0.11893831193447113
epoch£º585	 i:5 	 global-step:11705	 l-p:0.12498235702514648
epoch£º585	 i:6 	 global-step:11706	 l-p:0.14217838644981384
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11658965796232224
epoch£º585	 i:8 	 global-step:11708	 l-p:0.1358933001756668
epoch£º585	 i:9 	 global-step:11709	 l-p:0.07651907950639725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1845, 5.1839, 5.1845],
        [5.1845, 5.7281, 5.8061],
        [5.1845, 5.0573, 5.0601],
        [5.1845, 5.1533, 5.1770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.1372515857219696 
model_pd.l_d.mean(): -20.21535301208496 
model_pd.lagr.mean(): -20.078102111816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4472], device='cuda:0')), ('power', tensor([-20.8931], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.1372515857219696
epoch£º586	 i:1 	 global-step:11721	 l-p:0.1473681628704071
epoch£º586	 i:2 	 global-step:11722	 l-p:0.0922178253531456
epoch£º586	 i:3 	 global-step:11723	 l-p:0.2675229609012604
epoch£º586	 i:4 	 global-step:11724	 l-p:-0.024969058111310005
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11597512662410736
epoch£º586	 i:6 	 global-step:11726	 l-p:0.0937526673078537
epoch£º586	 i:7 	 global-step:11727	 l-p:0.14390946924686432
epoch£º586	 i:8 	 global-step:11728	 l-p:0.12375477701425552
epoch£º586	 i:9 	 global-step:11729	 l-p:0.14973857998847961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0557, 4.8990, 4.7683],
        [5.0557, 5.1208, 4.9179],
        [5.0557, 5.0610, 4.8437],
        [5.0557, 5.0013, 5.0356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12915532290935516 
model_pd.l_d.mean(): -20.192832946777344 
model_pd.lagr.mean(): -20.063676834106445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5050], device='cuda:0')), ('power', tensor([-20.9293], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12915532290935516
epoch£º587	 i:1 	 global-step:11741	 l-p:0.0795864686369896
epoch£º587	 i:2 	 global-step:11742	 l-p:0.04762772470712662
epoch£º587	 i:3 	 global-step:11743	 l-p:0.13480237126350403
epoch£º587	 i:4 	 global-step:11744	 l-p:0.13766704499721527
epoch£º587	 i:5 	 global-step:11745	 l-p:0.2815300524234772
epoch£º587	 i:6 	 global-step:11746	 l-p:0.19461876153945923
epoch£º587	 i:7 	 global-step:11747	 l-p:0.2831740379333496
epoch£º587	 i:8 	 global-step:11748	 l-p:0.13128356635570526
epoch£º587	 i:9 	 global-step:11749	 l-p:-18.55889320373535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9252, 4.9252, 4.9252],
        [4.9252, 4.8378, 4.8773],
        [4.9252, 4.7544, 4.6955],
        [4.9252, 4.8502, 4.8897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.14329794049263 
model_pd.l_d.mean(): -20.5058536529541 
model_pd.lagr.mean(): -20.36255645751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-21.2519], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.14329794049263
epoch£º588	 i:1 	 global-step:11761	 l-p:0.12385649234056473
epoch£º588	 i:2 	 global-step:11762	 l-p:0.12147695571184158
epoch£º588	 i:3 	 global-step:11763	 l-p:0.12511643767356873
epoch£º588	 i:4 	 global-step:11764	 l-p:0.07639464735984802
epoch£º588	 i:5 	 global-step:11765	 l-p:0.18073055148124695
epoch£º588	 i:6 	 global-step:11766	 l-p:0.008963794447481632
epoch£º588	 i:7 	 global-step:11767	 l-p:0.06448101997375488
epoch£º588	 i:8 	 global-step:11768	 l-p:0.15281255543231964
epoch£º588	 i:9 	 global-step:11769	 l-p:0.02600594237446785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8682, 4.8532, 4.8663],
        [4.8682, 4.8674, 4.8682],
        [4.8682, 4.8236, 4.8550],
        [4.8682, 4.8682, 4.8682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.0548604279756546 
model_pd.l_d.mean(): -20.266429901123047 
model_pd.lagr.mean(): -20.21156883239746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5575], device='cuda:0')), ('power', tensor([-21.0573], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.0548604279756546
epoch£º589	 i:1 	 global-step:11781	 l-p:0.13308316469192505
epoch£º589	 i:2 	 global-step:11782	 l-p:0.17430400848388672
epoch£º589	 i:3 	 global-step:11783	 l-p:0.11317861080169678
epoch£º589	 i:4 	 global-step:11784	 l-p:0.011844415217638016
epoch£º589	 i:5 	 global-step:11785	 l-p:0.21663230657577515
epoch£º589	 i:6 	 global-step:11786	 l-p:0.16246062517166138
epoch£º589	 i:7 	 global-step:11787	 l-p:0.1414073407649994
epoch£º589	 i:8 	 global-step:11788	 l-p:0.15105310082435608
epoch£º589	 i:9 	 global-step:11789	 l-p:0.13064056634902954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0503, 5.0163, 5.0419],
        [5.0503, 5.0118, 4.7891],
        [5.0503, 4.9940, 5.0290],
        [5.0503, 5.0405, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.16266191005706787 
model_pd.l_d.mean(): -20.36211395263672 
model_pd.lagr.mean(): -20.199451446533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4865], device='cuda:0')), ('power', tensor([-21.0815], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.16266191005706787
epoch£º590	 i:1 	 global-step:11801	 l-p:0.13995859026908875
epoch£º590	 i:2 	 global-step:11802	 l-p:0.12804146111011505
epoch£º590	 i:3 	 global-step:11803	 l-p:0.13611827790737152
epoch£º590	 i:4 	 global-step:11804	 l-p:0.147040456533432
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13421794772148132
epoch£º590	 i:6 	 global-step:11806	 l-p:0.1291085183620453
epoch£º590	 i:7 	 global-step:11807	 l-p:0.0886097326874733
epoch£º590	 i:8 	 global-step:11808	 l-p:0.00779504282400012
epoch£º590	 i:9 	 global-step:11809	 l-p:0.13828971982002258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0686, 5.1470, 4.9467],
        [5.0686, 4.9090, 4.7810],
        [5.0686, 5.0685, 5.0686],
        [5.0686, 5.0683, 5.0686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12405505776405334 
model_pd.l_d.mean(): -18.838855743408203 
model_pd.lagr.mean(): -18.714799880981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5285], device='cuda:0')), ('power', tensor([-19.5846], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12405505776405334
epoch£º591	 i:1 	 global-step:11821	 l-p:0.16800491511821747
epoch£º591	 i:2 	 global-step:11822	 l-p:0.11955337226390839
epoch£º591	 i:3 	 global-step:11823	 l-p:0.10960262268781662
epoch£º591	 i:4 	 global-step:11824	 l-p:0.02676951326429844
epoch£º591	 i:5 	 global-step:11825	 l-p:0.10420668125152588
epoch£º591	 i:6 	 global-step:11826	 l-p:0.09782818704843521
epoch£º591	 i:7 	 global-step:11827	 l-p:0.16860678791999817
epoch£º591	 i:8 	 global-step:11828	 l-p:0.08779899030923843
epoch£º591	 i:9 	 global-step:11829	 l-p:0.1300736367702484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.1570, 5.0069, 4.9717],
        [5.1570, 5.0068, 4.9714],
        [5.1570, 5.7749, 5.9091],
        [5.1570, 5.0014, 4.9394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12471427023410797 
model_pd.l_d.mean(): -20.14776611328125 
model_pd.lagr.mean(): -20.023052215576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12471427023410797
epoch£º592	 i:1 	 global-step:11841	 l-p:0.02472936548292637
epoch£º592	 i:2 	 global-step:11842	 l-p:0.18950599431991577
epoch£º592	 i:3 	 global-step:11843	 l-p:0.13364040851593018
epoch£º592	 i:4 	 global-step:11844	 l-p:0.34055057168006897
epoch£º592	 i:5 	 global-step:11845	 l-p:0.11762940138578415
epoch£º592	 i:6 	 global-step:11846	 l-p:0.13327418267726898
epoch£º592	 i:7 	 global-step:11847	 l-p:0.1258685290813446
epoch£º592	 i:8 	 global-step:11848	 l-p:0.15806595981121063
epoch£º592	 i:9 	 global-step:11849	 l-p:0.11067446321249008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1344, 5.0651, 5.1024],
        [5.1344, 5.0287, 5.0591],
        [5.1344, 5.1344, 5.1344],
        [5.1344, 5.1344, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.26144182682037354 
model_pd.l_d.mean(): -20.752164840698242 
model_pd.lagr.mean(): -20.49072265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150], device='cuda:0')), ('power', tensor([-21.4028], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.26144182682037354
epoch£º593	 i:1 	 global-step:11861	 l-p:0.09891428053379059
epoch£º593	 i:2 	 global-step:11862	 l-p:0.08541398495435715
epoch£º593	 i:3 	 global-step:11863	 l-p:0.16436108946800232
epoch£º593	 i:4 	 global-step:11864	 l-p:0.13449408113956451
epoch£º593	 i:5 	 global-step:11865	 l-p:0.09237125515937805
epoch£º593	 i:6 	 global-step:11866	 l-p:0.12067924439907074
epoch£º593	 i:7 	 global-step:11867	 l-p:0.15544068813323975
epoch£º593	 i:8 	 global-step:11868	 l-p:0.1462419331073761
epoch£º593	 i:9 	 global-step:11869	 l-p:0.09834177047014236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0652, 4.9644, 4.9994],
        [5.0652, 5.0651, 5.0652],
        [5.0652, 5.0045, 5.0409],
        [5.0652, 5.3130, 5.1933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.1059294268488884 
model_pd.l_d.mean(): -20.332866668701172 
model_pd.lagr.mean(): -20.22693634033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5041], device='cuda:0')), ('power', tensor([-21.0699], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.1059294268488884
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13113607466220856
epoch£º594	 i:2 	 global-step:11882	 l-p:0.117802195250988
epoch£º594	 i:3 	 global-step:11883	 l-p:0.13446731865406036
epoch£º594	 i:4 	 global-step:11884	 l-p:0.1450851857662201
epoch£º594	 i:5 	 global-step:11885	 l-p:0.07538441568613052
epoch£º594	 i:6 	 global-step:11886	 l-p:0.1415696144104004
epoch£º594	 i:7 	 global-step:11887	 l-p:0.21511264145374298
epoch£º594	 i:8 	 global-step:11888	 l-p:0.13247887790203094
epoch£º594	 i:9 	 global-step:11889	 l-p:0.15018567442893982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0128, 5.2484, 5.1229],
        [5.0128, 4.8508, 4.8181],
        [5.0128, 5.0128, 5.0128],
        [5.0128, 5.0112, 5.0128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.15716561675071716 
model_pd.l_d.mean(): -20.637144088745117 
model_pd.lagr.mean(): -20.479978561401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4621], device='cuda:0')), ('power', tensor([-21.3346], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.15716561675071716
epoch£º595	 i:1 	 global-step:11901	 l-p:0.1448621153831482
epoch£º595	 i:2 	 global-step:11902	 l-p:0.17049837112426758
epoch£º595	 i:3 	 global-step:11903	 l-p:0.23331980407238007
epoch£º595	 i:4 	 global-step:11904	 l-p:0.1254313588142395
epoch£º595	 i:5 	 global-step:11905	 l-p:0.12799902260303497
epoch£º595	 i:6 	 global-step:11906	 l-p:0.12475727498531342
epoch£º595	 i:7 	 global-step:11907	 l-p:0.11405151337385178
epoch£º595	 i:8 	 global-step:11908	 l-p:0.11993682384490967
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11685898154973984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0103, 4.9732, 5.0007],
        [5.0103, 5.0095, 5.0103],
        [5.0103, 4.9047, 4.6856],
        [5.0103, 5.0103, 5.0103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11102834343910217 
model_pd.l_d.mean(): -20.567731857299805 
model_pd.lagr.mean(): -20.456703186035156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-21.2661], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11102834343910217
epoch£º596	 i:1 	 global-step:11921	 l-p:0.1420578807592392
epoch£º596	 i:2 	 global-step:11922	 l-p:0.149207204580307
epoch£º596	 i:3 	 global-step:11923	 l-p:0.2184908241033554
epoch£º596	 i:4 	 global-step:11924	 l-p:0.1160024031996727
epoch£º596	 i:5 	 global-step:11925	 l-p:0.20871089398860931
epoch£º596	 i:6 	 global-step:11926	 l-p:0.2389400452375412
epoch£º596	 i:7 	 global-step:11927	 l-p:0.19800493121147156
epoch£º596	 i:8 	 global-step:11928	 l-p:0.8089163899421692
epoch£º596	 i:9 	 global-step:11929	 l-p:0.12903131544589996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9927, 4.8415, 4.8384],
        [4.9927, 4.9679, 4.9880],
        [4.9927, 4.9927, 4.9927],
        [4.9927, 4.9910, 4.9926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.1373947560787201 
model_pd.l_d.mean(): -20.467554092407227 
model_pd.lagr.mean(): -20.33016014099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.1802], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.1373947560787201
epoch£º597	 i:1 	 global-step:11941	 l-p:0.13932597637176514
epoch£º597	 i:2 	 global-step:11942	 l-p:0.12710437178611755
epoch£º597	 i:3 	 global-step:11943	 l-p:0.39955615997314453
epoch£º597	 i:4 	 global-step:11944	 l-p:0.16516222059726715
epoch£º597	 i:5 	 global-step:11945	 l-p:0.2888871133327484
epoch£º597	 i:6 	 global-step:11946	 l-p:0.08239573985338211
epoch£º597	 i:7 	 global-step:11947	 l-p:0.16720706224441528
epoch£º597	 i:8 	 global-step:11948	 l-p:0.13108979165554047
epoch£º597	 i:9 	 global-step:11949	 l-p:0.14392177760601044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0432, 5.0424, 5.0432],
        [5.0432, 5.1672, 4.9815],
        [5.0432, 4.9135, 4.9335],
        [5.0432, 5.4679, 5.4619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11486316472291946 
model_pd.l_d.mean(): -19.295440673828125 
model_pd.lagr.mean(): -19.180578231811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.0525], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11486316472291946
epoch£º598	 i:1 	 global-step:11961	 l-p:0.10367219895124435
epoch£º598	 i:2 	 global-step:11962	 l-p:0.10858363658189774
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13113459944725037
epoch£º598	 i:4 	 global-step:11964	 l-p:0.11620473861694336
epoch£º598	 i:5 	 global-step:11965	 l-p:0.12383677065372467
epoch£º598	 i:6 	 global-step:11966	 l-p:0.14718088507652283
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13967323303222656
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15728837251663208
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13226062059402466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0711, 5.0711, 5.0711],
        [5.0711, 4.9364, 4.9501],
        [5.0711, 5.3173, 5.1943],
        [5.0711, 5.0674, 5.0709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.14508672058582306 
model_pd.l_d.mean(): -19.844118118286133 
model_pd.lagr.mean(): -19.699031829833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5521], device='cuda:0')), ('power', tensor([-20.6249], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.14508672058582306
epoch£º599	 i:1 	 global-step:11981	 l-p:0.16177280247211456
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12297184020280838
epoch£º599	 i:3 	 global-step:11983	 l-p:0.18891963362693787
epoch£º599	 i:4 	 global-step:11984	 l-p:0.10293904691934586
epoch£º599	 i:5 	 global-step:11985	 l-p:0.08457368612289429
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13900145888328552
epoch£º599	 i:7 	 global-step:11987	 l-p:0.1380140632390976
epoch£º599	 i:8 	 global-step:11988	 l-p:0.13591748476028442
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12576963007450104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9863, 5.0250, 4.8051],
        [4.9863, 4.9863, 4.9863],
        [4.9863, 4.9863, 4.9863],
        [4.9863, 4.9863, 4.9863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.13558289408683777 
model_pd.l_d.mean(): -19.649614334106445 
model_pd.lagr.mean(): -19.5140323638916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-20.3707], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.13558289408683777
epoch£º600	 i:1 	 global-step:12001	 l-p:0.16212783753871918
epoch£º600	 i:2 	 global-step:12002	 l-p:0.42766305804252625
epoch£º600	 i:3 	 global-step:12003	 l-p:0.10567561537027359
epoch£º600	 i:4 	 global-step:12004	 l-p:0.12617629766464233
epoch£º600	 i:5 	 global-step:12005	 l-p:0.16704009473323822
epoch£º600	 i:6 	 global-step:12006	 l-p:0.31004980206489563
epoch£º600	 i:7 	 global-step:12007	 l-p:0.12823283672332764
epoch£º600	 i:8 	 global-step:12008	 l-p:0.13517385721206665
epoch£º600	 i:9 	 global-step:12009	 l-p:0.13486821949481964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9835, 4.9375, 4.7016],
        [4.9835, 4.8066, 4.6637],
        [4.9835, 4.9630, 4.9802],
        [4.9835, 4.9828, 4.9835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.14481084048748016 
model_pd.l_d.mean(): -20.674070358276367 
model_pd.lagr.mean(): -20.529258728027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.3631], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.14481084048748016
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12944583594799042
epoch£º601	 i:2 	 global-step:12022	 l-p:0.2521634101867676
epoch£º601	 i:3 	 global-step:12023	 l-p:0.16886591911315918
epoch£º601	 i:4 	 global-step:12024	 l-p:0.1339707374572754
epoch£º601	 i:5 	 global-step:12025	 l-p:0.1243266835808754
epoch£º601	 i:6 	 global-step:12026	 l-p:0.3619646430015564
epoch£º601	 i:7 	 global-step:12027	 l-p:0.11478409171104431
epoch£º601	 i:8 	 global-step:12028	 l-p:0.12235751748085022
epoch£º601	 i:9 	 global-step:12029	 l-p:0.14474183320999146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9707, 4.9707, 4.9707],
        [4.9707, 4.9854, 4.7578],
        [4.9707, 4.9463, 4.9663],
        [4.9707, 4.9429, 4.7074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.16729111969470978 
model_pd.l_d.mean(): -20.433509826660156 
model_pd.lagr.mean(): -20.266218185424805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5023], device='cuda:0')), ('power', tensor([-21.1698], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.16729111969470978
epoch£º602	 i:1 	 global-step:12041	 l-p:0.10329139232635498
epoch£º602	 i:2 	 global-step:12042	 l-p:0.12249626964330673
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11593499779701233
epoch£º602	 i:4 	 global-step:12044	 l-p:0.14405646920204163
epoch£º602	 i:5 	 global-step:12045	 l-p:0.30416586995124817
epoch£º602	 i:6 	 global-step:12046	 l-p:0.15613198280334473
epoch£º602	 i:7 	 global-step:12047	 l-p:0.11691894382238388
epoch£º602	 i:8 	 global-step:12048	 l-p:0.1630527675151825
epoch£º602	 i:9 	 global-step:12049	 l-p:0.25958406925201416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9951, 4.8731, 4.9032],
        [4.9951, 4.9840, 4.9939],
        [4.9951, 4.9950, 4.9950],
        [4.9951, 4.8810, 4.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.14302711188793182 
model_pd.l_d.mean(): -20.797744750976562 
model_pd.lagr.mean(): -20.65471839904785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4430], device='cuda:0')), ('power', tensor([-21.4775], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.14302711188793182
epoch£º603	 i:1 	 global-step:12061	 l-p:0.255065381526947
epoch£º603	 i:2 	 global-step:12062	 l-p:0.14783743023872375
epoch£º603	 i:3 	 global-step:12063	 l-p:0.13227520883083344
epoch£º603	 i:4 	 global-step:12064	 l-p:0.11663825064897537
epoch£º603	 i:5 	 global-step:12065	 l-p:0.1855373978614807
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14718662202358246
epoch£º603	 i:7 	 global-step:12067	 l-p:0.08914075791835785
epoch£º603	 i:8 	 global-step:12068	 l-p:0.1195516437292099
epoch£º603	 i:9 	 global-step:12069	 l-p:0.1260288804769516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0479, 5.3170, 5.2072],
        [5.0479, 5.0479, 5.0479],
        [5.0479, 4.9826, 5.0210],
        [5.0479, 4.8778, 4.8286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.07138536870479584 
model_pd.l_d.mean(): -20.556610107421875 
model_pd.lagr.mean(): -20.485225677490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.2515], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.07138536870479584
epoch£º604	 i:1 	 global-step:12081	 l-p:0.17201824486255646
epoch£º604	 i:2 	 global-step:12082	 l-p:0.13013187050819397
epoch£º604	 i:3 	 global-step:12083	 l-p:0.11632126569747925
epoch£º604	 i:4 	 global-step:12084	 l-p:0.1464424878358841
epoch£º604	 i:5 	 global-step:12085	 l-p:0.1152752935886383
epoch£º604	 i:6 	 global-step:12086	 l-p:0.05539187416434288
epoch£º604	 i:7 	 global-step:12087	 l-p:0.1185152605175972
epoch£º604	 i:8 	 global-step:12088	 l-p:0.14444778859615326
epoch£º604	 i:9 	 global-step:12089	 l-p:0.12103104591369629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1440, 5.1127, 4.8848],
        [5.1440, 5.0391, 5.0732],
        [5.1440, 5.0903, 5.1249],
        [5.1440, 5.1002, 5.1309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11277793347835541 
model_pd.l_d.mean(): -20.43681526184082 
model_pd.lagr.mean(): -20.324037551879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4378], device='cuda:0')), ('power', tensor([-21.1073], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11277793347835541
epoch£º605	 i:1 	 global-step:12101	 l-p:0.1297004669904709
epoch£º605	 i:2 	 global-step:12102	 l-p:0.1407979130744934
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12912258505821228
epoch£º605	 i:4 	 global-step:12104	 l-p:0.08598601073026657
epoch£º605	 i:5 	 global-step:12105	 l-p:0.14539659023284912
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11313536763191223
epoch£º605	 i:7 	 global-step:12107	 l-p:-0.2099866420030594
epoch£º605	 i:8 	 global-step:12108	 l-p:0.1266670972108841
epoch£º605	 i:9 	 global-step:12109	 l-p:0.04723404720425606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0956, 5.0956, 5.0956],
        [5.0956, 5.3488, 5.2277],
        [5.0956, 4.9287, 4.8796],
        [5.0956, 5.0953, 5.0956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.12260918319225311 
model_pd.l_d.mean(): -20.700332641601562 
model_pd.lagr.mean(): -20.577722549438477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4297], device='cuda:0')), ('power', tensor([-21.3654], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.12260918319225311
epoch£º606	 i:1 	 global-step:12121	 l-p:0.14509770274162292
epoch£º606	 i:2 	 global-step:12122	 l-p:0.1302068531513214
epoch£º606	 i:3 	 global-step:12123	 l-p:0.09401901066303253
epoch£º606	 i:4 	 global-step:12124	 l-p:0.14850831031799316
epoch£º606	 i:5 	 global-step:12125	 l-p:0.08505400270223618
epoch£º606	 i:6 	 global-step:12126	 l-p:0.144448384642601
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14614428579807281
epoch£º606	 i:8 	 global-step:12128	 l-p:0.13526350259780884
epoch£º606	 i:9 	 global-step:12129	 l-p:0.08286222070455551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0607, 5.0514, 5.0598],
        [5.0607, 4.8862, 4.8127],
        [5.0607, 5.0593, 5.0606],
        [5.0607, 4.8958, 4.8630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14722132682800293 
model_pd.l_d.mean(): -20.48334503173828 
model_pd.lagr.mean(): -20.336124420166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-21.1867], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14722132682800293
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13018739223480225
epoch£º607	 i:2 	 global-step:12142	 l-p:0.10747078061103821
epoch£º607	 i:3 	 global-step:12143	 l-p:0.14726588129997253
epoch£º607	 i:4 	 global-step:12144	 l-p:0.15585902333259583
epoch£º607	 i:5 	 global-step:12145	 l-p:0.1135110855102539
epoch£º607	 i:6 	 global-step:12146	 l-p:0.09477388113737106
epoch£º607	 i:7 	 global-step:12147	 l-p:0.12446782737970352
epoch£º607	 i:8 	 global-step:12148	 l-p:0.13622790575027466
epoch£º607	 i:9 	 global-step:12149	 l-p:0.1773654967546463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 5.0265, 5.0279],
        [5.0280, 5.0224, 5.0276],
        [5.0280, 4.9651, 5.0032],
        [5.0280, 5.2365, 5.0914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.18223336338996887 
model_pd.l_d.mean(): -19.590023040771484 
model_pd.lagr.mean(): -19.40778923034668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5546], device='cuda:0')), ('power', tensor([-20.3706], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.18223336338996887
epoch£º608	 i:1 	 global-step:12161	 l-p:0.14563311636447906
epoch£º608	 i:2 	 global-step:12162	 l-p:0.14317819476127625
epoch£º608	 i:3 	 global-step:12163	 l-p:0.18463680148124695
epoch£º608	 i:4 	 global-step:12164	 l-p:0.07739685475826263
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11932490766048431
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13533781468868256
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11811360716819763
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15726958215236664
epoch£º608	 i:9 	 global-step:12169	 l-p:0.13505740463733673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0372, 5.0372, 5.0372],
        [5.0372, 5.0197, 5.0346],
        [5.0372, 5.2476, 5.1031],
        [5.0372, 4.9263, 4.9616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.1471490114927292 
model_pd.l_d.mean(): -19.254783630371094 
model_pd.lagr.mean(): -19.107635498046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-20.0432], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.1471490114927292
epoch£º609	 i:1 	 global-step:12181	 l-p:0.1371360421180725
epoch£º609	 i:2 	 global-step:12182	 l-p:0.15974204242229462
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13277281820774078
epoch£º609	 i:4 	 global-step:12184	 l-p:0.11549932509660721
epoch£º609	 i:5 	 global-step:12185	 l-p:0.1431914120912552
epoch£º609	 i:6 	 global-step:12186	 l-p:0.1705508530139923
epoch£º609	 i:7 	 global-step:12187	 l-p:0.1375114917755127
epoch£º609	 i:8 	 global-step:12188	 l-p:0.10308279097080231
epoch£º609	 i:9 	 global-step:12189	 l-p:0.10975082963705063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0413, 4.9304, 4.9657],
        [5.0413, 5.0409, 5.0412],
        [5.0413, 5.0412, 5.0413],
        [5.0413, 5.0072, 5.0332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08237898349761963 
model_pd.l_d.mean(): -20.41464614868164 
model_pd.lagr.mean(): -20.33226776123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-21.1224], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.08237898349761963
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14342722296714783
epoch£º610	 i:2 	 global-step:12202	 l-p:0.1290246546268463
epoch£º610	 i:3 	 global-step:12203	 l-p:0.12511621415615082
epoch£º610	 i:4 	 global-step:12204	 l-p:0.12981672585010529
epoch£º610	 i:5 	 global-step:12205	 l-p:0.13000242412090302
epoch£º610	 i:6 	 global-step:12206	 l-p:0.04889800027012825
epoch£º610	 i:7 	 global-step:12207	 l-p:0.1884423941373825
epoch£º610	 i:8 	 global-step:12208	 l-p:0.15453463792800903
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14478270709514618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0927, 4.9222, 4.8664],
        [5.0927, 5.3834, 5.2840],
        [5.0927, 5.6144, 5.6731],
        [5.0927, 5.0916, 5.0926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.09784112870693207 
model_pd.l_d.mean(): -20.592330932617188 
model_pd.lagr.mean(): -20.494489669799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4196], device='cuda:0')), ('power', tensor([-21.2460], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.09784112870693207
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11959830671548843
epoch£º611	 i:2 	 global-step:12222	 l-p:0.09246545284986496
epoch£º611	 i:3 	 global-step:12223	 l-p:0.16174891591072083
epoch£º611	 i:4 	 global-step:12224	 l-p:0.14090555906295776
epoch£º611	 i:5 	 global-step:12225	 l-p:0.11441759020090103
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1280471235513687
epoch£º611	 i:7 	 global-step:12227	 l-p:0.1299390345811844
epoch£º611	 i:8 	 global-step:12228	 l-p:-0.03415500745177269
epoch£º611	 i:9 	 global-step:12229	 l-p:0.14988064765930176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0995, 5.0549, 4.8219],
        [5.0995, 5.0994, 5.0995],
        [5.0995, 4.9371, 4.9087],
        [5.0995, 5.0986, 5.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14588160812854767 
model_pd.l_d.mean(): -19.506589889526367 
model_pd.lagr.mean(): -19.360708236694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4895], device='cuda:0')), ('power', tensor([-20.2197], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14588160812854767
epoch£º612	 i:1 	 global-step:12241	 l-p:0.130248561501503
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09885823726654053
epoch£º612	 i:3 	 global-step:12243	 l-p:0.130517840385437
epoch£º612	 i:4 	 global-step:12244	 l-p:0.11549121886491776
epoch£º612	 i:5 	 global-step:12245	 l-p:0.1358933448791504
epoch£º612	 i:6 	 global-step:12246	 l-p:0.15139104425907135
epoch£º612	 i:7 	 global-step:12247	 l-p:0.15069924294948578
epoch£º612	 i:8 	 global-step:12248	 l-p:0.19735155999660492
epoch£º612	 i:9 	 global-step:12249	 l-p:0.0690193623304367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0450, 5.0448, 5.0450],
        [5.0450, 5.0205, 5.0405],
        [5.0450, 5.0418, 5.0448],
        [5.0450, 4.9883, 5.0247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.20900994539260864 
model_pd.l_d.mean(): -20.591167449951172 
model_pd.lagr.mean(): -20.382158279418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-21.2880], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.20900994539260864
epoch£º613	 i:1 	 global-step:12261	 l-p:0.1683466136455536
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12041638791561127
epoch£º613	 i:3 	 global-step:12263	 l-p:0.05200297385454178
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11418945342302322
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12218909710645676
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14054927229881287
epoch£º613	 i:7 	 global-step:12267	 l-p:0.0884200856089592
epoch£º613	 i:8 	 global-step:12268	 l-p:0.1062721237540245
epoch£º613	 i:9 	 global-step:12269	 l-p:0.13004353642463684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0972, 4.9993, 4.7738],
        [5.0972, 4.9215, 4.8295],
        [5.0972, 5.0939, 5.0971],
        [5.0972, 5.0956, 5.0972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13598480820655823 
model_pd.l_d.mean(): -20.440046310424805 
model_pd.lagr.mean(): -20.304061889648438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.1405], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13598480820655823
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14720691740512848
epoch£º614	 i:2 	 global-step:12282	 l-p:0.010266067460179329
epoch£º614	 i:3 	 global-step:12283	 l-p:0.1273505985736847
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12801258265972137
epoch£º614	 i:5 	 global-step:12285	 l-p:0.1391202062368393
epoch£º614	 i:6 	 global-step:12286	 l-p:0.17634829878807068
epoch£º614	 i:7 	 global-step:12287	 l-p:0.10728025436401367
epoch£º614	 i:8 	 global-step:12288	 l-p:0.1493619829416275
epoch£º614	 i:9 	 global-step:12289	 l-p:0.11474389582872391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0643, 4.8852, 4.7737],
        [5.0643, 4.8855, 4.8006],
        [5.0643, 5.1090, 4.8884],
        [5.0643, 5.0643, 5.0643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.1329207867383957 
model_pd.l_d.mean(): -19.140586853027344 
model_pd.lagr.mean(): -19.007665634155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.8784], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.1329207867383957
epoch£º615	 i:1 	 global-step:12301	 l-p:0.08651236444711685
epoch£º615	 i:2 	 global-step:12302	 l-p:0.13526862859725952
epoch£º615	 i:3 	 global-step:12303	 l-p:0.10339432209730148
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13104604184627533
epoch£º615	 i:5 	 global-step:12305	 l-p:0.12994390726089478
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14693263173103333
epoch£º615	 i:7 	 global-step:12307	 l-p:0.1719566285610199
epoch£º615	 i:8 	 global-step:12308	 l-p:0.11712398380041122
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11645365506410599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0650, 5.0630, 5.0649],
        [5.0650, 5.0324, 5.0576],
        [5.0650, 5.2238, 5.0500],
        [5.0650, 4.9219, 4.7190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.1321786493062973 
model_pd.l_d.mean(): -18.741962432861328 
model_pd.lagr.mean(): -18.609783172607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5993], device='cuda:0')), ('power', tensor([-19.5589], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.1321786493062973
epoch£º616	 i:1 	 global-step:12321	 l-p:0.154080331325531
epoch£º616	 i:2 	 global-step:12322	 l-p:0.15304552018642426
epoch£º616	 i:3 	 global-step:12323	 l-p:0.1419386863708496
epoch£º616	 i:4 	 global-step:12324	 l-p:0.06508617103099823
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10542649030685425
epoch£º616	 i:6 	 global-step:12326	 l-p:0.13095924258232117
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12322259694337845
epoch£º616	 i:8 	 global-step:12328	 l-p:0.13655641674995422
epoch£º616	 i:9 	 global-step:12329	 l-p:0.1294499933719635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0591, 4.9995, 5.0369],
        [5.0591, 4.9176, 4.9316],
        [5.0591, 5.5961, 5.6667],
        [5.0591, 5.0006, 5.0376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.14479412138462067 
model_pd.l_d.mean(): -20.256437301635742 
model_pd.lagr.mean(): -20.111642837524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4785], device='cuda:0')), ('power', tensor([-20.9666], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.14479412138462067
epoch£º617	 i:1 	 global-step:12341	 l-p:0.11091042309999466
epoch£º617	 i:2 	 global-step:12342	 l-p:0.1681479662656784
epoch£º617	 i:3 	 global-step:12343	 l-p:0.11655523627996445
epoch£º617	 i:4 	 global-step:12344	 l-p:0.1300673484802246
epoch£º617	 i:5 	 global-step:12345	 l-p:0.15186455845832825
epoch£º617	 i:6 	 global-step:12346	 l-p:0.12814319133758545
epoch£º617	 i:7 	 global-step:12347	 l-p:0.2133004367351532
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11994216591119766
epoch£º617	 i:9 	 global-step:12349	 l-p:0.14733560383319855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0197, 4.8346, 4.7310],
        [5.0197, 4.9582, 4.9964],
        [5.0197, 5.5187, 5.5627],
        [5.0197, 4.8494, 4.8210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.12959782779216766 
model_pd.l_d.mean(): -20.30314064025879 
model_pd.lagr.mean(): -20.173542022705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-21.0259], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.12959782779216766
epoch£º618	 i:1 	 global-step:12361	 l-p:0.1723628044128418
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10478044301271439
epoch£º618	 i:3 	 global-step:12363	 l-p:0.28823739290237427
epoch£º618	 i:4 	 global-step:12364	 l-p:0.12423227727413177
epoch£º618	 i:5 	 global-step:12365	 l-p:0.16435843706130981
epoch£º618	 i:6 	 global-step:12366	 l-p:0.13823385536670685
epoch£º618	 i:7 	 global-step:12367	 l-p:0.16082826256752014
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08998952805995941
epoch£º618	 i:9 	 global-step:12369	 l-p:0.16445700824260712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0075, 4.9481, 4.9857],
        [5.0075, 4.8921, 4.9281],
        [5.0075, 4.8881, 4.9222],
        [5.0075, 5.0869, 4.8770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.17856642603874207 
model_pd.l_d.mean(): -20.364927291870117 
model_pd.lagr.mean(): -20.18636131286621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-21.1112], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.17856642603874207
epoch£º619	 i:1 	 global-step:12381	 l-p:0.1572536677122116
epoch£º619	 i:2 	 global-step:12382	 l-p:0.1209348812699318
epoch£º619	 i:3 	 global-step:12383	 l-p:0.09448005259037018
epoch£º619	 i:4 	 global-step:12384	 l-p:0.1813206970691681
epoch£º619	 i:5 	 global-step:12385	 l-p:0.10960980504751205
epoch£º619	 i:6 	 global-step:12386	 l-p:0.14540071785449982
epoch£º619	 i:7 	 global-step:12387	 l-p:0.11724332720041275
epoch£º619	 i:8 	 global-step:12388	 l-p:0.1270710974931717
epoch£º619	 i:9 	 global-step:12389	 l-p:0.1737423539161682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0494, 5.0267, 5.0455],
        [5.0494, 4.9093, 4.9263],
        [5.0494, 5.0494, 5.0494],
        [5.0494, 5.0127, 4.7741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.13238991796970367 
model_pd.l_d.mean(): -20.874801635742188 
model_pd.lagr.mean(): -20.742412567138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.5480], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.13238991796970367
epoch£º620	 i:1 	 global-step:12401	 l-p:0.1890915185213089
epoch£º620	 i:2 	 global-step:12402	 l-p:0.1267019510269165
epoch£º620	 i:3 	 global-step:12403	 l-p:0.10898897051811218
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12394824624061584
epoch£º620	 i:5 	 global-step:12405	 l-p:0.10344737768173218
epoch£º620	 i:6 	 global-step:12406	 l-p:0.15486033260822296
epoch£º620	 i:7 	 global-step:12407	 l-p:0.1401907056570053
epoch£º620	 i:8 	 global-step:12408	 l-p:0.1422668993473053
epoch£º620	 i:9 	 global-step:12409	 l-p:0.13009369373321533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0508, 5.0977, 4.8761],
        [5.0508, 5.0499, 5.0508],
        [5.0508, 5.0508, 5.0508],
        [5.0508, 5.0508, 5.0508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.15147185325622559 
model_pd.l_d.mean(): -20.879459381103516 
model_pd.lagr.mean(): -20.72798728942871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145], device='cuda:0')), ('power', tensor([-21.5310], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.15147185325622559
epoch£º621	 i:1 	 global-step:12421	 l-p:0.13161441683769226
epoch£º621	 i:2 	 global-step:12422	 l-p:0.12997278571128845
epoch£º621	 i:3 	 global-step:12423	 l-p:0.19292034208774567
epoch£º621	 i:4 	 global-step:12424	 l-p:0.19195161759853363
epoch£º621	 i:5 	 global-step:12425	 l-p:0.0800231546163559
epoch£º621	 i:6 	 global-step:12426	 l-p:0.11296539008617401
epoch£º621	 i:7 	 global-step:12427	 l-p:0.1275695413351059
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14855332672595978
epoch£º621	 i:9 	 global-step:12429	 l-p:0.05950732156634331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0664, 5.0664, 5.0664],
        [5.0664, 4.8916, 4.8422],
        [5.0664, 4.9590, 4.7301],
        [5.0664, 4.9147, 4.9167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.16466140747070312 
model_pd.l_d.mean(): -20.774572372436523 
model_pd.lagr.mean(): -20.60991096496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.4433], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.16466140747070312
epoch£º622	 i:1 	 global-step:12441	 l-p:0.13380178809165955
epoch£º622	 i:2 	 global-step:12442	 l-p:0.13109901547431946
epoch£º622	 i:3 	 global-step:12443	 l-p:0.1073589101433754
epoch£º622	 i:4 	 global-step:12444	 l-p:0.1221526712179184
epoch£º622	 i:5 	 global-step:12445	 l-p:0.10908723622560501
epoch£º622	 i:6 	 global-step:12446	 l-p:0.17239409685134888
epoch£º622	 i:7 	 global-step:12447	 l-p:0.14962267875671387
epoch£º622	 i:8 	 global-step:12448	 l-p:0.12844309210777283
epoch£º622	 i:9 	 global-step:12449	 l-p:0.12463975697755814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0306, 4.9967, 5.0227],
        [5.0306, 4.9171, 4.9535],
        [5.0306, 5.0242, 5.0301],
        [5.0306, 5.0305, 5.0306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.15670977532863617 
model_pd.l_d.mean(): -20.307737350463867 
model_pd.lagr.mean(): -20.15102767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.15670977532863617
epoch£º623	 i:1 	 global-step:12461	 l-p:0.13003578782081604
epoch£º623	 i:2 	 global-step:12462	 l-p:0.17772391438484192
epoch£º623	 i:3 	 global-step:12463	 l-p:0.25268056988716125
epoch£º623	 i:4 	 global-step:12464	 l-p:0.12107864022254944
epoch£º623	 i:5 	 global-step:12465	 l-p:0.14815008640289307
epoch£º623	 i:6 	 global-step:12466	 l-p:0.13070428371429443
epoch£º623	 i:7 	 global-step:12467	 l-p:0.1292794793844223
epoch£º623	 i:8 	 global-step:12468	 l-p:0.14389848709106445
epoch£º623	 i:9 	 global-step:12469	 l-p:0.12217835336923599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0096, 5.0012, 5.0089],
        [5.0096, 5.3603, 5.3004],
        [5.0096, 5.0079, 5.0095],
        [5.0096, 5.0096, 5.0096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.1948416531085968 
model_pd.l_d.mean(): -20.511455535888672 
model_pd.lagr.mean(): -20.316614151000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4800], device='cuda:0')), ('power', tensor([-21.2259], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.1948416531085968
epoch£º624	 i:1 	 global-step:12481	 l-p:0.13085795938968658
epoch£º624	 i:2 	 global-step:12482	 l-p:0.154275044798851
epoch£º624	 i:3 	 global-step:12483	 l-p:0.11543336510658264
epoch£º624	 i:4 	 global-step:12484	 l-p:0.20776595175266266
epoch£º624	 i:5 	 global-step:12485	 l-p:0.14960245788097382
epoch£º624	 i:6 	 global-step:12486	 l-p:0.1488230675458908
epoch£º624	 i:7 	 global-step:12487	 l-p:0.09731616079807281
epoch£º624	 i:8 	 global-step:12488	 l-p:0.11082912236452103
epoch£º624	 i:9 	 global-step:12489	 l-p:0.15661735832691193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0583, 5.0583, 5.0583],
        [5.0583, 5.0460, 5.0569],
        [5.0583, 5.0571, 5.0583],
        [5.0583, 4.8809, 4.7286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.14901362359523773 
model_pd.l_d.mean(): -20.329219818115234 
model_pd.lagr.mean(): -20.180206298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4865], device='cuda:0')), ('power', tensor([-21.0483], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.14901362359523773
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09747765213251114
epoch£º625	 i:2 	 global-step:12502	 l-p:0.08240041881799698
epoch£º625	 i:3 	 global-step:12503	 l-p:0.13545145094394684
epoch£º625	 i:4 	 global-step:12504	 l-p:0.14348618686199188
epoch£º625	 i:5 	 global-step:12505	 l-p:0.15237562358379364
epoch£º625	 i:6 	 global-step:12506	 l-p:0.10068734735250473
epoch£º625	 i:7 	 global-step:12507	 l-p:0.1274053305387497
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12873101234436035
epoch£º625	 i:9 	 global-step:12509	 l-p:0.13059929013252258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0887, 5.0473, 5.0773],
        [5.0887, 4.9132, 4.8598],
        [5.0887, 5.0887, 5.0887],
        [5.0887, 5.4837, 5.4497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.04712184891104698 
model_pd.l_d.mean(): -20.83528709411621 
model_pd.lagr.mean(): -20.788166046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4125], device='cuda:0')), ('power', tensor([-21.4843], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.04712184891104698
epoch£º626	 i:1 	 global-step:12521	 l-p:0.1552397459745407
epoch£º626	 i:2 	 global-step:12522	 l-p:0.12383556365966797
epoch£º626	 i:3 	 global-step:12523	 l-p:0.1367725282907486
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10627178847789764
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11286750435829163
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10197794437408447
epoch£º626	 i:7 	 global-step:12527	 l-p:0.15869902074337006
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11938752233982086
epoch£º626	 i:9 	 global-step:12529	 l-p:0.1313527524471283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1004, 5.0058, 5.0463],
        [5.1004, 4.9278, 4.7720],
        [5.1004, 5.0130, 5.0542],
        [5.1004, 5.0689, 5.0934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.11096968501806259 
model_pd.l_d.mean(): -20.75303077697754 
model_pd.lagr.mean(): -20.642061233520508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.4004], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.11096968501806259
epoch£º627	 i:1 	 global-step:12541	 l-p:0.11225619912147522
epoch£º627	 i:2 	 global-step:12542	 l-p:0.15532632172107697
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11651289463043213
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12977908551692963
epoch£º627	 i:5 	 global-step:12545	 l-p:0.13109438121318817
epoch£º627	 i:6 	 global-step:12546	 l-p:0.15789559483528137
epoch£º627	 i:7 	 global-step:12547	 l-p:0.053239498287439346
epoch£º627	 i:8 	 global-step:12548	 l-p:0.1593005508184433
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15148216485977173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0669, 4.8828, 4.7883],
        [5.0669, 5.0666, 5.0669],
        [5.0669, 5.0660, 5.0669],
        [5.0669, 5.0022, 5.0413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07470779865980148 
model_pd.l_d.mean(): -20.784801483154297 
model_pd.lagr.mean(): -20.710094451904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4257], device='cuda:0')), ('power', tensor([-21.4467], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.07470779865980148
epoch£º628	 i:1 	 global-step:12561	 l-p:0.14572444558143616
epoch£º628	 i:2 	 global-step:12562	 l-p:0.17440804839134216
epoch£º628	 i:3 	 global-step:12563	 l-p:0.16762052476406097
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10110430419445038
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14021016657352448
epoch£º628	 i:6 	 global-step:12566	 l-p:0.12927736341953278
epoch£º628	 i:7 	 global-step:12567	 l-p:0.121439129114151
epoch£º628	 i:8 	 global-step:12568	 l-p:0.15085482597351074
epoch£º628	 i:9 	 global-step:12569	 l-p:0.10721772909164429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0508, 5.0499, 5.0508],
        [5.0508, 4.8722, 4.8236],
        [5.0508, 5.0463, 5.0506],
        [5.0508, 5.2515, 5.0975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.14477235078811646 
model_pd.l_d.mean(): -21.049152374267578 
model_pd.lagr.mean(): -20.904380798339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3927], device='cuda:0')), ('power', tensor([-21.6802], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.14477235078811646
epoch£º629	 i:1 	 global-step:12581	 l-p:0.12881192564964294
epoch£º629	 i:2 	 global-step:12582	 l-p:0.13070252537727356
epoch£º629	 i:3 	 global-step:12583	 l-p:0.12758734822273254
epoch£º629	 i:4 	 global-step:12584	 l-p:0.18406543135643005
epoch£º629	 i:5 	 global-step:12585	 l-p:0.09431209415197372
epoch£º629	 i:6 	 global-step:12586	 l-p:0.10501965880393982
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12016794085502625
epoch£º629	 i:8 	 global-step:12588	 l-p:0.2095727175474167
epoch£º629	 i:9 	 global-step:12589	 l-p:0.14688974618911743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0397, 5.0397, 5.0397],
        [5.0397, 4.8599, 4.8117],
        [5.0397, 4.8515, 4.7391],
        [5.0397, 5.0397, 5.0397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.15986409783363342 
model_pd.l_d.mean(): -20.063852310180664 
model_pd.lagr.mean(): -19.903987884521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.7883], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.15986409783363342
epoch£º630	 i:1 	 global-step:12601	 l-p:0.14851318299770355
epoch£º630	 i:2 	 global-step:12602	 l-p:0.20658385753631592
epoch£º630	 i:3 	 global-step:12603	 l-p:0.10686099529266357
epoch£º630	 i:4 	 global-step:12604	 l-p:0.11041188985109329
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13925769925117493
epoch£º630	 i:6 	 global-step:12606	 l-p:0.10173920542001724
epoch£º630	 i:7 	 global-step:12607	 l-p:0.12145769596099854
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13865455985069275
epoch£º630	 i:9 	 global-step:12609	 l-p:0.12269797176122665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0499, 5.0334, 5.0476],
        [5.0499, 4.9837, 4.7405],
        [5.0499, 5.0499, 5.0499],
        [5.0499, 4.9241, 4.9554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.11859536170959473 
model_pd.l_d.mean(): -20.992446899414062 
model_pd.lagr.mean(): -20.873851776123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3980], device='cuda:0')), ('power', tensor([-21.6283], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.11859536170959473
epoch£º631	 i:1 	 global-step:12621	 l-p:0.117289699614048
epoch£º631	 i:2 	 global-step:12622	 l-p:0.1522081345319748
epoch£º631	 i:3 	 global-step:12623	 l-p:0.15733174979686737
epoch£º631	 i:4 	 global-step:12624	 l-p:0.21495641767978668
epoch£º631	 i:5 	 global-step:12625	 l-p:0.18935716152191162
epoch£º631	 i:6 	 global-step:12626	 l-p:0.09517839550971985
epoch£º631	 i:7 	 global-step:12627	 l-p:0.1261318325996399
epoch£º631	 i:8 	 global-step:12628	 l-p:0.13034649193286896
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12751685082912445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0300, 4.9756, 5.0117],
        [5.0300, 5.0132, 5.0277],
        [5.0300, 5.0248, 5.0296],
        [5.0300, 5.0259, 5.0297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.13260751962661743 
model_pd.l_d.mean(): -19.508529663085938 
model_pd.lagr.mean(): -19.37592124938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.2398], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.13260751962661743
epoch£º632	 i:1 	 global-step:12641	 l-p:0.16008682548999786
epoch£º632	 i:2 	 global-step:12642	 l-p:0.1318901926279068
epoch£º632	 i:3 	 global-step:12643	 l-p:0.15108129382133484
epoch£º632	 i:4 	 global-step:12644	 l-p:0.14622868597507477
epoch£º632	 i:5 	 global-step:12645	 l-p:0.13655062019824982
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12653867900371552
epoch£º632	 i:7 	 global-step:12647	 l-p:0.23493780195713043
epoch£º632	 i:8 	 global-step:12648	 l-p:0.12010879069566727
epoch£º632	 i:9 	 global-step:12649	 l-p:0.23191966116428375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9914, 4.9771, 4.9897],
        [4.9914, 4.7989, 4.6627],
        [4.9914, 4.8803, 4.9202],
        [4.9914, 5.5200, 5.5842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.13826949894428253 
model_pd.l_d.mean(): -19.877098083496094 
model_pd.lagr.mean(): -19.738828659057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5189], device='cuda:0')), ('power', tensor([-20.6243], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.13826949894428253
epoch£º633	 i:1 	 global-step:12661	 l-p:0.18298080563545227
epoch£º633	 i:2 	 global-step:12662	 l-p:0.1356000155210495
epoch£º633	 i:3 	 global-step:12663	 l-p:0.2189875692129135
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12747274339199066
epoch£º633	 i:5 	 global-step:12665	 l-p:0.3632650673389435
epoch£º633	 i:6 	 global-step:12666	 l-p:0.15728119015693665
epoch£º633	 i:7 	 global-step:12667	 l-p:0.2108369767665863
epoch£º633	 i:8 	 global-step:12668	 l-p:0.15599514544010162
epoch£º633	 i:9 	 global-step:12669	 l-p:0.10226962715387344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0176, 5.0176, 5.0176],
        [5.0176, 5.0138, 5.0174],
        [5.0176, 4.9156, 4.9573],
        [5.0176, 4.9253, 4.9681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.1267111599445343 
model_pd.l_d.mean(): -20.085432052612305 
model_pd.lagr.mean(): -19.958721160888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5101], device='cuda:0')), ('power', tensor([-20.8259], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.1267111599445343
epoch£º634	 i:1 	 global-step:12681	 l-p:0.20719173550605774
epoch£º634	 i:2 	 global-step:12682	 l-p:0.18105405569076538
epoch£º634	 i:3 	 global-step:12683	 l-p:0.1694200038909912
epoch£º634	 i:4 	 global-step:12684	 l-p:0.06541255861520767
epoch£º634	 i:5 	 global-step:12685	 l-p:0.1511346846818924
epoch£º634	 i:6 	 global-step:12686	 l-p:0.13807964324951172
epoch£º634	 i:7 	 global-step:12687	 l-p:0.07525206357240677
epoch£º634	 i:8 	 global-step:12688	 l-p:0.11367245763540268
epoch£º634	 i:9 	 global-step:12689	 l-p:0.1414310336112976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1014, 4.9578, 4.9720],
        [5.1014, 4.9672, 4.9906],
        [5.1014, 5.1005, 5.1013],
        [5.1014, 4.9727, 4.7514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.1437266618013382 
model_pd.l_d.mean(): -20.62105941772461 
model_pd.lagr.mean(): -20.477333068847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.3071], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.1437266618013382
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13404305279254913
epoch£º635	 i:2 	 global-step:12702	 l-p:0.11419841647148132
epoch£º635	 i:3 	 global-step:12703	 l-p:0.14162702858448029
epoch£º635	 i:4 	 global-step:12704	 l-p:0.06513457000255585
epoch£º635	 i:5 	 global-step:12705	 l-p:0.104815773665905
epoch£º635	 i:6 	 global-step:12706	 l-p:0.14647383987903595
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12394336611032486
epoch£º635	 i:8 	 global-step:12708	 l-p:0.11108700186014175
epoch£º635	 i:9 	 global-step:12709	 l-p:1.5929186344146729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1409, 5.0820, 5.1192],
        [5.1409, 5.1236, 5.1384],
        [5.1409, 5.0911, 4.8521],
        [5.1409, 5.1408, 5.1409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11585662513971329 
model_pd.l_d.mean(): -20.498802185058594 
model_pd.lagr.mean(): -20.382946014404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4219], device='cuda:0')), ('power', tensor([-21.1537], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11585662513971329
epoch£º636	 i:1 	 global-step:12721	 l-p:0.12172924727201462
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10388586670160294
epoch£º636	 i:3 	 global-step:12723	 l-p:0.12183206528425217
epoch£º636	 i:4 	 global-step:12724	 l-p:0.06186947226524353
epoch£º636	 i:5 	 global-step:12725	 l-p:0.07192011177539825
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13349248468875885
epoch£º636	 i:7 	 global-step:12727	 l-p:-0.013576745986938477
epoch£º636	 i:8 	 global-step:12728	 l-p:0.18172359466552734
epoch£º636	 i:9 	 global-step:12729	 l-p:0.1599632203578949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1152, 4.9457, 4.9158],
        [5.1152, 5.6828, 5.7703],
        [5.1152, 4.9898, 5.0194],
        [5.1152, 5.0986, 5.1129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15670986473560333 
model_pd.l_d.mean(): -19.469078063964844 
model_pd.lagr.mean(): -19.312368392944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-20.1777], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15670986473560333
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12647931277751923
epoch£º637	 i:2 	 global-step:12742	 l-p:0.09879393130540848
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14443230628967285
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11787841469049454
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12480820715427399
epoch£º637	 i:6 	 global-step:12746	 l-p:0.004516639746725559
epoch£º637	 i:7 	 global-step:12747	 l-p:0.12269658595323563
epoch£º637	 i:8 	 global-step:12748	 l-p:0.17008572816848755
epoch£º637	 i:9 	 global-step:12749	 l-p:0.09768945723772049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0906, 5.0904, 5.0906],
        [5.0906, 5.0906, 5.0906],
        [5.0906, 5.2979, 5.1452],
        [5.0906, 5.5015, 5.4764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.14117242395877838 
model_pd.l_d.mean(): -20.42384910583496 
model_pd.lagr.mean(): -20.282676696777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5029], device='cuda:0')), ('power', tensor([-21.1607], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.14117242395877838
epoch£º638	 i:1 	 global-step:12761	 l-p:0.08924701809883118
epoch£º638	 i:2 	 global-step:12762	 l-p:0.05587991699576378
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12287916243076324
epoch£º638	 i:4 	 global-step:12764	 l-p:0.14299102127552032
epoch£º638	 i:5 	 global-step:12765	 l-p:0.11026166379451752
epoch£º638	 i:6 	 global-step:12766	 l-p:0.1605433225631714
epoch£º638	 i:7 	 global-step:12767	 l-p:0.1484743058681488
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14508947730064392
epoch£º638	 i:9 	 global-step:12769	 l-p:0.1319778710603714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0840, 5.5274, 5.5251],
        [5.0840, 5.0802, 5.0838],
        [5.0840, 4.9019, 4.8402],
        [5.0840, 4.9089, 4.8719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.10442224144935608 
model_pd.l_d.mean(): -18.349206924438477 
model_pd.lagr.mean(): -18.24478530883789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6339], device='cuda:0')), ('power', tensor([-19.1972], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.10442224144935608
epoch£º639	 i:1 	 global-step:12781	 l-p:0.18576696515083313
epoch£º639	 i:2 	 global-step:12782	 l-p:0.12993603944778442
epoch£º639	 i:3 	 global-step:12783	 l-p:0.08935155719518661
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13958200812339783
epoch£º639	 i:5 	 global-step:12785	 l-p:0.07024036347866058
epoch£º639	 i:6 	 global-step:12786	 l-p:0.14023111760616302
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14032508432865143
epoch£º639	 i:8 	 global-step:12788	 l-p:-1.7679990530014038
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13612782955169678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1397, 4.9643, 4.9138],
        [5.1397, 4.9606, 4.8245],
        [5.1397, 5.1397, 5.1397],
        [5.1397, 5.1223, 5.1373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.09754274785518646 
model_pd.l_d.mean(): -20.042909622192383 
model_pd.lagr.mean(): -19.94536781311035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5199], device='cuda:0')), ('power', tensor([-20.7930], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.09754274785518646
epoch£º640	 i:1 	 global-step:12801	 l-p:0.14062495529651642
epoch£º640	 i:2 	 global-step:12802	 l-p:-0.5569038391113281
epoch£º640	 i:3 	 global-step:12803	 l-p:0.0891757532954216
epoch£º640	 i:4 	 global-step:12804	 l-p:0.16568566858768463
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11784116178750992
epoch£º640	 i:6 	 global-step:12806	 l-p:0.1254294216632843
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12064201384782791
epoch£º640	 i:8 	 global-step:12808	 l-p:0.12604880332946777
epoch£º640	 i:9 	 global-step:12809	 l-p:0.13788814842700958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1333, 5.1346],
        [5.1347, 5.1006, 4.8600],
        [5.1347, 5.1346, 5.1347],
        [5.1347, 5.0753, 5.1129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.002509164856746793 
model_pd.l_d.mean(): -20.634965896606445 
model_pd.lagr.mean(): -20.632455825805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4449], device='cuda:0')), ('power', tensor([-21.3149], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.002509164856746793
epoch£º641	 i:1 	 global-step:12821	 l-p:0.09025275707244873
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10601487755775452
epoch£º641	 i:3 	 global-step:12823	 l-p:0.10782511532306671
epoch£º641	 i:4 	 global-step:12824	 l-p:0.14924241602420807
epoch£º641	 i:5 	 global-step:12825	 l-p:0.1749284863471985
epoch£º641	 i:6 	 global-step:12826	 l-p:0.1335071474313736
epoch£º641	 i:7 	 global-step:12827	 l-p:-0.07358810305595398
epoch£º641	 i:8 	 global-step:12828	 l-p:0.13266439735889435
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13549819588661194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1055, 5.1053, 5.1055],
        [5.1055, 4.9307, 4.8934],
        [5.1055, 5.1055, 5.1055],
        [5.1055, 5.5774, 5.5937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): -0.0023895406629890203 
model_pd.l_d.mean(): -19.196395874023438 
model_pd.lagr.mean(): -19.19878578186035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-19.9842], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:-0.0023895406629890203
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15755698084831238
epoch£º642	 i:2 	 global-step:12842	 l-p:0.18576262891292572
epoch£º642	 i:3 	 global-step:12843	 l-p:0.13054107129573822
epoch£º642	 i:4 	 global-step:12844	 l-p:0.0960959792137146
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14625512063503265
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12911111116409302
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11421361565589905
epoch£º642	 i:8 	 global-step:12848	 l-p:0.0976022481918335
epoch£º642	 i:9 	 global-step:12849	 l-p:0.18483081459999084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0406, 4.9741, 5.0144],
        [5.0406, 5.0406, 5.0406],
        [5.0406, 4.9108, 4.6777],
        [5.0406, 5.0406, 5.0406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.10371182858943939 
model_pd.l_d.mean(): -19.042821884155273 
model_pd.lagr.mean(): -18.939109802246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-19.8253], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.10371182858943939
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12236902117729187
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13254956901073456
epoch£º643	 i:3 	 global-step:12863	 l-p:0.10965774208307266
epoch£º643	 i:4 	 global-step:12864	 l-p:0.23618339002132416
epoch£º643	 i:5 	 global-step:12865	 l-p:0.10312007367610931
epoch£º643	 i:6 	 global-step:12866	 l-p:0.18481966853141785
epoch£º643	 i:7 	 global-step:12867	 l-p:0.12350860238075256
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13752275705337524
epoch£º643	 i:9 	 global-step:12869	 l-p:0.15693140029907227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0611, 4.8702, 4.7772],
        [5.0611, 4.9340, 4.7013],
        [5.0611, 4.8811, 4.8407],
        [5.0611, 4.9628, 5.0052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.150175541639328 
model_pd.l_d.mean(): -20.28207015991211 
model_pd.lagr.mean(): -20.131895065307617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5000], device='cuda:0')), ('power', tensor([-21.0144], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.150175541639328
epoch£º644	 i:1 	 global-step:12881	 l-p:0.181234672665596
epoch£º644	 i:2 	 global-step:12882	 l-p:0.14590461552143097
epoch£º644	 i:3 	 global-step:12883	 l-p:0.1277916431427002
epoch£º644	 i:4 	 global-step:12884	 l-p:0.15943902730941772
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14702746272087097
epoch£º644	 i:6 	 global-step:12886	 l-p:0.1330685168504715
epoch£º644	 i:7 	 global-step:12887	 l-p:0.03459616377949715
epoch£º644	 i:8 	 global-step:12888	 l-p:0.06841056793928146
epoch£º644	 i:9 	 global-step:12889	 l-p:0.0642169788479805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1179, 5.0606, 5.0977],
        [5.1179, 4.9528, 4.9387],
        [5.1179, 5.0949, 4.8529],
        [5.1179, 5.1147, 5.1178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14499813318252563 
model_pd.l_d.mean(): -20.062406539916992 
model_pd.lagr.mean(): -19.917407989501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.7909], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14499813318252563
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.06904245913028717
epoch£º645	 i:2 	 global-step:12902	 l-p:0.1172625720500946
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11236188560724258
epoch£º645	 i:4 	 global-step:12904	 l-p:0.1222296878695488
epoch£º645	 i:5 	 global-step:12905	 l-p:0.021430648863315582
epoch£º645	 i:6 	 global-step:12906	 l-p:0.14219510555267334
epoch£º645	 i:7 	 global-step:12907	 l-p:0.08170875161886215
epoch£º645	 i:8 	 global-step:12908	 l-p:0.1009952649474144
epoch£º645	 i:9 	 global-step:12909	 l-p:0.1257796287536621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1860, 5.1153, 4.8767],
        [5.1860, 5.6790, 5.7060],
        [5.1860, 5.7192, 5.7757],
        [5.1860, 5.0179, 4.9873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10202573239803314 
model_pd.l_d.mean(): -19.396068572998047 
model_pd.lagr.mean(): -19.294042587280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-20.1442], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10202573239803314
epoch£º646	 i:1 	 global-step:12921	 l-p:0.1588420271873474
epoch£º646	 i:2 	 global-step:12922	 l-p:0.1245412677526474
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13545282185077667
epoch£º646	 i:4 	 global-step:12924	 l-p:-2.467273712158203
epoch£º646	 i:5 	 global-step:12925	 l-p:0.04497876018285751
epoch£º646	 i:6 	 global-step:12926	 l-p:0.1205853596329689
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12392318248748779
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13420015573501587
epoch£º646	 i:9 	 global-step:12929	 l-p:0.21045638620853424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1627, 5.1627, 5.1627],
        [5.1627, 5.1624, 5.1627],
        [5.1627, 5.0456, 4.8182],
        [5.1627, 5.1624, 5.1627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.13603191077709198 
model_pd.l_d.mean(): -20.068944931030273 
model_pd.lagr.mean(): -19.932912826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5027], device='cuda:0')), ('power', tensor([-20.8017], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.13603191077709198
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1310126632452011
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12391073256731033
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12063605338335037
epoch£º647	 i:4 	 global-step:12944	 l-p:0.11030654609203339
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12839467823505402
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.016277918592095375
epoch£º647	 i:7 	 global-step:12947	 l-p:0.09911685436964035
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14723621308803558
epoch£º647	 i:9 	 global-step:12949	 l-p:0.0919002816081047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0835, 5.0566, 4.8117],
        [5.0835, 5.0834, 5.0835],
        [5.0835, 5.0459, 5.0742],
        [5.0835, 5.0789, 5.0832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.11858975142240524 
model_pd.l_d.mean(): -20.786090850830078 
model_pd.lagr.mean(): -20.66750144958496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.4600], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.11858975142240524
epoch£º648	 i:1 	 global-step:12961	 l-p:0.12751732766628265
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17384856939315796
epoch£º648	 i:3 	 global-step:12963	 l-p:0.14364516735076904
epoch£º648	 i:4 	 global-step:12964	 l-p:0.181523859500885
epoch£º648	 i:5 	 global-step:12965	 l-p:0.1467040479183197
epoch£º648	 i:6 	 global-step:12966	 l-p:0.1320662796497345
epoch£º648	 i:7 	 global-step:12967	 l-p:0.10095218569040298
epoch£º648	 i:8 	 global-step:12968	 l-p:0.12065329402685165
epoch£º648	 i:9 	 global-step:12969	 l-p:0.0976300910115242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0691, 5.0321, 5.0601],
        [5.0691, 5.0661, 5.0689],
        [5.0691, 5.0668, 5.0690],
        [5.0691, 4.8881, 4.8477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12279658019542694 
model_pd.l_d.mean(): -20.08824348449707 
model_pd.lagr.mean(): -19.96544647216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-20.7940], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12279658019542694
epoch£º649	 i:1 	 global-step:12981	 l-p:0.12184790521860123
epoch£º649	 i:2 	 global-step:12982	 l-p:0.1302752047777176
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15159554779529572
epoch£º649	 i:4 	 global-step:12984	 l-p:0.18571005761623383
epoch£º649	 i:5 	 global-step:12985	 l-p:0.1340857595205307
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13312779366970062
epoch£º649	 i:7 	 global-step:12987	 l-p:0.154669389128685
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06426285952329636
epoch£º649	 i:9 	 global-step:12989	 l-p:0.22330865263938904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0356, 4.9417, 4.9853],
        [5.0356, 4.9876, 5.0214],
        [5.0356, 4.8709, 4.6599],
        [5.0356, 4.8859, 4.9030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.1087624728679657 
model_pd.l_d.mean(): -20.731313705444336 
model_pd.lagr.mean(): -20.62255096435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4537], device='cuda:0')), ('power', tensor([-21.4212], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.1087624728679657
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11307691037654877
epoch£º650	 i:2 	 global-step:13002	 l-p:0.1674635261297226
epoch£º650	 i:3 	 global-step:13003	 l-p:0.0674968734383583
epoch£º650	 i:4 	 global-step:13004	 l-p:0.14003071188926697
epoch£º650	 i:5 	 global-step:13005	 l-p:0.15478982031345367
epoch£º650	 i:6 	 global-step:13006	 l-p:0.1545267403125763
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15468184649944305
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14936010539531708
epoch£º650	 i:9 	 global-step:13009	 l-p:0.15549059212207794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0693, 5.0693, 5.0693],
        [5.0693, 5.5035, 5.4930],
        [5.0693, 5.0693, 5.0693],
        [5.0693, 4.8996, 4.8866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.15985798835754395 
model_pd.l_d.mean(): -20.30424690246582 
model_pd.lagr.mean(): -20.14438819885254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-21.0342], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.15985798835754395
epoch£º651	 i:1 	 global-step:13021	 l-p:0.11916244029998779
epoch£º651	 i:2 	 global-step:13022	 l-p:0.08599022030830383
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11855289340019226
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12870794534683228
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13693663477897644
epoch£º651	 i:6 	 global-step:13026	 l-p:0.1617014855146408
epoch£º651	 i:7 	 global-step:13027	 l-p:0.1844935566186905
epoch£º651	 i:8 	 global-step:13028	 l-p:0.24279525876045227
epoch£º651	 i:9 	 global-step:13029	 l-p:0.10002733021974564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0349, 4.9228, 4.9637],
        [5.0349, 5.0231, 5.0336],
        [5.0349, 5.0349, 5.0349],
        [5.0349, 5.0335, 5.0348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.10654372721910477 
model_pd.l_d.mean(): -20.56478500366211 
model_pd.lagr.mean(): -20.458240509033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.2514], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.10654372721910477
epoch£º652	 i:1 	 global-step:13041	 l-p:0.1604756861925125
epoch£º652	 i:2 	 global-step:13042	 l-p:0.12814582884311676
epoch£º652	 i:3 	 global-step:13043	 l-p:0.15763890743255615
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08264684677124023
epoch£º652	 i:5 	 global-step:13045	 l-p:0.1830449253320694
epoch£º652	 i:6 	 global-step:13046	 l-p:0.15352144837379456
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1591208428144455
epoch£º652	 i:8 	 global-step:13048	 l-p:0.1265181452035904
epoch£º652	 i:9 	 global-step:13049	 l-p:0.10347521305084229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1131, 5.2390, 5.0415],
        [5.1131, 5.0145, 4.7722],
        [5.1131, 5.2467, 5.0529],
        [5.1131, 5.0467, 5.0868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.1685214638710022 
model_pd.l_d.mean(): -20.18401336669922 
model_pd.lagr.mean(): -20.015491485595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4785], device='cuda:0')), ('power', tensor([-20.8933], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.1685214638710022
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11974883824586868
epoch£º653	 i:2 	 global-step:13062	 l-p:0.1483418196439743
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12547700107097626
epoch£º653	 i:4 	 global-step:13064	 l-p:0.14139999449253082
epoch£º653	 i:5 	 global-step:13065	 l-p:0.025654105469584465
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08566144108772278
epoch£º653	 i:7 	 global-step:13067	 l-p:0.1312764286994934
epoch£º653	 i:8 	 global-step:13068	 l-p:0.11872775852680206
epoch£º653	 i:9 	 global-step:13069	 l-p:0.07001840323209763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1338, 5.2435, 5.0384],
        [5.1338, 4.9779, 4.9817],
        [5.1338, 5.1338, 5.1338],
        [5.1338, 4.9605, 4.9328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12429595738649368 
model_pd.l_d.mean(): -19.663799285888672 
model_pd.lagr.mean(): -19.53950309753418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.3796], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12429595738649368
epoch£º654	 i:1 	 global-step:13081	 l-p:0.058370333164930344
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14233675599098206
epoch£º654	 i:3 	 global-step:13083	 l-p:0.08731140941381454
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13818934559822083
epoch£º654	 i:5 	 global-step:13085	 l-p:0.07533690333366394
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12891250848770142
epoch£º654	 i:7 	 global-step:13087	 l-p:0.11347993463277817
epoch£º654	 i:8 	 global-step:13088	 l-p:0.32870352268218994
epoch£º654	 i:9 	 global-step:13089	 l-p:0.1373753696680069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1572, 4.9732, 4.8335],
        [5.1572, 5.0011, 4.7983],
        [5.1572, 4.9804, 4.9395],
        [5.1572, 5.0308, 5.0618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11652203649282455 
model_pd.l_d.mean(): -20.546104431152344 
model_pd.lagr.mean(): -20.429582595825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.2126], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11652203649282455
epoch£º655	 i:1 	 global-step:13101	 l-p:0.06618880480527878
epoch£º655	 i:2 	 global-step:13102	 l-p:0.1436295360326767
epoch£º655	 i:3 	 global-step:13103	 l-p:0.17772479355335236
epoch£º655	 i:4 	 global-step:13104	 l-p:0.053739603608846664
epoch£º655	 i:5 	 global-step:13105	 l-p:0.1301644891500473
epoch£º655	 i:6 	 global-step:13106	 l-p:0.13114720582962036
epoch£º655	 i:7 	 global-step:13107	 l-p:0.10692361742258072
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.07416467368602753
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12362935394048691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1170, 5.0966, 5.1138],
        [5.1170, 5.0845, 5.1099],
        [5.1170, 5.1023, 5.1152],
        [5.1170, 5.0074, 4.7672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.1183081865310669 
model_pd.l_d.mean(): -20.599571228027344 
model_pd.lagr.mean(): -20.48126220703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4338], device='cuda:0')), ('power', tensor([-21.2678], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.1183081865310669
epoch£º656	 i:1 	 global-step:13121	 l-p:0.1309398114681244
epoch£º656	 i:2 	 global-step:13122	 l-p:0.056906554847955704
epoch£º656	 i:3 	 global-step:13123	 l-p:0.13639864325523376
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13997355103492737
epoch£º656	 i:5 	 global-step:13125	 l-p:0.22412574291229248
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14742356538772583
epoch£º656	 i:7 	 global-step:13127	 l-p:0.06222250685095787
epoch£º656	 i:8 	 global-step:13128	 l-p:0.14440114796161652
epoch£º656	 i:9 	 global-step:13129	 l-p:0.12170696258544922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0783, 4.8998, 4.8713],
        [5.0783, 4.8834, 4.7603],
        [5.0783, 5.0609, 4.8142],
        [5.0783, 4.9781, 4.7313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.1418779492378235 
model_pd.l_d.mean(): -20.44146156311035 
model_pd.lagr.mean(): -20.299583435058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.1565], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.1418779492378235
epoch£º657	 i:1 	 global-step:13141	 l-p:0.09267596155405045
epoch£º657	 i:2 	 global-step:13142	 l-p:0.17470458149909973
epoch£º657	 i:3 	 global-step:13143	 l-p:0.14308756589889526
epoch£º657	 i:4 	 global-step:13144	 l-p:0.120188407599926
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10204286128282547
epoch£º657	 i:6 	 global-step:13146	 l-p:0.13954906165599823
epoch£º657	 i:7 	 global-step:13147	 l-p:0.12528173625469208
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1436973661184311
epoch£º657	 i:9 	 global-step:13149	 l-p:0.026389995589852333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.1144, 5.1144],
        [5.1144, 4.9262, 4.7777],
        [5.1144, 4.9290, 4.8728],
        [5.1144, 4.9251, 4.8497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07764184474945068 
model_pd.l_d.mean(): -20.226295471191406 
model_pd.lagr.mean(): -20.148653030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4835], device='cuda:0')), ('power', tensor([-20.9412], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.07764184474945068
epoch£º658	 i:1 	 global-step:13161	 l-p:0.1503596007823944
epoch£º658	 i:2 	 global-step:13162	 l-p:0.1638336479663849
epoch£º658	 i:3 	 global-step:13163	 l-p:0.1059141680598259
epoch£º658	 i:4 	 global-step:13164	 l-p:0.139463409781456
epoch£º658	 i:5 	 global-step:13165	 l-p:0.10842844098806381
epoch£º658	 i:6 	 global-step:13166	 l-p:0.0030115197878330946
epoch£º658	 i:7 	 global-step:13167	 l-p:0.127725750207901
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14896038174629211
epoch£º658	 i:9 	 global-step:13169	 l-p:0.12413976341485977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1223, 5.0971, 4.8507],
        [5.1223, 5.1220, 5.1223],
        [5.1223, 4.9307, 4.8148],
        [5.1223, 5.1214, 5.1223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12200427800416946 
model_pd.l_d.mean(): -20.48087501525879 
model_pd.lagr.mean(): -20.358871459960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4219], device='cuda:0')), ('power', tensor([-21.1356], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12200427800416946
epoch£º659	 i:1 	 global-step:13181	 l-p:0.10975532233715057
epoch£º659	 i:2 	 global-step:13182	 l-p:0.1616053730249405
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13279373943805695
epoch£º659	 i:4 	 global-step:13184	 l-p:0.11759401857852936
epoch£º659	 i:5 	 global-step:13185	 l-p:0.13168707489967346
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10838176310062408
epoch£º659	 i:7 	 global-step:13187	 l-p:0.12710680067539215
epoch£º659	 i:8 	 global-step:13188	 l-p:0.19437463581562042
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13242977857589722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0434, 5.2218, 5.0503],
        [5.0434, 4.9243, 4.9637],
        [5.0434, 5.0205, 4.7704],
        [5.0434, 5.0433, 5.0434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14440147578716278 
model_pd.l_d.mean(): -19.6840763092041 
model_pd.lagr.mean(): -19.539674758911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5657], device='cuda:0')), ('power', tensor([-20.4770], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14440147578716278
epoch£º660	 i:1 	 global-step:13201	 l-p:0.16013021767139435
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13757522404193878
epoch£º660	 i:3 	 global-step:13203	 l-p:0.12656697630882263
epoch£º660	 i:4 	 global-step:13204	 l-p:0.16523955762386322
epoch£º660	 i:5 	 global-step:13205	 l-p:0.1310470551252365
epoch£º660	 i:6 	 global-step:13206	 l-p:0.1141480803489685
epoch£º660	 i:7 	 global-step:13207	 l-p:0.12418802827596664
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12476735562086105
epoch£º660	 i:9 	 global-step:13209	 l-p:0.2082158923149109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0511, 5.0502, 5.0511],
        [5.0511, 4.9499, 4.9936],
        [5.0511, 5.0242, 5.0461],
        [5.0511, 4.9045, 4.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.1377342790365219 
model_pd.l_d.mean(): -19.972518920898438 
model_pd.lagr.mean(): -19.83478546142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4895], device='cuda:0')), ('power', tensor([-20.6907], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.1377342790365219
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13260053098201752
epoch£º661	 i:2 	 global-step:13222	 l-p:0.15348689258098602
epoch£º661	 i:3 	 global-step:13223	 l-p:0.19239330291748047
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11371930688619614
epoch£º661	 i:5 	 global-step:13225	 l-p:0.1102130264043808
epoch£º661	 i:6 	 global-step:13226	 l-p:0.1349291354417801
epoch£º661	 i:7 	 global-step:13227	 l-p:0.16811415553092957
epoch£º661	 i:8 	 global-step:13228	 l-p:0.10793091356754303
epoch£º661	 i:9 	 global-step:13229	 l-p:0.18554623425006866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0520, 5.0520, 5.0520],
        [5.0520, 5.0520, 5.0520],
        [5.0520, 5.0300, 5.0484],
        [5.0520, 4.8778, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.1359371393918991 
model_pd.l_d.mean(): -20.905778884887695 
model_pd.lagr.mean(): -20.76984214782715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-21.5493], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.1359371393918991
epoch£º662	 i:1 	 global-step:13241	 l-p:0.10288821905851364
epoch£º662	 i:2 	 global-step:13242	 l-p:0.14951640367507935
epoch£º662	 i:3 	 global-step:13243	 l-p:0.09940921515226364
epoch£º662	 i:4 	 global-step:13244	 l-p:0.10708915442228317
epoch£º662	 i:5 	 global-step:13245	 l-p:0.1282435804605484
epoch£º662	 i:6 	 global-step:13246	 l-p:0.15594086050987244
epoch£º662	 i:7 	 global-step:13247	 l-p:0.15270964801311493
epoch£º662	 i:8 	 global-step:13248	 l-p:0.14419640600681305
epoch£º662	 i:9 	 global-step:13249	 l-p:0.14763450622558594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0909, 5.2122, 5.0107],
        [5.0909, 5.0307, 5.0694],
        [5.0909, 4.8945, 4.7702],
        [5.0909, 5.0781, 5.0895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12813203036785126 
model_pd.l_d.mean(): -20.588897705078125 
model_pd.lagr.mean(): -20.460765838623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.2521], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12813203036785126
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11569524556398392
epoch£º663	 i:2 	 global-step:13262	 l-p:0.08340488374233246
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10838273167610168
epoch£º663	 i:4 	 global-step:13264	 l-p:0.13577739894390106
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19859641790390015
epoch£º663	 i:6 	 global-step:13266	 l-p:0.17107349634170532
epoch£º663	 i:7 	 global-step:13267	 l-p:0.1019589751958847
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10491804033517838
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12428867071866989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0974, 5.0974, 5.0974],
        [5.0974, 4.9044, 4.7551],
        [5.0974, 5.0957, 5.0974],
        [5.0974, 4.9326, 4.9315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.13218477368354797 
model_pd.l_d.mean(): -20.38332748413086 
model_pd.lagr.mean(): -20.251142501831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4720], device='cuda:0')), ('power', tensor([-21.0881], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.13218477368354797
epoch£º664	 i:1 	 global-step:13281	 l-p:0.15971776843070984
epoch£º664	 i:2 	 global-step:13282	 l-p:0.1256350874900818
epoch£º664	 i:3 	 global-step:13283	 l-p:0.11423145979642868
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12932701408863068
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12758855521678925
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16584017872810364
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12651661038398743
epoch£º664	 i:8 	 global-step:13288	 l-p:0.17101995646953583
epoch£º664	 i:9 	 global-step:13289	 l-p:0.05551648885011673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0629, 5.0606, 5.0628],
        [5.0629, 5.0605, 5.0628],
        [5.0629, 4.8640, 4.7233],
        [5.0629, 4.9889, 5.0316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.09923398494720459 
model_pd.l_d.mean(): -20.40873908996582 
model_pd.lagr.mean(): -20.309505462646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-21.1384], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.09923398494720459
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13785171508789062
epoch£º665	 i:2 	 global-step:13302	 l-p:0.17881226539611816
epoch£º665	 i:3 	 global-step:13303	 l-p:0.12855516374111176
epoch£º665	 i:4 	 global-step:13304	 l-p:0.1245117336511612
epoch£º665	 i:5 	 global-step:13305	 l-p:0.13627010583877563
epoch£º665	 i:6 	 global-step:13306	 l-p:0.1367107331752777
epoch£º665	 i:7 	 global-step:13307	 l-p:0.1765480935573578
epoch£º665	 i:8 	 global-step:13308	 l-p:0.10099657624959946
epoch£º665	 i:9 	 global-step:13309	 l-p:0.14371469616889954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0582, 5.0582, 5.0582],
        [5.0582, 4.9715, 4.7165],
        [5.0582, 4.9878, 4.7320],
        [5.0582, 4.8967, 4.9049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12448982149362564 
model_pd.l_d.mean(): -18.879558563232422 
model_pd.lagr.mean(): -18.755067825317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-19.6518], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12448982149362564
epoch£º666	 i:1 	 global-step:13321	 l-p:0.09302421659231186
epoch£º666	 i:2 	 global-step:13322	 l-p:0.13241426646709442
epoch£º666	 i:3 	 global-step:13323	 l-p:0.1550910472869873
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10685335099697113
epoch£º666	 i:5 	 global-step:13325	 l-p:0.16065441071987152
epoch£º666	 i:6 	 global-step:13326	 l-p:0.1585369110107422
epoch£º666	 i:7 	 global-step:13327	 l-p:0.13778287172317505
epoch£º666	 i:8 	 global-step:13328	 l-p:0.14271540939807892
epoch£º666	 i:9 	 global-step:13329	 l-p:0.20381195843219757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0530, 5.5328, 5.5522],
        [5.0530, 4.8563, 4.7905],
        [5.0530, 5.2947, 5.1571],
        [5.0530, 4.8544, 4.6964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.12907451391220093 
model_pd.l_d.mean(): -20.273826599121094 
model_pd.lagr.mean(): -20.144752502441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-21.0185], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.12907451391220093
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11505504697561264
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14077764749526978
epoch£º667	 i:3 	 global-step:13343	 l-p:0.1754182130098343
epoch£º667	 i:4 	 global-step:13344	 l-p:0.1639232635498047
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17773622274398804
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11837786436080933
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11113143712282181
epoch£º667	 i:8 	 global-step:13348	 l-p:0.13730593025684357
epoch£º667	 i:9 	 global-step:13349	 l-p:0.11427570134401321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0650, 5.0354, 5.0591],
        [5.0650, 5.0650, 5.0650],
        [5.0650, 4.8799, 4.8484],
        [5.0650, 4.9581, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11672637611627579 
model_pd.l_d.mean(): -19.970172882080078 
model_pd.lagr.mean(): -19.85344696044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182], device='cuda:0')), ('power', tensor([-20.7177], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11672637611627579
epoch£º668	 i:1 	 global-step:13361	 l-p:0.14096324145793915
epoch£º668	 i:2 	 global-step:13362	 l-p:0.12880441546440125
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15739548206329346
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12101470679044724
epoch£º668	 i:5 	 global-step:13365	 l-p:0.16156838834285736
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11915343999862671
epoch£º668	 i:7 	 global-step:13367	 l-p:0.1665007621049881
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11278697103261948
epoch£º668	 i:9 	 global-step:13369	 l-p:0.25327572226524353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0334, 4.8772, 4.8946],
        [5.0334, 5.0334, 5.0334],
        [5.0334, 5.0334, 5.0334],
        [5.0334, 4.8347, 4.6629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.1370396465063095 
model_pd.l_d.mean(): -20.105133056640625 
model_pd.lagr.mean(): -19.968093872070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-20.7713], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.1370396465063095
epoch£º669	 i:1 	 global-step:13381	 l-p:0.1661909520626068
epoch£º669	 i:2 	 global-step:13382	 l-p:0.13413088023662567
epoch£º669	 i:3 	 global-step:13383	 l-p:0.16069577634334564
epoch£º669	 i:4 	 global-step:13384	 l-p:0.16990900039672852
epoch£º669	 i:5 	 global-step:13385	 l-p:0.11090943217277527
epoch£º669	 i:6 	 global-step:13386	 l-p:0.1532667577266693
epoch£º669	 i:7 	 global-step:13387	 l-p:0.19204074144363403
epoch£º669	 i:8 	 global-step:13388	 l-p:0.1817544847726822
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11113223433494568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0473, 5.0473, 5.0473],
        [5.0473, 4.9766, 5.0190],
        [5.0473, 5.0459, 5.0473],
        [5.0473, 5.0473, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.2430325299501419 
model_pd.l_d.mean(): -20.619863510131836 
model_pd.lagr.mean(): -20.3768310546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-21.3376], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.2430325299501419
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11723736673593521
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13846178352832794
epoch£º670	 i:3 	 global-step:13403	 l-p:0.13250632584095
epoch£º670	 i:4 	 global-step:13404	 l-p:0.1506732553243637
epoch£º670	 i:5 	 global-step:13405	 l-p:0.09975366294384003
epoch£º670	 i:6 	 global-step:13406	 l-p:0.1515020728111267
epoch£º670	 i:7 	 global-step:13407	 l-p:0.1250927746295929
epoch£º670	 i:8 	 global-step:13408	 l-p:0.14352141320705414
epoch£º670	 i:9 	 global-step:13409	 l-p:0.15497399866580963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0334, 5.0226, 5.0324],
        [5.0334, 5.0298, 5.0333],
        [5.0334, 4.8384, 4.7896],
        [5.0334, 5.0297, 5.0333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.14279694855213165 
model_pd.l_d.mean(): -20.743576049804688 
model_pd.lagr.mean(): -20.600778579711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4361], device='cuda:0')), ('power', tensor([-21.4156], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.14279694855213165
epoch£º671	 i:1 	 global-step:13421	 l-p:0.1716342717409134
epoch£º671	 i:2 	 global-step:13422	 l-p:0.1165497675538063
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16711768507957458
epoch£º671	 i:4 	 global-step:13424	 l-p:0.28606975078582764
epoch£º671	 i:5 	 global-step:13425	 l-p:0.20630447566509247
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09650073200464249
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12378444522619247
epoch£º671	 i:8 	 global-step:13428	 l-p:0.18390361964702606
epoch£º671	 i:9 	 global-step:13429	 l-p:0.15033593773841858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0225, 5.0012, 5.0192],
        [5.0225, 4.8783, 4.6323],
        [5.0225, 5.0225, 5.0225],
        [5.0225, 4.8349, 4.8078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.2693537473678589 
model_pd.l_d.mean(): -20.131567001342773 
model_pd.lagr.mean(): -19.862213134765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5167], device='cuda:0')), ('power', tensor([-20.8793], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.2693537473678589
epoch£º672	 i:1 	 global-step:13441	 l-p:0.13611717522144318
epoch£º672	 i:2 	 global-step:13442	 l-p:0.15944799780845642
epoch£º672	 i:3 	 global-step:13443	 l-p:0.15924493968486786
epoch£º672	 i:4 	 global-step:13444	 l-p:0.10714653134346008
epoch£º672	 i:5 	 global-step:13445	 l-p:0.1180860623717308
epoch£º672	 i:6 	 global-step:13446	 l-p:0.149366557598114
epoch£º672	 i:7 	 global-step:13447	 l-p:0.13842323422431946
epoch£º672	 i:8 	 global-step:13448	 l-p:0.1200910434126854
epoch£º672	 i:9 	 global-step:13449	 l-p:0.24556319415569305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0204, 5.0202, 5.0204],
        [5.0204, 5.0204, 5.0204],
        [5.0204, 5.0039, 4.7496],
        [5.0204, 5.0099, 5.0194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.13452664017677307 
model_pd.l_d.mean(): -20.249975204467773 
model_pd.lagr.mean(): -20.115447998046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-20.9506], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.13452664017677307
epoch£º673	 i:1 	 global-step:13461	 l-p:0.16534732282161713
epoch£º673	 i:2 	 global-step:13462	 l-p:0.19806909561157227
epoch£º673	 i:3 	 global-step:13463	 l-p:0.3259015679359436
epoch£º673	 i:4 	 global-step:13464	 l-p:0.13226300477981567
epoch£º673	 i:5 	 global-step:13465	 l-p:0.15856687724590302
epoch£º673	 i:6 	 global-step:13466	 l-p:0.1790326088666916
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09536884725093842
epoch£º673	 i:8 	 global-step:13468	 l-p:0.15909311175346375
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09794265776872635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0324, 4.9203, 4.9641],
        [5.0324, 4.9347, 4.9804],
        [5.0324, 4.8840, 4.9097],
        [5.0324, 4.8246, 4.7064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.17124740779399872 
model_pd.l_d.mean(): -19.56106948852539 
model_pd.lagr.mean(): -19.389822006225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5596], device='cuda:0')), ('power', tensor([-20.3464], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.17124740779399872
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13257059454917908
epoch£º674	 i:2 	 global-step:13482	 l-p:0.10809940099716187
epoch£º674	 i:3 	 global-step:13483	 l-p:0.13448823988437653
epoch£º674	 i:4 	 global-step:13484	 l-p:0.07661214470863342
epoch£º674	 i:5 	 global-step:13485	 l-p:0.1374037265777588
epoch£º674	 i:6 	 global-step:13486	 l-p:0.2223178595304489
epoch£º674	 i:7 	 global-step:13487	 l-p:0.1674228310585022
epoch£º674	 i:8 	 global-step:13488	 l-p:0.13128773868083954
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13962945342063904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0938, 4.9278, 4.9310],
        [5.0938, 4.8937, 4.8073],
        [5.0938, 4.9995, 5.0444],
        [5.0938, 5.0216, 5.0642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10062531381845474 
model_pd.l_d.mean(): -20.164445877075195 
model_pd.lagr.mean(): -20.063819885253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4674], device='cuda:0')), ('power', tensor([-20.8623], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10062531381845474
epoch£º675	 i:1 	 global-step:13501	 l-p:0.13306203484535217
epoch£º675	 i:2 	 global-step:13502	 l-p:0.13601714372634888
epoch£º675	 i:3 	 global-step:13503	 l-p:0.12981687486171722
epoch£º675	 i:4 	 global-step:13504	 l-p:0.16002561151981354
epoch£º675	 i:5 	 global-step:13505	 l-p:0.15025141835212708
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12134385854005814
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12203281372785568
epoch£º675	 i:8 	 global-step:13508	 l-p:0.15794426202774048
epoch£º675	 i:9 	 global-step:13509	 l-p:0.04357143118977547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1020, 5.1019, 5.1020],
        [5.1020, 4.9107, 4.7307],
        [5.1020, 5.0900, 5.1007],
        [5.1020, 5.0980, 5.1018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.10147280246019363 
model_pd.l_d.mean(): -20.461755752563477 
model_pd.lagr.mean(): -20.36028289794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4475], device='cuda:0')), ('power', tensor([-21.1424], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.10147280246019363
epoch£º676	 i:1 	 global-step:13521	 l-p:0.14908021688461304
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18578018248081207
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12583793699741364
epoch£º676	 i:4 	 global-step:13524	 l-p:0.1464681625366211
epoch£º676	 i:5 	 global-step:13525	 l-p:0.07721703499555588
epoch£º676	 i:6 	 global-step:13526	 l-p:0.1947479248046875
epoch£º676	 i:7 	 global-step:13527	 l-p:0.13867312669754028
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12134318053722382
epoch£º676	 i:9 	 global-step:13529	 l-p:0.1264687180519104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0569, 5.0569, 5.0569],
        [5.0569, 4.9577, 5.0032],
        [5.0569, 4.8652, 4.8269],
        [5.0569, 5.0405, 5.0548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.12946684658527374 
model_pd.l_d.mean(): -20.59387969970703 
model_pd.lagr.mean(): -20.464412689208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4584], device='cuda:0')), ('power', tensor([-21.2872], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.12946684658527374
epoch£º677	 i:1 	 global-step:13541	 l-p:0.14830724895000458
epoch£º677	 i:2 	 global-step:13542	 l-p:0.1147848516702652
epoch£º677	 i:3 	 global-step:13543	 l-p:0.14350101351737976
epoch£º677	 i:4 	 global-step:13544	 l-p:0.18529081344604492
epoch£º677	 i:5 	 global-step:13545	 l-p:0.13101249933242798
epoch£º677	 i:6 	 global-step:13546	 l-p:0.4500073194503784
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.1950642615556717
epoch£º677	 i:8 	 global-step:13548	 l-p:0.16259655356407166
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11244045943021774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9881, 4.9564, 4.9816],
        [4.9881, 4.9865, 4.9880],
        [4.9881, 4.9602, 4.9829],
        [4.9881, 4.9481, 4.9783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.043645333498716354 
model_pd.l_d.mean(): -19.92574119567871 
model_pd.lagr.mean(): -19.96938705444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5695], device='cuda:0')), ('power', tensor([-20.7252], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.043645333498716354
epoch£º678	 i:1 	 global-step:13561	 l-p:0.18068692088127136
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13037613034248352
epoch£º678	 i:3 	 global-step:13563	 l-p:0.24277247488498688
epoch£º678	 i:4 	 global-step:13564	 l-p:0.129643052816391
epoch£º678	 i:5 	 global-step:13565	 l-p:0.13495874404907227
epoch£º678	 i:6 	 global-step:13566	 l-p:0.17228515446186066
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14697130024433136
epoch£º678	 i:8 	 global-step:13568	 l-p:0.14458557963371277
epoch£º678	 i:9 	 global-step:13569	 l-p:0.21775978803634644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9967, 4.9966, 4.9967],
        [4.9967, 4.9502, 4.6874],
        [4.9967, 4.9698, 4.9918],
        [4.9967, 4.9464, 4.9819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.18440887331962585 
model_pd.l_d.mean(): -20.444643020629883 
model_pd.lagr.mean(): -20.260234832763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4914], device='cuda:0')), ('power', tensor([-21.1700], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.18440887331962585
epoch£º679	 i:1 	 global-step:13581	 l-p:0.16509631276130676
epoch£º679	 i:2 	 global-step:13582	 l-p:0.17233054339885712
epoch£º679	 i:3 	 global-step:13583	 l-p:1.5188488960266113
epoch£º679	 i:4 	 global-step:13584	 l-p:0.2282511442899704
epoch£º679	 i:5 	 global-step:13585	 l-p:0.11947064846754074
epoch£º679	 i:6 	 global-step:13586	 l-p:0.16477271914482117
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11943847686052322
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12303487211465836
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11459821462631226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0328, 5.0260, 5.0324],
        [5.0328, 5.0304, 5.0327],
        [5.0328, 4.9952, 5.0240],
        [5.0328, 5.2191, 5.0480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.21045181155204773 
model_pd.l_d.mean(): -20.949989318847656 
model_pd.lagr.mean(): -20.739538192749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.6141], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.21045181155204773
epoch£º680	 i:1 	 global-step:13601	 l-p:0.141654372215271
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15403376519680023
epoch£º680	 i:3 	 global-step:13603	 l-p:0.1673104465007782
epoch£º680	 i:4 	 global-step:13604	 l-p:0.20797595381736755
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15308263897895813
epoch£º680	 i:6 	 global-step:13606	 l-p:0.1361696720123291
epoch£º680	 i:7 	 global-step:13607	 l-p:0.11517886817455292
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12360654771327972
epoch£º680	 i:9 	 global-step:13609	 l-p:0.11763475835323334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0900, 4.9117, 4.8989],
        [5.0900, 5.4749, 5.4241],
        [5.0900, 5.0891, 5.0900],
        [5.0900, 5.0891, 5.0900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.10591110587120056 
model_pd.l_d.mean(): -20.376218795776367 
model_pd.lagr.mean(): -20.270307540893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-21.0725], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.10591110587120056
epoch£º681	 i:1 	 global-step:13621	 l-p:0.1330493539571762
epoch£º681	 i:2 	 global-step:13622	 l-p:0.17322036623954773
epoch£º681	 i:3 	 global-step:13623	 l-p:0.14624455571174622
epoch£º681	 i:4 	 global-step:13624	 l-p:0.14769421517848969
epoch£º681	 i:5 	 global-step:13625	 l-p:0.08566516637802124
epoch£º681	 i:6 	 global-step:13626	 l-p:0.05551520362496376
epoch£º681	 i:7 	 global-step:13627	 l-p:0.14628419280052185
epoch£º681	 i:8 	 global-step:13628	 l-p:0.15969522297382355
epoch£º681	 i:9 	 global-step:13629	 l-p:0.11223660409450531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0808, 4.9855, 5.0310],
        [5.0808, 4.9578, 4.9980],
        [5.0808, 5.0784, 5.0807],
        [5.0808, 5.0658, 5.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14222711324691772 
model_pd.l_d.mean(): -20.54279899597168 
model_pd.lagr.mean(): -20.400571823120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.2395], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14222711324691772
epoch£º682	 i:1 	 global-step:13641	 l-p:0.12795932590961456
epoch£º682	 i:2 	 global-step:13642	 l-p:0.12487415969371796
epoch£º682	 i:3 	 global-step:13643	 l-p:0.21555744111537933
epoch£º682	 i:4 	 global-step:13644	 l-p:0.17787450551986694
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12959040701389313
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13705949485301971
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14158594608306885
epoch£º682	 i:8 	 global-step:13648	 l-p:0.11713185906410217
epoch£º682	 i:9 	 global-step:13649	 l-p:-0.4255220592021942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9678, 4.8474, 4.8916],
        [4.9678, 4.9313, 4.9595],
        [4.9678, 4.9678, 4.9678],
        [4.9678, 4.8137, 4.8403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.14752179384231567 
model_pd.l_d.mean(): -20.870288848876953 
model_pd.lagr.mean(): -20.722766876220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.5730], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.14752179384231567
epoch£º683	 i:1 	 global-step:13661	 l-p:0.3048224449157715
epoch£º683	 i:2 	 global-step:13662	 l-p:0.11406564712524414
epoch£º683	 i:3 	 global-step:13663	 l-p:0.14004774391651154
epoch£º683	 i:4 	 global-step:13664	 l-p:-0.1787133365869522
epoch£º683	 i:5 	 global-step:13665	 l-p:0.1260647475719452
epoch£º683	 i:6 	 global-step:13666	 l-p:0.10220702737569809
epoch£º683	 i:7 	 global-step:13667	 l-p:0.16922958195209503
epoch£º683	 i:8 	 global-step:13668	 l-p:0.24430476129055023
epoch£º683	 i:9 	 global-step:13669	 l-p:0.11720654368400574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9630, 4.7972, 4.5495],
        [4.9630, 4.9630, 4.9630],
        [4.9630, 5.1789, 5.0259],
        [4.9630, 4.9608, 4.9629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.1235445886850357 
model_pd.l_d.mean(): -20.367698669433594 
model_pd.lagr.mean(): -20.24415397644043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-21.1095], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.1235445886850357
epoch£º684	 i:1 	 global-step:13681	 l-p:0.126267671585083
epoch£º684	 i:2 	 global-step:13682	 l-p:0.1317453682422638
epoch£º684	 i:3 	 global-step:13683	 l-p:0.2572910189628601
epoch£º684	 i:4 	 global-step:13684	 l-p:0.12607097625732422
epoch£º684	 i:5 	 global-step:13685	 l-p:0.7785972952842712
epoch£º684	 i:6 	 global-step:13686	 l-p:-0.8978371024131775
epoch£º684	 i:7 	 global-step:13687	 l-p:-0.06013974919915199
epoch£º684	 i:8 	 global-step:13688	 l-p:0.1322527527809143
epoch£º684	 i:9 	 global-step:13689	 l-p:0.05247120559215546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9879, 4.9084, 4.9537],
        [4.9879, 5.3326, 5.2585],
        [4.9879, 4.7717, 4.6462],
        [4.9879, 4.9878, 4.9879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.22070090472698212 
model_pd.l_d.mean(): -20.637107849121094 
model_pd.lagr.mean(): -20.416406631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-21.3611], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.22070090472698212
epoch£º685	 i:1 	 global-step:13701	 l-p:0.18563805520534515
epoch£º685	 i:2 	 global-step:13702	 l-p:0.11986083537340164
epoch£º685	 i:3 	 global-step:13703	 l-p:0.3669191598892212
epoch£º685	 i:4 	 global-step:13704	 l-p:0.1320555955171585
epoch£º685	 i:5 	 global-step:13705	 l-p:0.11725334823131561
epoch£º685	 i:6 	 global-step:13706	 l-p:0.08789241313934326
epoch£º685	 i:7 	 global-step:13707	 l-p:0.1732124239206314
epoch£º685	 i:8 	 global-step:13708	 l-p:0.1024230420589447
epoch£º685	 i:9 	 global-step:13709	 l-p:0.1279676854610443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1025, 5.1022, 5.1025],
        [5.1025, 5.1020, 5.1025],
        [5.1025, 5.0719, 5.0963],
        [5.1025, 5.3356, 5.1884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.08767277747392654 
model_pd.l_d.mean(): -19.0440616607666 
model_pd.lagr.mean(): -18.956388473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5860], device='cuda:0')), ('power', tensor([-19.8507], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.08767277747392654
epoch£º686	 i:1 	 global-step:13721	 l-p:0.1395522654056549
epoch£º686	 i:2 	 global-step:13722	 l-p:0.10121428966522217
epoch£º686	 i:3 	 global-step:13723	 l-p:0.14908656477928162
epoch£º686	 i:4 	 global-step:13724	 l-p:0.11167798936367035
epoch£º686	 i:5 	 global-step:13725	 l-p:0.1121426671743393
epoch£º686	 i:6 	 global-step:13726	 l-p:0.09564563632011414
epoch£º686	 i:7 	 global-step:13727	 l-p:0.13946327567100525
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13537243008613586
epoch£º686	 i:9 	 global-step:13729	 l-p:0.1402915120124817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1396, 5.3448, 5.1807],
        [5.1396, 4.9652, 4.7544],
        [5.1396, 4.9509, 4.9107],
        [5.1396, 5.2298, 5.0093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07158631831407547 
model_pd.l_d.mean(): -19.753488540649414 
model_pd.lagr.mean(): -19.681901931762695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5471], device='cuda:0')), ('power', tensor([-20.5281], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.07158631831407547
epoch£º687	 i:1 	 global-step:13741	 l-p:0.1733543574810028
epoch£º687	 i:2 	 global-step:13742	 l-p:0.13746143877506256
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13086704909801483
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14951357245445251
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10526320338249207
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08757102489471436
epoch£º687	 i:7 	 global-step:13747	 l-p:0.13852739334106445
epoch£º687	 i:8 	 global-step:13748	 l-p:0.1407000571489334
epoch£º687	 i:9 	 global-step:13749	 l-p:0.01480832975357771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1206, 5.1206, 5.1206],
        [5.1206, 4.9243, 4.7505],
        [5.1206, 4.9648, 4.9821],
        [5.1206, 5.1205, 5.1206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.044379014521837234 
model_pd.l_d.mean(): -19.473302841186523 
model_pd.lagr.mean(): -19.428924560546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5254], device='cuda:0')), ('power', tensor([-20.2228], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.044379014521837234
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13398073613643646
epoch£º688	 i:2 	 global-step:13762	 l-p:0.14983078837394714
epoch£º688	 i:3 	 global-step:13763	 l-p:0.1763007640838623
epoch£º688	 i:4 	 global-step:13764	 l-p:0.15048252046108246
epoch£º688	 i:5 	 global-step:13765	 l-p:0.11985913664102554
epoch£º688	 i:6 	 global-step:13766	 l-p:0.03783825412392616
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13359488546848297
epoch£º688	 i:8 	 global-step:13768	 l-p:0.1221800297498703
epoch£º688	 i:9 	 global-step:13769	 l-p:0.13087712228298187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1033, 5.0382, 5.0792],
        [5.1033, 5.0962, 5.1028],
        [5.1033, 5.1033, 5.1033],
        [5.1033, 5.1033, 5.1033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.17598886787891388 
model_pd.l_d.mean(): -20.67106056213379 
model_pd.lagr.mean(): -20.495071411132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4429], device='cuda:0')), ('power', tensor([-21.3493], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.17598886787891388
epoch£º689	 i:1 	 global-step:13781	 l-p:0.1360468715429306
epoch£º689	 i:2 	 global-step:13782	 l-p:0.1210850179195404
epoch£º689	 i:3 	 global-step:13783	 l-p:0.12677563726902008
epoch£º689	 i:4 	 global-step:13784	 l-p:0.12016019225120544
epoch£º689	 i:5 	 global-step:13785	 l-p:0.12098167836666107
epoch£º689	 i:6 	 global-step:13786	 l-p:0.18972209095954895
epoch£º689	 i:7 	 global-step:13787	 l-p:0.09349897503852844
epoch£º689	 i:8 	 global-step:13788	 l-p:0.2513594925403595
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12664523720741272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0311, 5.0310, 5.0311],
        [5.0311, 5.4420, 5.4100],
        [5.0311, 5.0305, 5.0311],
        [5.0311, 5.0311, 5.0311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.1391587108373642 
model_pd.l_d.mean(): -20.613569259643555 
model_pd.lagr.mean(): -20.474411010742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4749], device='cuda:0')), ('power', tensor([-21.3239], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.1391587108373642
epoch£º690	 i:1 	 global-step:13801	 l-p:0.0794176384806633
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12477931380271912
epoch£º690	 i:3 	 global-step:13803	 l-p:0.39248111844062805
epoch£º690	 i:4 	 global-step:13804	 l-p:0.1322702020406723
epoch£º690	 i:5 	 global-step:13805	 l-p:0.19047589600086212
epoch£º690	 i:6 	 global-step:13806	 l-p:0.22825442254543304
epoch£º690	 i:7 	 global-step:13807	 l-p:0.17253592610359192
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13656772673130035
epoch£º690	 i:9 	 global-step:13809	 l-p:0.1335744857788086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 5.0409, 5.0410],
        [5.0410, 4.9354, 4.9816],
        [5.0410, 4.9366, 4.6720],
        [5.0410, 5.0387, 5.0409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.11128570139408112 
model_pd.l_d.mean(): -19.581892013549805 
model_pd.lagr.mean(): -19.470605850219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5208], device='cuda:0')), ('power', tensor([-20.3278], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.11128570139408112
epoch£º691	 i:1 	 global-step:13821	 l-p:0.09836944192647934
epoch£º691	 i:2 	 global-step:13822	 l-p:0.17091037333011627
epoch£º691	 i:3 	 global-step:13823	 l-p:0.17038102447986603
epoch£º691	 i:4 	 global-step:13824	 l-p:0.1918487250804901
epoch£º691	 i:5 	 global-step:13825	 l-p:0.10066723823547363
epoch£º691	 i:6 	 global-step:13826	 l-p:0.20909570157527924
epoch£º691	 i:7 	 global-step:13827	 l-p:0.11157504469156265
epoch£º691	 i:8 	 global-step:13828	 l-p:0.11727841943502426
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11847507953643799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0989, 5.0986, 5.0989],
        [5.0989, 5.0983, 5.0989],
        [5.0989, 5.0359, 5.0764],
        [5.0989, 5.0989, 5.0989]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.081293985247612 
model_pd.l_d.mean(): -20.844505310058594 
model_pd.lagr.mean(): -20.763212203979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-21.4873], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.081293985247612
epoch£º692	 i:1 	 global-step:13841	 l-p:0.12535609304904938
epoch£º692	 i:2 	 global-step:13842	 l-p:0.15940354764461517
epoch£º692	 i:3 	 global-step:13843	 l-p:0.14261707663536072
epoch£º692	 i:4 	 global-step:13844	 l-p:0.1059594452381134
epoch£º692	 i:5 	 global-step:13845	 l-p:0.1197899878025055
epoch£º692	 i:6 	 global-step:13846	 l-p:0.14505033195018768
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12345369160175323
epoch£º692	 i:8 	 global-step:13848	 l-p:0.19985385239124298
epoch£º692	 i:9 	 global-step:13849	 l-p:0.11304587125778198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0613, 5.0576, 5.0612],
        [5.0613, 5.1700, 4.9567],
        [5.0613, 5.4250, 5.3592],
        [5.0613, 5.4655, 5.4269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.09635357558727264 
model_pd.l_d.mean(): -19.56005096435547 
model_pd.lagr.mean(): -19.46369743347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5814], device='cuda:0')), ('power', tensor([-20.3677], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.09635357558727264
epoch£º693	 i:1 	 global-step:13861	 l-p:0.07540782541036606
epoch£º693	 i:2 	 global-step:13862	 l-p:0.15754523873329163
epoch£º693	 i:3 	 global-step:13863	 l-p:0.1595141887664795
epoch£º693	 i:4 	 global-step:13864	 l-p:0.1770852655172348
epoch£º693	 i:5 	 global-step:13865	 l-p:0.1318109780550003
epoch£º693	 i:6 	 global-step:13866	 l-p:0.12256378680467606
epoch£º693	 i:7 	 global-step:13867	 l-p:0.20712023973464966
epoch£º693	 i:8 	 global-step:13868	 l-p:0.21074581146240234
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11577653884887695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0546, 4.9905, 5.0317],
        [5.0546, 4.8451, 4.6772],
        [5.0546, 5.0545, 5.0546],
        [5.0546, 5.0546, 5.0546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.10136134177446365 
model_pd.l_d.mean(): -19.198610305786133 
model_pd.lagr.mean(): -19.097248077392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5568], device='cuda:0')), ('power', tensor([-19.9772], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.10136134177446365
epoch£º694	 i:1 	 global-step:13881	 l-p:0.17984460294246674
epoch£º694	 i:2 	 global-step:13882	 l-p:0.13337089121341705
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12839527428150177
epoch£º694	 i:4 	 global-step:13884	 l-p:0.1124439612030983
epoch£º694	 i:5 	 global-step:13885	 l-p:0.18797186017036438
epoch£º694	 i:6 	 global-step:13886	 l-p:0.14568862318992615
epoch£º694	 i:7 	 global-step:13887	 l-p:0.1505085974931717
epoch£º694	 i:8 	 global-step:13888	 l-p:0.13155809044837952
epoch£º694	 i:9 	 global-step:13889	 l-p:0.1356869786977768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0820, 4.8734, 4.7865],
        [5.0820, 4.9288, 4.6822],
        [5.0820, 4.9374, 4.6860],
        [5.0820, 5.0818, 5.0820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12584549188613892 
model_pd.l_d.mean(): -19.54973602294922 
model_pd.lagr.mean(): -19.423891067504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4963], device='cuda:0')), ('power', tensor([-20.2703], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12584549188613892
epoch£º695	 i:1 	 global-step:13901	 l-p:0.17796093225479126
epoch£º695	 i:2 	 global-step:13902	 l-p:0.09396396577358246
epoch£º695	 i:3 	 global-step:13903	 l-p:0.1333385556936264
epoch£º695	 i:4 	 global-step:13904	 l-p:0.1670304238796234
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12107907980680466
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13702796399593353
epoch£º695	 i:7 	 global-step:13907	 l-p:0.09817685931921005
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13159972429275513
epoch£º695	 i:9 	 global-step:13909	 l-p:0.13267183303833008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0954, 5.0866, 5.0947],
        [5.0954, 4.9439, 4.6975],
        [5.0954, 5.0512, 5.0837],
        [5.0954, 5.4471, 5.3716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.18949805200099945 
model_pd.l_d.mean(): -20.804317474365234 
model_pd.lagr.mean(): -20.614818572998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.4675], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.18949805200099945
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11851298063993454
epoch£º696	 i:2 	 global-step:13922	 l-p:0.1639138013124466
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11918266117572784
epoch£º696	 i:4 	 global-step:13924	 l-p:0.1414669156074524
epoch£º696	 i:5 	 global-step:13925	 l-p:0.1834753304719925
epoch£º696	 i:6 	 global-step:13926	 l-p:0.12714290618896484
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11712827533483505
epoch£º696	 i:8 	 global-step:13928	 l-p:0.1367410272359848
epoch£º696	 i:9 	 global-step:13929	 l-p:0.11079760640859604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0262, 5.0262, 5.0262],
        [5.0262, 5.0261, 5.0262],
        [5.0262, 5.1083, 4.8823],
        [5.0262, 5.0253, 5.0262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.13215689361095428 
model_pd.l_d.mean(): -20.116985321044922 
model_pd.lagr.mean(): -19.98482894897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.13215689361095428
epoch£º697	 i:1 	 global-step:13941	 l-p:0.1644936352968216
epoch£º697	 i:2 	 global-step:13942	 l-p:0.7689950466156006
epoch£º697	 i:3 	 global-step:13943	 l-p:0.12647856771945953
epoch£º697	 i:4 	 global-step:13944	 l-p:0.22269412875175476
epoch£º697	 i:5 	 global-step:13945	 l-p:0.1469084918498993
epoch£º697	 i:6 	 global-step:13946	 l-p:0.1779220551252365
epoch£º697	 i:7 	 global-step:13947	 l-p:0.15201304852962494
epoch£º697	 i:8 	 global-step:13948	 l-p:0.1404363065958023
epoch£º697	 i:9 	 global-step:13949	 l-p:0.1687111258506775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0378, 5.0013, 5.0295],
        [5.0378, 5.0353, 5.0377],
        [5.0378, 4.8519, 4.8414],
        [5.0378, 4.8207, 4.7008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.14442835748195648 
model_pd.l_d.mean(): -20.58294105529785 
model_pd.lagr.mean(): -20.438512802124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.2790], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.14442835748195648
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11810149252414703
epoch£º698	 i:2 	 global-step:13962	 l-p:0.2050573229789734
epoch£º698	 i:3 	 global-step:13963	 l-p:0.1483420729637146
epoch£º698	 i:4 	 global-step:13964	 l-p:0.13164004683494568
epoch£º698	 i:5 	 global-step:13965	 l-p:0.17013965547084808
epoch£º698	 i:6 	 global-step:13966	 l-p:0.2652724087238312
epoch£º698	 i:7 	 global-step:13967	 l-p:0.1651058942079544
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09469117969274521
epoch£º698	 i:9 	 global-step:13969	 l-p:0.11973583698272705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0580, 5.0510, 5.0575],
        [5.0580, 4.9943, 5.0354],
        [5.0580, 4.8429, 4.7008],
        [5.0580, 5.0580, 5.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.14000408351421356 
model_pd.l_d.mean(): -20.4417781829834 
model_pd.lagr.mean(): -20.301774978637695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4827], device='cuda:0')), ('power', tensor([-21.1581], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.14000408351421356
epoch£º699	 i:1 	 global-step:13981	 l-p:0.1235819011926651
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14947693049907684
epoch£º699	 i:3 	 global-step:13983	 l-p:0.1331792175769806
epoch£º699	 i:4 	 global-step:13984	 l-p:0.17914806306362152
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06648324429988861
epoch£º699	 i:6 	 global-step:13986	 l-p:0.10323676466941833
epoch£º699	 i:7 	 global-step:13987	 l-p:0.18184049427509308
epoch£º699	 i:8 	 global-step:13988	 l-p:0.14838309586048126
epoch£º699	 i:9 	 global-step:13989	 l-p:0.13490158319473267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1179, 5.1130, 5.1177],
        [5.1179, 4.9123, 4.8357],
        [5.1179, 4.9276, 4.8993],
        [5.1179, 5.0531, 5.0943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.11059105396270752 
model_pd.l_d.mean(): -20.438125610351562 
model_pd.lagr.mean(): -20.327533721923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.1031], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.11059105396270752
epoch£º700	 i:1 	 global-step:14001	 l-p:0.12229066342115402
epoch£º700	 i:2 	 global-step:14002	 l-p:0.08280522376298904
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12466233968734741
epoch£º700	 i:4 	 global-step:14004	 l-p:0.123805470764637
epoch£º700	 i:5 	 global-step:14005	 l-p:0.16000178456306458
epoch£º700	 i:6 	 global-step:14006	 l-p:0.0914468765258789
epoch£º700	 i:7 	 global-step:14007	 l-p:0.1480703204870224
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11808717250823975
epoch£º700	 i:9 	 global-step:14009	 l-p:0.15715397894382477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1228, 5.1155, 5.1222],
        [5.1228, 5.0436, 5.0885],
        [5.1228, 5.1228, 5.1228],
        [5.1228, 4.9577, 4.9698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.12474441528320312 
model_pd.l_d.mean(): -20.126325607299805 
model_pd.lagr.mean(): -20.0015811920166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-20.8428], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.12474441528320312
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14660470187664032
epoch£º701	 i:2 	 global-step:14022	 l-p:0.1356990933418274
epoch£º701	 i:3 	 global-step:14023	 l-p:0.16341136395931244
epoch£º701	 i:4 	 global-step:14024	 l-p:0.09103868901729584
epoch£º701	 i:5 	 global-step:14025	 l-p:0.09511642903089523
epoch£º701	 i:6 	 global-step:14026	 l-p:0.1435963660478592
epoch£º701	 i:7 	 global-step:14027	 l-p:0.0814518928527832
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12937551736831665
epoch£º701	 i:9 	 global-step:14029	 l-p:0.1612038016319275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1055, 5.0420, 5.0830],
        [5.1055, 5.2742, 5.0880],
        [5.1055, 5.0440, 5.0842],
        [5.1055, 5.1055, 5.1055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.1280367374420166 
model_pd.l_d.mean(): -20.2987003326416 
model_pd.lagr.mean(): -20.170663833618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-21.0113], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.1280367374420166
epoch£º702	 i:1 	 global-step:14041	 l-p:0.19225135445594788
epoch£º702	 i:2 	 global-step:14042	 l-p:0.04382617026567459
epoch£º702	 i:3 	 global-step:14043	 l-p:0.09861490875482559
epoch£º702	 i:4 	 global-step:14044	 l-p:0.13226710259914398
epoch£º702	 i:5 	 global-step:14045	 l-p:0.14036989212036133
epoch£º702	 i:6 	 global-step:14046	 l-p:0.10553537309169769
epoch£º702	 i:7 	 global-step:14047	 l-p:0.16503188014030457
epoch£º702	 i:8 	 global-step:14048	 l-p:0.15553832054138184
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12639427185058594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0792, 5.0763, 5.0791],
        [5.0792, 5.3170, 5.1699],
        [5.0792, 5.0667, 5.0779],
        [5.0792, 5.3417, 5.2092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.14560569822788239 
model_pd.l_d.mean(): -20.462968826293945 
model_pd.lagr.mean(): -20.317363739013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-21.1870], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.14560569822788239
epoch£º703	 i:1 	 global-step:14061	 l-p:0.13058675825595856
epoch£º703	 i:2 	 global-step:14062	 l-p:0.08608562499284744
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13689634203910828
epoch£º703	 i:4 	 global-step:14064	 l-p:0.2011813074350357
epoch£º703	 i:5 	 global-step:14065	 l-p:0.18525049090385437
epoch£º703	 i:6 	 global-step:14066	 l-p:0.11824449896812439
epoch£º703	 i:7 	 global-step:14067	 l-p:0.12302818149328232
epoch£º703	 i:8 	 global-step:14068	 l-p:0.0664878711104393
epoch£º703	 i:9 	 global-step:14069	 l-p:0.15203604102134705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1061, 5.1061, 5.1061],
        [5.1061, 5.0615, 5.0943],
        [5.1061, 5.0255, 5.0710],
        [5.1061, 4.9327, 4.6985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13201917707920074 
model_pd.l_d.mean(): -21.062387466430664 
model_pd.lagr.mean(): -20.930368423461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3720], device='cuda:0')), ('power', tensor([-21.6725], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13201917707920074
epoch£º704	 i:1 	 global-step:14081	 l-p:0.09027457982301712
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16453120112419128
epoch£º704	 i:3 	 global-step:14083	 l-p:0.08468399196863174
epoch£º704	 i:4 	 global-step:14084	 l-p:0.14971140027046204
epoch£º704	 i:5 	 global-step:14085	 l-p:0.12911251187324524
epoch£º704	 i:6 	 global-step:14086	 l-p:0.10563976317644119
epoch£º704	 i:7 	 global-step:14087	 l-p:0.14118580520153046
epoch£º704	 i:8 	 global-step:14088	 l-p:0.1101129874587059
epoch£º704	 i:9 	 global-step:14089	 l-p:0.13469889760017395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1591, 5.1591, 5.1591],
        [5.1591, 5.1547, 5.1589],
        [5.1591, 5.0028, 4.7619],
        [5.1591, 5.2623, 5.0428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): -0.22504012286663055 
model_pd.l_d.mean(): -20.141475677490234 
model_pd.lagr.mean(): -20.36651611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5016], device='cuda:0')), ('power', tensor([-20.8740], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:-0.22504012286663055
epoch£º705	 i:1 	 global-step:14101	 l-p:0.1504853367805481
epoch£º705	 i:2 	 global-step:14102	 l-p:0.1356506645679474
epoch£º705	 i:3 	 global-step:14103	 l-p:0.14764289557933807
epoch£º705	 i:4 	 global-step:14104	 l-p:0.11281631886959076
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11329153925180435
epoch£º705	 i:6 	 global-step:14106	 l-p:0.1289723515510559
epoch£º705	 i:7 	 global-step:14107	 l-p:0.13972462713718414
epoch£º705	 i:8 	 global-step:14108	 l-p:0.08009587973356247
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12282723933458328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1203, 5.0454, 5.0897],
        [5.1203, 5.3044, 5.1252],
        [5.1203, 5.0839, 5.1121],
        [5.1203, 4.9071, 4.7761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.1552284210920334 
model_pd.l_d.mean(): -20.512126922607422 
model_pd.lagr.mean(): -20.35689926147461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-21.2112], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.1552284210920334
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12514278292655945
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14586858451366425
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10726936161518097
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10443086177110672
epoch£º706	 i:5 	 global-step:14125	 l-p:0.20840412378311157
epoch£º706	 i:6 	 global-step:14126	 l-p:0.1235496774315834
epoch£º706	 i:7 	 global-step:14127	 l-p:0.32095617055892944
epoch£º706	 i:8 	 global-step:14128	 l-p:0.22030264139175415
epoch£º706	 i:9 	 global-step:14129	 l-p:0.13773290812969208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 5.0180, 5.0183],
        [5.0183, 5.0183, 5.0183],
        [5.0183, 5.0183, 5.0183],
        [5.0183, 4.9092, 4.9572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 1.3001164197921753 
model_pd.l_d.mean(): -20.919078826904297 
model_pd.lagr.mean(): -19.61896324157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-21.5950], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:1.3001164197921753
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11277135461568832
epoch£º707	 i:2 	 global-step:14142	 l-p:0.22226428985595703
epoch£º707	 i:3 	 global-step:14143	 l-p:0.1370486617088318
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12801817059516907
epoch£º707	 i:5 	 global-step:14145	 l-p:0.2739067077636719
epoch£º707	 i:6 	 global-step:14146	 l-p:0.1915583610534668
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14586377143859863
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12639258801937103
epoch£º707	 i:9 	 global-step:14149	 l-p:0.11409696191549301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0398, 4.9655, 5.0104],
        [5.0398, 4.8685, 4.8826],
        [5.0398, 4.9859, 4.7143],
        [5.0398, 4.9647, 5.0097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13556629419326782 
model_pd.l_d.mean(): -20.37879180908203 
model_pd.lagr.mean(): -20.24322509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-21.1093], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13556629419326782
epoch£º708	 i:1 	 global-step:14161	 l-p:0.11394790560007095
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1662026196718216
epoch£º708	 i:3 	 global-step:14163	 l-p:0.17424529790878296
epoch£º708	 i:4 	 global-step:14164	 l-p:0.1388661116361618
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06649637222290039
epoch£º708	 i:6 	 global-step:14166	 l-p:0.31725263595581055
epoch£º708	 i:7 	 global-step:14167	 l-p:0.19149602949619293
epoch£º708	 i:8 	 global-step:14168	 l-p:0.14585453271865845
epoch£º708	 i:9 	 global-step:14169	 l-p:0.1719079166650772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0847, 4.8767, 4.8182],
        [5.0847, 5.0469, 5.0759],
        [5.0847, 5.5473, 5.5450],
        [5.0847, 4.9090, 4.6681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10532573610544205 
model_pd.l_d.mean(): -20.187040328979492 
model_pd.lagr.mean(): -20.081714630126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4776], device='cuda:0')), ('power', tensor([-20.8954], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10532573610544205
epoch£º709	 i:1 	 global-step:14181	 l-p:0.1503300666809082
epoch£º709	 i:2 	 global-step:14182	 l-p:0.06953646242618561
epoch£º709	 i:3 	 global-step:14183	 l-p:0.09650452435016632
epoch£º709	 i:4 	 global-step:14184	 l-p:0.1566396802663803
epoch£º709	 i:5 	 global-step:14185	 l-p:0.11701802909374237
epoch£º709	 i:6 	 global-step:14186	 l-p:0.13903175294399261
epoch£º709	 i:7 	 global-step:14187	 l-p:0.1488642692565918
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12530988454818726
epoch£º709	 i:9 	 global-step:14189	 l-p:0.1587287038564682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1267, 5.1096, 5.1245],
        [5.1267, 4.9230, 4.8690],
        [5.1267, 4.9545, 4.9617],
        [5.1267, 5.0994, 5.1218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12231307476758957 
model_pd.l_d.mean(): -20.50465202331543 
model_pd.lagr.mean(): -20.382339477539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4312], device='cuda:0')), ('power', tensor([-21.1692], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12231307476758957
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12681062519550323
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12621928751468658
epoch£º710	 i:3 	 global-step:14203	 l-p:0.07369174063205719
epoch£º710	 i:4 	 global-step:14204	 l-p:0.15603919327259064
epoch£º710	 i:5 	 global-step:14205	 l-p:0.1002860888838768
epoch£º710	 i:6 	 global-step:14206	 l-p:0.22191451489925385
epoch£º710	 i:7 	 global-step:14207	 l-p:0.14582739770412445
epoch£º710	 i:8 	 global-step:14208	 l-p:0.15970566868782043
epoch£º710	 i:9 	 global-step:14209	 l-p:0.1390352100133896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0767, 5.0657, 5.0757],
        [5.0767, 5.1384, 4.8999],
        [5.0767, 5.0647, 4.8015],
        [5.0767, 5.0767, 5.0767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.152201846241951 
model_pd.l_d.mean(): -19.753496170043945 
model_pd.lagr.mean(): -19.601293563842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5439], device='cuda:0')), ('power', tensor([-20.5249], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.152201846241951
epoch£º711	 i:1 	 global-step:14221	 l-p:0.1934262216091156
epoch£º711	 i:2 	 global-step:14222	 l-p:0.10709301382303238
epoch£º711	 i:3 	 global-step:14223	 l-p:0.16003966331481934
epoch£º711	 i:4 	 global-step:14224	 l-p:0.14887070655822754
epoch£º711	 i:5 	 global-step:14225	 l-p:0.15419088304042816
epoch£º711	 i:6 	 global-step:14226	 l-p:0.029950851574540138
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12210974097251892
epoch£º711	 i:8 	 global-step:14228	 l-p:0.08721550554037094
epoch£º711	 i:9 	 global-step:14229	 l-p:0.118754543364048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.1154, 4.8490],
        [5.1664, 5.2659, 5.0430],
        [5.1664, 5.0937, 5.1374],
        [5.1664, 5.3990, 5.2443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): -0.29912522435188293 
model_pd.l_d.mean(): -20.212074279785156 
model_pd.lagr.mean(): -20.511199951171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.9532], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:-0.29912522435188293
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11684032529592514
epoch£º712	 i:2 	 global-step:14242	 l-p:0.1326320469379425
epoch£º712	 i:3 	 global-step:14243	 l-p:0.1396481990814209
epoch£º712	 i:4 	 global-step:14244	 l-p:0.11657411605119705
epoch£º712	 i:5 	 global-step:14245	 l-p:0.1235613152384758
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13817991316318512
epoch£º712	 i:7 	 global-step:14247	 l-p:0.08420976251363754
epoch£º712	 i:8 	 global-step:14248	 l-p:0.07154960185289383
epoch£º712	 i:9 	 global-step:14249	 l-p:0.1239837035536766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 5.1512, 5.1576],
        [5.1581, 4.9463, 4.8001],
        [5.1581, 5.1581, 5.1581],
        [5.1581, 4.9810, 4.7511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08628331869840622 
model_pd.l_d.mean(): -20.884416580200195 
model_pd.lagr.mean(): -20.798133850097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3842], device='cuda:0')), ('power', tensor([-21.5051], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.08628331869840622
epoch£º713	 i:1 	 global-step:14261	 l-p:0.03832034394145012
epoch£º713	 i:2 	 global-step:14262	 l-p:0.1355128139257431
epoch£º713	 i:3 	 global-step:14263	 l-p:0.13209643959999084
epoch£º713	 i:4 	 global-step:14264	 l-p:0.1563079059123993
epoch£º713	 i:5 	 global-step:14265	 l-p:0.14128516614437103
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12037748098373413
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12339802831411362
epoch£º713	 i:8 	 global-step:14268	 l-p:0.06646884977817535
epoch£º713	 i:9 	 global-step:14269	 l-p:0.1828322559595108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1038, 5.1038, 5.1038],
        [5.1038, 5.0385, 4.7668],
        [5.1038, 4.9329, 4.6874],
        [5.1038, 5.0996, 5.1036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15016379952430725 
model_pd.l_d.mean(): -20.602928161621094 
model_pd.lagr.mean(): -20.4527645111084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4494], device='cuda:0')), ('power', tensor([-21.2870], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15016379952430725
epoch£º714	 i:1 	 global-step:14281	 l-p:0.16957080364227295
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13802848756313324
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14234864711761475
epoch£º714	 i:4 	 global-step:14284	 l-p:0.1959129422903061
epoch£º714	 i:5 	 global-step:14285	 l-p:0.19805896282196045
epoch£º714	 i:6 	 global-step:14286	 l-p:0.10942807048559189
epoch£º714	 i:7 	 global-step:14287	 l-p:0.10063352435827255
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12565192580223083
epoch£º714	 i:9 	 global-step:14289	 l-p:0.09810536354780197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0429, 5.0308, 5.0417],
        [5.0429, 5.4048, 5.3338],
        [5.0429, 4.8177, 4.6757],
        [5.0429, 5.0429, 5.0429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.1397274136543274 
model_pd.l_d.mean(): -20.393714904785156 
model_pd.lagr.mean(): -20.25398826599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4870], device='cuda:0')), ('power', tensor([-21.1140], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.1397274136543274
epoch£º715	 i:1 	 global-step:14301	 l-p:0.21212802827358246
epoch£º715	 i:2 	 global-step:14302	 l-p:0.12187108397483826
epoch£º715	 i:3 	 global-step:14303	 l-p:0.1613396853208542
epoch£º715	 i:4 	 global-step:14304	 l-p:0.19242888689041138
epoch£º715	 i:5 	 global-step:14305	 l-p:0.19757512211799622
epoch£º715	 i:6 	 global-step:14306	 l-p:0.13150493800640106
epoch£º715	 i:7 	 global-step:14307	 l-p:0.728411853313446
epoch£º715	 i:8 	 global-step:14308	 l-p:0.13308101892471313
epoch£º715	 i:9 	 global-step:14309	 l-p:0.12948092818260193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0397, 4.9178, 4.6429],
        [5.0397, 5.0371, 5.0396],
        [5.0397, 5.0397, 5.0397],
        [5.0397, 5.3807, 5.2961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.2093917280435562 
model_pd.l_d.mean(): -20.360212326049805 
model_pd.lagr.mean(): -20.150819778442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5055], device='cuda:0')), ('power', tensor([-21.0991], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.2093917280435562
epoch£º716	 i:1 	 global-step:14321	 l-p:0.13966506719589233
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14436449110507965
epoch£º716	 i:3 	 global-step:14323	 l-p:0.13142862915992737
epoch£º716	 i:4 	 global-step:14324	 l-p:0.24688291549682617
epoch£º716	 i:5 	 global-step:14325	 l-p:0.12354213744401932
epoch£º716	 i:6 	 global-step:14326	 l-p:0.13445888459682465
epoch£º716	 i:7 	 global-step:14327	 l-p:0.1715564727783203
epoch£º716	 i:8 	 global-step:14328	 l-p:0.08634591847658157
epoch£º716	 i:9 	 global-step:14329	 l-p:0.13215312361717224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0829, 4.9818, 5.0301],
        [5.0829, 4.9542, 4.9976],
        [5.0829, 5.0776, 5.0826],
        [5.0829, 5.1998, 4.9846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.1484331488609314 
model_pd.l_d.mean(): -20.862932205200195 
model_pd.lagr.mean(): -20.71449851989746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4175], device='cuda:0')), ('power', tensor([-21.5174], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.1484331488609314
epoch£º717	 i:1 	 global-step:14341	 l-p:0.1088259294629097
epoch£º717	 i:2 	 global-step:14342	 l-p:0.1981104463338852
epoch£º717	 i:3 	 global-step:14343	 l-p:0.10330434888601303
epoch£º717	 i:4 	 global-step:14344	 l-p:0.1270606368780136
epoch£º717	 i:5 	 global-step:14345	 l-p:0.06477777659893036
epoch£º717	 i:6 	 global-step:14346	 l-p:0.15627244114875793
epoch£º717	 i:7 	 global-step:14347	 l-p:0.18486030399799347
epoch£º717	 i:8 	 global-step:14348	 l-p:0.22152522206306458
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12032397836446762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0775, 4.8766, 4.8460],
        [5.0775, 4.9116, 4.9319],
        [5.0775, 5.4857, 5.4441],
        [5.0775, 4.9392, 4.9788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.13017848134040833 
model_pd.l_d.mean(): -20.868545532226562 
model_pd.lagr.mean(): -20.738367080688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.5173], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.13017848134040833
epoch£º718	 i:1 	 global-step:14361	 l-p:0.1012013629078865
epoch£º718	 i:2 	 global-step:14362	 l-p:0.19645388424396515
epoch£º718	 i:3 	 global-step:14363	 l-p:0.14867502450942993
epoch£º718	 i:4 	 global-step:14364	 l-p:0.1717890202999115
epoch£º718	 i:5 	 global-step:14365	 l-p:0.10498275607824326
epoch£º718	 i:6 	 global-step:14366	 l-p:0.1232772246003151
epoch£º718	 i:7 	 global-step:14367	 l-p:0.10608888417482376
epoch£º718	 i:8 	 global-step:14368	 l-p:0.2474013864994049
epoch£º718	 i:9 	 global-step:14369	 l-p:0.11857577413320541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0680, 4.9188, 4.9529],
        [5.0680, 4.9737, 4.6975],
        [5.0680, 5.1457, 4.9124],
        [5.0680, 5.0456, 5.0646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.12543126940727234 
model_pd.l_d.mean(): -18.66492462158203 
model_pd.lagr.mean(): -18.539493560791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6018], device='cuda:0')), ('power', tensor([-19.4836], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.12543126940727234
epoch£º719	 i:1 	 global-step:14381	 l-p:0.18489661812782288
epoch£º719	 i:2 	 global-step:14382	 l-p:0.13070224225521088
epoch£º719	 i:3 	 global-step:14383	 l-p:0.1038258969783783
epoch£º719	 i:4 	 global-step:14384	 l-p:0.14019426703453064
epoch£º719	 i:5 	 global-step:14385	 l-p:0.17206279933452606
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13613659143447876
epoch£º719	 i:7 	 global-step:14387	 l-p:0.10261981934309006
epoch£º719	 i:8 	 global-step:14388	 l-p:0.15602901577949524
epoch£º719	 i:9 	 global-step:14389	 l-p:0.1440373957157135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1076, 5.0381, 5.0815],
        [5.1076, 4.9080, 4.8765],
        [5.1076, 4.9300, 4.6872],
        [5.1076, 4.8891, 4.7896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.11280009895563126 
model_pd.l_d.mean(): -20.357044219970703 
model_pd.lagr.mean(): -20.244243621826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.0526], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.11280009895563126
epoch£º720	 i:1 	 global-step:14401	 l-p:0.12546765804290771
epoch£º720	 i:2 	 global-step:14402	 l-p:0.056373849511146545
epoch£º720	 i:3 	 global-step:14403	 l-p:0.137324258685112
epoch£º720	 i:4 	 global-step:14404	 l-p:0.1228649839758873
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13948491215705872
epoch£º720	 i:6 	 global-step:14406	 l-p:0.14224058389663696
epoch£º720	 i:7 	 global-step:14407	 l-p:0.18843208253383636
epoch£º720	 i:8 	 global-step:14408	 l-p:0.14537547528743744
epoch£º720	 i:9 	 global-step:14409	 l-p:0.11912635713815689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.1128, 4.9750, 4.7091],
        [5.1128, 5.5309, 5.4943],
        [5.1128, 4.8926, 4.7730],
        [5.1128, 4.8966, 4.7249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11709194630384445 
model_pd.l_d.mean(): -19.609195709228516 
model_pd.lagr.mean(): -19.492103576660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5086], device='cuda:0')), ('power', tensor([-20.3430], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11709194630384445
epoch£º721	 i:1 	 global-step:14421	 l-p:0.0878872498869896
epoch£º721	 i:2 	 global-step:14422	 l-p:0.1436680555343628
epoch£º721	 i:3 	 global-step:14423	 l-p:0.14862625300884247
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10740217566490173
epoch£º721	 i:5 	 global-step:14425	 l-p:0.12989282608032227
epoch£º721	 i:6 	 global-step:14426	 l-p:0.1578008383512497
epoch£º721	 i:7 	 global-step:14427	 l-p:0.12104154378175735
epoch£º721	 i:8 	 global-step:14428	 l-p:0.1363993138074875
epoch£º721	 i:9 	 global-step:14429	 l-p:0.17755328118801117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0799, 5.0799, 5.0799],
        [5.0799, 5.0774, 5.0798],
        [5.0799, 5.0799, 5.0799],
        [5.0799, 5.0328, 5.0672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.10092058777809143 
model_pd.l_d.mean(): -20.227373123168945 
model_pd.lagr.mean(): -20.126453399658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5007], device='cuda:0')), ('power', tensor([-20.9598], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.10092058777809143
epoch£º722	 i:1 	 global-step:14441	 l-p:0.13840675354003906
epoch£º722	 i:2 	 global-step:14442	 l-p:0.16315622627735138
epoch£º722	 i:3 	 global-step:14443	 l-p:0.1316278576850891
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13755740225315094
epoch£º722	 i:5 	 global-step:14445	 l-p:0.2818394601345062
epoch£º722	 i:6 	 global-step:14446	 l-p:0.122107595205307
epoch£º722	 i:7 	 global-step:14447	 l-p:0.13401541113853455
epoch£º722	 i:8 	 global-step:14448	 l-p:0.13834379613399506
epoch£º722	 i:9 	 global-step:14449	 l-p:-0.32824721932411194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0080, 4.9426, 4.9853],
        [5.0080, 4.9059, 4.6242],
        [5.0080, 5.0079, 5.0080],
        [5.0080, 5.0081, 5.0081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.11862535029649734 
model_pd.l_d.mean(): -20.53913688659668 
model_pd.lagr.mean(): -20.42051124572754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.2661], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.11862535029649734
epoch£º723	 i:1 	 global-step:14461	 l-p:0.21431544423103333
epoch£º723	 i:2 	 global-step:14462	 l-p:0.26327764987945557
epoch£º723	 i:3 	 global-step:14463	 l-p:0.13574153184890747
epoch£º723	 i:4 	 global-step:14464	 l-p:-0.7658215165138245
epoch£º723	 i:5 	 global-step:14465	 l-p:0.2145976424217224
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10782331228256226
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12821578979492188
epoch£º723	 i:8 	 global-step:14468	 l-p:-0.1908426284790039
epoch£º723	 i:9 	 global-step:14469	 l-p:0.16279356181621552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0249, 5.0221, 5.0248],
        [5.0249, 4.9586, 4.6796],
        [5.0249, 5.0130, 5.0237],
        [5.0249, 4.8075, 4.7498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.12155309319496155 
model_pd.l_d.mean(): -19.828893661499023 
model_pd.lagr.mean(): -19.707340240478516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.5747], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.12155309319496155
epoch£º724	 i:1 	 global-step:14481	 l-p:0.11665766686201096
epoch£º724	 i:2 	 global-step:14482	 l-p:0.2519576847553253
epoch£º724	 i:3 	 global-step:14483	 l-p:0.15452684462070465
epoch£º724	 i:4 	 global-step:14484	 l-p:0.12361247837543488
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14853821694850922
epoch£º724	 i:6 	 global-step:14486	 l-p:0.13372348248958588
epoch£º724	 i:7 	 global-step:14487	 l-p:0.15438427031040192
epoch£º724	 i:8 	 global-step:14488	 l-p:0.24363262951374054
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10665679723024368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0753, 5.0711, 5.0751],
        [5.0753, 4.9291, 4.6605],
        [5.0753, 4.8919, 4.6456],
        [5.0753, 4.9450, 4.9888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13211335241794586 
model_pd.l_d.mean(): -20.574909210205078 
model_pd.lagr.mean(): -20.44279670715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-21.2432], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13211335241794586
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12688209116458893
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13120503723621368
epoch£º725	 i:3 	 global-step:14503	 l-p:0.1684381067752838
epoch£º725	 i:4 	 global-step:14504	 l-p:0.13166867196559906
epoch£º725	 i:5 	 global-step:14505	 l-p:0.32215407490730286
epoch£º725	 i:6 	 global-step:14506	 l-p:0.20836810767650604
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10478409379720688
epoch£º725	 i:8 	 global-step:14508	 l-p:0.11380406469106674
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12641820311546326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0346, 4.9097, 4.9564],
        [5.0346, 4.8043, 4.6720],
        [5.0346, 4.9875, 4.7113],
        [5.0346, 5.2328, 5.0596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.19460569322109222 
model_pd.l_d.mean(): -19.22627067565918 
model_pd.lagr.mean(): -19.031665802001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5736], device='cuda:0')), ('power', tensor([-20.0222], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.19460569322109222
epoch£º726	 i:1 	 global-step:14521	 l-p:0.1928899884223938
epoch£º726	 i:2 	 global-step:14522	 l-p:0.20378512144088745
epoch£º726	 i:3 	 global-step:14523	 l-p:0.18944379687309265
epoch£º726	 i:4 	 global-step:14524	 l-p:0.15017499029636383
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13113276660442352
epoch£º726	 i:6 	 global-step:14526	 l-p:0.23286032676696777
epoch£º726	 i:7 	 global-step:14527	 l-p:0.06848352402448654
epoch£º726	 i:8 	 global-step:14528	 l-p:0.15218350291252136
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12266033887863159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0846, 5.0846, 5.0846],
        [5.0846, 5.2313, 5.0291],
        [5.0846, 5.4940, 5.4517],
        [5.0846, 5.0846, 5.0846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.1596946120262146 
model_pd.l_d.mean(): -20.530887603759766 
model_pd.lagr.mean(): -20.371192932128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.2390], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.1596946120262146
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12378857284784317
epoch£º727	 i:2 	 global-step:14542	 l-p:0.14235801994800568
epoch£º727	 i:3 	 global-step:14543	 l-p:0.11537114530801773
epoch£º727	 i:4 	 global-step:14544	 l-p:0.14086835086345673
epoch£º727	 i:5 	 global-step:14545	 l-p:0.2159106433391571
epoch£º727	 i:6 	 global-step:14546	 l-p:0.12783333659172058
epoch£º727	 i:7 	 global-step:14547	 l-p:0.16520047187805176
epoch£º727	 i:8 	 global-step:14548	 l-p:0.15148374438285828
epoch£º727	 i:9 	 global-step:14549	 l-p:0.07391763478517532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1149, 5.1015, 5.1135],
        [5.1149, 5.1142, 5.1149],
        [5.1149, 4.9347, 4.6903],
        [5.1149, 5.5651, 5.5496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.13051855564117432 
model_pd.l_d.mean(): -20.468774795532227 
model_pd.lagr.mean(): -20.3382568359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4609], device='cuda:0')), ('power', tensor([-21.1632], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.13051855564117432
epoch£º728	 i:1 	 global-step:14561	 l-p:0.10162603110074997
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12586410343647003
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1448453664779663
epoch£º728	 i:4 	 global-step:14564	 l-p:0.136239692568779
epoch£º728	 i:5 	 global-step:14565	 l-p:0.1322227418422699
epoch£º728	 i:6 	 global-step:14566	 l-p:2.4106369018554688
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13638171553611755
epoch£º728	 i:8 	 global-step:14568	 l-p:0.11150794476270676
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10078893601894379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1916, 5.0569, 5.0960],
        [5.1916, 5.1190, 5.1631],
        [5.1916, 5.0293, 5.0487],
        [5.1916, 5.1916, 5.1916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.46815812587738037 
model_pd.l_d.mean(): -19.306673049926758 
model_pd.lagr.mean(): -18.83851432800293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5640], device='cuda:0')), ('power', tensor([-20.0937], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.46815812587738037
epoch£º729	 i:1 	 global-step:14581	 l-p:0.11121492087841034
epoch£º729	 i:2 	 global-step:14582	 l-p:0.15789109468460083
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12699256837368011
epoch£º729	 i:4 	 global-step:14584	 l-p:0.13865439593791962
epoch£º729	 i:5 	 global-step:14585	 l-p:0.11231038719415665
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12440855801105499
epoch£º729	 i:7 	 global-step:14587	 l-p:0.021097226068377495
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13448478281497955
epoch£º729	 i:9 	 global-step:14589	 l-p:0.11922546476125717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1294, 5.0549, 4.7794],
        [5.1294, 5.3415, 5.1732],
        [5.1294, 4.9553, 4.7070],
        [5.1294, 4.9645, 4.7096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.0363045260310173 
model_pd.l_d.mean(): -19.29509162902832 
model_pd.lagr.mean(): -19.258787155151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5506], device='cuda:0')), ('power', tensor([-20.0684], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.0363045260310173
epoch£º730	 i:1 	 global-step:14601	 l-p:0.1375015377998352
epoch£º730	 i:2 	 global-step:14602	 l-p:0.17054717242717743
epoch£º730	 i:3 	 global-step:14603	 l-p:0.12341398000717163
epoch£º730	 i:4 	 global-step:14604	 l-p:0.14264725148677826
epoch£º730	 i:5 	 global-step:14605	 l-p:0.13344934582710266
epoch£º730	 i:6 	 global-step:14606	 l-p:0.136253222823143
epoch£º730	 i:7 	 global-step:14607	 l-p:0.13877543807029724
epoch£º730	 i:8 	 global-step:14608	 l-p:0.27776429057121277
epoch£º730	 i:9 	 global-step:14609	 l-p:0.0901612713932991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0424, 5.3510, 5.2432],
        [5.0424, 4.8599, 4.8682],
        [5.0424, 4.8113, 4.6681],
        [5.0424, 5.0315, 5.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.11905547976493835 
model_pd.l_d.mean(): -20.273164749145508 
model_pd.lagr.mean(): -20.154109954833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5160], device='cuda:0')), ('power', tensor([-21.0217], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.11905547976493835
epoch£º731	 i:1 	 global-step:14621	 l-p:0.12918488681316376
epoch£º731	 i:2 	 global-step:14622	 l-p:0.1959795206785202
epoch£º731	 i:3 	 global-step:14623	 l-p:0.23542441427707672
epoch£º731	 i:4 	 global-step:14624	 l-p:0.13741862773895264
epoch£º731	 i:5 	 global-step:14625	 l-p:0.23441365361213684
epoch£º731	 i:6 	 global-step:14626	 l-p:-0.014712867327034473
epoch£º731	 i:7 	 global-step:14627	 l-p:1.4504406452178955
epoch£º731	 i:8 	 global-step:14628	 l-p:0.026672443374991417
epoch£º731	 i:9 	 global-step:14629	 l-p:0.12014331668615341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9967, 4.9196, 4.6359],
        [4.9967, 4.9943, 4.9966],
        [4.9967, 4.9967, 4.9967],
        [4.9967, 4.9929, 4.9965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.3592141568660736 
model_pd.l_d.mean(): -19.5334415435791 
model_pd.lagr.mean(): -19.174226760864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5546], device='cuda:0')), ('power', tensor([-20.3134], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.3592141568660736
epoch£º732	 i:1 	 global-step:14641	 l-p:0.2899807095527649
epoch£º732	 i:2 	 global-step:14642	 l-p:0.12531432509422302
epoch£º732	 i:3 	 global-step:14643	 l-p:0.1323561817407608
epoch£º732	 i:4 	 global-step:14644	 l-p:0.2765538692474365
epoch£º732	 i:5 	 global-step:14645	 l-p:0.1164400726556778
epoch£º732	 i:6 	 global-step:14646	 l-p:0.18787036836147308
epoch£º732	 i:7 	 global-step:14647	 l-p:0.09480644762516022
epoch£º732	 i:8 	 global-step:14648	 l-p:0.1377875804901123
epoch£º732	 i:9 	 global-step:14649	 l-p:0.23552049696445465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0639, 5.0639, 5.0639],
        [5.0639, 5.0519, 5.0627],
        [5.0639, 4.9858, 5.0322],
        [5.0639, 5.0311, 5.0573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.1326918601989746 
model_pd.l_d.mean(): -19.852703094482422 
model_pd.lagr.mean(): -19.72001075744629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5147], device='cuda:0')), ('power', tensor([-20.5954], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.1326918601989746
epoch£º733	 i:1 	 global-step:14661	 l-p:0.106136254966259
epoch£º733	 i:2 	 global-step:14662	 l-p:0.2203572541475296
epoch£º733	 i:3 	 global-step:14663	 l-p:0.16989870369434357
epoch£º733	 i:4 	 global-step:14664	 l-p:0.06973075121641159
epoch£º733	 i:5 	 global-step:14665	 l-p:0.1495305448770523
epoch£º733	 i:6 	 global-step:14666	 l-p:0.16391699016094208
epoch£º733	 i:7 	 global-step:14667	 l-p:0.10659205913543701
epoch£º733	 i:8 	 global-step:14668	 l-p:0.1642753928899765
epoch£º733	 i:9 	 global-step:14669	 l-p:0.1058274433016777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1299, 5.1287, 5.1299],
        [5.1299, 5.2515, 5.0353],
        [5.1299, 5.4700, 5.3792],
        [5.1299, 4.9299, 4.7091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.15553675591945648 
model_pd.l_d.mean(): -20.436145782470703 
model_pd.lagr.mean(): -20.280609130859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4555], device='cuda:0')), ('power', tensor([-21.1248], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.15553675591945648
epoch£º734	 i:1 	 global-step:14681	 l-p:0.11071257293224335
epoch£º734	 i:2 	 global-step:14682	 l-p:0.10220371931791306
epoch£º734	 i:3 	 global-step:14683	 l-p:0.16135644912719727
epoch£º734	 i:4 	 global-step:14684	 l-p:0.10988075286149979
epoch£º734	 i:5 	 global-step:14685	 l-p:0.13949905335903168
epoch£º734	 i:6 	 global-step:14686	 l-p:0.07057078927755356
epoch£º734	 i:7 	 global-step:14687	 l-p:0.10580369085073471
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13122418522834778
epoch£º734	 i:9 	 global-step:14689	 l-p:0.22872573137283325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0938, 4.8691, 4.7692],
        [5.0938, 5.0107, 5.0581],
        [5.0938, 5.0038, 4.7244],
        [5.0938, 4.9734, 4.6962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.14546500146389008 
model_pd.l_d.mean(): -20.02168083190918 
model_pd.lagr.mean(): -19.8762149810791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4770], device='cuda:0')), ('power', tensor([-20.7277], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.14546500146389008
epoch£º735	 i:1 	 global-step:14701	 l-p:0.09708014875650406
epoch£º735	 i:2 	 global-step:14702	 l-p:0.10527530312538147
epoch£º735	 i:3 	 global-step:14703	 l-p:0.13115283846855164
epoch£º735	 i:4 	 global-step:14704	 l-p:0.2447366714477539
epoch£º735	 i:5 	 global-step:14705	 l-p:0.1343991756439209
epoch£º735	 i:6 	 global-step:14706	 l-p:0.12387681752443314
epoch£º735	 i:7 	 global-step:14707	 l-p:0.1608649343252182
epoch£º735	 i:8 	 global-step:14708	 l-p:0.15113772451877594
epoch£º735	 i:9 	 global-step:14709	 l-p:0.24903467297554016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0688, 5.3971, 5.3002],
        [5.0688, 5.0022, 5.0453],
        [5.0688, 4.8390, 4.6863],
        [5.0688, 4.8386, 4.6948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16627836227416992 
model_pd.l_d.mean(): -20.448606491088867 
model_pd.lagr.mean(): -20.28232765197754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-21.1742], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16627836227416992
epoch£º736	 i:1 	 global-step:14721	 l-p:0.10980038344860077
epoch£º736	 i:2 	 global-step:14722	 l-p:0.18974170088768005
epoch£º736	 i:3 	 global-step:14723	 l-p:0.13887546956539154
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09401903301477432
epoch£º736	 i:5 	 global-step:14725	 l-p:0.1292983740568161
epoch£º736	 i:6 	 global-step:14726	 l-p:0.1371125429868698
epoch£º736	 i:7 	 global-step:14727	 l-p:0.1277155727148056
epoch£º736	 i:8 	 global-step:14728	 l-p:0.09211250394582748
epoch£º736	 i:9 	 global-step:14729	 l-p:0.16535943746566772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 5.0895, 5.1159],
        [5.1227, 5.5653, 5.5427],
        [5.1227, 5.0214, 4.7434],
        [5.1227, 5.4972, 5.4284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12242227047681808 
model_pd.l_d.mean(): -20.681140899658203 
model_pd.lagr.mean(): -20.558717727661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-21.3309], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12242227047681808
epoch£º737	 i:1 	 global-step:14741	 l-p:0.18601280450820923
epoch£º737	 i:2 	 global-step:14742	 l-p:0.06099925935268402
epoch£º737	 i:3 	 global-step:14743	 l-p:0.10671079158782959
epoch£º737	 i:4 	 global-step:14744	 l-p:0.16190677881240845
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12587518990039825
epoch£º737	 i:6 	 global-step:14746	 l-p:0.13274738192558289
epoch£º737	 i:7 	 global-step:14747	 l-p:0.12937474250793457
epoch£º737	 i:8 	 global-step:14748	 l-p:0.13966681063175201
epoch£º737	 i:9 	 global-step:14749	 l-p:0.11746826767921448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1164, 5.1042, 5.1152],
        [5.1164, 5.1139, 5.1163],
        [5.1164, 5.1157, 5.1164],
        [5.1164, 5.1164, 5.1164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.10272013396024704 
model_pd.l_d.mean(): -20.27031898498535 
model_pd.lagr.mean(): -20.167598724365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.9873], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.10272013396024704
epoch£º738	 i:1 	 global-step:14761	 l-p:0.13593938946723938
epoch£º738	 i:2 	 global-step:14762	 l-p:0.09051450341939926
epoch£º738	 i:3 	 global-step:14763	 l-p:0.14511370658874512
epoch£º738	 i:4 	 global-step:14764	 l-p:0.1287606805562973
epoch£º738	 i:5 	 global-step:14765	 l-p:0.14703480899333954
epoch£º738	 i:6 	 global-step:14766	 l-p:0.14574097096920013
epoch£º738	 i:7 	 global-step:14767	 l-p:0.15390335023403168
epoch£º738	 i:8 	 global-step:14768	 l-p:0.15256118774414062
epoch£º738	 i:9 	 global-step:14769	 l-p:0.17287111282348633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0865, 5.0865, 5.0865],
        [5.0865, 5.0865, 5.0865],
        [5.0865, 5.0198, 4.7391],
        [5.0865, 4.9533, 4.9977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.15441462397575378 
model_pd.l_d.mean(): -19.72365379333496 
model_pd.lagr.mean(): -19.569238662719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5320], device='cuda:0')), ('power', tensor([-20.4826], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.15441462397575378
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13085535168647766
epoch£º739	 i:2 	 global-step:14782	 l-p:0.1816604733467102
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15819308161735535
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12607957422733307
epoch£º739	 i:5 	 global-step:14785	 l-p:0.17658358812332153
epoch£º739	 i:6 	 global-step:14786	 l-p:0.08817397803068161
epoch£º739	 i:7 	 global-step:14787	 l-p:0.13140495121479034
epoch£º739	 i:8 	 global-step:14788	 l-p:0.06546900421380997
epoch£º739	 i:9 	 global-step:14789	 l-p:0.09158477932214737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1467, 5.1467, 5.1467],
        [5.1467, 5.1355, 5.1457],
        [5.1467, 4.9363, 4.8860],
        [5.1467, 4.9443, 4.7232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.1295478641986847 
model_pd.l_d.mean(): -19.217910766601562 
model_pd.lagr.mean(): -19.088363647460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5253], device='cuda:0')), ('power', tensor([-19.9645], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.1295478641986847
epoch£º740	 i:1 	 global-step:14801	 l-p:0.05061262100934982
epoch£º740	 i:2 	 global-step:14802	 l-p:0.10027331858873367
epoch£º740	 i:3 	 global-step:14803	 l-p:0.06205141544342041
epoch£º740	 i:4 	 global-step:14804	 l-p:0.1350604146718979
epoch£º740	 i:5 	 global-step:14805	 l-p:0.12836605310440063
epoch£º740	 i:6 	 global-step:14806	 l-p:0.1436755657196045
epoch£º740	 i:7 	 global-step:14807	 l-p:0.14680856466293335
epoch£º740	 i:8 	 global-step:14808	 l-p:0.13330109417438507
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15159255266189575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1352, 4.9712, 4.7094],
        [5.1352, 5.5504, 5.5076],
        [5.1352, 4.9793, 4.7133],
        [5.1352, 5.1352, 5.1352]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.1163594052195549 
model_pd.l_d.mean(): -20.33555793762207 
model_pd.lagr.mean(): -20.21919822692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4815], device='cuda:0')), ('power', tensor([-21.0496], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.1163594052195549
epoch£º741	 i:1 	 global-step:14821	 l-p:0.10747715085744858
epoch£º741	 i:2 	 global-step:14822	 l-p:0.1280948370695114
epoch£º741	 i:3 	 global-step:14823	 l-p:0.15818080306053162
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07550308853387833
epoch£º741	 i:5 	 global-step:14825	 l-p:0.11563734710216522
epoch£º741	 i:6 	 global-step:14826	 l-p:0.14828692376613617
epoch£º741	 i:7 	 global-step:14827	 l-p:0.14906997978687286
epoch£º741	 i:8 	 global-step:14828	 l-p:0.16703177988529205
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14921385049819946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0794, 5.4779, 5.4256],
        [5.0794, 4.9522, 4.9990],
        [5.0794, 5.0753, 5.0792],
        [5.0794, 4.8518, 4.7619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.1387181282043457 
model_pd.l_d.mean(): -19.848636627197266 
model_pd.lagr.mean(): -19.709918975830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5533], device='cuda:0')), ('power', tensor([-20.6307], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.1387181282043457
epoch£º742	 i:1 	 global-step:14841	 l-p:0.12054010480642319
epoch£º742	 i:2 	 global-step:14842	 l-p:0.11584068089723587
epoch£º742	 i:3 	 global-step:14843	 l-p:0.1343057006597519
epoch£º742	 i:4 	 global-step:14844	 l-p:0.2481287270784378
epoch£º742	 i:5 	 global-step:14845	 l-p:0.14702054858207703
epoch£º742	 i:6 	 global-step:14846	 l-p:0.2182610034942627
epoch£º742	 i:7 	 global-step:14847	 l-p:0.12323205918073654
epoch£º742	 i:8 	 global-step:14848	 l-p:0.07345651090145111
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12552765011787415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1032, 5.1032, 5.1032],
        [5.1032, 5.0384, 5.0809],
        [5.1032, 5.0614, 4.7838],
        [5.1032, 4.9264, 4.6667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.06180884689092636 
model_pd.l_d.mean(): -19.886089324951172 
model_pd.lagr.mean(): -19.82427978515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5227], device='cuda:0')), ('power', tensor([-20.6373], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.06180884689092636
epoch£º743	 i:1 	 global-step:14861	 l-p:0.12944231927394867
epoch£º743	 i:2 	 global-step:14862	 l-p:0.1375531107187271
epoch£º743	 i:3 	 global-step:14863	 l-p:0.13754630088806152
epoch£º743	 i:4 	 global-step:14864	 l-p:0.13011720776557922
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12619757652282715
epoch£º743	 i:6 	 global-step:14866	 l-p:0.16226401925086975
epoch£º743	 i:7 	 global-step:14867	 l-p:0.14094839990139008
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12342163920402527
epoch£º743	 i:9 	 global-step:14869	 l-p:0.15802651643753052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.0293, 5.0773],
        [5.1141, 4.9792, 5.0229],
        [5.1141, 5.0308, 5.0785],
        [5.1141, 5.0758, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.1322755068540573 
model_pd.l_d.mean(): -20.913944244384766 
model_pd.lagr.mean(): -20.78166961669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900], device='cuda:0')), ('power', tensor([-21.5409], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.1322755068540573
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13007140159606934
epoch£º744	 i:2 	 global-step:14882	 l-p:0.10856802761554718
epoch£º744	 i:3 	 global-step:14883	 l-p:0.10957589000463486
epoch£º744	 i:4 	 global-step:14884	 l-p:0.20357085764408112
epoch£º744	 i:5 	 global-step:14885	 l-p:0.09091816842556
epoch£º744	 i:6 	 global-step:14886	 l-p:0.15008537471294403
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13302750885486603
epoch£º744	 i:8 	 global-step:14888	 l-p:2.710134744644165
epoch£º744	 i:9 	 global-step:14889	 l-p:0.26483532786369324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0294, 4.9958, 5.0226],
        [5.0294, 5.0293, 5.0294],
        [5.0294, 5.4329, 5.3850],
        [5.0294, 4.8711, 4.5912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.26087963581085205 
model_pd.l_d.mean(): -19.69414710998535 
model_pd.lagr.mean(): -19.43326759338379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5520], device='cuda:0')), ('power', tensor([-20.4732], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.26087963581085205
epoch£º745	 i:1 	 global-step:14901	 l-p:0.1377810835838318
epoch£º745	 i:2 	 global-step:14902	 l-p:0.25995633006095886
epoch£º745	 i:3 	 global-step:14903	 l-p:0.10931960493326187
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13365091383457184
epoch£º745	 i:5 	 global-step:14905	 l-p:0.12099053710699081
epoch£º745	 i:6 	 global-step:14906	 l-p:0.31012842059135437
epoch£º745	 i:7 	 global-step:14907	 l-p:0.15585772693157196
epoch£º745	 i:8 	 global-step:14908	 l-p:0.18132737278938293
epoch£º745	 i:9 	 global-step:14909	 l-p:0.11844595521688461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0802, 5.0736, 5.0798],
        [5.0802, 5.0705, 5.0794],
        [5.0802, 5.1222, 4.8684],
        [5.0802, 5.0802, 5.0802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.1461026966571808 
model_pd.l_d.mean(): -19.261442184448242 
model_pd.lagr.mean(): -19.115339279174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5534], device='cuda:0')), ('power', tensor([-20.0372], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.1461026966571808
epoch£º746	 i:1 	 global-step:14921	 l-p:0.20057575404644012
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12727445363998413
epoch£º746	 i:3 	 global-step:14923	 l-p:0.13307996094226837
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12438458949327469
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12826162576675415
epoch£º746	 i:6 	 global-step:14926	 l-p:0.13462696969509125
epoch£º746	 i:7 	 global-step:14927	 l-p:0.10769372433423996
epoch£º746	 i:8 	 global-step:14928	 l-p:0.11034189909696579
epoch£º746	 i:9 	 global-step:14929	 l-p:0.12128710746765137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1377, 5.0844, 5.1221],
        [5.1377, 5.0994, 5.1290],
        [5.1377, 5.1438, 4.8780],
        [5.1377, 5.1645, 4.9054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18140250444412231 
model_pd.l_d.mean(): -19.936363220214844 
model_pd.lagr.mean(): -19.754961013793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.6654], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18140250444412231
epoch£º747	 i:1 	 global-step:14941	 l-p:0.1636463850736618
epoch£º747	 i:2 	 global-step:14942	 l-p:0.13582167029380798
epoch£º747	 i:3 	 global-step:14943	 l-p:0.10498834401369095
epoch£º747	 i:4 	 global-step:14944	 l-p:0.13269931077957153
epoch£º747	 i:5 	 global-step:14945	 l-p:0.10054809600114822
epoch£º747	 i:6 	 global-step:14946	 l-p:0.1536608785390854
epoch£º747	 i:7 	 global-step:14947	 l-p:0.08300492912530899
epoch£º747	 i:8 	 global-step:14948	 l-p:0.15004855394363403
epoch£º747	 i:9 	 global-step:14949	 l-p:0.14315463602542877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0781, 4.8841, 4.8816],
        [5.0781, 5.4724, 5.4160],
        [5.0781, 4.9674, 5.0176],
        [5.0781, 4.8694, 4.8424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.16270825266838074 
model_pd.l_d.mean(): -20.73686408996582 
model_pd.lagr.mean(): -20.574155807495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-21.4127], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.16270825266838074
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15160921216011047
epoch£º748	 i:2 	 global-step:14962	 l-p:0.17967258393764496
epoch£º748	 i:3 	 global-step:14963	 l-p:0.13069234788417816
epoch£º748	 i:4 	 global-step:14964	 l-p:0.09194742143154144
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2517404556274414
epoch£º748	 i:6 	 global-step:14966	 l-p:0.10222861170768738
epoch£º748	 i:7 	 global-step:14967	 l-p:0.16697078943252563
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12526565790176392
epoch£º748	 i:9 	 global-step:14969	 l-p:0.1247231587767601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0797, 5.4343, 5.3513],
        [5.0797, 5.0565, 5.0761],
        [5.0797, 5.0523, 5.0749],
        [5.0797, 4.8541, 4.6501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.13699927926063538 
model_pd.l_d.mean(): -20.092592239379883 
model_pd.lagr.mean(): -19.95559310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5259], device='cuda:0')), ('power', tensor([-20.8494], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.13699927926063538
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12981028854846954
epoch£º749	 i:2 	 global-step:14982	 l-p:0.1306646466255188
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13431483507156372
epoch£º749	 i:4 	 global-step:14984	 l-p:0.1093737930059433
epoch£º749	 i:5 	 global-step:14985	 l-p:0.1519581824541092
epoch£º749	 i:6 	 global-step:14986	 l-p:0.27275797724723816
epoch£º749	 i:7 	 global-step:14987	 l-p:0.19786357879638672
epoch£º749	 i:8 	 global-step:14988	 l-p:0.14305919408798218
epoch£º749	 i:9 	 global-step:14989	 l-p:0.12426155060529709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0822, 4.9857, 4.6991],
        [5.0822, 5.3952, 5.2853],
        [5.0822, 5.0777, 5.0819],
        [5.0822, 5.0767, 5.0819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11980924010276794 
model_pd.l_d.mean(): -20.413450241088867 
model_pd.lagr.mean(): -20.29364013671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4768], device='cuda:0')), ('power', tensor([-21.1235], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11980924010276794
epoch£º750	 i:1 	 global-step:15001	 l-p:0.19566531479358673
epoch£º750	 i:2 	 global-step:15002	 l-p:0.1974278837442398
epoch£º750	 i:3 	 global-step:15003	 l-p:0.19662311673164368
epoch£º750	 i:4 	 global-step:15004	 l-p:0.038507599383592606
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13650989532470703
epoch£º750	 i:6 	 global-step:15006	 l-p:0.1243557333946228
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12230437248945236
epoch£º750	 i:8 	 global-step:15008	 l-p:0.12397398054599762
epoch£º750	 i:9 	 global-step:15009	 l-p:0.13323275744915009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1179, 5.0049, 5.0545],
        [5.1179, 5.0508, 4.7678],
        [5.1179, 5.1179, 5.1179],
        [5.1179, 4.9464, 4.9679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.15186840295791626 
model_pd.l_d.mean(): -20.675987243652344 
model_pd.lagr.mean(): -20.524118423461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.3674], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.15186840295791626
epoch£º751	 i:1 	 global-step:15021	 l-p:0.12682093679904938
epoch£º751	 i:2 	 global-step:15022	 l-p:0.16044849157333374
epoch£º751	 i:3 	 global-step:15023	 l-p:0.17203228175640106
epoch£º751	 i:4 	 global-step:15024	 l-p:0.0809686928987503
epoch£º751	 i:5 	 global-step:15025	 l-p:0.11772201955318451
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14471910893917084
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13096436858177185
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14871706068515778
epoch£º751	 i:9 	 global-step:15029	 l-p:0.0978318601846695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1100, 5.1100, 5.1100],
        [5.1100, 5.0691, 5.1003],
        [5.1100, 4.9820, 5.0292],
        [5.1100, 5.1056, 5.1098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.105019710958004 
model_pd.l_d.mean(): -19.46246910095215 
model_pd.lagr.mean(): -19.35744857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5419], device='cuda:0')), ('power', tensor([-20.2286], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.105019710958004
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12210407108068466
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09292851388454437
epoch£º752	 i:3 	 global-step:15043	 l-p:0.12900279462337494
epoch£º752	 i:4 	 global-step:15044	 l-p:0.16970539093017578
epoch£º752	 i:5 	 global-step:15045	 l-p:0.17660808563232422
epoch£º752	 i:6 	 global-step:15046	 l-p:0.13006719946861267
epoch£º752	 i:7 	 global-step:15047	 l-p:0.1361861675977707
epoch£º752	 i:8 	 global-step:15048	 l-p:0.1291298270225525
epoch£º752	 i:9 	 global-step:15049	 l-p:0.12100864201784134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 5.1332, 5.1351],
        [5.1351, 5.1351, 5.1351],
        [5.1351, 5.0019, 5.0470],
        [5.1351, 4.9177, 4.8638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13503308594226837 
model_pd.l_d.mean(): -20.586830139160156 
model_pd.lagr.mean(): -20.451797485351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.2797], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13503308594226837
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14204102754592896
epoch£º753	 i:2 	 global-step:15062	 l-p:0.12994986772537231
epoch£º753	 i:3 	 global-step:15063	 l-p:0.15680436789989471
epoch£º753	 i:4 	 global-step:15064	 l-p:0.1549941599369049
epoch£º753	 i:5 	 global-step:15065	 l-p:0.06898441165685654
epoch£º753	 i:6 	 global-step:15066	 l-p:0.20149438083171844
epoch£º753	 i:7 	 global-step:15067	 l-p:0.1546054631471634
epoch£º753	 i:8 	 global-step:15068	 l-p:0.13978469371795654
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12746351957321167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0852, 5.0797, 5.0849],
        [5.0852, 5.0810, 5.0850],
        [5.0852, 5.1760, 4.9411],
        [5.0852, 5.0850, 5.0852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.11885661631822586 
model_pd.l_d.mean(): -20.517560958862305 
model_pd.lagr.mean(): -20.398704528808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-21.2094], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.11885661631822586
epoch£º754	 i:1 	 global-step:15081	 l-p:0.11040232330560684
epoch£º754	 i:2 	 global-step:15082	 l-p:0.1594991683959961
epoch£º754	 i:3 	 global-step:15083	 l-p:0.22338201105594635
epoch£º754	 i:4 	 global-step:15084	 l-p:0.19740994274616241
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12347614765167236
epoch£º754	 i:6 	 global-step:15086	 l-p:0.13014592230319977
epoch£º754	 i:7 	 global-step:15087	 l-p:0.15005099773406982
epoch£º754	 i:8 	 global-step:15088	 l-p:0.14524221420288086
epoch£º754	 i:9 	 global-step:15089	 l-p:0.11260582506656647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0735, 4.9084, 4.9390],
        [5.0735, 5.0312, 5.0634],
        [5.0735, 4.9662, 4.6770],
        [5.0735, 4.8433, 4.7661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10898076742887497 
model_pd.l_d.mean(): -20.1202335357666 
model_pd.lagr.mean(): -20.011253356933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5027], device='cuda:0')), ('power', tensor([-20.8536], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10898076742887497
epoch£º755	 i:1 	 global-step:15101	 l-p:0.145332470536232
epoch£º755	 i:2 	 global-step:15102	 l-p:0.1554848849773407
epoch£º755	 i:3 	 global-step:15103	 l-p:0.1699714958667755
epoch£º755	 i:4 	 global-step:15104	 l-p:0.15012143552303314
epoch£º755	 i:5 	 global-step:15105	 l-p:-0.21144485473632812
epoch£º755	 i:6 	 global-step:15106	 l-p:0.12258417159318924
epoch£º755	 i:7 	 global-step:15107	 l-p:0.14360788464546204
epoch£º755	 i:8 	 global-step:15108	 l-p:-0.2327946573495865
epoch£º755	 i:9 	 global-step:15109	 l-p:0.7130907773971558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9982, 4.8497, 4.5582],
        [4.9982, 4.9982, 4.9982],
        [4.9982, 4.7536, 4.5689],
        [4.9982, 4.7583, 4.6701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.12463557720184326 
model_pd.l_d.mean(): -20.657230377197266 
model_pd.lagr.mean(): -20.532594680786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4668], device='cuda:0')), ('power', tensor([-21.3597], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.12463557720184326
epoch£º756	 i:1 	 global-step:15121	 l-p:-0.02795327641069889
epoch£º756	 i:2 	 global-step:15122	 l-p:-1.867413878440857
epoch£º756	 i:3 	 global-step:15123	 l-p:0.2238648235797882
epoch£º756	 i:4 	 global-step:15124	 l-p:0.3312126100063324
epoch£º756	 i:5 	 global-step:15125	 l-p:0.17575760185718536
epoch£º756	 i:6 	 global-step:15126	 l-p:0.11262468248605728
epoch£º756	 i:7 	 global-step:15127	 l-p:0.14215317368507385
epoch£º756	 i:8 	 global-step:15128	 l-p:0.07964933663606644
epoch£º756	 i:9 	 global-step:15129	 l-p:0.16092853248119354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0701, 4.8672, 4.6141],
        [5.0701, 4.9611, 4.6711],
        [5.0701, 4.9342, 4.9806],
        [5.0701, 5.0701, 5.0701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.14699174463748932 
model_pd.l_d.mean(): -18.732229232788086 
model_pd.lagr.mean(): -18.585237503051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5647], device='cuda:0')), ('power', tensor([-19.5137], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.14699174463748932
epoch£º757	 i:1 	 global-step:15141	 l-p:0.15775257349014282
epoch£º757	 i:2 	 global-step:15142	 l-p:0.14925652742385864
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12070441991090775
epoch£º757	 i:4 	 global-step:15144	 l-p:0.16272109746932983
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13169141113758087
epoch£º757	 i:6 	 global-step:15146	 l-p:0.11516951024532318
epoch£º757	 i:7 	 global-step:15147	 l-p:0.07552700489759445
epoch£º757	 i:8 	 global-step:15148	 l-p:0.11472831666469574
epoch£º757	 i:9 	 global-step:15149	 l-p:0.16159258782863617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 4.9674, 4.9695],
        [5.1559, 4.9937, 5.0229],
        [5.1559, 5.0848, 5.1296],
        [5.1559, 4.9270, 4.8253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.04204900562763214 
model_pd.l_d.mean(): -20.760162353515625 
model_pd.lagr.mean(): -20.71811294555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4090], device='cuda:0')), ('power', tensor([-21.4048], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.04204900562763214
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13373231887817383
epoch£º758	 i:2 	 global-step:15162	 l-p:0.1522860825061798
epoch£º758	 i:3 	 global-step:15163	 l-p:0.09023775160312653
epoch£º758	 i:4 	 global-step:15164	 l-p:0.1689847707748413
epoch£º758	 i:5 	 global-step:15165	 l-p:0.10857478529214859
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11132737994194031
epoch£º758	 i:7 	 global-step:15167	 l-p:0.08396840840578079
epoch£º758	 i:8 	 global-step:15168	 l-p:0.03222837299108505
epoch£º758	 i:9 	 global-step:15169	 l-p:0.12878142297267914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1983, 5.1982, 5.1983],
        [5.1983, 5.1983, 5.1983],
        [5.1983, 5.0984, 4.8166],
        [5.1983, 5.1983, 5.1983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.029424451291561127 
model_pd.l_d.mean(): -19.8416805267334 
model_pd.lagr.mean(): -19.812255859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-20.5169], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.029424451291561127
epoch£º759	 i:1 	 global-step:15181	 l-p:0.14283998310565948
epoch£º759	 i:2 	 global-step:15182	 l-p:0.12304969877004623
epoch£º759	 i:3 	 global-step:15183	 l-p:0.14385229349136353
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11910130828619003
epoch£º759	 i:5 	 global-step:15185	 l-p:0.15395177900791168
epoch£º759	 i:6 	 global-step:15186	 l-p:0.08106015622615814
epoch£º759	 i:7 	 global-step:15187	 l-p:0.1284351497888565
epoch£º759	 i:8 	 global-step:15188	 l-p:-0.00740955350920558
epoch£º759	 i:9 	 global-step:15189	 l-p:0.11694428324699402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 5.1255, 5.1516],
        [5.1581, 4.9695, 4.9721],
        [5.1581, 4.9298, 4.7510],
        [5.1581, 5.0886, 5.1330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.06147502362728119 
model_pd.l_d.mean(): -19.27592658996582 
model_pd.lagr.mean(): -19.21445083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5486], device='cuda:0')), ('power', tensor([-20.0470], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.06147502362728119
epoch£º760	 i:1 	 global-step:15201	 l-p:0.1293121725320816
epoch£º760	 i:2 	 global-step:15202	 l-p:0.07500147819519043
epoch£º760	 i:3 	 global-step:15203	 l-p:0.15048161149024963
epoch£º760	 i:4 	 global-step:15204	 l-p:0.1387796252965927
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12315406650304794
epoch£º760	 i:6 	 global-step:15206	 l-p:0.09651972353458405
epoch£º760	 i:7 	 global-step:15207	 l-p:0.14279429614543915
epoch£º760	 i:8 	 global-step:15208	 l-p:0.14779308438301086
epoch£º760	 i:9 	 global-step:15209	 l-p:0.12020831555128098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1004, 5.0836, 5.0984],
        [5.1004, 4.9014, 4.6466],
        [5.1004, 4.8875, 4.8572],
        [5.1004, 5.0476, 4.7622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.14181572198867798 
model_pd.l_d.mean(): -18.436094284057617 
model_pd.lagr.mean(): -18.294279098510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6144], device='cuda:0')), ('power', tensor([-19.2651], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.14181572198867798
epoch£º761	 i:1 	 global-step:15221	 l-p:0.0965474471449852
epoch£º761	 i:2 	 global-step:15222	 l-p:0.1269751340150833
epoch£º761	 i:3 	 global-step:15223	 l-p:0.13996240496635437
epoch£º761	 i:4 	 global-step:15224	 l-p:0.13601462543010712
epoch£º761	 i:5 	 global-step:15225	 l-p:-0.02456003613770008
epoch£º761	 i:6 	 global-step:15226	 l-p:0.20188575983047485
epoch£º761	 i:7 	 global-step:15227	 l-p:0.1966802179813385
epoch£º761	 i:8 	 global-step:15228	 l-p:0.11573757231235504
epoch£º761	 i:9 	 global-step:15229	 l-p:-16.347259521484375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9801, 4.7733, 4.7685],
        [4.9801, 4.7607, 4.7356],
        [4.9801, 4.7412, 4.5164],
        [4.9801, 4.9801, 4.9801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13866698741912842 
model_pd.l_d.mean(): -20.736425399780273 
model_pd.lagr.mean(): -20.597759246826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.4508], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13866698741912842
epoch£º762	 i:1 	 global-step:15241	 l-p:-0.08713144063949585
epoch£º762	 i:2 	 global-step:15242	 l-p:0.14578811824321747
epoch£º762	 i:3 	 global-step:15243	 l-p:0.13418246805667877
epoch£º762	 i:4 	 global-step:15244	 l-p:0.13637425005435944
epoch£º762	 i:5 	 global-step:15245	 l-p:0.13336673378944397
epoch£º762	 i:6 	 global-step:15246	 l-p:0.9809394478797913
epoch£º762	 i:7 	 global-step:15247	 l-p:0.03145044669508934
epoch£º762	 i:8 	 global-step:15248	 l-p:0.7596917748451233
epoch£º762	 i:9 	 global-step:15249	 l-p:0.06989782303571701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9925, 4.7423, 4.6072],
        [4.9925, 4.9871, 4.9922],
        [4.9925, 4.9502, 4.9826],
        [4.9925, 5.1648, 4.9711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.15546198189258575 
model_pd.l_d.mean(): -19.4687442779541 
model_pd.lagr.mean(): -19.313282012939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-20.2428], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.15546198189258575
epoch£º763	 i:1 	 global-step:15261	 l-p:-0.1666892170906067
epoch£º763	 i:2 	 global-step:15262	 l-p:0.10888557136058807
epoch£º763	 i:3 	 global-step:15263	 l-p:0.12216585129499435
epoch£º763	 i:4 	 global-step:15264	 l-p:0.14719261229038239
epoch£º763	 i:5 	 global-step:15265	 l-p:0.12709084153175354
epoch£º763	 i:6 	 global-step:15266	 l-p:0.4361399710178375
epoch£º763	 i:7 	 global-step:15267	 l-p:0.15534846484661102
epoch£º763	 i:8 	 global-step:15268	 l-p:0.14587996900081635
epoch£º763	 i:9 	 global-step:15269	 l-p:0.1439717411994934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0855, 5.0086, 5.0558],
        [5.0855, 5.0817, 5.0853],
        [5.0855, 4.8622, 4.6352],
        [5.0855, 4.8709, 4.8411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.1764073669910431 
model_pd.l_d.mean(): -20.803314208984375 
model_pd.lagr.mean(): -20.626907348632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4404], device='cuda:0')), ('power', tensor([-21.4805], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.1764073669910431
epoch£º764	 i:1 	 global-step:15281	 l-p:0.10130739957094193
epoch£º764	 i:2 	 global-step:15282	 l-p:0.14720268547534943
epoch£º764	 i:3 	 global-step:15283	 l-p:0.17413291335105896
epoch£º764	 i:4 	 global-step:15284	 l-p:0.12885931134223938
epoch£º764	 i:5 	 global-step:15285	 l-p:0.12883059680461884
epoch£º764	 i:6 	 global-step:15286	 l-p:0.1320192664861679
epoch£º764	 i:7 	 global-step:15287	 l-p:0.1754639595746994
epoch£º764	 i:8 	 global-step:15288	 l-p:0.0674591064453125
epoch£º764	 i:9 	 global-step:15289	 l-p:0.07999705523252487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.0448, 5.0954],
        [5.1498, 5.1438, 5.1494],
        [5.1498, 5.1496, 5.1498],
        [5.1498, 5.0309, 5.0804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.11806020140647888 
model_pd.l_d.mean(): -18.580059051513672 
model_pd.lagr.mean(): -18.461997985839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5726], device='cuda:0')), ('power', tensor([-19.3680], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.11806020140647888
epoch£º765	 i:1 	 global-step:15301	 l-p:0.1772826761007309
epoch£º765	 i:2 	 global-step:15302	 l-p:0.09208028763532639
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11937189102172852
epoch£º765	 i:4 	 global-step:15304	 l-p:0.1596461683511734
epoch£º765	 i:5 	 global-step:15305	 l-p:0.13098619878292084
epoch£º765	 i:6 	 global-step:15306	 l-p:0.46555426716804504
epoch£º765	 i:7 	 global-step:15307	 l-p:0.129014790058136
epoch£º765	 i:8 	 global-step:15308	 l-p:0.07519712299108505
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08748050779104233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2232, 5.5948, 5.5152],
        [5.2232, 4.9953, 4.8690],
        [5.2232, 5.1761, 5.2107],
        [5.2232, 5.3487, 5.1265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12290864437818527 
model_pd.l_d.mean(): -20.516050338745117 
model_pd.lagr.mean(): -20.39314079284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.1784], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12290864437818527
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12464376538991928
epoch£º766	 i:2 	 global-step:15322	 l-p:0.14143404364585876
epoch£º766	 i:3 	 global-step:15323	 l-p:0.22429080307483673
epoch£º766	 i:4 	 global-step:15324	 l-p:0.13737133145332336
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12465544790029526
epoch£º766	 i:6 	 global-step:15326	 l-p:0.09699061512947083
epoch£º766	 i:7 	 global-step:15327	 l-p:0.11817176640033722
epoch£º766	 i:8 	 global-step:15328	 l-p:0.07929309457540512
epoch£º766	 i:9 	 global-step:15329	 l-p:0.12881366908550262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2119, 5.1494, 5.1911],
        [5.2119, 5.2118, 5.2119],
        [5.2119, 5.2119, 5.2119],
        [5.2119, 5.4753, 5.3278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.11718560755252838 
model_pd.l_d.mean(): -20.788265228271484 
model_pd.lagr.mean(): -20.671079635620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.4136], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.11718560755252838
epoch£º767	 i:1 	 global-step:15341	 l-p:0.06707855314016342
epoch£º767	 i:2 	 global-step:15342	 l-p:0.11407341063022614
epoch£º767	 i:3 	 global-step:15343	 l-p:0.07375865429639816
epoch£º767	 i:4 	 global-step:15344	 l-p:0.13160614669322968
epoch£º767	 i:5 	 global-step:15345	 l-p:0.13644234836101532
epoch£º767	 i:6 	 global-step:15346	 l-p:0.17394205927848816
epoch£º767	 i:7 	 global-step:15347	 l-p:0.1531301885843277
epoch£º767	 i:8 	 global-step:15348	 l-p:0.11594036966562271
epoch£º767	 i:9 	 global-step:15349	 l-p:0.004908323287963867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1686, 4.9341, 4.8005],
        [5.1686, 5.6728, 5.6860],
        [5.1686, 5.1208, 5.1559],
        [5.1686, 5.1019, 4.8157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.15296918153762817 
model_pd.l_d.mean(): -19.49078369140625 
model_pd.lagr.mean(): -19.337814331054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.2055], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.15296918153762817
epoch£º768	 i:1 	 global-step:15361	 l-p:0.12374479323625565
epoch£º768	 i:2 	 global-step:15362	 l-p:0.13141410052776337
epoch£º768	 i:3 	 global-step:15363	 l-p:0.09158117324113846
epoch£º768	 i:4 	 global-step:15364	 l-p:0.10896281152963638
epoch£º768	 i:5 	 global-step:15365	 l-p:-0.11103719472885132
epoch£º768	 i:6 	 global-step:15366	 l-p:0.13221466541290283
epoch£º768	 i:7 	 global-step:15367	 l-p:0.1707930713891983
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13457979261875153
epoch£º768	 i:9 	 global-step:15369	 l-p:0.11040791124105453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1529, 4.9227, 4.7295],
        [5.1529, 5.1304, 4.8510],
        [5.1529, 5.1220, 4.8406],
        [5.1529, 5.1389, 5.1514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12543559074401855 
model_pd.l_d.mean(): -20.255929946899414 
model_pd.lagr.mean(): -20.130495071411133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-20.9585], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12543559074401855
epoch£º769	 i:1 	 global-step:15381	 l-p:0.1543673723936081
epoch£º769	 i:2 	 global-step:15382	 l-p:0.09786813706159592
epoch£º769	 i:3 	 global-step:15383	 l-p:0.22467204928398132
epoch£º769	 i:4 	 global-step:15384	 l-p:0.1118539422750473
epoch£º769	 i:5 	 global-step:15385	 l-p:0.12584204971790314
epoch£º769	 i:6 	 global-step:15386	 l-p:0.16292457282543182
epoch£º769	 i:7 	 global-step:15387	 l-p:0.12472503632307053
epoch£º769	 i:8 	 global-step:15388	 l-p:0.1940973997116089
epoch£º769	 i:9 	 global-step:15389	 l-p:0.13528390228748322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0444, 5.0444, 5.0444],
        [5.0444, 4.9730, 4.6792],
        [5.0444, 4.8068, 4.5901],
        [5.0444, 4.9271, 4.9793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): -5.993791580200195 
model_pd.l_d.mean(): -18.34312629699707 
model_pd.lagr.mean(): -24.336917877197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6297], device='cuda:0')), ('power', tensor([-19.1868], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:-5.993791580200195
epoch£º770	 i:1 	 global-step:15401	 l-p:0.15503409504890442
epoch£º770	 i:2 	 global-step:15402	 l-p:0.1225828304886818
epoch£º770	 i:3 	 global-step:15403	 l-p:0.7384354472160339
epoch£º770	 i:4 	 global-step:15404	 l-p:0.20334281027317047
epoch£º770	 i:5 	 global-step:15405	 l-p:0.13122344017028809
epoch£º770	 i:6 	 global-step:15406	 l-p:0.13921526074409485
epoch£º770	 i:7 	 global-step:15407	 l-p:0.15708068013191223
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10994140058755875
epoch£º770	 i:9 	 global-step:15409	 l-p:-0.4217446744441986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9920, 4.9919, 4.9920],
        [4.9920, 4.9482, 4.9815],
        [4.9920, 4.9872, 4.9917],
        [4.9920, 4.7387, 4.5600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.1295468807220459 
model_pd.l_d.mean(): -20.819272994995117 
model_pd.lagr.mean(): -20.689725875854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4410], device='cuda:0')), ('power', tensor([-21.4972], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.1295468807220459
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11805459856987
epoch£º771	 i:2 	 global-step:15422	 l-p:0.029196662828326225
epoch£º771	 i:3 	 global-step:15423	 l-p:0.14131766557693481
epoch£º771	 i:4 	 global-step:15424	 l-p:-0.11527948081493378
epoch£º771	 i:5 	 global-step:15425	 l-p:0.17463119328022003
epoch£º771	 i:6 	 global-step:15426	 l-p:0.15079766511917114
epoch£º771	 i:7 	 global-step:15427	 l-p:0.20832423865795135
epoch£º771	 i:8 	 global-step:15428	 l-p:-3.694849729537964
epoch£º771	 i:9 	 global-step:15429	 l-p:0.31698641180992126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9942, 4.9518, 4.9843],
        [4.9942, 4.9195, 4.9668],
        [4.9942, 5.2610, 5.1206],
        [4.9942, 4.9503, 4.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.12329057604074478 
model_pd.l_d.mean(): -20.834135055541992 
model_pd.lagr.mean(): -20.710844039916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4567], device='cuda:0')), ('power', tensor([-21.5283], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.12329057604074478
epoch£º772	 i:1 	 global-step:15441	 l-p:0.15880319476127625
epoch£º772	 i:2 	 global-step:15442	 l-p:0.10580381006002426
epoch£º772	 i:3 	 global-step:15443	 l-p:0.1472884565591812
epoch£º772	 i:4 	 global-step:15444	 l-p:0.22031541168689728
epoch£º772	 i:5 	 global-step:15445	 l-p:0.11669664084911346
epoch£º772	 i:6 	 global-step:15446	 l-p:0.11594893783330917
epoch£º772	 i:7 	 global-step:15447	 l-p:0.18105663359165192
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07814525067806244
epoch£º772	 i:9 	 global-step:15449	 l-p:0.16449731588363647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1344, 5.1345, 5.1345],
        [5.1344, 5.0969, 5.1263],
        [5.1344, 4.9027, 4.8241],
        [5.1344, 4.9055, 4.8370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13484825193881989 
model_pd.l_d.mean(): -21.015790939331055 
model_pd.lagr.mean(): -20.880943298339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3822], device='cuda:0')), ('power', tensor([-21.6358], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13484825193881989
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13174179196357727
epoch£º773	 i:2 	 global-step:15462	 l-p:0.08733676373958588
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07345122843980789
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14066182076931
epoch£º773	 i:5 	 global-step:15465	 l-p:0.15646979212760925
epoch£º773	 i:6 	 global-step:15466	 l-p:0.5585499405860901
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13174861669540405
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11653853952884674
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10861819982528687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2284, 5.1306, 5.1806],
        [5.2284, 5.1886, 5.2192],
        [5.2284, 4.9974, 4.8706],
        [5.2284, 5.2092, 5.2258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.08357515186071396 
model_pd.l_d.mean(): -20.7198543548584 
model_pd.lagr.mean(): -20.636280059814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4046], device='cuda:0')), ('power', tensor([-21.3596], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.08357515186071396
epoch£º774	 i:1 	 global-step:15481	 l-p:0.11835227906703949
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12804679572582245
epoch£º774	 i:3 	 global-step:15483	 l-p:0.21904391050338745
epoch£º774	 i:4 	 global-step:15484	 l-p:0.13617204129695892
epoch£º774	 i:5 	 global-step:15485	 l-p:0.06859474629163742
epoch£º774	 i:6 	 global-step:15486	 l-p:0.13732962310314178
epoch£º774	 i:7 	 global-step:15487	 l-p:3.0878374576568604
epoch£º774	 i:8 	 global-step:15488	 l-p:0.12737709283828735
epoch£º774	 i:9 	 global-step:15489	 l-p:0.11737951636314392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2619, 5.0367, 4.9405],
        [5.2619, 5.0517, 5.0107],
        [5.2619, 5.2619, 5.2619],
        [5.2619, 5.3920, 5.1698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.1265527456998825 
model_pd.l_d.mean(): -18.81344985961914 
model_pd.lagr.mean(): -18.68689727783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5989], device='cuda:0')), ('power', tensor([-19.6308], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.1265527456998825
epoch£º775	 i:1 	 global-step:15501	 l-p:0.1427333652973175
epoch£º775	 i:2 	 global-step:15502	 l-p:0.09742029756307602
epoch£º775	 i:3 	 global-step:15503	 l-p:0.5638161301612854
epoch£º775	 i:4 	 global-step:15504	 l-p:0.0937051773071289
epoch£º775	 i:5 	 global-step:15505	 l-p:0.11380230635404587
epoch£º775	 i:6 	 global-step:15506	 l-p:0.19767074286937714
epoch£º775	 i:7 	 global-step:15507	 l-p:0.1401301771402359
epoch£º775	 i:8 	 global-step:15508	 l-p:0.1400640606880188
epoch£º775	 i:9 	 global-step:15509	 l-p:0.06669611483812332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2121, 5.0392, 5.0612],
        [5.2121, 4.9924, 4.9372],
        [5.2121, 4.9980, 4.9579],
        [5.2121, 5.5746, 5.4875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.5105646252632141 
model_pd.l_d.mean(): -20.61806869506836 
model_pd.lagr.mean(): -20.10750389099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.2834], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.5105646252632141
epoch£º776	 i:1 	 global-step:15521	 l-p:0.12074277549982071
epoch£º776	 i:2 	 global-step:15522	 l-p:0.14132651686668396
epoch£º776	 i:3 	 global-step:15523	 l-p:0.12851454317569733
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12716670334339142
epoch£º776	 i:5 	 global-step:15525	 l-p:0.11755599826574326
epoch£º776	 i:6 	 global-step:15526	 l-p:0.1327066272497177
epoch£º776	 i:7 	 global-step:15527	 l-p:0.1173529252409935
epoch£º776	 i:8 	 global-step:15528	 l-p:0.1627499759197235
epoch£º776	 i:9 	 global-step:15529	 l-p:0.1414140909910202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1234, 4.9099, 4.8856],
        [5.1234, 5.0387, 5.0880],
        [5.1234, 4.8986, 4.8498],
        [5.1234, 4.8801, 4.7286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.1199202761054039 
model_pd.l_d.mean(): -20.619861602783203 
model_pd.lagr.mean(): -20.499940872192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4438], device='cuda:0')), ('power', tensor([-21.2985], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.1199202761054039
epoch£º777	 i:1 	 global-step:15541	 l-p:0.14506784081459045
epoch£º777	 i:2 	 global-step:15542	 l-p:0.15883253514766693
epoch£º777	 i:3 	 global-step:15543	 l-p:0.16715554893016815
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16467422246932983
epoch£º777	 i:5 	 global-step:15545	 l-p:0.09351859241724014
epoch£º777	 i:6 	 global-step:15546	 l-p:0.1433737874031067
epoch£º777	 i:7 	 global-step:15547	 l-p:0.25614041090011597
epoch£º777	 i:8 	 global-step:15548	 l-p:0.05469844117760658
epoch£º777	 i:9 	 global-step:15549	 l-p:0.17154334485530853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0788, 5.0779, 5.0788],
        [5.0788, 4.9996, 5.0480],
        [5.0788, 5.0617, 5.0768],
        [5.0788, 5.0720, 5.0784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.19171077013015747 
model_pd.l_d.mean(): -19.052330017089844 
model_pd.lagr.mean(): -18.860618591308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6154], device='cuda:0')), ('power', tensor([-19.8892], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.19171077013015747
epoch£º778	 i:1 	 global-step:15561	 l-p:0.15142710506916046
epoch£º778	 i:2 	 global-step:15562	 l-p:0.09083722531795502
epoch£º778	 i:3 	 global-step:15563	 l-p:0.20727956295013428
epoch£º778	 i:4 	 global-step:15564	 l-p:0.09820371121168137
epoch£º778	 i:5 	 global-step:15565	 l-p:0.08111749589443207
epoch£º778	 i:6 	 global-step:15566	 l-p:0.1770610362291336
epoch£º778	 i:7 	 global-step:15567	 l-p:0.14590728282928467
epoch£º778	 i:8 	 global-step:15568	 l-p:0.14053428173065186
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14972011744976044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1317, 4.9726, 5.0097],
        [5.1317, 4.8920, 4.7069],
        [5.1317, 5.1316, 5.1317],
        [5.1317, 5.0532, 5.1010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.11467724293470383 
model_pd.l_d.mean(): -20.498519897460938 
model_pd.lagr.mean(): -20.38384246826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518], device='cuda:0')), ('power', tensor([-21.1840], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.11467724293470383
epoch£º779	 i:1 	 global-step:15581	 l-p:0.17051072418689728
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13254491984844208
epoch£º779	 i:3 	 global-step:15583	 l-p:0.1225273460149765
epoch£º779	 i:4 	 global-step:15584	 l-p:0.15362882614135742
epoch£º779	 i:5 	 global-step:15585	 l-p:0.05694505572319031
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13168364763259888
epoch£º779	 i:7 	 global-step:15587	 l-p:0.13659149408340454
epoch£º779	 i:8 	 global-step:15588	 l-p:0.1517975628376007
epoch£º779	 i:9 	 global-step:15589	 l-p:0.15875253081321716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1196, 5.1246],
        [5.1249, 5.1063, 5.1225],
        [5.1249, 5.2713, 5.0578],
        [5.1249, 5.2645, 5.0477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.17221179604530334 
model_pd.l_d.mean(): -18.651357650756836 
model_pd.lagr.mean(): -18.479145050048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5696], device='cuda:0')), ('power', tensor([-19.4370], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.17221179604530334
epoch£º780	 i:1 	 global-step:15601	 l-p:0.059377994388341904
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14004510641098022
epoch£º780	 i:3 	 global-step:15603	 l-p:0.10961103439331055
epoch£º780	 i:4 	 global-step:15604	 l-p:0.15567496418952942
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11199618875980377
epoch£º780	 i:6 	 global-step:15606	 l-p:0.16999073326587677
epoch£º780	 i:7 	 global-step:15607	 l-p:0.11077788472175598
epoch£º780	 i:8 	 global-step:15608	 l-p:0.06568002700805664
epoch£º780	 i:9 	 global-step:15609	 l-p:0.1293409764766693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2001, 5.3652, 5.1602],
        [5.2001, 5.2001, 5.2001],
        [5.2001, 5.1941, 5.1997],
        [5.2001, 5.1974, 5.2000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.16181254386901855 
model_pd.l_d.mean(): -20.49867820739746 
model_pd.lagr.mean(): -20.33686637878418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-21.1788], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.16181254386901855
epoch£º781	 i:1 	 global-step:15621	 l-p:0.11938129365444183
epoch£º781	 i:2 	 global-step:15622	 l-p:0.11465566605329514
epoch£º781	 i:3 	 global-step:15623	 l-p:0.09094910323619843
epoch£º781	 i:4 	 global-step:15624	 l-p:0.12253789603710175
epoch£º781	 i:5 	 global-step:15625	 l-p:0.029650211334228516
epoch£º781	 i:6 	 global-step:15626	 l-p:0.10942162573337555
epoch£º781	 i:7 	 global-step:15627	 l-p:0.586962878704071
epoch£º781	 i:8 	 global-step:15628	 l-p:0.13498905301094055
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11848842352628708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2147, 5.1966, 5.2124],
        [5.2147, 5.2803, 5.0282],
        [5.2147, 5.1511, 5.1936],
        [5.2147, 5.1473, 5.1912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): -0.01693391054868698 
model_pd.l_d.mean(): -20.11867904663086 
model_pd.lagr.mean(): -20.13561248779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4161], device='cuda:0')), ('power', tensor([-20.7636], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:-0.01693391054868698
epoch£º782	 i:1 	 global-step:15641	 l-p:0.14073923230171204
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12205595523118973
epoch£º782	 i:3 	 global-step:15643	 l-p:0.12963441014289856
epoch£º782	 i:4 	 global-step:15644	 l-p:0.1294364631175995
epoch£º782	 i:5 	 global-step:15645	 l-p:0.11069261282682419
epoch£º782	 i:6 	 global-step:15646	 l-p:0.04862729087471962
epoch£º782	 i:7 	 global-step:15647	 l-p:0.1271573007106781
epoch£º782	 i:8 	 global-step:15648	 l-p:0.14597837626934052
epoch£º782	 i:9 	 global-step:15649	 l-p:0.16518302261829376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1357, 5.0838, 4.7935],
        [5.1357, 4.9072, 4.8502],
        [5.1357, 5.0500, 4.7552],
        [5.1357, 5.0911, 5.1247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.1585446000099182 
model_pd.l_d.mean(): -20.670515060424805 
model_pd.lagr.mean(): -20.51197052001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.3232], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.1585446000099182
epoch£º783	 i:1 	 global-step:15661	 l-p:0.15562616288661957
epoch£º783	 i:2 	 global-step:15662	 l-p:0.12478666752576828
epoch£º783	 i:3 	 global-step:15663	 l-p:0.1174611821770668
epoch£º783	 i:4 	 global-step:15664	 l-p:0.12893331050872803
epoch£º783	 i:5 	 global-step:15665	 l-p:0.10170120000839233
epoch£º783	 i:6 	 global-step:15666	 l-p:0.15454703569412231
epoch£º783	 i:7 	 global-step:15667	 l-p:0.14848893880844116
epoch£º783	 i:8 	 global-step:15668	 l-p:0.09072986245155334
epoch£º783	 i:9 	 global-step:15669	 l-p:0.1828443855047226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1008, 5.0381, 4.7441],
        [5.1008, 5.1008, 5.1008],
        [5.1008, 4.9977, 5.0502],
        [5.1008, 4.9897, 5.0423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.12510313093662262 
model_pd.l_d.mean(): -20.230710983276367 
model_pd.lagr.mean(): -20.105607986450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5158], device='cuda:0')), ('power', tensor([-20.9786], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.12510313093662262
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13812275230884552
epoch£º784	 i:2 	 global-step:15682	 l-p:0.1065051406621933
epoch£º784	 i:3 	 global-step:15683	 l-p:0.18517912924289703
epoch£º784	 i:4 	 global-step:15684	 l-p:0.09969406574964523
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13641996681690216
epoch£º784	 i:6 	 global-step:15686	 l-p:0.1706731617450714
epoch£º784	 i:7 	 global-step:15687	 l-p:0.15694501996040344
epoch£º784	 i:8 	 global-step:15688	 l-p:0.10760924965143204
epoch£º784	 i:9 	 global-step:15689	 l-p:0.13099336624145508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1304, 5.1304, 5.1304],
        [5.1304, 5.1259, 5.1302],
        [5.1304, 4.9154, 4.6655],
        [5.1304, 4.9600, 4.9906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.09696064889431 
model_pd.l_d.mean(): -19.94306182861328 
model_pd.lagr.mean(): -19.846101760864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4544], device='cuda:0')), ('power', tensor([-20.6251], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.09696064889431
epoch£º785	 i:1 	 global-step:15701	 l-p:0.1523345559835434
epoch£º785	 i:2 	 global-step:15702	 l-p:0.08677399158477783
epoch£º785	 i:3 	 global-step:15703	 l-p:0.11094704270362854
epoch£º785	 i:4 	 global-step:15704	 l-p:0.13444219529628754
epoch£º785	 i:5 	 global-step:15705	 l-p:0.22269338369369507
epoch£º785	 i:6 	 global-step:15706	 l-p:0.16497863829135895
epoch£º785	 i:7 	 global-step:15707	 l-p:0.19074371457099915
epoch£º785	 i:8 	 global-step:15708	 l-p:0.12350169569253922
epoch£º785	 i:9 	 global-step:15709	 l-p:0.17463494837284088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0962, 5.0962, 5.0962],
        [5.0962, 5.4707, 5.3934],
        [5.0962, 5.0679, 4.7810],
        [5.0962, 5.0308, 5.0746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.1315341591835022 
model_pd.l_d.mean(): -20.73442268371582 
model_pd.lagr.mean(): -20.602888107299805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.4010], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.1315341591835022
epoch£º786	 i:1 	 global-step:15721	 l-p:0.1400177925825119
epoch£º786	 i:2 	 global-step:15722	 l-p:0.15915067493915558
epoch£º786	 i:3 	 global-step:15723	 l-p:0.2123255431652069
epoch£º786	 i:4 	 global-step:15724	 l-p:0.12194085866212845
epoch£º786	 i:5 	 global-step:15725	 l-p:0.11539828777313232
epoch£º786	 i:6 	 global-step:15726	 l-p:0.12799322605133057
epoch£º786	 i:7 	 global-step:15727	 l-p:0.15818439424037933
epoch£º786	 i:8 	 global-step:15728	 l-p:0.11548447608947754
epoch£º786	 i:9 	 global-step:15729	 l-p:0.15295562148094177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1395, 5.0781, 4.7854],
        [5.1395, 5.0948, 5.1285],
        [5.1395, 5.1211, 5.1371],
        [5.1395, 5.0595, 5.1080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.11551237106323242 
model_pd.l_d.mean(): -19.420454025268555 
model_pd.lagr.mean(): -19.304941177368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5621], device='cuda:0')), ('power', tensor([-20.2068], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.11551237106323242
epoch£º787	 i:1 	 global-step:15741	 l-p:0.11257580667734146
epoch£º787	 i:2 	 global-step:15742	 l-p:0.14021645486354828
epoch£º787	 i:3 	 global-step:15743	 l-p:0.09979462623596191
epoch£º787	 i:4 	 global-step:15744	 l-p:0.09686747193336487
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14119693636894226
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11769694089889526
epoch£º787	 i:7 	 global-step:15747	 l-p:0.14625538885593414
epoch£º787	 i:8 	 global-step:15748	 l-p:0.15902669727802277
epoch£º787	 i:9 	 global-step:15749	 l-p:0.134147047996521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1338, 4.8940, 4.8032],
        [5.1338, 5.1338, 5.1338],
        [5.1338, 4.8888, 4.7646],
        [5.1338, 5.2162, 4.9703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.136528879404068 
model_pd.l_d.mean(): -20.705293655395508 
model_pd.lagr.mean(): -20.56876564025879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.3577], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.136528879404068
epoch£º788	 i:1 	 global-step:15761	 l-p:0.14576099812984467
epoch£º788	 i:2 	 global-step:15762	 l-p:0.1162937730550766
epoch£º788	 i:3 	 global-step:15763	 l-p:0.1140470877289772
epoch£º788	 i:4 	 global-step:15764	 l-p:0.2074057012796402
epoch£º788	 i:5 	 global-step:15765	 l-p:0.1900947391986847
epoch£º788	 i:6 	 global-step:15766	 l-p:0.1425245702266693
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12621763348579407
epoch£º788	 i:8 	 global-step:15768	 l-p:0.1534571349620819
epoch£º788	 i:9 	 global-step:15769	 l-p:0.13976168632507324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0753, 5.0288, 5.0637],
        [5.0753, 4.8380, 4.7750],
        [5.0753, 5.0753, 5.0753],
        [5.0753, 5.0753, 5.0753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.12376558035612106 
model_pd.l_d.mean(): -20.433124542236328 
model_pd.lagr.mean(): -20.309358596801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.1397], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.12376558035612106
epoch£º789	 i:1 	 global-step:15781	 l-p:0.1588253229856491
epoch£º789	 i:2 	 global-step:15782	 l-p:0.7723939418792725
epoch£º789	 i:3 	 global-step:15783	 l-p:0.18415451049804688
epoch£º789	 i:4 	 global-step:15784	 l-p:0.23997780680656433
epoch£º789	 i:5 	 global-step:15785	 l-p:0.1575455665588379
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11777522414922714
epoch£º789	 i:7 	 global-step:15787	 l-p:0.1221766471862793
epoch£º789	 i:8 	 global-step:15788	 l-p:0.16319900751113892
epoch£º789	 i:9 	 global-step:15789	 l-p:0.15026789903640747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0897, 5.0155, 5.0626],
        [5.0897, 5.0897, 5.0897],
        [5.0897, 5.0327, 4.7377],
        [5.0897, 4.8532, 4.7900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.12085329741239548 
model_pd.l_d.mean(): -20.942106246948242 
model_pd.lagr.mean(): -20.821252822875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4126], device='cuda:0')), ('power', tensor([-21.5924], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.12085329741239548
epoch£º790	 i:1 	 global-step:15801	 l-p:0.20428046584129333
epoch£º790	 i:2 	 global-step:15802	 l-p:0.14862103760242462
epoch£º790	 i:3 	 global-step:15803	 l-p:0.14233621954917908
epoch£º790	 i:4 	 global-step:15804	 l-p:0.14467136561870575
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12835536897182465
epoch£º790	 i:6 	 global-step:15806	 l-p:0.16424964368343353
epoch£º790	 i:7 	 global-step:15807	 l-p:0.1254878044128418
epoch£º790	 i:8 	 global-step:15808	 l-p:0.14121471345424652
epoch£º790	 i:9 	 global-step:15809	 l-p:0.08378767967224121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1396, 4.9165, 4.8777],
        [5.1396, 5.5753, 5.5373],
        [5.1396, 5.0312, 5.0837],
        [5.1396, 4.9456, 4.9543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.1592676192522049 
model_pd.l_d.mean(): -20.650619506835938 
model_pd.lagr.mean(): -20.491352081298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4106], device='cuda:0')), ('power', tensor([-21.2957], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.1592676192522049
epoch£º791	 i:1 	 global-step:15821	 l-p:0.14939957857131958
epoch£º791	 i:2 	 global-step:15822	 l-p:0.06306428462266922
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12223826348781586
epoch£º791	 i:4 	 global-step:15824	 l-p:0.10060717165470123
epoch£º791	 i:5 	 global-step:15825	 l-p:0.13456714153289795
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09786283224821091
epoch£º791	 i:7 	 global-step:15827	 l-p:0.1527092009782791
epoch£º791	 i:8 	 global-step:15828	 l-p:0.12797102332115173
epoch£º791	 i:9 	 global-step:15829	 l-p:0.15572281181812286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1404, 4.8989, 4.7024],
        [5.1404, 5.0110, 4.7144],
        [5.1404, 5.0714, 5.1165],
        [5.1404, 5.0010, 5.0487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.1333109587430954 
model_pd.l_d.mean(): -20.795669555664062 
model_pd.lagr.mean(): -20.6623592376709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4103], device='cuda:0')), ('power', tensor([-21.4420], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.1333109587430954
epoch£º792	 i:1 	 global-step:15841	 l-p:0.07420731335878372
epoch£º792	 i:2 	 global-step:15842	 l-p:0.14297232031822205
epoch£º792	 i:3 	 global-step:15843	 l-p:0.1440441757440567
epoch£º792	 i:4 	 global-step:15844	 l-p:0.11958106607198715
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12725093960762024
epoch£º792	 i:6 	 global-step:15846	 l-p:0.09677080810070038
epoch£º792	 i:7 	 global-step:15847	 l-p:0.24976173043251038
epoch£º792	 i:8 	 global-step:15848	 l-p:0.16751964390277863
epoch£º792	 i:9 	 global-step:15849	 l-p:-0.3051312267780304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0510, 5.0510, 5.0510],
        [5.0510, 5.0482, 5.0509],
        [5.0510, 5.0510, 5.0510],
        [5.0510, 5.0034, 5.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13314004242420197 
model_pd.l_d.mean(): -20.796396255493164 
model_pd.lagr.mean(): -20.66325569152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4189], device='cuda:0')), ('power', tensor([-21.4515], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13314004242420197
epoch£º793	 i:1 	 global-step:15861	 l-p:0.17059503495693207
epoch£º793	 i:2 	 global-step:15862	 l-p:0.1180083230137825
epoch£º793	 i:3 	 global-step:15863	 l-p:0.12002488225698471
epoch£º793	 i:4 	 global-step:15864	 l-p:0.12189774215221405
epoch£º793	 i:5 	 global-step:15865	 l-p:0.05555738881230354
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13645978271961212
epoch£º793	 i:7 	 global-step:15867	 l-p:0.3848315477371216
epoch£º793	 i:8 	 global-step:15868	 l-p:0.19762183725833893
epoch£º793	 i:9 	 global-step:15869	 l-p:-0.20775192975997925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0265, 4.7667, 4.6175],
        [5.0265, 5.0045, 5.0234],
        [5.0265, 4.7687, 4.6437],
        [5.0265, 5.0266, 5.0266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.20344141125679016 
model_pd.l_d.mean(): -20.27406120300293 
model_pd.lagr.mean(): -20.070619583129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-21.0253], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.20344141125679016
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11573073267936707
epoch£º794	 i:2 	 global-step:15882	 l-p:0.26453718543052673
epoch£º794	 i:3 	 global-step:15883	 l-p:0.13216759264469147
epoch£º794	 i:4 	 global-step:15884	 l-p:0.287596195936203
epoch£º794	 i:5 	 global-step:15885	 l-p:-2.4230196475982666
epoch£º794	 i:6 	 global-step:15886	 l-p:0.11325912177562714
epoch£º794	 i:7 	 global-step:15887	 l-p:0.20626942813396454
epoch£º794	 i:8 	 global-step:15888	 l-p:0.1189805120229721
epoch£º794	 i:9 	 global-step:15889	 l-p:0.18332317471504211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0859, 5.0859, 5.0859],
        [5.0859, 4.8751, 4.6048],
        [5.0859, 5.0433, 5.0760],
        [5.0859, 5.0452, 5.0768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.1127202957868576 
model_pd.l_d.mean(): -20.393970489501953 
model_pd.lagr.mean(): -20.28125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4703], device='cuda:0')), ('power', tensor([-21.0972], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.1127202957868576
epoch£º795	 i:1 	 global-step:15901	 l-p:0.16396689414978027
epoch£º795	 i:2 	 global-step:15902	 l-p:0.1665208786725998
epoch£º795	 i:3 	 global-step:15903	 l-p:0.14404970407485962
epoch£º795	 i:4 	 global-step:15904	 l-p:0.15842507779598236
epoch£º795	 i:5 	 global-step:15905	 l-p:0.10668721050024033
epoch£º795	 i:6 	 global-step:15906	 l-p:0.19598208367824554
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12182949483394623
epoch£º795	 i:8 	 global-step:15908	 l-p:0.11386249959468842
epoch£º795	 i:9 	 global-step:15909	 l-p:0.1211862713098526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1483, 5.0975, 5.1346],
        [5.1483, 5.1017, 4.8098],
        [5.1483, 5.0172, 5.0674],
        [5.1483, 5.1482, 5.1483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.1376708745956421 
model_pd.l_d.mean(): -19.96675682067871 
model_pd.lagr.mean(): -19.829086303710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4848], device='cuda:0')), ('power', tensor([-20.6801], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.1376708745956421
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12301813066005707
epoch£º796	 i:2 	 global-step:15922	 l-p:0.09851470589637756
epoch£º796	 i:3 	 global-step:15923	 l-p:0.13520686328411102
epoch£º796	 i:4 	 global-step:15924	 l-p:0.15271060168743134
epoch£º796	 i:5 	 global-step:15925	 l-p:0.10325085371732712
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11343266069889069
epoch£º796	 i:7 	 global-step:15927	 l-p:0.14184044301509857
epoch£º796	 i:8 	 global-step:15928	 l-p:0.1352100670337677
epoch£º796	 i:9 	 global-step:15929	 l-p:0.16071884334087372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1432, 5.0540, 5.1048],
        [5.1432, 5.1268, 5.1413],
        [5.1432, 5.1426, 5.1432],
        [5.1432, 5.1432, 5.1432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.1173635944724083 
model_pd.l_d.mean(): -20.519704818725586 
model_pd.lagr.mean(): -20.402341842651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4519], device='cuda:0')), ('power', tensor([-21.2055], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.1173635944724083
epoch£º797	 i:1 	 global-step:15941	 l-p:0.11740745604038239
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14466114342212677
epoch£º797	 i:3 	 global-step:15943	 l-p:0.16046880185604095
epoch£º797	 i:4 	 global-step:15944	 l-p:0.17944976687431335
epoch£º797	 i:5 	 global-step:15945	 l-p:0.11992795765399933
epoch£º797	 i:6 	 global-step:15946	 l-p:0.14858704805374146
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11357284337282181
epoch£º797	 i:8 	 global-step:15948	 l-p:0.17249689996242523
epoch£º797	 i:9 	 global-step:15949	 l-p:0.18563564121723175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0751, 5.1011, 4.8296],
        [5.0751, 5.0705, 5.0749],
        [5.0751, 4.9401, 4.9913],
        [5.0751, 5.3383, 5.1892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.15099605917930603 
model_pd.l_d.mean(): -19.9083309173584 
model_pd.lagr.mean(): -19.757335662841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-20.6164], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.15099605917930603
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11435071378946304
epoch£º798	 i:2 	 global-step:15962	 l-p:0.24962371587753296
epoch£º798	 i:3 	 global-step:15963	 l-p:0.14386354386806488
epoch£º798	 i:4 	 global-step:15964	 l-p:-0.1382836252450943
epoch£º798	 i:5 	 global-step:15965	 l-p:0.2003626972436905
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11666717380285263
epoch£º798	 i:7 	 global-step:15967	 l-p:0.22400934994220734
epoch£º798	 i:8 	 global-step:15968	 l-p:0.26344549655914307
epoch£º798	 i:9 	 global-step:15969	 l-p:0.20368801057338715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0472, 5.0041, 5.0372],
        [5.0472, 5.0459, 5.0472],
        [5.0472, 5.0472, 5.0472],
        [5.0472, 5.0445, 5.0471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.1157306358218193 
model_pd.l_d.mean(): -21.020681381225586 
model_pd.lagr.mean(): -20.904951095581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4036], device='cuda:0')), ('power', tensor([-21.6627], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.1157306358218193
epoch£º799	 i:1 	 global-step:15981	 l-p:-0.20017871260643005
epoch£º799	 i:2 	 global-step:15982	 l-p:0.17053154110908508
epoch£º799	 i:3 	 global-step:15983	 l-p:0.12253822386264801
epoch£º799	 i:4 	 global-step:15984	 l-p:0.13118186593055725
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10904799401760101
epoch£º799	 i:6 	 global-step:15986	 l-p:0.12121742218732834
epoch£º799	 i:7 	 global-step:15987	 l-p:0.12216319143772125
epoch£º799	 i:8 	 global-step:15988	 l-p:0.34390121698379517
epoch£º799	 i:9 	 global-step:15989	 l-p:0.1332642287015915
