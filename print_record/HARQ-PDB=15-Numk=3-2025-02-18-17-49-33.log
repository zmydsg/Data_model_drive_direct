
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0001], device='cuda:0')), ('power', tensor([0.9999], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.27698108553886414
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802683353424
epoch£º0	 i:4 	 global-step:4	 l-p:-0.7105809450149536
epoch£º0	 i:5 	 global-step:5	 l-p:-2.5309512615203857
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505249977111816
epoch£º0	 i:7 	 global-step:7	 l-p:0.4916812479496002
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606581211090088
epoch£º0	 i:9 	 global-step:9	 l-p:0.056802768260240555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.377257376909256 
model_pd.l_d.mean(): -18.863727569580078 
model_pd.lagr.mean(): -18.486469268798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0057], device='cuda:0')), ('power', tensor([0.9952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.377257376909256
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797874927521
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878843605518341
epoch£º1	 i:3 	 global-step:23	 l-p:0.24289998412132263
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980881869792938
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688207864761353
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738257348537445
epoch£º1	 i:7 	 global-step:27	 l-p:0.21875056624412537
epoch£º1	 i:8 	 global-step:28	 l-p:0.22574172914028168
epoch£º1	 i:9 	 global-step:29	 l-p:0.11670563369989395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 5.9885, 6.5145],
        [4.9151, 5.0129, 4.9528],
        [4.9151, 5.7268, 5.9878],
        [4.9151, 4.9970, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13362102210521698 
model_pd.l_d.mean(): -20.285629272460938 
model_pd.lagr.mean(): -20.152008056640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0097], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13362102210521698
epoch£º2	 i:1 	 global-step:41	 l-p:0.10827109217643738
epoch£º2	 i:2 	 global-step:42	 l-p:0.10365663468837738
epoch£º2	 i:3 	 global-step:43	 l-p:0.10691878944635391
epoch£º2	 i:4 	 global-step:44	 l-p:0.11889299005270004
epoch£º2	 i:5 	 global-step:45	 l-p:0.10603377968072891
epoch£º2	 i:6 	 global-step:46	 l-p:0.11256347596645355
epoch£º2	 i:7 	 global-step:47	 l-p:-0.5110549330711365
epoch£º2	 i:8 	 global-step:48	 l-p:0.10819506645202637
epoch£º2	 i:9 	 global-step:49	 l-p:0.11759164184331894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3947, 6.4841, 6.9565],
        [5.3947, 5.4457, 5.4069],
        [5.3947, 5.5007, 5.4350],
        [5.3947, 6.9665, 8.0013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08163745701313019 
model_pd.l_d.mean(): -18.719287872314453 
model_pd.lagr.mean(): -18.637649536132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0105], device='cuda:0')), ('power', tensor([0.9893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08163745701313019
epoch£º3	 i:1 	 global-step:61	 l-p:0.08724100142717361
epoch£º3	 i:2 	 global-step:62	 l-p:0.0003931140818167478
epoch£º3	 i:3 	 global-step:63	 l-p:0.12234185636043549
epoch£º3	 i:4 	 global-step:64	 l-p:0.11887037009000778
epoch£º3	 i:5 	 global-step:65	 l-p:0.11695913225412369
epoch£º3	 i:6 	 global-step:66	 l-p:0.11347240209579468
epoch£º3	 i:7 	 global-step:67	 l-p:0.118540458381176
epoch£º3	 i:8 	 global-step:68	 l-p:0.12620234489440918
epoch£º3	 i:9 	 global-step:69	 l-p:0.12451382726430893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9813, 5.7048, 5.8822],
        [4.9813, 5.8649, 6.1865],
        [4.9813, 6.5220, 7.6084],
        [4.9813, 5.0696, 5.0129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11540451645851135 
model_pd.l_d.mean(): -19.0565185546875 
model_pd.lagr.mean(): -18.94111442565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-19.7592], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11540451645851135
epoch£º4	 i:1 	 global-step:81	 l-p:0.1313904970884323
epoch£º4	 i:2 	 global-step:82	 l-p:0.169345885515213
epoch£º4	 i:3 	 global-step:83	 l-p:0.13243471086025238
epoch£º4	 i:4 	 global-step:84	 l-p:0.15472614765167236
epoch£º4	 i:5 	 global-step:85	 l-p:0.12686507403850555
epoch£º4	 i:6 	 global-step:86	 l-p:0.1303710788488388
epoch£º4	 i:7 	 global-step:87	 l-p:0.13439692556858063
epoch£º4	 i:8 	 global-step:88	 l-p:0.1405920535326004
epoch£º4	 i:9 	 global-step:89	 l-p:0.16175095736980438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6544, 4.6545, 4.6544],
        [4.6544, 4.6605, 4.6548],
        [4.6544, 4.7478, 4.6909],
        [4.6544, 4.7883, 4.7203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18698804080486298 
model_pd.l_d.mean(): -20.1062068939209 
model_pd.lagr.mean(): -19.919218063354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.8780], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18698804080486298
epoch£º5	 i:1 	 global-step:101	 l-p:0.15168964862823486
epoch£º5	 i:2 	 global-step:102	 l-p:0.16872388124465942
epoch£º5	 i:3 	 global-step:103	 l-p:0.08486378192901611
epoch£º5	 i:4 	 global-step:104	 l-p:0.1332530826330185
epoch£º5	 i:5 	 global-step:105	 l-p:0.15382957458496094
epoch£º5	 i:6 	 global-step:106	 l-p:0.12070516496896744
epoch£º5	 i:7 	 global-step:107	 l-p:0.13906459510326385
epoch£º5	 i:8 	 global-step:108	 l-p:0.13696502149105072
epoch£º5	 i:9 	 global-step:109	 l-p:0.13650542497634888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8654, 4.8931, 4.8703],
        [4.8654, 4.8654, 4.8654],
        [4.8654, 6.2734, 7.2129],
        [4.8654, 4.8702, 4.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.1327129453420639 
model_pd.l_d.mean(): -20.583019256591797 
model_pd.lagr.mean(): -20.450305938720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.2438], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.1327129453420639
epoch£º6	 i:1 	 global-step:121	 l-p:0.13211944699287415
epoch£º6	 i:2 	 global-step:122	 l-p:0.13903598487377167
epoch£º6	 i:3 	 global-step:123	 l-p:0.15075430274009705
epoch£º6	 i:4 	 global-step:124	 l-p:0.12953269481658936
epoch£º6	 i:5 	 global-step:125	 l-p:0.15238818526268005
epoch£º6	 i:6 	 global-step:126	 l-p:0.13753648102283478
epoch£º6	 i:7 	 global-step:127	 l-p:0.12962599098682404
epoch£º6	 i:8 	 global-step:128	 l-p:0.1311018466949463
epoch£º6	 i:9 	 global-step:129	 l-p:0.12418383359909058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 5.5407, 5.6706],
        [4.8894, 4.9519, 4.9077],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 4.9105, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.12779361009597778 
model_pd.l_d.mean(): -20.454792022705078 
model_pd.lagr.mean(): -20.326997756958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.1229], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.12779361009597778
epoch£º7	 i:1 	 global-step:141	 l-p:0.11821639537811279
epoch£º7	 i:2 	 global-step:142	 l-p:0.13610459864139557
epoch£º7	 i:3 	 global-step:143	 l-p:0.13800419867038727
epoch£º7	 i:4 	 global-step:144	 l-p:0.14886674284934998
epoch£º7	 i:5 	 global-step:145	 l-p:-0.23914305865764618
epoch£º7	 i:6 	 global-step:146	 l-p:0.12930671870708466
epoch£º7	 i:7 	 global-step:147	 l-p:0.1389794647693634
epoch£º7	 i:8 	 global-step:148	 l-p:0.16634248197078705
epoch£º7	 i:9 	 global-step:149	 l-p:0.14020870625972748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6979, 4.6979, 4.6979],
        [4.6979, 4.6980, 4.6979],
        [4.6979, 5.4962, 5.7741],
        [4.6979, 4.8409, 4.7713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13395299017429352 
model_pd.l_d.mean(): -18.755706787109375 
model_pd.lagr.mean(): -18.621753692626953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-19.5433], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13395299017429352
epoch£º8	 i:1 	 global-step:161	 l-p:0.10942572355270386
epoch£º8	 i:2 	 global-step:162	 l-p:0.1394367814064026
epoch£º8	 i:3 	 global-step:163	 l-p:0.1424546092748642
epoch£º8	 i:4 	 global-step:164	 l-p:0.12771360576152802
epoch£º8	 i:5 	 global-step:165	 l-p:0.1480615735054016
epoch£º8	 i:6 	 global-step:166	 l-p:0.12972122430801392
epoch£º8	 i:7 	 global-step:167	 l-p:0.13172902166843414
epoch£º8	 i:8 	 global-step:168	 l-p:0.14077462255954742
epoch£º8	 i:9 	 global-step:169	 l-p:0.14067493379116058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8656, 4.8674, 4.8656],
        [4.8656, 4.8675, 4.8656],
        [4.8656, 5.0135, 4.9413],
        [4.8656, 4.8864, 4.8686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.1353059858083725 
model_pd.l_d.mean(): -17.66032600402832 
model_pd.lagr.mean(): -17.525020599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5548], device='cuda:0')), ('power', tensor([-18.4200], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.1353059858083725
epoch£º9	 i:1 	 global-step:181	 l-p:0.12824425101280212
epoch£º9	 i:2 	 global-step:182	 l-p:0.13838240504264832
epoch£º9	 i:3 	 global-step:183	 l-p:0.13025569915771484
epoch£º9	 i:4 	 global-step:184	 l-p:0.12077602744102478
epoch£º9	 i:5 	 global-step:185	 l-p:0.14665479958057404
epoch£º9	 i:6 	 global-step:186	 l-p:0.1652500182390213
epoch£º9	 i:7 	 global-step:187	 l-p:0.12909728288650513
epoch£º9	 i:8 	 global-step:188	 l-p:0.12787218391895294
epoch£º9	 i:9 	 global-step:189	 l-p:0.10969669371843338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8094, 6.0339, 6.7596],
        [4.8094, 5.6447, 5.9466],
        [4.8094, 4.8145, 4.8098],
        [4.8094, 4.8094, 4.8094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15206681191921234 
model_pd.l_d.mean(): -20.224409103393555 
model_pd.lagr.mean(): -20.072341918945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.9380], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15206681191921234
epoch£º10	 i:1 	 global-step:201	 l-p:0.13919959962368011
epoch£º10	 i:2 	 global-step:202	 l-p:0.11491088569164276
epoch£º10	 i:3 	 global-step:203	 l-p:0.13253693282604218
epoch£º10	 i:4 	 global-step:204	 l-p:0.1327715516090393
epoch£º10	 i:5 	 global-step:205	 l-p:0.13973239064216614
epoch£º10	 i:6 	 global-step:206	 l-p:0.14992018043994904
epoch£º10	 i:7 	 global-step:207	 l-p:0.1646292805671692
epoch£º10	 i:8 	 global-step:208	 l-p:-0.05705166608095169
epoch£º10	 i:9 	 global-step:209	 l-p:0.141854390501976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7934, 5.3127, 5.3589],
        [4.7934, 6.1000, 6.9314],
        [4.7934, 5.5043, 5.6960],
        [4.7934, 4.8165, 4.7971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.1184033751487732 
model_pd.l_d.mean(): -20.091371536254883 
model_pd.lagr.mean(): -19.97296905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.8150], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.1184033751487732
epoch£º11	 i:1 	 global-step:221	 l-p:0.1498481184244156
epoch£º11	 i:2 	 global-step:222	 l-p:0.12453683465719223
epoch£º11	 i:3 	 global-step:223	 l-p:0.12744592130184174
epoch£º11	 i:4 	 global-step:224	 l-p:0.1430954933166504
epoch£º11	 i:5 	 global-step:225	 l-p:0.13841676712036133
epoch£º11	 i:6 	 global-step:226	 l-p:0.12570175528526306
epoch£º11	 i:7 	 global-step:227	 l-p:0.20622022449970245
epoch£º11	 i:8 	 global-step:228	 l-p:0.1232600063085556
epoch£º11	 i:9 	 global-step:229	 l-p:0.15526577830314636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9188, 5.3213, 5.2957],
        [4.9188, 4.9555, 4.9265],
        [4.9188, 4.9499, 4.9247],
        [4.9188, 5.0891, 5.0141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12307965755462646 
model_pd.l_d.mean(): -19.340322494506836 
model_pd.lagr.mean(): -19.217243194580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.0808], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12307965755462646
epoch£º12	 i:1 	 global-step:241	 l-p:0.13600987195968628
epoch£º12	 i:2 	 global-step:242	 l-p:0.1200462207198143
epoch£º12	 i:3 	 global-step:243	 l-p:0.13655757904052734
epoch£º12	 i:4 	 global-step:244	 l-p:0.13908511400222778
epoch£º12	 i:5 	 global-step:245	 l-p:0.13875554502010345
epoch£º12	 i:6 	 global-step:246	 l-p:0.13550694286823273
epoch£º12	 i:7 	 global-step:247	 l-p:0.13369393348693848
epoch£º12	 i:8 	 global-step:248	 l-p:0.13904595375061035
epoch£º12	 i:9 	 global-step:249	 l-p:0.09761044383049011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8530, 5.9949, 6.6194],
        [4.8530, 4.8530, 4.8530],
        [4.8530, 4.8563, 4.8532],
        [4.8530, 5.2476, 5.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12329404801130295 
model_pd.l_d.mean(): -18.70701026916504 
model_pd.lagr.mean(): -18.583715438842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-19.4482], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12329404801130295
epoch£º13	 i:1 	 global-step:261	 l-p:0.18997308611869812
epoch£º13	 i:2 	 global-step:262	 l-p:0.12834173440933228
epoch£º13	 i:3 	 global-step:263	 l-p:0.1305588185787201
epoch£º13	 i:4 	 global-step:264	 l-p:0.13821491599082947
epoch£º13	 i:5 	 global-step:265	 l-p:0.14281263947486877
epoch£º13	 i:6 	 global-step:266	 l-p:0.11233365535736084
epoch£º13	 i:7 	 global-step:267	 l-p:0.1348501741886139
epoch£º13	 i:8 	 global-step:268	 l-p:0.1386839896440506
epoch£º13	 i:9 	 global-step:269	 l-p:0.12354254722595215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8309, 4.8310, 4.8309],
        [4.8309, 5.7266, 6.0897],
        [4.8309, 4.8650, 4.8379],
        [4.8309, 5.6607, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.15095080435276031 
model_pd.l_d.mean(): -18.576196670532227 
model_pd.lagr.mean(): -18.42524528503418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-19.3109], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.15095080435276031
epoch£º14	 i:1 	 global-step:281	 l-p:0.12489312887191772
epoch£º14	 i:2 	 global-step:282	 l-p:0.16707393527030945
epoch£º14	 i:3 	 global-step:283	 l-p:0.14096592366695404
epoch£º14	 i:4 	 global-step:284	 l-p:0.13556167483329773
epoch£º14	 i:5 	 global-step:285	 l-p:0.12305828183889389
epoch£º14	 i:6 	 global-step:286	 l-p:0.11791884154081345
epoch£º14	 i:7 	 global-step:287	 l-p:0.1315152496099472
epoch£º14	 i:8 	 global-step:288	 l-p:0.1315719485282898
epoch£º14	 i:9 	 global-step:289	 l-p:0.015735134482383728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7477, 4.7786, 4.7538],
        [4.7477, 4.7514, 4.7479],
        [4.7477, 4.9185, 4.8467],
        [4.7477, 4.7582, 4.7488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.1557796448469162 
model_pd.l_d.mean(): -20.54967498779297 
model_pd.lagr.mean(): -20.393896102905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2513], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.1557796448469162
epoch£º15	 i:1 	 global-step:301	 l-p:0.1621803194284439
epoch£º15	 i:2 	 global-step:302	 l-p:0.12420010566711426
epoch£º15	 i:3 	 global-step:303	 l-p:0.14429372549057007
epoch£º15	 i:4 	 global-step:304	 l-p:0.15843601524829865
epoch£º15	 i:5 	 global-step:305	 l-p:0.14607253670692444
epoch£º15	 i:6 	 global-step:306	 l-p:0.11911492794752121
epoch£º15	 i:7 	 global-step:307	 l-p:0.14245352149009705
epoch£º15	 i:8 	 global-step:308	 l-p:0.13309887051582336
epoch£º15	 i:9 	 global-step:309	 l-p:0.20727227628231049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 4.8642, 4.8624],
        [4.8623, 4.8884, 4.8668],
        [4.8623, 4.8627, 4.8623],
        [4.8623, 5.2191, 5.1793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12061798572540283 
model_pd.l_d.mean(): -18.991838455200195 
model_pd.lagr.mean(): -18.871219635009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-19.6958], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12061798572540283
epoch£º16	 i:1 	 global-step:321	 l-p:0.11633148789405823
epoch£º16	 i:2 	 global-step:322	 l-p:0.1511363536119461
epoch£º16	 i:3 	 global-step:323	 l-p:0.135508194565773
epoch£º16	 i:4 	 global-step:324	 l-p:0.12454830855131149
epoch£º16	 i:5 	 global-step:325	 l-p:0.11478723585605621
epoch£º16	 i:6 	 global-step:326	 l-p:0.13394205272197723
epoch£º16	 i:7 	 global-step:327	 l-p:0.154728963971138
epoch£º16	 i:8 	 global-step:328	 l-p:0.13002778589725494
epoch£º16	 i:9 	 global-step:329	 l-p:0.12865930795669556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9090, 5.8365, 6.2249],
        [4.9090, 5.2229, 5.1669],
        [4.9090, 4.9091, 4.9090],
        [4.9090, 5.8243, 6.2004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.16058342158794403 
model_pd.l_d.mean(): -20.177383422851562 
model_pd.lagr.mean(): -20.016799926757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.8588], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.16058342158794403
epoch£º17	 i:1 	 global-step:341	 l-p:0.12124841660261154
epoch£º17	 i:2 	 global-step:342	 l-p:0.13692814111709595
epoch£º17	 i:3 	 global-step:343	 l-p:0.13877080380916595
epoch£º17	 i:4 	 global-step:344	 l-p:0.13086701929569244
epoch£º17	 i:5 	 global-step:345	 l-p:0.12558455765247345
epoch£º17	 i:6 	 global-step:346	 l-p:0.13737726211547852
epoch£º17	 i:7 	 global-step:347	 l-p:0.14227212965488434
epoch£º17	 i:8 	 global-step:348	 l-p:0.13048991560935974
epoch£º17	 i:9 	 global-step:349	 l-p:0.13335993885993958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7509, 4.7988, 4.7634],
        [4.7509, 4.7511, 4.7509],
        [4.7509, 4.8621, 4.8004],
        [4.7509, 4.7632, 4.7522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14244858920574188 
model_pd.l_d.mean(): -20.05828094482422 
model_pd.lagr.mean(): -19.91583251953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8015], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14244858920574188
epoch£º18	 i:1 	 global-step:361	 l-p:0.19642014801502228
epoch£º18	 i:2 	 global-step:362	 l-p:0.13525114953517914
epoch£º18	 i:3 	 global-step:363	 l-p:0.17519471049308777
epoch£º18	 i:4 	 global-step:364	 l-p:0.1319628208875656
epoch£º18	 i:5 	 global-step:365	 l-p:0.13759176433086395
epoch£º18	 i:6 	 global-step:366	 l-p:0.13375209271907806
epoch£º18	 i:7 	 global-step:367	 l-p:0.055890634655952454
epoch£º18	 i:8 	 global-step:368	 l-p:0.14062738418579102
epoch£º18	 i:9 	 global-step:369	 l-p:0.14228597283363342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7707, 4.9428, 4.8715],
        [4.7707, 4.8001, 4.7763],
        [4.7707, 4.7707, 4.7707],
        [4.7707, 4.9752, 4.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.14762663841247559 
model_pd.l_d.mean(): -20.522296905517578 
model_pd.lagr.mean(): -20.374670028686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.2100], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.14762663841247559
epoch£º19	 i:1 	 global-step:381	 l-p:0.14677514135837555
epoch£º19	 i:2 	 global-step:382	 l-p:0.133392795920372
epoch£º19	 i:3 	 global-step:383	 l-p:0.13147057592868805
epoch£º19	 i:4 	 global-step:384	 l-p:0.12496726959943771
epoch£º19	 i:5 	 global-step:385	 l-p:0.14536027610301971
epoch£º19	 i:6 	 global-step:386	 l-p:0.1297917664051056
epoch£º19	 i:7 	 global-step:387	 l-p:0.12461086362600327
epoch£º19	 i:8 	 global-step:388	 l-p:0.1097223088145256
epoch£º19	 i:9 	 global-step:389	 l-p:0.1706894338130951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9244, 4.9271, 4.9245],
        [4.9244, 5.3463, 5.3350],
        [4.9244, 4.9666, 4.9343],
        [4.9244, 4.9853, 4.9425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12436629831790924 
model_pd.l_d.mean(): -20.474315643310547 
model_pd.lagr.mean(): -20.34994888305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.1301], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12436629831790924
epoch£º20	 i:1 	 global-step:401	 l-p:0.1276494413614273
epoch£º20	 i:2 	 global-step:402	 l-p:0.12204654514789581
epoch£º20	 i:3 	 global-step:403	 l-p:0.1947028487920761
epoch£º20	 i:4 	 global-step:404	 l-p:0.14041435718536377
epoch£º20	 i:5 	 global-step:405	 l-p:0.13922639191150665
epoch£º20	 i:6 	 global-step:406	 l-p:0.12474102526903152
epoch£º20	 i:7 	 global-step:407	 l-p:0.13122022151947021
epoch£º20	 i:8 	 global-step:408	 l-p:0.12551279366016388
epoch£º20	 i:9 	 global-step:409	 l-p:0.12827931344509125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9206, 6.3322, 7.2834],
        [4.9206, 4.9820, 4.9390],
        [4.9206, 4.9206, 4.9206],
        [4.9206, 5.0205, 4.9614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13278913497924805 
model_pd.l_d.mean(): -20.315629959106445 
model_pd.lagr.mean(): -20.18284034729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13278913497924805
epoch£º21	 i:1 	 global-step:421	 l-p:0.12889745831489563
epoch£º21	 i:2 	 global-step:422	 l-p:0.13648803532123566
epoch£º21	 i:3 	 global-step:423	 l-p:0.13070148229599
epoch£º21	 i:4 	 global-step:424	 l-p:0.16298243403434753
epoch£º21	 i:5 	 global-step:425	 l-p:0.1352016180753708
epoch£º21	 i:6 	 global-step:426	 l-p:0.15799680352210999
epoch£º21	 i:7 	 global-step:427	 l-p:0.16642898321151733
epoch£º21	 i:8 	 global-step:428	 l-p:0.18877048790454865
epoch£º21	 i:9 	 global-step:429	 l-p:0.13467787206172943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7039, 4.8001, 4.7437],
        [4.7039, 4.9843, 4.9283],
        [4.7039, 4.7060, 4.7040],
        [4.7039, 4.7340, 4.7099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18138812482357025 
model_pd.l_d.mean(): -20.52960968017578 
model_pd.lagr.mean(): -20.348220825195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-21.2463], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18138812482357025
epoch£º22	 i:1 	 global-step:441	 l-p:0.13949161767959595
epoch£º22	 i:2 	 global-step:442	 l-p:0.13412916660308838
epoch£º22	 i:3 	 global-step:443	 l-p:0.13338403403759003
epoch£º22	 i:4 	 global-step:444	 l-p:0.11404068768024445
epoch£º22	 i:5 	 global-step:445	 l-p:0.1460408866405487
epoch£º22	 i:6 	 global-step:446	 l-p:0.11981398612260818
epoch£º22	 i:7 	 global-step:447	 l-p:0.13960473239421844
epoch£º22	 i:8 	 global-step:448	 l-p:0.13568980991840363
epoch£º22	 i:9 	 global-step:449	 l-p:0.12130007147789001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0005, 6.4325, 7.3950],
        [5.0005, 5.5550, 5.6191],
        [5.0005, 5.0010, 5.0005],
        [5.0005, 5.9315, 6.3177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10507181286811829 
model_pd.l_d.mean(): -19.999608993530273 
model_pd.lagr.mean(): -19.8945369720459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-20.6708], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10507181286811829
epoch£º23	 i:1 	 global-step:461	 l-p:0.13177454471588135
epoch£º23	 i:2 	 global-step:462	 l-p:0.13836415112018585
epoch£º23	 i:3 	 global-step:463	 l-p:0.11578373610973358
epoch£º23	 i:4 	 global-step:464	 l-p:0.13820092380046844
epoch£º23	 i:5 	 global-step:465	 l-p:0.13183212280273438
epoch£º23	 i:6 	 global-step:466	 l-p:0.15632280707359314
epoch£º23	 i:7 	 global-step:467	 l-p:0.12825077772140503
epoch£º23	 i:8 	 global-step:468	 l-p:0.1431431621313095
epoch£º23	 i:9 	 global-step:469	 l-p:0.14557777345180511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8297, 5.2773, 5.2875],
        [4.8297, 5.1130, 5.0544],
        [4.8297, 4.9098, 4.8588],
        [4.8297, 4.8297, 4.8297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.1381000131368637 
model_pd.l_d.mean(): -20.95723533630371 
model_pd.lagr.mean(): -20.819135665893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4010], device='cuda:0')), ('power', tensor([-21.5958], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.1381000131368637
epoch£º24	 i:1 	 global-step:481	 l-p:0.14638936519622803
epoch£º24	 i:2 	 global-step:482	 l-p:0.12133977562189102
epoch£º24	 i:3 	 global-step:483	 l-p:0.1202172189950943
epoch£º24	 i:4 	 global-step:484	 l-p:0.1419588178396225
epoch£º24	 i:5 	 global-step:485	 l-p:0.11674606800079346
epoch£º24	 i:6 	 global-step:486	 l-p:0.14352989196777344
epoch£º24	 i:7 	 global-step:487	 l-p:0.1408439427614212
epoch£º24	 i:8 	 global-step:488	 l-p:0.1871345043182373
epoch£º24	 i:9 	 global-step:489	 l-p:0.12445337325334549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 6.0145, 6.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13400670886039734 
model_pd.l_d.mean(): -18.3502140045166 
model_pd.lagr.mean(): -18.21620750427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5760], device='cuda:0')), ('power', tensor([-19.1391], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13400670886039734
epoch£º25	 i:1 	 global-step:501	 l-p:0.13481579720973969
epoch£º25	 i:2 	 global-step:502	 l-p:0.1424388289451599
epoch£º25	 i:3 	 global-step:503	 l-p:0.12411126494407654
epoch£º25	 i:4 	 global-step:504	 l-p:0.38717398047447205
epoch£º25	 i:5 	 global-step:505	 l-p:0.1474734991788864
epoch£º25	 i:6 	 global-step:506	 l-p:0.13004812598228455
epoch£º25	 i:7 	 global-step:507	 l-p:0.1287354975938797
epoch£º25	 i:8 	 global-step:508	 l-p:0.149006187915802
epoch£º25	 i:9 	 global-step:509	 l-p:0.12858454883098602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8317, 5.0213, 4.9503],
        [4.8317, 5.4288, 5.5392],
        [4.8317, 4.8906, 4.8494],
        [4.8317, 5.3466, 5.3990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15572401881217957 
model_pd.l_d.mean(): -20.57049560546875 
model_pd.lagr.mean(): -20.414772033691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15572401881217957
epoch£º26	 i:1 	 global-step:521	 l-p:0.1385820209980011
epoch£º26	 i:2 	 global-step:522	 l-p:0.13959145545959473
epoch£º26	 i:3 	 global-step:523	 l-p:0.12689203023910522
epoch£º26	 i:4 	 global-step:524	 l-p:0.005153617821633816
epoch£º26	 i:5 	 global-step:525	 l-p:0.15632964670658112
epoch£º26	 i:6 	 global-step:526	 l-p:0.14128834009170532
epoch£º26	 i:7 	 global-step:527	 l-p:0.14351819455623627
epoch£º26	 i:8 	 global-step:528	 l-p:0.1405021846294403
epoch£º26	 i:9 	 global-step:529	 l-p:0.12953275442123413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8516, 4.9670, 4.9046],
        [4.8516, 5.1808, 5.1367],
        [4.8516, 4.8516, 4.8516],
        [4.8516, 4.8541, 4.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.1248539611697197 
model_pd.l_d.mean(): -18.25809097290039 
model_pd.lagr.mean(): -18.133237838745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5422], device='cuda:0')), ('power', tensor([-19.0114], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.1248539611697197
epoch£º27	 i:1 	 global-step:541	 l-p:0.13130296766757965
epoch£º27	 i:2 	 global-step:542	 l-p:0.11955617368221283
epoch£º27	 i:3 	 global-step:543	 l-p:0.140310138463974
epoch£º27	 i:4 	 global-step:544	 l-p:0.13030831515789032
epoch£º27	 i:5 	 global-step:545	 l-p:0.13881303369998932
epoch£º27	 i:6 	 global-step:546	 l-p:0.13480719923973083
epoch£º27	 i:7 	 global-step:547	 l-p:0.14729686081409454
epoch£º27	 i:8 	 global-step:548	 l-p:0.3801153600215912
epoch£º27	 i:9 	 global-step:549	 l-p:0.13564196228981018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8318, 5.0296, 4.9593],
        [4.8318, 5.4246, 5.5333],
        [4.8318, 4.8796, 4.8444],
        [4.8318, 6.1340, 6.9726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13329283893108368 
model_pd.l_d.mean(): -19.907880783081055 
model_pd.lagr.mean(): -19.774587631225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.6358], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13329283893108368
epoch£º28	 i:1 	 global-step:561	 l-p:0.16080470383167267
epoch£º28	 i:2 	 global-step:562	 l-p:0.12945446372032166
epoch£º28	 i:3 	 global-step:563	 l-p:0.12828058004379272
epoch£º28	 i:4 	 global-step:564	 l-p:0.13639262318611145
epoch£º28	 i:5 	 global-step:565	 l-p:0.16054610908031464
epoch£º28	 i:6 	 global-step:566	 l-p:0.12322444468736649
epoch£º28	 i:7 	 global-step:567	 l-p:0.12307042628526688
epoch£º28	 i:8 	 global-step:568	 l-p:0.12756823003292084
epoch£º28	 i:9 	 global-step:569	 l-p:0.13800297677516937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0150, 5.0150, 5.0150],
        [5.0150, 5.2199, 5.1464],
        [5.0150, 5.6643, 5.8007],
        [5.0150, 5.0193, 5.0152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.13617467880249023 
model_pd.l_d.mean(): -19.668392181396484 
model_pd.lagr.mean(): -19.532217025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-20.3400], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.13617467880249023
epoch£º29	 i:1 	 global-step:581	 l-p:0.10737056285142899
epoch£º29	 i:2 	 global-step:582	 l-p:0.11962785571813583
epoch£º29	 i:3 	 global-step:583	 l-p:0.13836070895195007
epoch£º29	 i:4 	 global-step:584	 l-p:0.1320611983537674
epoch£º29	 i:5 	 global-step:585	 l-p:0.13992604613304138
epoch£º29	 i:6 	 global-step:586	 l-p:0.2121720314025879
epoch£º29	 i:7 	 global-step:587	 l-p:0.1248352974653244
epoch£º29	 i:8 	 global-step:588	 l-p:0.12869447469711304
epoch£º29	 i:9 	 global-step:589	 l-p:0.13239255547523499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 4.8349, 4.8334],
        [4.8334, 6.1192, 6.9391],
        [4.8334, 5.0449, 4.9758],
        [4.8334, 6.0008, 6.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.15087290108203888 
model_pd.l_d.mean(): -18.95175552368164 
model_pd.lagr.mean(): -18.80088233947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5613], device='cuda:0')), ('power', tensor([-19.7322], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.15087290108203888
epoch£º30	 i:1 	 global-step:601	 l-p:0.1221628338098526
epoch£º30	 i:2 	 global-step:602	 l-p:0.12749530375003815
epoch£º30	 i:3 	 global-step:603	 l-p:0.012070865370333195
epoch£º30	 i:4 	 global-step:604	 l-p:0.13114163279533386
epoch£º30	 i:5 	 global-step:605	 l-p:0.16880202293395996
epoch£º30	 i:6 	 global-step:606	 l-p:0.1379488706588745
epoch£º30	 i:7 	 global-step:607	 l-p:0.1307258903980255
epoch£º30	 i:8 	 global-step:608	 l-p:0.12976482510566711
epoch£º30	 i:9 	 global-step:609	 l-p:0.15520355105400085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7230, 4.7230, 4.7230],
        [4.7230, 4.7309, 4.7237],
        [4.7230, 6.0393, 6.9206],
        [4.7230, 4.7768, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13403606414794922 
model_pd.l_d.mean(): -18.069232940673828 
model_pd.lagr.mean(): -17.935195922851562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6331], device='cuda:0')), ('power', tensor([-18.9134], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13403606414794922
epoch£º31	 i:1 	 global-step:621	 l-p:0.1709807962179184
epoch£º31	 i:2 	 global-step:622	 l-p:0.1206585094332695
epoch£º31	 i:3 	 global-step:623	 l-p:0.13284114003181458
epoch£º31	 i:4 	 global-step:624	 l-p:0.13170944154262543
epoch£º31	 i:5 	 global-step:625	 l-p:0.1265157014131546
epoch£º31	 i:6 	 global-step:626	 l-p:0.14885330200195312
epoch£º31	 i:7 	 global-step:627	 l-p:0.2063954472541809
epoch£º31	 i:8 	 global-step:628	 l-p:0.14710357785224915
epoch£º31	 i:9 	 global-step:629	 l-p:0.1636175662279129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 6.2453, 7.2551],
        [4.8148, 5.7434, 6.1608],
        [4.8148, 4.8712, 4.8316],
        [4.8148, 4.8162, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13486388325691223 
model_pd.l_d.mean(): -19.91747283935547 
model_pd.lagr.mean(): -19.782608032226562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.6232], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13486388325691223
epoch£º32	 i:1 	 global-step:641	 l-p:0.14056812226772308
epoch£º32	 i:2 	 global-step:642	 l-p:0.15270812809467316
epoch£º32	 i:3 	 global-step:643	 l-p:0.12265192717313766
epoch£º32	 i:4 	 global-step:644	 l-p:0.17840057611465454
epoch£º32	 i:5 	 global-step:645	 l-p:0.13909026980400085
epoch£º32	 i:6 	 global-step:646	 l-p:0.1540030688047409
epoch£º32	 i:7 	 global-step:647	 l-p:0.28057363629341125
epoch£º32	 i:8 	 global-step:648	 l-p:0.11922383308410645
epoch£º32	 i:9 	 global-step:649	 l-p:0.13175439834594727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9064, 4.9066, 4.9064],
        [4.9064, 5.1508, 5.0846],
        [4.9064, 4.9070, 4.9064],
        [4.9064, 5.1510, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11790171265602112 
model_pd.l_d.mean(): -20.35063934326172 
model_pd.lagr.mean(): -20.232738494873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4456], device='cuda:0')), ('power', tensor([-21.0282], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11790171265602112
epoch£º33	 i:1 	 global-step:661	 l-p:0.20177046954631805
epoch£º33	 i:2 	 global-step:662	 l-p:0.13896043598651886
epoch£º33	 i:3 	 global-step:663	 l-p:0.11760112643241882
epoch£º33	 i:4 	 global-step:664	 l-p:0.14047475159168243
epoch£º33	 i:5 	 global-step:665	 l-p:0.1333068311214447
epoch£º33	 i:6 	 global-step:666	 l-p:0.14870116114616394
epoch£º33	 i:7 	 global-step:667	 l-p:0.15926702320575714
epoch£º33	 i:8 	 global-step:668	 l-p:0.13533766567707062
epoch£º33	 i:9 	 global-step:669	 l-p:0.1996229737997055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 5.3299, 5.4378],
        [4.7523, 4.7524, 4.7523],
        [4.7523, 5.0221, 4.9659],
        [4.7523, 4.8057, 4.7678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.15001285076141357 
model_pd.l_d.mean(): -20.410314559936523 
model_pd.lagr.mean(): -20.26030158996582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-21.1281], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.15001285076141357
epoch£º34	 i:1 	 global-step:681	 l-p:0.14427439868450165
epoch£º34	 i:2 	 global-step:682	 l-p:0.13363519310951233
epoch£º34	 i:3 	 global-step:683	 l-p:0.12760819494724274
epoch£º34	 i:4 	 global-step:684	 l-p:0.1579214632511139
epoch£º34	 i:5 	 global-step:685	 l-p:0.1421368420124054
epoch£º34	 i:6 	 global-step:686	 l-p:0.1400430053472519
epoch£º34	 i:7 	 global-step:687	 l-p:0.13657207787036896
epoch£º34	 i:8 	 global-step:688	 l-p:0.12412820756435394
epoch£º34	 i:9 	 global-step:689	 l-p:0.17167536914348602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.3872, 5.4008],
        [4.9326, 4.9326, 4.9326],
        [4.9326, 5.0646, 4.9986],
        [4.9326, 6.2882, 7.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.11937913298606873 
model_pd.l_d.mean(): -19.902978897094727 
model_pd.lagr.mean(): -19.783599853515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.6084], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.11937913298606873
epoch£º35	 i:1 	 global-step:701	 l-p:0.13346029818058014
epoch£º35	 i:2 	 global-step:702	 l-p:0.15160129964351654
epoch£º35	 i:3 	 global-step:703	 l-p:0.12814617156982422
epoch£º35	 i:4 	 global-step:704	 l-p:0.1159842237830162
epoch£º35	 i:5 	 global-step:705	 l-p:0.1319243311882019
epoch£º35	 i:6 	 global-step:706	 l-p:0.14844603836536407
epoch£º35	 i:7 	 global-step:707	 l-p:0.1274743378162384
epoch£º35	 i:8 	 global-step:708	 l-p:0.1074056401848793
epoch£º35	 i:9 	 global-step:709	 l-p:0.12258855253458023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8828, 4.8829, 4.8828],
        [4.8828, 4.8828, 4.8828],
        [4.8828, 5.5177, 5.6592],
        [4.8828, 4.9286, 4.8947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.13557147979736328 
model_pd.l_d.mean(): -19.0329647064209 
model_pd.lagr.mean(): -18.89739227294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-19.7650], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.13557147979736328
epoch£º36	 i:1 	 global-step:721	 l-p:0.11875178664922714
epoch£º36	 i:2 	 global-step:722	 l-p:0.14396093785762787
epoch£º36	 i:3 	 global-step:723	 l-p:0.1194518655538559
epoch£º36	 i:4 	 global-step:724	 l-p:0.13529916107654572
epoch£º36	 i:5 	 global-step:725	 l-p:0.15217415988445282
epoch£º36	 i:6 	 global-step:726	 l-p:0.15418721735477448
epoch£º36	 i:7 	 global-step:727	 l-p:0.13982173800468445
epoch£º36	 i:8 	 global-step:728	 l-p:-0.2499726116657257
epoch£º36	 i:9 	 global-step:729	 l-p:0.13167324662208557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8259, 4.8204],
        [4.8199, 6.0991, 6.9193],
        [4.8199, 5.2085, 5.1949],
        [4.8199, 5.3073, 5.3491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.12874698638916016 
model_pd.l_d.mean(): -18.535175323486328 
model_pd.lagr.mean(): -18.406429290771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.3218], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.12874698638916016
epoch£º37	 i:1 	 global-step:741	 l-p:0.12683475017547607
epoch£º37	 i:2 	 global-step:742	 l-p:0.23317866027355194
epoch£º37	 i:3 	 global-step:743	 l-p:0.12618297338485718
epoch£º37	 i:4 	 global-step:744	 l-p:0.14028586447238922
epoch£º37	 i:5 	 global-step:745	 l-p:0.13209809362888336
epoch£º37	 i:6 	 global-step:746	 l-p:0.12678875029087067
epoch£º37	 i:7 	 global-step:747	 l-p:0.11798841506242752
epoch£º37	 i:8 	 global-step:748	 l-p:0.12296479940414429
epoch£º37	 i:9 	 global-step:749	 l-p:0.11887971311807632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9074, 5.0170, 4.9567],
        [4.9074, 4.9725, 4.9285],
        [4.9074, 4.9326, 4.9120],
        [4.9074, 4.9696, 4.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.1629635989665985 
model_pd.l_d.mean(): -19.890819549560547 
model_pd.lagr.mean(): -19.727855682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4836], device='cuda:0')), ('power', tensor([-20.6022], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.1629635989665985
epoch£º38	 i:1 	 global-step:761	 l-p:0.141570046544075
epoch£º38	 i:2 	 global-step:762	 l-p:0.12921111285686493
epoch£º38	 i:3 	 global-step:763	 l-p:0.12469564378261566
epoch£º38	 i:4 	 global-step:764	 l-p:0.1333252638578415
epoch£º38	 i:5 	 global-step:765	 l-p:0.1236858144402504
epoch£º38	 i:6 	 global-step:766	 l-p:0.17689959704875946
epoch£º38	 i:7 	 global-step:767	 l-p:0.13975360989570618
epoch£º38	 i:8 	 global-step:768	 l-p:0.14744101464748383
epoch£º38	 i:9 	 global-step:769	 l-p:0.12311683595180511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7669, 4.7669, 4.7669],
        [4.7669, 5.8097, 6.3630],
        [4.7669, 4.7669, 4.7669],
        [4.7669, 4.7688, 4.7670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13791784644126892 
model_pd.l_d.mean(): -20.780925750732422 
model_pd.lagr.mean(): -20.643007278442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.4596], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13791784644126892
epoch£º39	 i:1 	 global-step:781	 l-p:0.13355830311775208
epoch£º39	 i:2 	 global-step:782	 l-p:0.10831555724143982
epoch£º39	 i:3 	 global-step:783	 l-p:0.13879753649234772
epoch£º39	 i:4 	 global-step:784	 l-p:0.16921548545360565
epoch£º39	 i:5 	 global-step:785	 l-p:0.19251887500286102
epoch£º39	 i:6 	 global-step:786	 l-p:0.14551407098770142
epoch£º39	 i:7 	 global-step:787	 l-p:0.14439640939235687
epoch£º39	 i:8 	 global-step:788	 l-p:0.13691386580467224
epoch£º39	 i:9 	 global-step:789	 l-p:0.12353179603815079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9624, 4.9626, 4.9624],
        [4.9624, 4.9809, 4.9652],
        [4.9624, 5.0747, 5.0134],
        [4.9624, 4.9699, 4.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.11194141209125519 
model_pd.l_d.mean(): -20.549823760986328 
model_pd.lagr.mean(): -20.437881469726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112], device='cuda:0')), ('power', tensor([-21.1944], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.11194141209125519
epoch£º40	 i:1 	 global-step:801	 l-p:0.13013297319412231
epoch£º40	 i:2 	 global-step:802	 l-p:0.11617770791053772
epoch£º40	 i:3 	 global-step:803	 l-p:0.15148364007472992
epoch£º40	 i:4 	 global-step:804	 l-p:0.12384544312953949
epoch£º40	 i:5 	 global-step:805	 l-p:0.1167902946472168
epoch£º40	 i:6 	 global-step:806	 l-p:0.12567059695720673
epoch£º40	 i:7 	 global-step:807	 l-p:0.19722571969032288
epoch£º40	 i:8 	 global-step:808	 l-p:0.1291155368089676
epoch£º40	 i:9 	 global-step:809	 l-p:0.14513877034187317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9346, 4.9372, 4.9347],
        [4.9346, 4.9349, 4.9346],
        [4.9346, 5.1178, 5.0478],
        [4.9346, 5.0489, 4.9874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.1423846334218979 
model_pd.l_d.mean(): -20.475444793701172 
model_pd.lagr.mean(): -20.333059310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.1287], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.1423846334218979
epoch£º41	 i:1 	 global-step:821	 l-p:0.1218782514333725
epoch£º41	 i:2 	 global-step:822	 l-p:0.11777929961681366
epoch£º41	 i:3 	 global-step:823	 l-p:0.13271352648735046
epoch£º41	 i:4 	 global-step:824	 l-p:0.22767023742198944
epoch£º41	 i:5 	 global-step:825	 l-p:0.12362241744995117
epoch£º41	 i:6 	 global-step:826	 l-p:0.13086803257465363
epoch£º41	 i:7 	 global-step:827	 l-p:0.12329777330160141
epoch£º41	 i:8 	 global-step:828	 l-p:0.1310683637857437
epoch£º41	 i:9 	 global-step:829	 l-p:0.1632247120141983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 5.1116, 5.0574],
        [4.8334, 4.8334, 4.8334],
        [4.8334, 6.1941, 7.1179],
        [4.8334, 4.8490, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12239082902669907 
model_pd.l_d.mean(): -18.991313934326172 
model_pd.lagr.mean(): -18.86892318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5236], device='cuda:0')), ('power', tensor([-19.7336], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.12239082902669907
epoch£º42	 i:1 	 global-step:841	 l-p:0.1351308971643448
epoch£º42	 i:2 	 global-step:842	 l-p:0.13941867649555206
epoch£º42	 i:3 	 global-step:843	 l-p:0.2128986269235611
epoch£º42	 i:4 	 global-step:844	 l-p:0.15255475044250488
epoch£º42	 i:5 	 global-step:845	 l-p:0.1472131758928299
epoch£º42	 i:6 	 global-step:846	 l-p:0.13498792052268982
epoch£º42	 i:7 	 global-step:847	 l-p:0.09510616213083267
epoch£º42	 i:8 	 global-step:848	 l-p:0.13840554654598236
epoch£º42	 i:9 	 global-step:849	 l-p:0.16126468777656555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7678, 5.9701, 6.7115],
        [4.7678, 5.3089, 5.3940],
        [4.7678, 5.0661, 5.0204],
        [4.7678, 5.9103, 6.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.06238681077957153 
model_pd.l_d.mean(): -19.928783416748047 
model_pd.lagr.mean(): -19.866395950317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.6763], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.06238681077957153
epoch£º43	 i:1 	 global-step:861	 l-p:0.14900687336921692
epoch£º43	 i:2 	 global-step:862	 l-p:0.13461309671401978
epoch£º43	 i:3 	 global-step:863	 l-p:0.132412850856781
epoch£º43	 i:4 	 global-step:864	 l-p:0.1403106302022934
epoch£º43	 i:5 	 global-step:865	 l-p:0.12816913425922394
epoch£º43	 i:6 	 global-step:866	 l-p:0.13030599057674408
epoch£º43	 i:7 	 global-step:867	 l-p:0.14207778871059418
epoch£º43	 i:8 	 global-step:868	 l-p:0.12005607038736343
epoch£º43	 i:9 	 global-step:869	 l-p:0.14094305038452148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8878, 5.0136, 4.9502],
        [4.8878, 4.8948, 4.8884],
        [4.8878, 4.8901, 4.8879],
        [4.8878, 4.8879, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13652320206165314 
model_pd.l_d.mean(): -20.439420700073242 
model_pd.lagr.mean(): -20.302898406982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1046], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13652320206165314
epoch£º44	 i:1 	 global-step:881	 l-p:0.13807985186576843
epoch£º44	 i:2 	 global-step:882	 l-p:-0.031938180327415466
epoch£º44	 i:3 	 global-step:883	 l-p:0.14037750661373138
epoch£º44	 i:4 	 global-step:884	 l-p:0.13725832104682922
epoch£º44	 i:5 	 global-step:885	 l-p:0.12938593327999115
epoch£º44	 i:6 	 global-step:886	 l-p:0.1125878319144249
epoch£º44	 i:7 	 global-step:887	 l-p:0.15188303589820862
epoch£º44	 i:8 	 global-step:888	 l-p:0.1658264547586441
epoch£º44	 i:9 	 global-step:889	 l-p:0.12697604298591614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8619, 4.8619, 4.8619],
        [4.8619, 4.8629, 4.8619],
        [4.8619, 5.0938, 5.0297],
        [4.8619, 4.8619, 4.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.1325896978378296 
model_pd.l_d.mean(): -20.828224182128906 
model_pd.lagr.mean(): -20.695634841918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.4759], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.1325896978378296
epoch£º45	 i:1 	 global-step:901	 l-p:0.1308077573776245
epoch£º45	 i:2 	 global-step:902	 l-p:0.11158271878957748
epoch£º45	 i:3 	 global-step:903	 l-p:0.1257406622171402
epoch£º45	 i:4 	 global-step:904	 l-p:0.14954012632369995
epoch£º45	 i:5 	 global-step:905	 l-p:-0.007561349775642157
epoch£º45	 i:6 	 global-step:906	 l-p:0.14164231717586517
epoch£º45	 i:7 	 global-step:907	 l-p:0.14175434410572052
epoch£º45	 i:8 	 global-step:908	 l-p:0.16079379618167877
epoch£º45	 i:9 	 global-step:909	 l-p:0.12867426872253418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8885, 4.8845],
        [4.8843, 4.9106, 4.8892],
        [4.8843, 4.8843, 4.8843],
        [4.8843, 4.9100, 4.8891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11819028854370117 
model_pd.l_d.mean(): -19.682544708251953 
model_pd.lagr.mean(): -19.564353942871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.4005], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11819028854370117
epoch£º46	 i:1 	 global-step:921	 l-p:0.24162885546684265
epoch£º46	 i:2 	 global-step:922	 l-p:0.1281239092350006
epoch£º46	 i:3 	 global-step:923	 l-p:0.12558495998382568
epoch£º46	 i:4 	 global-step:924	 l-p:0.140862837433815
epoch£º46	 i:5 	 global-step:925	 l-p:0.1160467341542244
epoch£º46	 i:6 	 global-step:926	 l-p:0.14509223401546478
epoch£º46	 i:7 	 global-step:927	 l-p:0.13225087523460388
epoch£º46	 i:8 	 global-step:928	 l-p:0.1291024088859558
epoch£º46	 i:9 	 global-step:929	 l-p:0.1555924266576767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8283, 4.8840, 4.8452],
        [4.8283, 4.9345, 4.8765],
        [4.8283, 4.8301, 4.8284],
        [4.8283, 5.1694, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.12015414983034134 
model_pd.l_d.mean(): -18.42574119567871 
model_pd.lagr.mean(): -18.305587768554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-19.1709], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.12015414983034134
epoch£º47	 i:1 	 global-step:941	 l-p:0.1428689807653427
epoch£º47	 i:2 	 global-step:942	 l-p:0.1649964451789856
epoch£º47	 i:3 	 global-step:943	 l-p:0.13090990483760834
epoch£º47	 i:4 	 global-step:944	 l-p:0.13788242638111115
epoch£º47	 i:5 	 global-step:945	 l-p:0.1136614978313446
epoch£º47	 i:6 	 global-step:946	 l-p:0.13483527302742004
epoch£º47	 i:7 	 global-step:947	 l-p:0.13506494462490082
epoch£º47	 i:8 	 global-step:948	 l-p:0.1337069720029831
epoch£º47	 i:9 	 global-step:949	 l-p:0.17011424899101257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7862, 5.1243, 5.0940],
        [4.7862, 4.9667, 4.9006],
        [4.7862, 4.7862, 4.7862],
        [4.7862, 5.0162, 4.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17270879447460175 
model_pd.l_d.mean(): -20.852394104003906 
model_pd.lagr.mean(): -20.679685592651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.5241], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17270879447460175
epoch£º48	 i:1 	 global-step:961	 l-p:0.12782061100006104
epoch£º48	 i:2 	 global-step:962	 l-p:0.1354924887418747
epoch£º48	 i:3 	 global-step:963	 l-p:0.12013494223356247
epoch£º48	 i:4 	 global-step:964	 l-p:0.13807503879070282
epoch£º48	 i:5 	 global-step:965	 l-p:0.2121937870979309
epoch£º48	 i:6 	 global-step:966	 l-p:0.12402292340993881
epoch£º48	 i:7 	 global-step:967	 l-p:0.12953267991542816
epoch£º48	 i:8 	 global-step:968	 l-p:0.11851285398006439
epoch£º48	 i:9 	 global-step:969	 l-p:0.11745243519544601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9221, 4.9565, 4.9297],
        [4.9221, 4.9234, 4.9221],
        [4.9221, 5.0805, 5.0129],
        [4.9221, 4.9283, 4.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12594272196292877 
model_pd.l_d.mean(): -20.845367431640625 
model_pd.lagr.mean(): -20.719425201416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3920], device='cuda:0')), ('power', tensor([-21.4735], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12594272196292877
epoch£º49	 i:1 	 global-step:981	 l-p:0.13945697247982025
epoch£º49	 i:2 	 global-step:982	 l-p:0.1596854329109192
epoch£º49	 i:3 	 global-step:983	 l-p:0.13263048231601715
epoch£º49	 i:4 	 global-step:984	 l-p:0.14662562310695648
epoch£º49	 i:5 	 global-step:985	 l-p:0.14537793397903442
epoch£º49	 i:6 	 global-step:986	 l-p:0.13938236236572266
epoch£º49	 i:7 	 global-step:987	 l-p:0.19168692827224731
epoch£º49	 i:8 	 global-step:988	 l-p:0.11981593072414398
epoch£º49	 i:9 	 global-step:989	 l-p:0.14000676572322845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8120, 5.6421, 5.9736],
        [4.8120, 4.8121, 4.8120],
        [4.8120, 5.1291, 5.0902],
        [4.8120, 4.8138, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.13871052861213684 
model_pd.l_d.mean(): -20.09473419189453 
model_pd.lagr.mean(): -19.956024169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.8194], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.13871052861213684
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13916274905204773
epoch£º50	 i:2 	 global-step:1002	 l-p:0.1439143419265747
epoch£º50	 i:3 	 global-step:1003	 l-p:0.1403169482946396
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12623269855976105
epoch£º50	 i:5 	 global-step:1005	 l-p:0.1219882145524025
epoch£º50	 i:6 	 global-step:1006	 l-p:0.13390593230724335
epoch£º50	 i:7 	 global-step:1007	 l-p:0.14577625691890717
epoch£º50	 i:8 	 global-step:1008	 l-p:0.12407293915748596
epoch£º50	 i:9 	 global-step:1009	 l-p:0.13592977821826935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 5.5699, 5.6900],
        [4.9641, 6.2928, 7.1561],
        [4.9641, 4.9642, 4.9641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11433084309101105 
model_pd.l_d.mean(): -18.682985305786133 
model_pd.lagr.mean(): -18.568655014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.4081], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11433084309101105
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10962001234292984
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11620575934648514
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12692944705486298
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12341006845235825
epoch£º51	 i:5 	 global-step:1025	 l-p:0.12480376660823822
epoch£º51	 i:6 	 global-step:1026	 l-p:0.13821537792682648
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18906201422214508
epoch£º51	 i:8 	 global-step:1028	 l-p:0.15353059768676758
epoch£º51	 i:9 	 global-step:1029	 l-p:0.15874481201171875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7054, 5.2840, 5.4070],
        [4.7054, 5.1214, 5.1349],
        [4.7054, 4.7539, 4.7193],
        [4.7054, 4.7054, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.1433434933423996 
model_pd.l_d.mean(): -20.545391082763672 
model_pd.lagr.mean(): -20.402048110961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.2487], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.1433434933423996
epoch£º52	 i:1 	 global-step:1041	 l-p:0.15665388107299805
epoch£º52	 i:2 	 global-step:1042	 l-p:0.13932852447032928
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14809931814670563
epoch£º52	 i:4 	 global-step:1044	 l-p:0.040698956698179245
epoch£º52	 i:5 	 global-step:1045	 l-p:0.14602233469486237
epoch£º52	 i:6 	 global-step:1046	 l-p:0.16352860629558563
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.14820194244384766
epoch£º52	 i:8 	 global-step:1048	 l-p:0.19629371166229248
epoch£º52	 i:9 	 global-step:1049	 l-p:0.1079588308930397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6464, 4.6543, 4.6471],
        [4.6464, 4.6464, 4.6464],
        [4.6464, 4.6470, 4.6464],
        [4.6464, 4.6468, 4.6464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.15337525308132172 
model_pd.l_d.mean(): -19.069427490234375 
model_pd.lagr.mean(): -18.916051864624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-19.8513], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.15337525308132172
epoch£º53	 i:1 	 global-step:1061	 l-p:0.13261768221855164
epoch£º53	 i:2 	 global-step:1062	 l-p:0.13291142880916595
epoch£º53	 i:3 	 global-step:1063	 l-p:-0.6125677227973938
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12235695868730545
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11968018859624863
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12427116185426712
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12091786414384842
epoch£º53	 i:8 	 global-step:1068	 l-p:0.13446836173534393
epoch£º53	 i:9 	 global-step:1069	 l-p:0.13642700016498566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0137, 5.0137, 5.0137],
        [5.0137, 5.7297, 5.9356],
        [5.0137, 5.7817, 6.0332],
        [5.0137, 5.1209, 5.0616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1283033937215805 
model_pd.l_d.mean(): -20.593904495239258 
model_pd.lagr.mean(): -20.465600967407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2171], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1283033937215805
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12306030094623566
epoch£º54	 i:2 	 global-step:1082	 l-p:0.12654836475849152
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11655144393444061
epoch£º54	 i:4 	 global-step:1084	 l-p:0.1251312643289566
epoch£º54	 i:5 	 global-step:1085	 l-p:0.1412605494260788
epoch£º54	 i:6 	 global-step:1086	 l-p:0.04986855760216713
epoch£º54	 i:7 	 global-step:1087	 l-p:0.15599001944065094
epoch£º54	 i:8 	 global-step:1088	 l-p:0.13251300156116486
epoch£º54	 i:9 	 global-step:1089	 l-p:0.14125102758407593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8611, 5.5963, 5.8361],
        [4.8611, 5.8973, 6.4395],
        [4.8611, 5.2947, 5.3095],
        [4.8611, 5.6053, 5.8533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.15564413368701935 
model_pd.l_d.mean(): -20.037389755249023 
model_pd.lagr.mean(): -19.881746292114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-20.7506], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.15564413368701935
epoch£º55	 i:1 	 global-step:1101	 l-p:-0.7833299040794373
epoch£º55	 i:2 	 global-step:1102	 l-p:0.14931800961494446
epoch£º55	 i:3 	 global-step:1103	 l-p:0.15564240515232086
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10965940356254578
epoch£º55	 i:5 	 global-step:1105	 l-p:0.11472295224666595
epoch£º55	 i:6 	 global-step:1106	 l-p:0.1134544238448143
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12120155245065689
epoch£º55	 i:8 	 global-step:1108	 l-p:0.1133360043168068
epoch£º55	 i:9 	 global-step:1109	 l-p:0.14994370937347412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9237, 6.2317, 7.0814],
        [4.9237, 4.9238, 4.9237],
        [4.9237, 5.1621, 5.0997],
        [4.9237, 5.0004, 4.9521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.1831447184085846 
model_pd.l_d.mean(): -20.000200271606445 
model_pd.lagr.mean(): -19.817054748535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.7045], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.1831447184085846
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13572971522808075
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12788935005664825
epoch£º56	 i:3 	 global-step:1123	 l-p:0.12440966069698334
epoch£º56	 i:4 	 global-step:1124	 l-p:0.1221800372004509
epoch£º56	 i:5 	 global-step:1125	 l-p:0.12963561713695526
epoch£º56	 i:6 	 global-step:1126	 l-p:0.13438674807548523
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14420658349990845
epoch£º56	 i:8 	 global-step:1128	 l-p:0.16602858901023865
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14165887236595154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 5.8063, 6.2925],
        [4.8277, 5.9211, 6.5343],
        [4.8277, 4.8278, 4.8277],
        [4.8277, 4.8937, 4.8504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.1610056608915329 
model_pd.l_d.mean(): -20.344396591186523 
model_pd.lagr.mean(): -20.183391571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.1610056608915329
epoch£º57	 i:1 	 global-step:1141	 l-p:0.13890348374843597
epoch£º57	 i:2 	 global-step:1142	 l-p:0.16307611763477325
epoch£º57	 i:3 	 global-step:1143	 l-p:0.059473853558301926
epoch£º57	 i:4 	 global-step:1144	 l-p:0.12492971122264862
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14687636494636536
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14737452566623688
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11315145343542099
epoch£º57	 i:8 	 global-step:1148	 l-p:0.1268625408411026
epoch£º57	 i:9 	 global-step:1149	 l-p:0.11401433497667313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9294, 5.2412, 5.1980],
        [4.9294, 5.1477, 5.0827],
        [4.9294, 6.1823, 6.9652],
        [4.9294, 4.9294, 4.9294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.130782350897789 
model_pd.l_d.mean(): -20.614959716796875 
model_pd.lagr.mean(): -20.484176635742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.130782350897789
epoch£º58	 i:1 	 global-step:1161	 l-p:0.139971524477005
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12424612045288086
epoch£º58	 i:3 	 global-step:1163	 l-p:0.1302543431520462
epoch£º58	 i:4 	 global-step:1164	 l-p:0.15095220506191254
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13278956711292267
epoch£º58	 i:6 	 global-step:1166	 l-p:0.13467714190483093
epoch£º58	 i:7 	 global-step:1167	 l-p:0.1439695954322815
epoch£º58	 i:8 	 global-step:1168	 l-p:0.15477603673934937
epoch£º58	 i:9 	 global-step:1169	 l-p:0.087773397564888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6159, 4.9342, 4.9072],
        [4.6159, 4.6179, 4.6160],
        [4.6159, 4.6159, 4.6159],
        [4.6159, 4.7686, 4.7074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.13096041977405548 
model_pd.l_d.mean(): -18.834447860717773 
model_pd.lagr.mean(): -18.703487396240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5796], device='cuda:0')), ('power', tensor([-19.6323], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.13096041977405548
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14609472453594208
epoch£º59	 i:2 	 global-step:1182	 l-p:0.1655767858028412
epoch£º59	 i:3 	 global-step:1183	 l-p:0.3579756021499634
epoch£º59	 i:4 	 global-step:1184	 l-p:0.15379056334495544
epoch£º59	 i:5 	 global-step:1185	 l-p:0.1806541383266449
epoch£º59	 i:6 	 global-step:1186	 l-p:0.13994164764881134
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12406093627214432
epoch£º59	 i:8 	 global-step:1188	 l-p:0.13059894740581512
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13422566652297974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0033, 5.1118, 5.0527],
        [5.0033, 5.0033, 5.0033],
        [5.0033, 5.0053, 5.0034],
        [5.0033, 5.3503, 5.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15150237083435059 
model_pd.l_d.mean(): -18.20465087890625 
model_pd.lagr.mean(): -18.05314826965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5589], device='cuda:0')), ('power', tensor([-18.9745], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.15150237083435059
epoch£º60	 i:1 	 global-step:1201	 l-p:0.13278675079345703
epoch£º60	 i:2 	 global-step:1202	 l-p:0.11342380940914154
epoch£º60	 i:3 	 global-step:1203	 l-p:0.1155736967921257
epoch£º60	 i:4 	 global-step:1204	 l-p:0.1295892745256424
epoch£º60	 i:5 	 global-step:1205	 l-p:0.12156879156827927
epoch£º60	 i:6 	 global-step:1206	 l-p:0.1259913295507431
epoch£º60	 i:7 	 global-step:1207	 l-p:0.11328820884227753
epoch£º60	 i:8 	 global-step:1208	 l-p:0.14462293684482574
epoch£º60	 i:9 	 global-step:1209	 l-p:0.11627904325723648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8475, 4.8475, 4.8475],
        [4.8475, 5.4710, 5.6208],
        [4.8475, 4.8540, 4.8480],
        [4.8475, 5.5587, 5.7823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.1513148695230484 
model_pd.l_d.mean(): -19.23111915588379 
model_pd.lagr.mean(): -19.079803466796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-19.9577], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.1513148695230484
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14555740356445312
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12608535587787628
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17639903724193573
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13829804956912994
epoch£º61	 i:5 	 global-step:1225	 l-p:0.1172332763671875
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13532571494579315
epoch£º61	 i:7 	 global-step:1227	 l-p:0.12993554770946503
epoch£º61	 i:8 	 global-step:1228	 l-p:0.13775862753391266
epoch£º61	 i:9 	 global-step:1229	 l-p:0.1525101363658905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8236, 4.8550, 4.8305],
        [4.8236, 5.7478, 6.1805],
        [4.8236, 4.9538, 4.8922],
        [4.8236, 5.0357, 4.9733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.16190223395824432 
model_pd.l_d.mean(): -19.9556827545166 
model_pd.lagr.mean(): -19.793781280517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.6929], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.16190223395824432
epoch£º62	 i:1 	 global-step:1241	 l-p:0.13106375932693481
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10828685760498047
epoch£º62	 i:3 	 global-step:1243	 l-p:0.12257890403270721
epoch£º62	 i:4 	 global-step:1244	 l-p:0.17873111367225647
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12623675167560577
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13354641199111938
epoch£º62	 i:7 	 global-step:1247	 l-p:0.13899332284927368
epoch£º62	 i:8 	 global-step:1248	 l-p:0.13777528703212738
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13401314616203308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9335, 4.9361, 4.9336],
        [4.9335, 6.2783, 7.1770],
        [4.9335, 6.2831, 7.1876],
        [4.9335, 4.9334, 4.9334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.1327262818813324 
model_pd.l_d.mean(): -20.042781829833984 
model_pd.lagr.mean(): -19.91005516052246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4594], device='cuda:0')), ('power', tensor([-20.7311], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.1327262818813324
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14917361736297607
epoch£º63	 i:2 	 global-step:1262	 l-p:0.03693312034010887
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15349198877811432
epoch£º63	 i:4 	 global-step:1264	 l-p:0.1160094141960144
epoch£º63	 i:5 	 global-step:1265	 l-p:0.13936839997768402
epoch£º63	 i:6 	 global-step:1266	 l-p:0.1434931606054306
epoch£º63	 i:7 	 global-step:1267	 l-p:0.18242737650871277
epoch£º63	 i:8 	 global-step:1268	 l-p:0.11514609307050705
epoch£º63	 i:9 	 global-step:1269	 l-p:0.021055851131677628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6644, 4.6660, 4.6644],
        [4.6644, 4.6644, 4.6644],
        [4.6644, 4.6905, 4.6697],
        [4.6644, 4.6644, 4.6644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.13140448927879333 
model_pd.l_d.mean(): -19.788148880004883 
model_pd.lagr.mean(): -19.6567440032959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.5652], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.13140448927879333
epoch£º64	 i:1 	 global-step:1281	 l-p:0.14742042124271393
epoch£º64	 i:2 	 global-step:1282	 l-p:0.2516597509384155
epoch£º64	 i:3 	 global-step:1283	 l-p:0.22086749970912933
epoch£º64	 i:4 	 global-step:1284	 l-p:0.11135023087263107
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16896909475326538
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13798896968364716
epoch£º64	 i:7 	 global-step:1287	 l-p:0.1017824336886406
epoch£º64	 i:8 	 global-step:1288	 l-p:0.14322665333747864
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11565317213535309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9468, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0190, 6.5946],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12496856600046158 
model_pd.l_d.mean(): -20.00864028930664 
model_pd.lagr.mean(): -19.883670806884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7012], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12496856600046158
epoch£º65	 i:1 	 global-step:1301	 l-p:0.1392754167318344
epoch£º65	 i:2 	 global-step:1302	 l-p:0.1350025087594986
epoch£º65	 i:3 	 global-step:1303	 l-p:0.13672427833080292
epoch£º65	 i:4 	 global-step:1304	 l-p:0.12113344669342041
epoch£º65	 i:5 	 global-step:1305	 l-p:0.16671538352966309
epoch£º65	 i:6 	 global-step:1306	 l-p:0.1353394240140915
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14534635841846466
epoch£º65	 i:8 	 global-step:1308	 l-p:0.1482008397579193
epoch£º65	 i:9 	 global-step:1309	 l-p:0.28089800477027893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7085, 4.7085, 4.7085],
        [4.7085, 5.6968, 6.2182],
        [4.7085, 5.2369, 5.3288],
        [4.7085, 4.7323, 4.7130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14083263278007507 
model_pd.l_d.mean(): -20.467388153076172 
model_pd.lagr.mean(): -20.326555252075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4909], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14083263278007507
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1394396871328354
epoch£º66	 i:2 	 global-step:1322	 l-p:0.13220196962356567
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12715034186840057
epoch£º66	 i:4 	 global-step:1324	 l-p:0.1659439206123352
epoch£º66	 i:5 	 global-step:1325	 l-p:0.1336546391248703
epoch£º66	 i:6 	 global-step:1326	 l-p:0.13651476800441742
epoch£º66	 i:7 	 global-step:1327	 l-p:0.18176200985908508
epoch£º66	 i:8 	 global-step:1328	 l-p:0.1300731599330902
epoch£º66	 i:9 	 global-step:1329	 l-p:0.1533932387828827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8988, 4.8988, 4.8988],
        [4.8988, 4.8988, 4.8988],
        [4.8988, 5.8759, 6.3563],
        [4.8988, 5.6692, 5.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.1661679744720459 
model_pd.l_d.mean(): -20.00439453125 
model_pd.lagr.mean(): -19.838226318359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4795], device='cuda:0')), ('power', tensor([-20.7127], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.1661679744720459
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11301664263010025
epoch£º67	 i:2 	 global-step:1342	 l-p:0.12897971272468567
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12960119545459747
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13698656857013702
epoch£º67	 i:5 	 global-step:1345	 l-p:0.21121497452259064
epoch£º67	 i:6 	 global-step:1346	 l-p:0.1339307427406311
epoch£º67	 i:7 	 global-step:1347	 l-p:0.12879148125648499
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14463527500629425
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10992683470249176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8884, 6.1549, 6.9688],
        [4.8884, 6.2119, 7.0961],
        [4.8884, 4.8982, 4.8895],
        [4.8884, 5.9383, 6.4999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.120887391269207 
model_pd.l_d.mean(): -19.09025001525879 
model_pd.lagr.mean(): -18.969362258911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-19.8516], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.120887391269207
epoch£º68	 i:1 	 global-step:1361	 l-p:0.35985296964645386
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15067929029464722
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11881551146507263
epoch£º68	 i:4 	 global-step:1364	 l-p:0.1276542693376541
epoch£º68	 i:5 	 global-step:1365	 l-p:0.12374522536993027
epoch£º68	 i:6 	 global-step:1366	 l-p:0.1309642195701599
epoch£º68	 i:7 	 global-step:1367	 l-p:0.15561920404434204
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11845096200704575
epoch£º68	 i:9 	 global-step:1369	 l-p:0.16050876677036285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8355, 4.9999, 4.9361],
        [4.8355, 5.9071, 6.5025],
        [4.8355, 5.0575, 4.9975],
        [4.8355, 4.8355, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.15231022238731384 
model_pd.l_d.mean(): -18.61509132385254 
model_pd.lagr.mean(): -18.46278190612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5706], device='cuda:0')), ('power', tensor([-19.4014], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.15231022238731384
epoch£º69	 i:1 	 global-step:1381	 l-p:0.13183729350566864
epoch£º69	 i:2 	 global-step:1382	 l-p:-0.19903810322284698
epoch£º69	 i:3 	 global-step:1383	 l-p:0.15426766872406006
epoch£º69	 i:4 	 global-step:1384	 l-p:0.12037503719329834
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12480597198009491
epoch£º69	 i:6 	 global-step:1386	 l-p:0.11934345215559006
epoch£º69	 i:7 	 global-step:1387	 l-p:0.13282178342342377
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12728847563266754
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12115192413330078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9306, 6.3162, 7.2714],
        [4.9306, 4.9583, 4.9362],
        [4.9306, 4.9306, 4.9306],
        [4.9306, 6.2265, 7.0700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10400022566318512 
model_pd.l_d.mean(): -17.98586082458496 
model_pd.lagr.mean(): -17.881860733032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5762], device='cuda:0')), ('power', tensor([-18.7710], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10400022566318512
epoch£º70	 i:1 	 global-step:1401	 l-p:0.12656144797801971
epoch£º70	 i:2 	 global-step:1402	 l-p:0.1290477216243744
epoch£º70	 i:3 	 global-step:1403	 l-p:0.14046551287174225
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15240786969661713
epoch£º70	 i:5 	 global-step:1405	 l-p:0.1852770894765854
epoch£º70	 i:6 	 global-step:1406	 l-p:0.1652243435382843
epoch£º70	 i:7 	 global-step:1407	 l-p:0.1028582900762558
epoch£º70	 i:8 	 global-step:1408	 l-p:0.126621812582016
epoch£º70	 i:9 	 global-step:1409	 l-p:0.15000930428504944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8210, 5.0123, 4.9496],
        [4.8210, 4.8240, 4.8211],
        [4.8210, 4.8210, 4.8209],
        [4.8210, 5.4243, 5.5656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.13794618844985962 
model_pd.l_d.mean(): -20.32706642150879 
model_pd.lagr.mean(): -20.189119338989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-21.0375], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.13794618844985962
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1292579174041748
epoch£º71	 i:2 	 global-step:1422	 l-p:0.09325934201478958
epoch£º71	 i:3 	 global-step:1423	 l-p:0.12163195013999939
epoch£º71	 i:4 	 global-step:1424	 l-p:0.16066604852676392
epoch£º71	 i:5 	 global-step:1425	 l-p:0.1391419768333435
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14205442368984222
epoch£º71	 i:7 	 global-step:1427	 l-p:0.16101837158203125
epoch£º71	 i:8 	 global-step:1428	 l-p:0.14341890811920166
epoch£º71	 i:9 	 global-step:1429	 l-p:0.15501441061496735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8509, 4.8509, 4.8509],
        [4.8509, 4.8509, 4.8509],
        [4.8509, 4.8554, 4.8512],
        [4.8509, 4.8509, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12231585383415222 
model_pd.l_d.mean(): -19.196983337402344 
model_pd.lagr.mean(): -19.07466697692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12231585383415222
epoch£º72	 i:1 	 global-step:1441	 l-p:0.14672519266605377
epoch£º72	 i:2 	 global-step:1442	 l-p:0.13569647073745728
epoch£º72	 i:3 	 global-step:1443	 l-p:0.14201027154922485
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14734193682670593
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11704427748918533
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15368564426898956
epoch£º72	 i:7 	 global-step:1447	 l-p:0.1330123245716095
epoch£º72	 i:8 	 global-step:1448	 l-p:0.15642113983631134
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.04157344624400139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8868, 4.8868, 4.8868],
        [4.8868, 5.0443, 4.9803],
        [4.8868, 5.8553, 6.3325],
        [4.8868, 5.1080, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1317950189113617 
model_pd.l_d.mean(): -19.311697006225586 
model_pd.lagr.mean(): -19.179901123046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-20.0034], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.1317950189113617
epoch£º73	 i:1 	 global-step:1461	 l-p:0.14397592842578888
epoch£º73	 i:2 	 global-step:1462	 l-p:0.22139176726341248
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11550021916627884
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12857313454151154
epoch£º73	 i:5 	 global-step:1465	 l-p:0.12094045430421829
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13365907967090607
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14521685242652893
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10627502202987671
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12822923064231873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9009, 4.9188, 4.9037],
        [4.9009, 5.2702, 5.2564],
        [4.9009, 5.6302, 5.8714],
        [4.9009, 5.1214, 5.0604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13521279394626617 
model_pd.l_d.mean(): -19.75572967529297 
model_pd.lagr.mean(): -19.62051773071289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.4845], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13521279394626617
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11785313487052917
epoch£º74	 i:2 	 global-step:1482	 l-p:0.14480061829090118
epoch£º74	 i:3 	 global-step:1483	 l-p:0.1340389996767044
epoch£º74	 i:4 	 global-step:1484	 l-p:0.1252591460943222
epoch£º74	 i:5 	 global-step:1485	 l-p:0.1811923384666443
epoch£º74	 i:6 	 global-step:1486	 l-p:0.18467359244823456
epoch£º74	 i:7 	 global-step:1487	 l-p:0.108695849776268
epoch£º74	 i:8 	 global-step:1488	 l-p:0.1622314602136612
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13272583484649658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8920, 5.2251, 5.1962],
        [4.8920, 5.1021, 5.0402],
        [4.8920, 4.8920, 4.8920],
        [4.8920, 5.6745, 5.9659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.1364307999610901 
model_pd.l_d.mean(): -20.096939086914062 
model_pd.lagr.mean(): -19.960508346557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.1364307999610901
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12809041142463684
epoch£º75	 i:2 	 global-step:1502	 l-p:0.14080646634101868
epoch£º75	 i:3 	 global-step:1503	 l-p:0.14227819442749023
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13767971098423004
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11957218497991562
epoch£º75	 i:6 	 global-step:1506	 l-p:0.16405917704105377
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13617174327373505
epoch£º75	 i:8 	 global-step:1508	 l-p:0.0989271029829979
epoch£º75	 i:9 	 global-step:1509	 l-p:0.1453585922718048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8378, 4.8383, 4.8378],
        [4.8378, 4.8378, 4.8378],
        [4.8378, 5.5728, 5.8274],
        [4.8378, 5.0654, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12408145517110825 
model_pd.l_d.mean(): -19.41663360595703 
model_pd.lagr.mean(): -19.292552947998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1678], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12408145517110825
epoch£º76	 i:1 	 global-step:1521	 l-p:0.1384497731924057
epoch£º76	 i:2 	 global-step:1522	 l-p:0.2667461633682251
epoch£º76	 i:3 	 global-step:1523	 l-p:0.12712188065052032
epoch£º76	 i:4 	 global-step:1524	 l-p:0.11962025612592697
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11829330027103424
epoch£º76	 i:6 	 global-step:1526	 l-p:0.11112577468156815
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11747150123119354
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12500230967998505
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16276681423187256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9340, 4.9511, 4.9366],
        [4.9340, 4.9343, 4.9340],
        [4.9340, 4.9758, 4.9450],
        [4.9340, 5.0350, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12634332478046417 
model_pd.l_d.mean(): -20.453214645385742 
model_pd.lagr.mean(): -20.326871871948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.1179], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12634332478046417
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11698198318481445
epoch£º77	 i:2 	 global-step:1542	 l-p:0.1516093611717224
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10698387771844864
epoch£º77	 i:4 	 global-step:1544	 l-p:0.13720321655273438
epoch£º77	 i:5 	 global-step:1545	 l-p:0.19167126715183258
epoch£º77	 i:6 	 global-step:1546	 l-p:0.1266685277223587
epoch£º77	 i:7 	 global-step:1547	 l-p:0.17548426985740662
epoch£º77	 i:8 	 global-step:1548	 l-p:0.2001255750656128
epoch£º77	 i:9 	 global-step:1549	 l-p:0.1402096003293991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8063, 5.3959, 5.5314],
        [4.8063, 5.6717, 6.0568],
        [4.8063, 4.9062, 4.8523],
        [4.8063, 4.8570, 4.8217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.1439308375120163 
model_pd.l_d.mean(): -19.923086166381836 
model_pd.lagr.mean(): -19.779155731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.6659], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.1439308375120163
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13526515662670135
epoch£º78	 i:2 	 global-step:1562	 l-p:0.1694849580526352
epoch£º78	 i:3 	 global-step:1563	 l-p:0.1343766152858734
epoch£º78	 i:4 	 global-step:1564	 l-p:0.16377493739128113
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12506103515625
epoch£º78	 i:6 	 global-step:1566	 l-p:0.08515740185976028
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13314372301101685
epoch£º78	 i:8 	 global-step:1568	 l-p:0.14538829028606415
epoch£º78	 i:9 	 global-step:1569	 l-p:0.1405046284198761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8254, 5.4681, 5.6451],
        [4.8254, 4.8254, 4.8254],
        [4.8254, 4.8270, 4.8254],
        [4.8254, 4.8296, 4.8257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14757387340068817 
model_pd.l_d.mean(): -20.45145606994629 
model_pd.lagr.mean(): -20.303882598876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1535], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14757387340068817
epoch£º79	 i:1 	 global-step:1581	 l-p:0.17189069092273712
epoch£º79	 i:2 	 global-step:1582	 l-p:0.13692837953567505
epoch£º79	 i:3 	 global-step:1583	 l-p:0.13718944787979126
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12769107520580292
epoch£º79	 i:5 	 global-step:1585	 l-p:0.19374839961528778
epoch£º79	 i:6 	 global-step:1586	 l-p:0.2300557643175125
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14395691454410553
epoch£º79	 i:8 	 global-step:1588	 l-p:0.1554255485534668
epoch£º79	 i:9 	 global-step:1589	 l-p:0.1019587516784668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8129, 6.1495, 7.0742],
        [4.8129, 4.8290, 4.8153],
        [4.8129, 4.8129, 4.8129],
        [4.8129, 5.7480, 6.2052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.17077438533306122 
model_pd.l_d.mean(): -19.077167510986328 
model_pd.lagr.mean(): -18.90639305114746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8170], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.17077438533306122
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12721002101898193
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13936732709407806
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12071208655834198
epoch£º80	 i:4 	 global-step:1604	 l-p:0.24613571166992188
epoch£º80	 i:5 	 global-step:1605	 l-p:0.13247403502464294
epoch£º80	 i:6 	 global-step:1606	 l-p:0.1286044716835022
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13222678005695343
epoch£º80	 i:8 	 global-step:1608	 l-p:0.12685231864452362
epoch£º80	 i:9 	 global-step:1609	 l-p:0.1311706006526947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 4.9184, 4.9130],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 5.8766, 6.3501],
        [4.9125, 4.9827, 4.9382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12236098200082779 
model_pd.l_d.mean(): -20.220550537109375 
model_pd.lagr.mean(): -20.098190307617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.12236098200082779
epoch£º81	 i:1 	 global-step:1621	 l-p:0.14380495250225067
epoch£º81	 i:2 	 global-step:1622	 l-p:0.13110943138599396
epoch£º81	 i:3 	 global-step:1623	 l-p:0.1558549702167511
epoch£º81	 i:4 	 global-step:1624	 l-p:0.1652332991361618
epoch£º81	 i:5 	 global-step:1625	 l-p:0.5328207612037659
epoch£º81	 i:6 	 global-step:1626	 l-p:0.13285210728645325
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.012265338562428951
epoch£º81	 i:8 	 global-step:1628	 l-p:-1.4960047006607056
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13620352745056152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 4.7941, 4.7523],
        [4.7284, 5.3703, 5.5580],
        [4.7284, 4.9282, 4.8703],
        [4.7284, 5.7550, 6.3262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.25181496143341064 
model_pd.l_d.mean(): -20.29587745666504 
model_pd.lagr.mean(): -20.0440616607666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-21.0370], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.25181496143341064
epoch£º82	 i:1 	 global-step:1641	 l-p:0.13042466342449188
epoch£º82	 i:2 	 global-step:1642	 l-p:0.153426393866539
epoch£º82	 i:3 	 global-step:1643	 l-p:0.16693000495433807
epoch£º82	 i:4 	 global-step:1644	 l-p:0.13605596125125885
epoch£º82	 i:5 	 global-step:1645	 l-p:0.1375766545534134
epoch£º82	 i:6 	 global-step:1646	 l-p:0.1504659205675125
epoch£º82	 i:7 	 global-step:1647	 l-p:0.1373671144247055
epoch£º82	 i:8 	 global-step:1648	 l-p:0.12850777804851532
epoch£º82	 i:9 	 global-step:1649	 l-p:0.1381448358297348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9474, 5.8657, 6.2876],
        [4.9474, 5.1824, 5.1241],
        [4.9474, 4.9480, 4.9474],
        [4.9474, 5.4525, 5.5167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11888934671878815 
model_pd.l_d.mean(): -19.251291275024414 
model_pd.lagr.mean(): -19.132402420043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9706], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11888934671878815
epoch£º83	 i:1 	 global-step:1661	 l-p:0.10654883086681366
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13716958463191986
epoch£º83	 i:3 	 global-step:1663	 l-p:0.13136903941631317
epoch£º83	 i:4 	 global-step:1664	 l-p:0.127244234085083
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12627343833446503
epoch£º83	 i:6 	 global-step:1666	 l-p:0.13229279220104218
epoch£º83	 i:7 	 global-step:1667	 l-p:0.15468184649944305
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.19513587653636932
epoch£º83	 i:9 	 global-step:1669	 l-p:0.13777323067188263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8804, 4.8806, 4.8804],
        [4.8804, 4.8888, 4.8813],
        [4.8804, 4.9535, 4.9081],
        [4.8804, 5.1099, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13601097464561462 
model_pd.l_d.mean(): -20.28618812561035 
model_pd.lagr.mean(): -20.150177001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-20.9916], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13601097464561462
epoch£º84	 i:1 	 global-step:1681	 l-p:0.12280212342739105
epoch£º84	 i:2 	 global-step:1682	 l-p:0.13972364366054535
epoch£º84	 i:3 	 global-step:1683	 l-p:0.1486397534608841
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12543366849422455
epoch£º84	 i:5 	 global-step:1685	 l-p:0.14656971395015717
epoch£º84	 i:6 	 global-step:1686	 l-p:0.1846621036529541
epoch£º84	 i:7 	 global-step:1687	 l-p:0.0674673467874527
epoch£º84	 i:8 	 global-step:1688	 l-p:0.13643640279769897
epoch£º84	 i:9 	 global-step:1689	 l-p:0.1347390115261078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8291, 5.5559, 5.8086],
        [4.8291, 5.2917, 5.3377],
        [4.8291, 5.6603, 6.0119],
        [4.8291, 5.1527, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.1614164113998413 
model_pd.l_d.mean(): -20.015277862548828 
model_pd.lagr.mean(): -19.85386085510254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.1614164113998413
epoch£º85	 i:1 	 global-step:1701	 l-p:0.13557301461696625
epoch£º85	 i:2 	 global-step:1702	 l-p:0.0914052352309227
epoch£º85	 i:3 	 global-step:1703	 l-p:0.15843717753887177
epoch£º85	 i:4 	 global-step:1704	 l-p:0.15323865413665771
epoch£º85	 i:5 	 global-step:1705	 l-p:0.16742481291294098
epoch£º85	 i:6 	 global-step:1706	 l-p:0.1187339723110199
epoch£º85	 i:7 	 global-step:1707	 l-p:0.12003085762262344
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13899661600589752
epoch£º85	 i:9 	 global-step:1709	 l-p:0.12462026625871658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9069, 5.7601, 6.1237],
        [4.9069, 4.9165, 4.9079],
        [4.9069, 4.9069, 4.9069],
        [4.9069, 5.2179, 5.1828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.5806422233581543 
model_pd.l_d.mean(): -17.353965759277344 
model_pd.lagr.mean(): -16.77332305908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6600], device='cuda:0')), ('power', tensor([-18.2177], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.5806422233581543
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11675962805747986
epoch£º86	 i:2 	 global-step:1722	 l-p:0.13129590451717377
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14097940921783447
epoch£º86	 i:4 	 global-step:1724	 l-p:0.12560829520225525
epoch£º86	 i:5 	 global-step:1725	 l-p:0.12544699013233185
epoch£º86	 i:6 	 global-step:1726	 l-p:0.1308078020811081
epoch£º86	 i:7 	 global-step:1727	 l-p:0.13612017035484314
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14059151709079742
epoch£º86	 i:9 	 global-step:1729	 l-p:0.19269892573356628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6877, 4.7752, 4.7262],
        [4.6877, 4.6917, 4.6880],
        [4.6877, 4.6881, 4.6877],
        [4.6877, 5.1693, 5.2391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.152975931763649 
model_pd.l_d.mean(): -19.670846939086914 
model_pd.lagr.mean(): -19.517871856689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5793], device='cuda:0')), ('power', tensor([-20.4775], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.152975931763649
epoch£º87	 i:1 	 global-step:1741	 l-p:0.13629524409770966
epoch£º87	 i:2 	 global-step:1742	 l-p:0.13600771129131317
epoch£º87	 i:3 	 global-step:1743	 l-p:0.17059384286403656
epoch£º87	 i:4 	 global-step:1744	 l-p:0.05628447234630585
epoch£º87	 i:5 	 global-step:1745	 l-p:0.010132646188139915
epoch£º87	 i:6 	 global-step:1746	 l-p:0.13078603148460388
epoch£º87	 i:7 	 global-step:1747	 l-p:0.221623033285141
epoch£º87	 i:8 	 global-step:1748	 l-p:0.1490994393825531
epoch£º87	 i:9 	 global-step:1749	 l-p:0.1465933620929718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8849, 4.8849, 4.8849],
        [4.8849, 5.1777, 5.1376],
        [4.8849, 4.9856, 4.9315],
        [4.8849, 4.8902, 4.8853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.15383805334568024 
model_pd.l_d.mean(): -20.64870834350586 
model_pd.lagr.mean(): -20.494871139526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.3139], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.15383805334568024
epoch£º88	 i:1 	 global-step:1761	 l-p:0.1266869306564331
epoch£º88	 i:2 	 global-step:1762	 l-p:0.129228413105011
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13921025395393372
epoch£º88	 i:4 	 global-step:1764	 l-p:0.13166815042495728
epoch£º88	 i:5 	 global-step:1765	 l-p:0.14828196167945862
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14860358834266663
epoch£º88	 i:7 	 global-step:1767	 l-p:0.1481907218694687
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11828794330358505
epoch£º88	 i:9 	 global-step:1769	 l-p:0.11790736019611359
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7805, 5.7454, 6.2442],
        [4.7805, 4.8045, 4.7853],
        [4.7805, 4.8130, 4.7883],
        [4.7805, 4.9300, 4.8700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11386767774820328 
model_pd.l_d.mean(): -19.35983657836914 
model_pd.lagr.mean(): -19.245969772338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5823], device='cuda:0')), ('power', tensor([-20.1662], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11386767774820328
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15337347984313965
epoch£º89	 i:2 	 global-step:1782	 l-p:0.1326214224100113
epoch£º89	 i:3 	 global-step:1783	 l-p:0.1501663625240326
epoch£º89	 i:4 	 global-step:1784	 l-p:0.21535566449165344
epoch£º89	 i:5 	 global-step:1785	 l-p:0.1411198526620865
epoch£º89	 i:6 	 global-step:1786	 l-p:0.14539697766304016
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.1791847199201584
epoch£º89	 i:8 	 global-step:1788	 l-p:0.11794480681419373
epoch£º89	 i:9 	 global-step:1789	 l-p:0.1275351494550705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9758, 6.0879, 6.7186],
        [4.9758, 4.9758, 4.9758],
        [4.9758, 4.9759, 4.9758],
        [4.9758, 5.0137, 4.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.1687004119157791 
model_pd.l_d.mean(): -19.37898826599121 
model_pd.lagr.mean(): -19.21028709411621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-20.0688], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.1687004119157791
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11208038777112961
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12222911417484283
epoch£º90	 i:3 	 global-step:1803	 l-p:0.11916191130876541
epoch£º90	 i:4 	 global-step:1804	 l-p:0.12066420912742615
epoch£º90	 i:5 	 global-step:1805	 l-p:0.13653209805488586
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12312009185552597
epoch£º90	 i:7 	 global-step:1807	 l-p:0.1575876921415329
epoch£º90	 i:8 	 global-step:1808	 l-p:0.1470315307378769
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13849371671676636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7746, 4.7746, 4.7746],
        [4.7746, 4.7749, 4.7746],
        [4.7746, 4.7805, 4.7751],
        [4.7746, 5.5271, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13219788670539856 
model_pd.l_d.mean(): -19.322614669799805 
model_pd.lagr.mean(): -19.19041633605957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.0467], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13219788670539856
epoch£º91	 i:1 	 global-step:1821	 l-p:0.1866142600774765
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2720552682876587
epoch£º91	 i:3 	 global-step:1823	 l-p:0.1359407752752304
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14104920625686646
epoch£º91	 i:5 	 global-step:1825	 l-p:0.16389533877372742
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10022560507059097
epoch£º91	 i:7 	 global-step:1827	 l-p:0.1293569952249527
epoch£º91	 i:8 	 global-step:1828	 l-p:0.13096600770950317
epoch£º91	 i:9 	 global-step:1829	 l-p:0.15265703201293945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8991, 4.8991, 4.8991],
        [4.8991, 5.9394, 6.5036],
        [4.8991, 5.0452, 4.9839],
        [4.8991, 4.9582, 4.9189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.13247406482696533 
model_pd.l_d.mean(): -19.615846633911133 
model_pd.lagr.mean(): -19.48337173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.3764], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.13247406482696533
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13203538954257965
epoch£º92	 i:2 	 global-step:1842	 l-p:0.11994174867868423
epoch£º92	 i:3 	 global-step:1843	 l-p:-1.6540980339050293
epoch£º92	 i:4 	 global-step:1844	 l-p:0.1377527266740799
epoch£º92	 i:5 	 global-step:1845	 l-p:0.14413277804851532
epoch£º92	 i:6 	 global-step:1846	 l-p:0.15493127703666687
epoch£º92	 i:7 	 global-step:1847	 l-p:0.14308792352676392
epoch£º92	 i:8 	 global-step:1848	 l-p:0.10595330595970154
epoch£º92	 i:9 	 global-step:1849	 l-p:0.1284068524837494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9038, 4.9038, 4.9038],
        [4.9038, 5.2691, 5.2589],
        [4.9038, 5.3029, 5.3097],
        [4.9038, 5.1776, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12621325254440308 
model_pd.l_d.mean(): -19.67447853088379 
model_pd.lagr.mean(): -19.54826545715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.3844], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12621325254440308
epoch£º93	 i:1 	 global-step:1861	 l-p:0.1303117722272873
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12462875247001648
epoch£º93	 i:3 	 global-step:1863	 l-p:0.1452396959066391
epoch£º93	 i:4 	 global-step:1864	 l-p:0.15493358671665192
epoch£º93	 i:5 	 global-step:1865	 l-p:0.188743457198143
epoch£º93	 i:6 	 global-step:1866	 l-p:0.14262641966342926
epoch£º93	 i:7 	 global-step:1867	 l-p:0.14957430958747864
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.16093741357326508
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12663023173809052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9840, 5.5303, 5.6246],
        [4.9840, 4.9840, 4.9840],
        [4.9840, 5.0285, 4.9964],
        [4.9840, 4.9985, 4.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.14908672869205475 
model_pd.l_d.mean(): -20.69589614868164 
model_pd.lagr.mean(): -20.546810150146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.3255], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.14908672869205475
epoch£º94	 i:1 	 global-step:1881	 l-p:0.1132446750998497
epoch£º94	 i:2 	 global-step:1882	 l-p:0.17031750082969666
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12375131249427795
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12355716526508331
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11606143414974213
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11329995840787888
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12080714851617813
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13050827383995056
epoch£º94	 i:9 	 global-step:1889	 l-p:0.15641695261001587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8952, 4.8953, 4.8952],
        [4.8952, 4.8952, 4.8952],
        [4.8952, 4.8966, 4.8952],
        [4.8952, 4.9106, 4.8975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12278854101896286 
model_pd.l_d.mean(): -19.264141082763672 
model_pd.lagr.mean(): -19.1413516998291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-20.0075], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.12278854101896286
epoch£º95	 i:1 	 global-step:1901	 l-p:0.14851945638656616
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13646864891052246
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12951670587062836
epoch£º95	 i:4 	 global-step:1904	 l-p:0.12773524224758148
epoch£º95	 i:5 	 global-step:1905	 l-p:0.2341083288192749
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13593223690986633
epoch£º95	 i:7 	 global-step:1907	 l-p:0.13735422492027283
epoch£º95	 i:8 	 global-step:1908	 l-p:0.1359027475118637
epoch£º95	 i:9 	 global-step:1909	 l-p:0.3693349063396454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7908, 5.9199, 6.6059],
        [4.7908, 4.7914, 4.7908],
        [4.7908, 5.5351, 5.8141],
        [4.7908, 4.7908, 4.7908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.10403774678707123 
model_pd.l_d.mean(): -18.36155128479004 
model_pd.lagr.mean(): -18.25751304626465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6453], device='cuda:0')), ('power', tensor([-19.2214], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.10403774678707123
epoch£º96	 i:1 	 global-step:1921	 l-p:0.15209586918354034
epoch£º96	 i:2 	 global-step:1922	 l-p:0.12140300124883652
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13412059843540192
epoch£º96	 i:4 	 global-step:1924	 l-p:0.13842962682247162
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13624054193496704
epoch£º96	 i:6 	 global-step:1926	 l-p:0.1773214340209961
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12007714807987213
epoch£º96	 i:8 	 global-step:1928	 l-p:0.1398763656616211
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12690073251724243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.5216, 5.5699],
        [5.0394, 5.9005, 6.2611],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 6.3564, 7.2213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.1471766233444214 
model_pd.l_d.mean(): -20.299833297729492 
model_pd.lagr.mean(): -20.15265655517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-20.9457], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.1471766233444214
epoch£º97	 i:1 	 global-step:1941	 l-p:0.1223020926117897
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10776962339878082
epoch£º97	 i:3 	 global-step:1943	 l-p:0.1482992321252823
epoch£º97	 i:4 	 global-step:1944	 l-p:0.18553447723388672
epoch£º97	 i:5 	 global-step:1945	 l-p:0.13937632739543915
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11608065664768219
epoch£º97	 i:7 	 global-step:1947	 l-p:0.13259337842464447
epoch£º97	 i:8 	 global-step:1948	 l-p:0.14327242970466614
epoch£º97	 i:9 	 global-step:1949	 l-p:0.12731705605983734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 5.0911, 5.0908],
        [4.7284, 4.7284, 4.7284],
        [4.7284, 4.7677, 4.7392],
        [4.7284, 5.3218, 5.4769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13279296457767487 
model_pd.l_d.mean(): -20.321035385131836 
model_pd.lagr.mean(): -20.188241958618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-21.0655], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13279296457767487
epoch£º98	 i:1 	 global-step:1961	 l-p:0.14916272461414337
epoch£º98	 i:2 	 global-step:1962	 l-p:0.12943878769874573
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.030753087252378464
epoch£º98	 i:4 	 global-step:1964	 l-p:0.17162902653217316
epoch£º98	 i:5 	 global-step:1965	 l-p:0.1813340187072754
epoch£º98	 i:6 	 global-step:1966	 l-p:0.1363552063703537
epoch£º98	 i:7 	 global-step:1967	 l-p:0.14896631240844727
epoch£º98	 i:8 	 global-step:1968	 l-p:0.12327378988265991
epoch£º98	 i:9 	 global-step:1969	 l-p:0.13717719912528992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9241, 4.9241],
        [4.9241, 4.9242, 4.9241],
        [4.9241, 5.3863, 5.4309],
        [4.9241, 4.9241, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.14524108171463013 
model_pd.l_d.mean(): -19.399749755859375 
model_pd.lagr.mean(): -19.25450897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4568], device='cuda:0')), ('power', tensor([-20.0783], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.14524108171463013
epoch£º99	 i:1 	 global-step:1981	 l-p:0.1409168541431427
epoch£º99	 i:2 	 global-step:1982	 l-p:0.11232909560203552
epoch£º99	 i:3 	 global-step:1983	 l-p:0.14403539896011353
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12521050870418549
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.04702020436525345
epoch£º99	 i:6 	 global-step:1986	 l-p:0.1229366883635521
epoch£º99	 i:7 	 global-step:1987	 l-p:0.1597285121679306
epoch£º99	 i:8 	 global-step:1988	 l-p:0.12660224735736847
epoch£º99	 i:9 	 global-step:1989	 l-p:0.13951082527637482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8764, 4.8770, 4.8764],
        [4.8764, 5.3748, 5.4471],
        [4.8764, 4.9642, 4.9146],
        [4.8764, 4.8764, 4.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.1619422286748886 
model_pd.l_d.mean(): -20.743160247802734 
model_pd.lagr.mean(): -20.581218719482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.4021], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.1619422286748886
epoch£º100	 i:1 	 global-step:2001	 l-p:0.13343745470046997
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11211994290351868
epoch£º100	 i:3 	 global-step:2003	 l-p:0.11762907356023788
epoch£º100	 i:4 	 global-step:2004	 l-p:0.1396699994802475
epoch£º100	 i:5 	 global-step:2005	 l-p:0.14430555701255798
epoch£º100	 i:6 	 global-step:2006	 l-p:0.13814547657966614
epoch£º100	 i:7 	 global-step:2007	 l-p:2.388664722442627
epoch£º100	 i:8 	 global-step:2008	 l-p:-0.6517614126205444
epoch£º100	 i:9 	 global-step:2009	 l-p:0.1707194447517395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8123, 4.8712, 4.8328],
        [4.8123, 4.8124, 4.8123],
        [4.8123, 4.8144, 4.8124],
        [4.8123, 4.8123, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.13606299459934235 
model_pd.l_d.mean(): -20.105234146118164 
model_pd.lagr.mean(): -19.96917152404785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.13606299459934235
epoch£º101	 i:1 	 global-step:2021	 l-p:0.11696898192167282
epoch£º101	 i:2 	 global-step:2022	 l-p:0.1056220531463623
epoch£º101	 i:3 	 global-step:2023	 l-p:0.040979333221912384
epoch£º101	 i:4 	 global-step:2024	 l-p:0.13702110946178436
epoch£º101	 i:5 	 global-step:2025	 l-p:0.13371717929840088
epoch£º101	 i:6 	 global-step:2026	 l-p:0.1180359274148941
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12559863924980164
epoch£º101	 i:8 	 global-step:2028	 l-p:0.14421938359737396
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1384148746728897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9760, 5.6923, 5.9283],
        [4.9760, 5.6971, 5.9374],
        [4.9760, 6.2049, 6.9787],
        [4.9760, 5.1470, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11542589217424393 
model_pd.l_d.mean(): -20.078229904174805 
model_pd.lagr.mean(): -19.962804794311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.7715], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11542589217424393
epoch£º102	 i:1 	 global-step:2041	 l-p:0.16333810985088348
epoch£º102	 i:2 	 global-step:2042	 l-p:0.015342039987444878
epoch£º102	 i:3 	 global-step:2043	 l-p:0.14636169373989105
epoch£º102	 i:4 	 global-step:2044	 l-p:0.14009656012058258
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13479092717170715
epoch£º102	 i:6 	 global-step:2046	 l-p:0.14273516833782196
epoch£º102	 i:7 	 global-step:2047	 l-p:0.1274956464767456
epoch£º102	 i:8 	 global-step:2048	 l-p:0.14488916099071503
epoch£º102	 i:9 	 global-step:2049	 l-p:0.23212814331054688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6879, 4.7791, 4.7302],
        [4.6879, 5.0149, 5.0012],
        [4.6879, 4.7728, 4.7256],
        [4.6879, 4.6979, 4.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.12454938888549805 
model_pd.l_d.mean(): -19.899656295776367 
model_pd.lagr.mean(): -19.77510643005371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5609], device='cuda:0')), ('power', tensor([-20.6900], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.12454938888549805
epoch£º103	 i:1 	 global-step:2061	 l-p:0.07462145388126373
epoch£º103	 i:2 	 global-step:2062	 l-p:0.15335874259471893
epoch£º103	 i:3 	 global-step:2063	 l-p:0.1610567271709442
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13235916197299957
epoch£º103	 i:5 	 global-step:2065	 l-p:0.13750559091567993
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13019506633281708
epoch£º103	 i:7 	 global-step:2067	 l-p:0.15221750736236572
epoch£º103	 i:8 	 global-step:2068	 l-p:0.1304822713136673
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12791956961154938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9658, 4.9676, 4.9658],
        [4.9658, 5.1130, 5.0521],
        [4.9658, 5.0240, 4.9853],
        [4.9658, 5.8964, 6.3411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12891142070293427 
model_pd.l_d.mean(): -20.71617317199707 
model_pd.lagr.mean(): -20.587261199951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3973], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12891142070293427
epoch£º104	 i:1 	 global-step:2081	 l-p:0.3811779320240021
epoch£º104	 i:2 	 global-step:2082	 l-p:0.15308988094329834
epoch£º104	 i:3 	 global-step:2083	 l-p:0.15865318477153778
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12984693050384521
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12669938802719116
epoch£º104	 i:6 	 global-step:2086	 l-p:0.1257614642381668
epoch£º104	 i:7 	 global-step:2087	 l-p:0.14561471343040466
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12466847151517868
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13258740305900574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8395, 4.8395, 4.8395],
        [4.8395, 4.8395, 4.8395],
        [4.8395, 5.0425, 4.9861],
        [4.8395, 5.3607, 5.4540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.16077537834644318 
model_pd.l_d.mean(): -20.348169326782227 
model_pd.lagr.mean(): -20.187393188476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.0498], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.16077537834644318
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16111865639686584
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12355159223079681
epoch£º105	 i:3 	 global-step:2103	 l-p:0.13996334373950958
epoch£º105	 i:4 	 global-step:2104	 l-p:0.14281649887561798
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18187789618968964
epoch£º105	 i:6 	 global-step:2106	 l-p:0.1642792820930481
epoch£º105	 i:7 	 global-step:2107	 l-p:0.16648246347904205
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07361219078302383
epoch£º105	 i:9 	 global-step:2109	 l-p:0.12873420119285583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9102, 4.9131, 4.9104],
        [4.9102, 5.6171, 5.8542],
        [4.9102, 4.9287, 4.9134],
        [4.9102, 5.8249, 6.2619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13306091725826263 
model_pd.l_d.mean(): -18.520103454589844 
model_pd.lagr.mean(): -18.387042999267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-19.2983], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13306091725826263
epoch£º106	 i:1 	 global-step:2121	 l-p:0.14283104240894318
epoch£º106	 i:2 	 global-step:2122	 l-p:0.13301736116409302
epoch£º106	 i:3 	 global-step:2123	 l-p:0.13347317278385162
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12378199398517609
epoch£º106	 i:5 	 global-step:2125	 l-p:0.12922613322734833
epoch£º106	 i:6 	 global-step:2126	 l-p:-0.8462599515914917
epoch£º106	 i:7 	 global-step:2127	 l-p:0.1347506046295166
epoch£º106	 i:8 	 global-step:2128	 l-p:0.1327640861272812
epoch£º106	 i:9 	 global-step:2129	 l-p:0.11549317091703415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9003, 4.9029, 4.9004],
        [4.9003, 4.9054, 4.9007],
        [4.9003, 5.9268, 6.4851],
        [4.9003, 5.7418, 6.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.1452321857213974 
model_pd.l_d.mean(): -19.375402450561523 
model_pd.lagr.mean(): -19.23017120361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.1036], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.1452321857213974
epoch£º107	 i:1 	 global-step:2141	 l-p:0.1210818737745285
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.02908065728843212
epoch£º107	 i:3 	 global-step:2143	 l-p:0.1228322759270668
epoch£º107	 i:4 	 global-step:2144	 l-p:0.12857891619205475
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13161137700080872
epoch£º107	 i:6 	 global-step:2146	 l-p:0.17274130880832672
epoch£º107	 i:7 	 global-step:2147	 l-p:0.1509428322315216
epoch£º107	 i:8 	 global-step:2148	 l-p:0.12692920863628387
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1382361799478531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8342, 4.8253],
        [4.8240, 4.8242, 4.8240],
        [4.8240, 4.9191, 4.8684],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13067962229251862 
model_pd.l_d.mean(): -18.854278564453125 
model_pd.lagr.mean(): -18.72359848022461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-19.6127], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13067962229251862
epoch£º108	 i:1 	 global-step:2161	 l-p:0.16373342275619507
epoch£º108	 i:2 	 global-step:2162	 l-p:0.105313740670681
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13608865439891815
epoch£º108	 i:4 	 global-step:2164	 l-p:0.13890914618968964
epoch£º108	 i:5 	 global-step:2165	 l-p:0.1549154371023178
epoch£º108	 i:6 	 global-step:2166	 l-p:0.12788403034210205
epoch£º108	 i:7 	 global-step:2167	 l-p:0.12684416770935059
epoch£º108	 i:8 	 global-step:2168	 l-p:0.13363395631313324
epoch£º108	 i:9 	 global-step:2169	 l-p:0.16246314346790314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9291, 5.0128, 4.9646],
        [4.9291, 4.9306, 4.9291],
        [4.9291, 6.1867, 7.0080],
        [4.9291, 4.9291, 4.9291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11599677056074142 
model_pd.l_d.mean(): -20.014297485351562 
model_pd.lagr.mean(): -19.898300170898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.7177], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11599677056074142
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13467486202716827
epoch£º109	 i:2 	 global-step:2182	 l-p:0.13122156262397766
epoch£º109	 i:3 	 global-step:2183	 l-p:0.14612101018428802
epoch£º109	 i:4 	 global-step:2184	 l-p:0.39368167519569397
epoch£º109	 i:5 	 global-step:2185	 l-p:0.1397675722837448
epoch£º109	 i:6 	 global-step:2186	 l-p:0.14309854805469513
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12363670021295547
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13457995653152466
epoch£º109	 i:9 	 global-step:2189	 l-p:0.11706561595201492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9407, 4.9412, 4.9407],
        [4.9407, 4.9407, 4.9407],
        [4.9407, 4.9417, 4.9407],
        [4.9407, 4.9612, 4.9445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.6605889797210693 
model_pd.l_d.mean(): -20.809101104736328 
model_pd.lagr.mean(): -20.14851188659668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.4399], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.6605889797210693
epoch£º110	 i:1 	 global-step:2201	 l-p:0.13157081604003906
epoch£º110	 i:2 	 global-step:2202	 l-p:0.0985344797372818
epoch£º110	 i:3 	 global-step:2203	 l-p:0.1346808224916458
epoch£º110	 i:4 	 global-step:2204	 l-p:0.1439870297908783
epoch£º110	 i:5 	 global-step:2205	 l-p:0.13493800163269043
epoch£º110	 i:6 	 global-step:2206	 l-p:0.139212504029274
epoch£º110	 i:7 	 global-step:2207	 l-p:0.13349464535713196
epoch£º110	 i:8 	 global-step:2208	 l-p:0.23315437138080597
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21813641488552094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8588, 4.8631, 4.8591],
        [4.8588, 5.5366, 5.7565],
        [4.8588, 5.3721, 5.4610],
        [4.8588, 5.7227, 6.1183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.17197780311107635 
model_pd.l_d.mean(): -20.088855743408203 
model_pd.lagr.mean(): -19.91687774658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.17197780311107635
epoch£º111	 i:1 	 global-step:2221	 l-p:0.13511906564235687
epoch£º111	 i:2 	 global-step:2222	 l-p:0.4400617182254791
epoch£º111	 i:3 	 global-step:2223	 l-p:0.1273731142282486
epoch£º111	 i:4 	 global-step:2224	 l-p:0.11795735359191895
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12329547852277756
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11508512496948242
epoch£º111	 i:7 	 global-step:2227	 l-p:0.1404256671667099
epoch£º111	 i:8 	 global-step:2228	 l-p:0.1358635425567627
epoch£º111	 i:9 	 global-step:2229	 l-p:0.11055329442024231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9609, 5.0253, 4.9843],
        [4.9609, 5.0233, 4.9831],
        [4.9609, 4.9609, 4.9609],
        [4.9609, 4.9831, 4.9652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.1473640352487564 
model_pd.l_d.mean(): -20.548364639282227 
model_pd.lagr.mean(): -20.4010009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4242], device='cuda:0')), ('power', tensor([-21.2062], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.1473640352487564
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.12829183042049408
epoch£º112	 i:2 	 global-step:2242	 l-p:0.15494130551815033
epoch£º112	 i:3 	 global-step:2243	 l-p:0.13487379252910614
epoch£º112	 i:4 	 global-step:2244	 l-p:0.13465411961078644
epoch£º112	 i:5 	 global-step:2245	 l-p:0.15541304647922516
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11933114379644394
epoch£º112	 i:7 	 global-step:2247	 l-p:0.16730618476867676
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13128671050071716
epoch£º112	 i:9 	 global-step:2249	 l-p:0.13383810222148895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7995, 4.7995, 4.7995],
        [4.7995, 5.4674, 5.6862],
        [4.7995, 4.8595, 4.8211],
        [4.7995, 4.7995, 4.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.09870222955942154 
model_pd.l_d.mean(): -19.49880599975586 
model_pd.lagr.mean(): -19.400104522705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5252], device='cuda:0')), ('power', tensor([-20.2484], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.09870222955942154
epoch£º113	 i:1 	 global-step:2261	 l-p:0.13664694130420685
epoch£º113	 i:2 	 global-step:2262	 l-p:0.13010933995246887
epoch£º113	 i:3 	 global-step:2263	 l-p:0.14400073885917664
epoch£º113	 i:4 	 global-step:2264	 l-p:0.13899356126785278
epoch£º113	 i:5 	 global-step:2265	 l-p:0.1642368733882904
epoch£º113	 i:6 	 global-step:2266	 l-p:0.5322644114494324
epoch£º113	 i:7 	 global-step:2267	 l-p:0.13766203820705414
epoch£º113	 i:8 	 global-step:2268	 l-p:0.14826340973377228
epoch£º113	 i:9 	 global-step:2269	 l-p:-1.449659824371338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8028, 4.8028, 4.8028],
        [4.8028, 4.8043, 4.8029],
        [4.8028, 4.8036, 4.8028],
        [4.8028, 5.6696, 6.0778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1260077804327011 
model_pd.l_d.mean(): -19.596532821655273 
model_pd.lagr.mean(): -19.47052574157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-20.3751], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.1260077804327011
epoch£º114	 i:1 	 global-step:2281	 l-p:0.14143063127994537
epoch£º114	 i:2 	 global-step:2282	 l-p:0.1423628032207489
epoch£º114	 i:3 	 global-step:2283	 l-p:0.15760132670402527
epoch£º114	 i:4 	 global-step:2284	 l-p:0.1312251091003418
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13056360185146332
epoch£º114	 i:6 	 global-step:2286	 l-p:0.12767869234085083
epoch£º114	 i:7 	 global-step:2287	 l-p:0.13334356248378754
epoch£º114	 i:8 	 global-step:2288	 l-p:0.08849801868200302
epoch£º114	 i:9 	 global-step:2289	 l-p:0.13609205186367035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8589, 4.8589, 4.8589],
        [4.8589, 4.8589, 4.8589],
        [4.8589, 4.8589, 4.8589],
        [4.8589, 5.0529, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.12731671333312988 
model_pd.l_d.mean(): -20.264652252197266 
model_pd.lagr.mean(): -20.1373348236084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765], device='cuda:0')), ('power', tensor([-20.9728], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.12731671333312988
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11605200171470642
epoch£º115	 i:2 	 global-step:2302	 l-p:0.12111946195363998
epoch£º115	 i:3 	 global-step:2303	 l-p:0.6389082074165344
epoch£º115	 i:4 	 global-step:2304	 l-p:0.17218412458896637
epoch£º115	 i:5 	 global-step:2305	 l-p:0.13116450607776642
epoch£º115	 i:6 	 global-step:2306	 l-p:0.13663890957832336
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13648095726966858
epoch£º115	 i:8 	 global-step:2308	 l-p:0.44330325722694397
epoch£º115	 i:9 	 global-step:2309	 l-p:0.19258679449558258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7769, 4.8521, 4.8082],
        [4.7769, 4.8114, 4.7860],
        [4.7769, 4.7770, 4.7769],
        [4.7769, 4.9688, 4.9146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11802554875612259 
model_pd.l_d.mean(): -19.752229690551758 
model_pd.lagr.mean(): -19.634204864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-20.5174], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11802554875612259
epoch£º116	 i:1 	 global-step:2321	 l-p:0.1410110592842102
epoch£º116	 i:2 	 global-step:2322	 l-p:0.3659290671348572
epoch£º116	 i:3 	 global-step:2323	 l-p:0.15108545124530792
epoch£º116	 i:4 	 global-step:2324	 l-p:0.1030912920832634
epoch£º116	 i:5 	 global-step:2325	 l-p:0.1740378737449646
epoch£º116	 i:6 	 global-step:2326	 l-p:0.153314009308815
epoch£º116	 i:7 	 global-step:2327	 l-p:0.13738548755645752
epoch£º116	 i:8 	 global-step:2328	 l-p:0.10913736373186111
epoch£º116	 i:9 	 global-step:2329	 l-p:0.13311998546123505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0272, 5.0282, 5.0272],
        [5.0272, 5.0892, 5.0491],
        [5.0272, 5.0272, 5.0272],
        [5.0272, 5.0272, 5.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12944605946540833 
model_pd.l_d.mean(): -20.76484489440918 
model_pd.lagr.mean(): -20.635398864746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3770], device='cuda:0')), ('power', tensor([-21.3768], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12944605946540833
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13372111320495605
epoch£º117	 i:2 	 global-step:2342	 l-p:0.13443970680236816
epoch£º117	 i:3 	 global-step:2343	 l-p:0.11467546224594116
epoch£º117	 i:4 	 global-step:2344	 l-p:0.1423608511686325
epoch£º117	 i:5 	 global-step:2345	 l-p:0.1243453174829483
epoch£º117	 i:6 	 global-step:2346	 l-p:0.2800382077693939
epoch£º117	 i:7 	 global-step:2347	 l-p:0.1401093602180481
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.279222697019577
epoch£º117	 i:9 	 global-step:2349	 l-p:0.1542818248271942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7120, 5.8614, 6.5996],
        [4.7120, 5.0060, 4.9815],
        [4.7120, 4.7311, 4.7156],
        [4.7120, 4.7120, 4.7120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.1221831664443016 
model_pd.l_d.mean(): -19.743013381958008 
model_pd.lagr.mean(): -19.620830535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5612], device='cuda:0')), ('power', tensor([-20.5320], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.1221831664443016
epoch£º118	 i:1 	 global-step:2361	 l-p:0.1412990391254425
epoch£º118	 i:2 	 global-step:2362	 l-p:0.14146068692207336
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19656552374362946
epoch£º118	 i:4 	 global-step:2364	 l-p:0.1445373296737671
epoch£º118	 i:5 	 global-step:2365	 l-p:0.1523429900407791
epoch£º118	 i:6 	 global-step:2366	 l-p:0.1516590118408203
epoch£º118	 i:7 	 global-step:2367	 l-p:-3.6424500942230225
epoch£º118	 i:8 	 global-step:2368	 l-p:0.491413414478302
epoch£º118	 i:9 	 global-step:2369	 l-p:0.1805795431137085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8881, 5.0809, 5.0246],
        [4.8881, 4.8959, 4.8889],
        [4.8881, 5.4221, 5.5268],
        [4.8881, 5.5573, 5.7707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16676431894302368 
model_pd.l_d.mean(): -20.72416877746582 
model_pd.lagr.mean(): -20.557403564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.3858], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16676431894302368
epoch£º119	 i:1 	 global-step:2381	 l-p:0.1333216428756714
epoch£º119	 i:2 	 global-step:2382	 l-p:0.12292828410863876
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11854087561368942
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11453156918287277
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11662670224905014
epoch£º119	 i:6 	 global-step:2386	 l-p:0.12515948712825775
epoch£º119	 i:7 	 global-step:2387	 l-p:0.14094819128513336
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.12446155399084091
epoch£º119	 i:9 	 global-step:2389	 l-p:0.14612087607383728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 4.9117, 4.9116],
        [4.9116, 4.9118, 4.9116],
        [4.9116, 4.9125, 4.9117],
        [4.9116, 5.3191, 5.3397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.13102629780769348 
model_pd.l_d.mean(): -20.306060791015625 
model_pd.lagr.mean(): -20.175033569335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0005], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.13102629780769348
epoch£º120	 i:1 	 global-step:2401	 l-p:0.12613466382026672
epoch£º120	 i:2 	 global-step:2402	 l-p:0.1277754157781601
epoch£º120	 i:3 	 global-step:2403	 l-p:0.1521228402853012
epoch£º120	 i:4 	 global-step:2404	 l-p:0.1474153995513916
epoch£º120	 i:5 	 global-step:2405	 l-p:0.1271064579486847
epoch£º120	 i:6 	 global-step:2406	 l-p:0.14593712985515594
epoch£º120	 i:7 	 global-step:2407	 l-p:0.12629367411136627
epoch£º120	 i:8 	 global-step:2408	 l-p:0.08761697262525558
epoch£º120	 i:9 	 global-step:2409	 l-p:0.04906109720468521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 4.9189, 4.8649],
        [4.7389, 4.7646, 4.7447],
        [4.7389, 4.7389, 4.7389],
        [4.7389, 4.7394, 4.7389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.14721235632896423 
model_pd.l_d.mean(): -19.512346267700195 
model_pd.lagr.mean(): -19.36513328552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.2340], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.14721235632896423
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10670056194067001
epoch£º121	 i:2 	 global-step:2422	 l-p:0.13416695594787598
epoch£º121	 i:3 	 global-step:2423	 l-p:0.13525795936584473
epoch£º121	 i:4 	 global-step:2424	 l-p:0.20098605751991272
epoch£º121	 i:5 	 global-step:2425	 l-p:0.12883536517620087
epoch£º121	 i:6 	 global-step:2426	 l-p:0.13878287374973297
epoch£º121	 i:7 	 global-step:2427	 l-p:0.16520418226718903
epoch£º121	 i:8 	 global-step:2428	 l-p:0.1252584159374237
epoch£º121	 i:9 	 global-step:2429	 l-p:0.13324196636676788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9520, 4.9786, 4.9578],
        [4.9520, 4.9616, 4.9531],
        [4.9520, 4.9520, 4.9520],
        [4.9520, 4.9587, 4.9526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.1325395703315735 
model_pd.l_d.mean(): -18.616865158081055 
model_pd.lagr.mean(): -18.484325408935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.1325395703315735
epoch£º122	 i:1 	 global-step:2441	 l-p:0.1131492331624031
epoch£º122	 i:2 	 global-step:2442	 l-p:0.14786961674690247
epoch£º122	 i:3 	 global-step:2443	 l-p:0.13475358486175537
epoch£º122	 i:4 	 global-step:2444	 l-p:0.13365693390369415
epoch£º122	 i:5 	 global-step:2445	 l-p:0.11102622747421265
epoch£º122	 i:6 	 global-step:2446	 l-p:0.14292225241661072
epoch£º122	 i:7 	 global-step:2447	 l-p:0.13064837455749512
epoch£º122	 i:8 	 global-step:2448	 l-p:0.11589353531599045
epoch£º122	 i:9 	 global-step:2449	 l-p:0.13310985267162323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7854, 4.7858, 4.7854],
        [4.7854, 4.8368, 4.8026],
        [4.7854, 4.7911, 4.7859],
        [4.7854, 4.7854, 4.7854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13925933837890625 
model_pd.l_d.mean(): -20.52492904663086 
model_pd.lagr.mean(): -20.385669708251953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.2382], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13925933837890625
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13565769791603088
epoch£º123	 i:2 	 global-step:2462	 l-p:0.17591339349746704
epoch£º123	 i:3 	 global-step:2463	 l-p:0.143191859126091
epoch£º123	 i:4 	 global-step:2464	 l-p:0.06843817979097366
epoch£º123	 i:5 	 global-step:2465	 l-p:0.14538121223449707
epoch£º123	 i:6 	 global-step:2466	 l-p:0.21024909615516663
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14650912582874298
epoch£º123	 i:8 	 global-step:2468	 l-p:5.315640926361084
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08912244439125061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6756, 4.6759, 4.6756],
        [4.6756, 5.3672, 5.6271],
        [4.6756, 4.7073, 4.6838],
        [4.6756, 4.7228, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.21989744901657104 
model_pd.l_d.mean(): -20.14457893371582 
model_pd.lagr.mean(): -19.924680709838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.9062], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.21989744901657104
epoch£º124	 i:1 	 global-step:2481	 l-p:-0.18225479125976562
epoch£º124	 i:2 	 global-step:2482	 l-p:0.1810005158185959
epoch£º124	 i:3 	 global-step:2483	 l-p:0.1314888596534729
epoch£º124	 i:4 	 global-step:2484	 l-p:0.14065949618816376
epoch£º124	 i:5 	 global-step:2485	 l-p:0.10328510403633118
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11696938425302505
epoch£º124	 i:7 	 global-step:2487	 l-p:0.1089060828089714
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11589908599853516
epoch£º124	 i:9 	 global-step:2489	 l-p:0.11167434602975845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2932, 5.3083, 5.2954],
        [5.2932, 6.7116, 7.6682],
        [5.2932, 5.3123, 5.2964],
        [5.2932, 5.7490, 5.7753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.10583294928073883 
model_pd.l_d.mean(): -19.020742416381836 
model_pd.lagr.mean(): -18.91490936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3972], device='cuda:0')), ('power', tensor([-19.6343], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.10583294928073883
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11200854927301407
epoch£º125	 i:2 	 global-step:2502	 l-p:0.11222085356712341
epoch£º125	 i:3 	 global-step:2503	 l-p:0.12959742546081543
epoch£º125	 i:4 	 global-step:2504	 l-p:0.1151122897863388
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12890280783176422
epoch£º125	 i:6 	 global-step:2506	 l-p:0.12400821596384048
epoch£º125	 i:7 	 global-step:2507	 l-p:0.19420380890369415
epoch£º125	 i:8 	 global-step:2508	 l-p:0.14393112063407898
epoch£º125	 i:9 	 global-step:2509	 l-p:0.15763482451438904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6927, 5.3561, 5.5890],
        [4.6927, 5.1113, 5.1548],
        [4.6927, 4.6927, 4.6927],
        [4.6927, 4.6977, 4.6931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.14195846021175385 
model_pd.l_d.mean(): -20.695846557617188 
model_pd.lagr.mean(): -20.55388832092285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-21.4088], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.14195846021175385
epoch£º126	 i:1 	 global-step:2521	 l-p:0.16387268900871277
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16865621507167816
epoch£º126	 i:3 	 global-step:2523	 l-p:0.1756870597600937
epoch£º126	 i:4 	 global-step:2524	 l-p:0.14735183119773865
epoch£º126	 i:5 	 global-step:2525	 l-p:0.1555631458759308
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11348157376050949
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17040613293647766
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.046224575489759445
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.20496825873851776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6878, 4.7974, 4.7464],
        [4.6878, 4.7628, 4.7200],
        [4.6878, 4.8728, 4.8213],
        [4.6878, 5.4594, 5.7949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09849180281162262 
model_pd.l_d.mean(): -19.7321834564209 
model_pd.lagr.mean(): -19.633691787719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5375], device='cuda:0')), ('power', tensor([-20.4968], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09849180281162262
epoch£º127	 i:1 	 global-step:2541	 l-p:0.2437477856874466
epoch£º127	 i:2 	 global-step:2542	 l-p:0.13802772760391235
epoch£º127	 i:3 	 global-step:2543	 l-p:0.14092087745666504
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11358045041561127
epoch£º127	 i:5 	 global-step:2545	 l-p:0.1462121307849884
epoch£º127	 i:6 	 global-step:2546	 l-p:0.11908029019832611
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11689642816781998
epoch£º127	 i:8 	 global-step:2548	 l-p:0.1189693734049797
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11203263700008392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2055, 5.6831, 5.7288],
        [5.2055, 5.3938, 5.3313],
        [5.2055, 5.2084, 5.2056],
        [5.2055, 5.2062, 5.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09624262899160385 
model_pd.l_d.mean(): -19.658735275268555 
model_pd.lagr.mean(): -19.56249237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-20.3178], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09624262899160385
epoch£º128	 i:1 	 global-step:2561	 l-p:0.12331906706094742
epoch£º128	 i:2 	 global-step:2562	 l-p:0.12177828699350357
epoch£º128	 i:3 	 global-step:2563	 l-p:0.19872474670410156
epoch£º128	 i:4 	 global-step:2564	 l-p:0.1420932412147522
epoch£º128	 i:5 	 global-step:2565	 l-p:0.14447668194770813
epoch£º128	 i:6 	 global-step:2566	 l-p:0.156662717461586
epoch£º128	 i:7 	 global-step:2567	 l-p:0.12645649909973145
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14116044342517853
epoch£º128	 i:9 	 global-step:2569	 l-p:-0.05603272467851639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7560, 4.7567, 4.7560],
        [4.7560, 4.7727, 4.7589],
        [4.7560, 4.7560, 4.7560],
        [4.7560, 4.8818, 4.8281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.1193341612815857 
model_pd.l_d.mean(): -19.98590660095215 
model_pd.lagr.mean(): -19.866573333740234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-20.7640], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.1193341612815857
epoch£º129	 i:1 	 global-step:2581	 l-p:0.23789779841899872
epoch£º129	 i:2 	 global-step:2582	 l-p:0.14636656641960144
epoch£º129	 i:3 	 global-step:2583	 l-p:0.1554674506187439
epoch£º129	 i:4 	 global-step:2584	 l-p:0.13004010915756226
epoch£º129	 i:5 	 global-step:2585	 l-p:0.12597958743572235
epoch£º129	 i:6 	 global-step:2586	 l-p:0.15478992462158203
epoch£º129	 i:7 	 global-step:2587	 l-p:0.15062949061393738
epoch£º129	 i:8 	 global-step:2588	 l-p:0.1467047929763794
epoch£º129	 i:9 	 global-step:2589	 l-p:0.1279127597808838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8930, 5.0316, 4.9750],
        [4.8930, 4.8930, 4.8930],
        [4.8930, 4.8930, 4.8930],
        [4.8930, 4.8941, 4.8931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.053320713341236115 
model_pd.l_d.mean(): -18.75168228149414 
model_pd.lagr.mean(): -18.698362350463867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5354], device='cuda:0')), ('power', tensor([-19.5035], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.053320713341236115
epoch£º130	 i:1 	 global-step:2601	 l-p:0.16439828276634216
epoch£º130	 i:2 	 global-step:2602	 l-p:0.13260582089424133
epoch£º130	 i:3 	 global-step:2603	 l-p:0.1383672058582306
epoch£º130	 i:4 	 global-step:2604	 l-p:0.12867426872253418
epoch£º130	 i:5 	 global-step:2605	 l-p:0.1563519388437271
epoch£º130	 i:6 	 global-step:2606	 l-p:0.12385066598653793
epoch£º130	 i:7 	 global-step:2607	 l-p:0.2065642923116684
epoch£º130	 i:8 	 global-step:2608	 l-p:0.12918606400489807
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13685595989227295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9149, 4.9149, 4.9149],
        [4.9149, 4.9903, 4.9460],
        [4.9149, 4.9150, 4.9149],
        [4.9149, 4.9149, 4.9149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13942673802375793 
model_pd.l_d.mean(): -19.717121124267578 
model_pd.lagr.mean(): -19.577693939208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-20.4417], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.13942673802375793
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16038945317268372
epoch£º131	 i:2 	 global-step:2622	 l-p:0.15112639963626862
epoch£º131	 i:3 	 global-step:2623	 l-p:0.08014212548732758
epoch£º131	 i:4 	 global-step:2624	 l-p:0.12559626996517181
epoch£º131	 i:5 	 global-step:2625	 l-p:0.1487726867198944
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13280607759952545
epoch£º131	 i:7 	 global-step:2627	 l-p:0.15970326960086823
epoch£º131	 i:8 	 global-step:2628	 l-p:0.1252439320087433
epoch£º131	 i:9 	 global-step:2629	 l-p:0.1552509069442749
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9342, 4.9342, 4.9342],
        [4.9342, 6.0200, 6.6522],
        [4.9342, 4.9342, 4.9342],
        [4.9342, 5.3588, 5.3916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.117124043405056 
model_pd.l_d.mean(): -19.119667053222656 
model_pd.lagr.mean(): -19.00254249572754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-19.8686], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.117124043405056
epoch£º132	 i:1 	 global-step:2641	 l-p:0.1180599182844162
epoch£º132	 i:2 	 global-step:2642	 l-p:0.1222749799489975
epoch£º132	 i:3 	 global-step:2643	 l-p:0.12845918536186218
epoch£º132	 i:4 	 global-step:2644	 l-p:0.1399656981229782
epoch£º132	 i:5 	 global-step:2645	 l-p:0.1298363208770752
epoch£º132	 i:6 	 global-step:2646	 l-p:0.13265515863895416
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.32510772347450256
epoch£º132	 i:8 	 global-step:2648	 l-p:0.12532372772693634
epoch£º132	 i:9 	 global-step:2649	 l-p:0.16938495635986328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9633, 4.9633, 4.9633],
        [4.9633, 4.9633, 4.9633],
        [4.9633, 5.2694, 5.2421],
        [4.9633, 5.0746, 5.0208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14211557805538177 
model_pd.l_d.mean(): -18.58534049987793 
model_pd.lagr.mean(): -18.443225860595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-19.3119], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14211557805538177
epoch£º133	 i:1 	 global-step:2661	 l-p:0.1233861893415451
epoch£º133	 i:2 	 global-step:2662	 l-p:0.13563095033168793
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13458606600761414
epoch£º133	 i:4 	 global-step:2664	 l-p:0.1339874565601349
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12387873977422714
epoch£º133	 i:6 	 global-step:2666	 l-p:0.1572754830121994
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10742123425006866
epoch£º133	 i:8 	 global-step:2668	 l-p:0.15949639678001404
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13607925176620483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9565, 5.2468, 5.2143],
        [4.9565, 5.0353, 4.9898],
        [4.9565, 4.9721, 4.9590],
        [4.9565, 4.9589, 4.9566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13902877271175385 
model_pd.l_d.mean(): -20.33600425720215 
model_pd.lagr.mean(): -20.196975708007812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.0090], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13902877271175385
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12881220877170563
epoch£º134	 i:2 	 global-step:2682	 l-p:0.12976308166980743
epoch£º134	 i:3 	 global-step:2683	 l-p:0.1936643123626709
epoch£º134	 i:4 	 global-step:2684	 l-p:0.12775307893753052
epoch£º134	 i:5 	 global-step:2685	 l-p:0.11525274068117142
epoch£º134	 i:6 	 global-step:2686	 l-p:0.12981481850147247
epoch£º134	 i:7 	 global-step:2687	 l-p:0.11707902699708939
epoch£º134	 i:8 	 global-step:2688	 l-p:0.11491493135690689
epoch£º134	 i:9 	 global-step:2689	 l-p:0.1498432755470276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9485, 4.9485],
        [4.9485, 6.0586, 6.7180],
        [4.9485, 5.0732, 5.0177],
        [4.9485, 5.8683, 6.3215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.1252777874469757 
model_pd.l_d.mean(): -18.7529239654541 
model_pd.lagr.mean(): -18.62764549255371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.5188], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.1252777874469757
epoch£º135	 i:1 	 global-step:2701	 l-p:0.09573347121477127
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12516286969184875
epoch£º135	 i:3 	 global-step:2703	 l-p:0.1798514872789383
epoch£º135	 i:4 	 global-step:2704	 l-p:0.12893863022327423
epoch£º135	 i:5 	 global-step:2705	 l-p:0.1572650820016861
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16908100247383118
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11557478457689285
epoch£º135	 i:8 	 global-step:2708	 l-p:0.1288623809814453
epoch£º135	 i:9 	 global-step:2709	 l-p:0.1410854458808899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9325, 4.9326, 4.9325],
        [4.9325, 5.0752, 5.0185],
        [4.9325, 4.9332, 4.9325],
        [4.9325, 5.1043, 5.0475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.14106379449367523 
model_pd.l_d.mean(): -20.93817710876465 
model_pd.lagr.mean(): -20.7971134185791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3873], device='cuda:0')), ('power', tensor([-21.5626], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.14106379449367523
epoch£º136	 i:1 	 global-step:2721	 l-p:0.14095737040042877
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15687550604343414
epoch£º136	 i:3 	 global-step:2723	 l-p:0.12047084420919418
epoch£º136	 i:4 	 global-step:2724	 l-p:0.14837227761745453
epoch£º136	 i:5 	 global-step:2725	 l-p:0.11636902391910553
epoch£º136	 i:6 	 global-step:2726	 l-p:0.18499450385570526
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1346064656972885
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17907866835594177
epoch£º136	 i:9 	 global-step:2729	 l-p:0.1164395809173584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9626, 4.9626, 4.9626],
        [4.9626, 4.9791, 4.9654],
        [4.9626, 4.9626, 4.9626],
        [4.9626, 5.9679, 6.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.21658724546432495 
model_pd.l_d.mean(): -20.639198303222656 
model_pd.lagr.mean(): -20.855785369873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.2908], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.21658724546432495
epoch£º137	 i:1 	 global-step:2741	 l-p:0.1347791999578476
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11629194021224976
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12511739134788513
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11656076461076736
epoch£º137	 i:5 	 global-step:2745	 l-p:0.13058972358703613
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11374787986278534
epoch£º137	 i:7 	 global-step:2747	 l-p:0.109071746468544
epoch£º137	 i:8 	 global-step:2748	 l-p:0.14096391201019287
epoch£º137	 i:9 	 global-step:2749	 l-p:0.1456715315580368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9449, 5.0152, 4.9730],
        [4.9449, 4.9449, 4.9449],
        [4.9449, 4.9449, 4.9449],
        [4.9449, 4.9449, 4.9449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13198040425777435 
model_pd.l_d.mean(): -20.644487380981445 
model_pd.lagr.mean(): -20.51250648498535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153], device='cuda:0')), ('power', tensor([-21.2943], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13198040425777435
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12657451629638672
epoch£º138	 i:2 	 global-step:2762	 l-p:0.13945983350276947
epoch£º138	 i:3 	 global-step:2763	 l-p:0.2631123960018158
epoch£º138	 i:4 	 global-step:2764	 l-p:0.1514470875263214
epoch£º138	 i:5 	 global-step:2765	 l-p:0.13411030173301697
epoch£º138	 i:6 	 global-step:2766	 l-p:0.06285920739173889
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.004371456801891327
epoch£º138	 i:8 	 global-step:2768	 l-p:0.13681165874004364
epoch£º138	 i:9 	 global-step:2769	 l-p:0.20630240440368652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8998, 5.8801, 6.4066],
        [4.8998, 6.0689, 6.8069],
        [4.8998, 4.8998, 4.8998],
        [4.8998, 4.9018, 4.8999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.08526568114757538 
model_pd.l_d.mean(): -18.71599006652832 
model_pd.lagr.mean(): -18.63072395324707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-19.4725], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.08526568114757538
epoch£º139	 i:1 	 global-step:2781	 l-p:0.14947611093521118
epoch£º139	 i:2 	 global-step:2782	 l-p:0.13185447454452515
epoch£º139	 i:3 	 global-step:2783	 l-p:0.13533230125904083
epoch£º139	 i:4 	 global-step:2784	 l-p:0.12864290177822113
epoch£º139	 i:5 	 global-step:2785	 l-p:0.11740442365407944
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13436496257781982
epoch£º139	 i:7 	 global-step:2787	 l-p:0.12016475200653076
epoch£º139	 i:8 	 global-step:2788	 l-p:0.16961455345153809
epoch£º139	 i:9 	 global-step:2789	 l-p:0.162974551320076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8771, 5.9064, 6.4901],
        [4.8771, 4.9107, 4.8860],
        [4.8771, 5.4737, 5.6371],
        [4.8771, 5.9257, 6.5308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.14227591454982758 
model_pd.l_d.mean(): -20.090476989746094 
model_pd.lagr.mean(): -19.948200225830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.8152], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.14227591454982758
epoch£º140	 i:1 	 global-step:2801	 l-p:0.1365612894296646
epoch£º140	 i:2 	 global-step:2802	 l-p:0.13427935540676117
epoch£º140	 i:3 	 global-step:2803	 l-p:0.1721792221069336
epoch£º140	 i:4 	 global-step:2804	 l-p:0.10805211961269379
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13066788017749786
epoch£º140	 i:6 	 global-step:2806	 l-p:0.13725721836090088
epoch£º140	 i:7 	 global-step:2807	 l-p:0.13688288629055023
epoch£º140	 i:8 	 global-step:2808	 l-p:0.1262415647506714
epoch£º140	 i:9 	 global-step:2809	 l-p:0.1292961835861206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0309, 5.0310, 5.0309],
        [5.0309, 5.0310, 5.0309],
        [5.0309, 5.0309, 5.0309],
        [5.0309, 5.1604, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13617879152297974 
model_pd.l_d.mean(): -20.141679763793945 
model_pd.lagr.mean(): -20.00550079345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13617879152297974
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11883923411369324
epoch£º141	 i:2 	 global-step:2822	 l-p:0.4117206335067749
epoch£º141	 i:3 	 global-step:2823	 l-p:0.12850072979927063
epoch£º141	 i:4 	 global-step:2824	 l-p:0.1370968520641327
epoch£º141	 i:5 	 global-step:2825	 l-p:0.1296786218881607
epoch£º141	 i:6 	 global-step:2826	 l-p:0.1302197426557541
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12636347115039825
epoch£º141	 i:8 	 global-step:2828	 l-p:0.04820677638053894
epoch£º141	 i:9 	 global-step:2829	 l-p:0.1994229406118393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7061, 4.7986, 4.7516],
        [4.7061, 4.8646, 4.8123],
        [4.7061, 4.8480, 4.7954],
        [4.7061, 4.7066, 4.7062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.4738781750202179 
model_pd.l_d.mean(): -20.265745162963867 
model_pd.lagr.mean(): -19.791866302490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-21.0234], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.4738781750202179
epoch£º142	 i:1 	 global-step:2841	 l-p:0.16610349714756012
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14317573606967926
epoch£º142	 i:3 	 global-step:2843	 l-p:-0.3123466372489929
epoch£º142	 i:4 	 global-step:2844	 l-p:0.15266020596027374
epoch£º142	 i:5 	 global-step:2845	 l-p:0.1558857560157776
epoch£º142	 i:6 	 global-step:2846	 l-p:0.11304543167352676
epoch£º142	 i:7 	 global-step:2847	 l-p:0.11818468570709229
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12784036993980408
epoch£º142	 i:9 	 global-step:2849	 l-p:0.11206838488578796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.9433, 6.2307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.1171703115105629 
model_pd.l_d.mean(): -18.953060150146484 
model_pd.lagr.mean(): -18.83588981628418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-19.6299], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.1171703115105629
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11353200674057007
epoch£º143	 i:2 	 global-step:2862	 l-p:0.1167263388633728
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13291287422180176
epoch£º143	 i:4 	 global-step:2864	 l-p:0.12743249535560608
epoch£º143	 i:5 	 global-step:2865	 l-p:0.12337848544120789
epoch£º143	 i:6 	 global-step:2866	 l-p:0.15727843344211578
epoch£º143	 i:7 	 global-step:2867	 l-p:0.1408601552248001
epoch£º143	 i:8 	 global-step:2868	 l-p:0.10172517597675323
epoch£º143	 i:9 	 global-step:2869	 l-p:0.17130906879901886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 5.8968, 6.6537],
        [4.7389, 5.2827, 5.4187],
        [4.7389, 4.8285, 4.7820],
        [4.7389, 4.7441, 4.7394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13801898062229156 
model_pd.l_d.mean(): -19.545509338378906 
model_pd.lagr.mean(): -19.407489776611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-20.2945], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13801898062229156
epoch£º144	 i:1 	 global-step:2881	 l-p:0.1373155266046524
epoch£º144	 i:2 	 global-step:2882	 l-p:0.8935236930847168
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11640813201665878
epoch£º144	 i:4 	 global-step:2884	 l-p:0.04436459392309189
epoch£º144	 i:5 	 global-step:2885	 l-p:0.17069755494594574
epoch£º144	 i:6 	 global-step:2886	 l-p:0.15110239386558533
epoch£º144	 i:7 	 global-step:2887	 l-p:0.09981085360050201
epoch£º144	 i:8 	 global-step:2888	 l-p:0.1567991077899933
epoch£º144	 i:9 	 global-step:2889	 l-p:0.12231792509555817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.7362, 5.9853],
        [5.0241, 5.8313, 6.1681],
        [5.0241, 5.0329, 5.0251],
        [5.0241, 6.1065, 6.7281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12291701883077621 
model_pd.l_d.mean(): -20.473691940307617 
model_pd.lagr.mean(): -20.35077476501465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.1255], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12291701883077621
epoch£º145	 i:1 	 global-step:2901	 l-p:0.1927921324968338
epoch£º145	 i:2 	 global-step:2902	 l-p:0.11853747069835663
epoch£º145	 i:3 	 global-step:2903	 l-p:0.12223242223262787
epoch£º145	 i:4 	 global-step:2904	 l-p:0.13435573875904083
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11446309834718704
epoch£º145	 i:6 	 global-step:2906	 l-p:0.1353585422039032
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10579968988895416
epoch£º145	 i:8 	 global-step:2908	 l-p:0.14327239990234375
epoch£º145	 i:9 	 global-step:2909	 l-p:0.14317138493061066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8910, 4.8910, 4.8910],
        [4.8910, 4.9224, 4.8990],
        [4.8910, 5.1415, 5.1011],
        [4.8910, 4.9501, 4.9128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.1529962569475174 
model_pd.l_d.mean(): -19.63755226135254 
model_pd.lagr.mean(): -19.484556198120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.3594], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.1529962569475174
epoch£º146	 i:1 	 global-step:2921	 l-p:0.1673261821269989
epoch£º146	 i:2 	 global-step:2922	 l-p:0.13910126686096191
epoch£º146	 i:3 	 global-step:2923	 l-p:0.13617821037769318
epoch£º146	 i:4 	 global-step:2924	 l-p:-0.021706942468881607
epoch£º146	 i:5 	 global-step:2925	 l-p:0.14187964797019958
epoch£º146	 i:6 	 global-step:2926	 l-p:0.16152766346931458
epoch£º146	 i:7 	 global-step:2927	 l-p:0.14258959889411926
epoch£º146	 i:8 	 global-step:2928	 l-p:0.17254096269607544
epoch£º146	 i:9 	 global-step:2929	 l-p:0.19415725767612457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8221, 4.9905, 4.9371],
        [4.8221, 4.8221, 4.8221],
        [4.8221, 4.8659, 4.8358],
        [4.8221, 5.0702, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.1366308331489563 
model_pd.l_d.mean(): -20.784469604492188 
model_pd.lagr.mean(): -20.647838592529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-21.4616], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.1366308331489563
epoch£º147	 i:1 	 global-step:2941	 l-p:0.13038530945777893
epoch£º147	 i:2 	 global-step:2942	 l-p:0.13309802114963531
epoch£º147	 i:3 	 global-step:2943	 l-p:0.1609720140695572
epoch£º147	 i:4 	 global-step:2944	 l-p:0.13943730294704437
epoch£º147	 i:5 	 global-step:2945	 l-p:0.14718635380268097
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05773177370429039
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11968770623207092
epoch£º147	 i:8 	 global-step:2948	 l-p:0.1423853635787964
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12378434836864471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0169, 5.0381, 5.0212],
        [5.0169, 5.0267, 5.0182],
        [5.0169, 6.1193, 6.7655],
        [5.0169, 5.0169, 5.0169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12316083908081055 
model_pd.l_d.mean(): -20.50149917602539 
model_pd.lagr.mean(): -20.378337860107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.1461], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12316083908081055
epoch£º148	 i:1 	 global-step:2961	 l-p:0.15079818665981293
epoch£º148	 i:2 	 global-step:2962	 l-p:0.1283288598060608
epoch£º148	 i:3 	 global-step:2963	 l-p:0.16931958496570587
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1370006501674652
epoch£º148	 i:5 	 global-step:2965	 l-p:0.1186026930809021
epoch£º148	 i:6 	 global-step:2966	 l-p:0.14176684617996216
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10499885678291321
epoch£º148	 i:8 	 global-step:2968	 l-p:0.12056659162044525
epoch£º148	 i:9 	 global-step:2969	 l-p:0.13967254757881165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 4.9000, 4.8622],
        [4.8391, 4.8681, 4.8463],
        [4.8391, 5.8638, 6.4534],
        [4.8391, 5.2855, 5.3448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.1945418268442154 
model_pd.l_d.mean(): -18.755502700805664 
model_pd.lagr.mean(): -18.56096076965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-19.5561], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.1945418268442154
epoch£º149	 i:1 	 global-step:2981	 l-p:0.1428433358669281
epoch£º149	 i:2 	 global-step:2982	 l-p:0.23036344349384308
epoch£º149	 i:3 	 global-step:2983	 l-p:0.13947325944900513
epoch£º149	 i:4 	 global-step:2984	 l-p:0.1039571538567543
epoch£º149	 i:5 	 global-step:2985	 l-p:0.14260460436344147
epoch£º149	 i:6 	 global-step:2986	 l-p:0.13735896348953247
epoch£º149	 i:7 	 global-step:2987	 l-p:0.17951110005378723
epoch£º149	 i:8 	 global-step:2988	 l-p:0.1162395104765892
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14684626460075378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8550, 5.1441, 5.1190],
        [4.8550, 5.2683, 5.3059],
        [4.8550, 5.1379, 5.1104],
        [4.8550, 4.8568, 4.8551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13421140611171722 
model_pd.l_d.mean(): -18.662302017211914 
model_pd.lagr.mean(): -18.528091430664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-19.4304], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13421140611171722
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17186076939105988
epoch£º150	 i:2 	 global-step:3002	 l-p:0.06388518959283829
epoch£º150	 i:3 	 global-step:3003	 l-p:0.12168869376182556
epoch£º150	 i:4 	 global-step:3004	 l-p:0.120904840528965
epoch£º150	 i:5 	 global-step:3005	 l-p:0.12666301429271698
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12320193648338318
epoch£º150	 i:7 	 global-step:3007	 l-p:0.12729907035827637
epoch£º150	 i:8 	 global-step:3008	 l-p:0.1284324675798416
epoch£º150	 i:9 	 global-step:3009	 l-p:0.16401290893554688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9225, 5.2712, 5.2696],
        [4.9225, 4.9225, 4.9225],
        [4.9225, 5.6900, 6.0042],
        [4.9225, 5.3899, 5.4578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13014639914035797 
model_pd.l_d.mean(): -20.736303329467773 
model_pd.lagr.mean(): -20.606157302856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.3896], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13014639914035797
epoch£º151	 i:1 	 global-step:3021	 l-p:0.1409524381160736
epoch£º151	 i:2 	 global-step:3022	 l-p:0.2200513482093811
epoch£º151	 i:3 	 global-step:3023	 l-p:0.019673271104693413
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3461624085903168
epoch£º151	 i:5 	 global-step:3025	 l-p:0.15860997140407562
epoch£º151	 i:6 	 global-step:3026	 l-p:0.14770470559597015
epoch£º151	 i:7 	 global-step:3027	 l-p:0.06821184605360031
epoch£º151	 i:8 	 global-step:3028	 l-p:0.14477181434631348
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12967772781848907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9094, 5.0421, 4.9876],
        [4.9094, 5.8391, 6.3169],
        [4.9094, 5.0815, 5.0270],
        [4.9094, 5.0765, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.098838210105896 
model_pd.l_d.mean(): -19.85956382751465 
model_pd.lagr.mean(): -19.760725021362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-20.6108], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.098838210105896
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11932840943336487
epoch£º152	 i:2 	 global-step:3042	 l-p:0.13062432408332825
epoch£º152	 i:3 	 global-step:3043	 l-p:0.12601642310619354
epoch£º152	 i:4 	 global-step:3044	 l-p:0.13486848771572113
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12301613390445709
epoch£º152	 i:6 	 global-step:3046	 l-p:0.1286831647157669
epoch£º152	 i:7 	 global-step:3047	 l-p:0.14273792505264282
epoch£º152	 i:8 	 global-step:3048	 l-p:0.16293616592884064
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12502308189868927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9620, 4.9658, 4.9623],
        [4.9620, 4.9621, 4.9620],
        [4.9620, 4.9723, 4.9634],
        [4.9620, 4.9620, 4.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.17590734362602234 
model_pd.l_d.mean(): -19.374603271484375 
model_pd.lagr.mean(): -19.19869613647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.0581], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.17590734362602234
epoch£º153	 i:1 	 global-step:3061	 l-p:0.12960655987262726
epoch£º153	 i:2 	 global-step:3062	 l-p:0.06714621931314468
epoch£º153	 i:3 	 global-step:3063	 l-p:0.14171741902828217
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13708814978599548
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11809127777814865
epoch£º153	 i:6 	 global-step:3066	 l-p:0.14354458451271057
epoch£º153	 i:7 	 global-step:3067	 l-p:0.1217445433139801
epoch£º153	 i:8 	 global-step:3068	 l-p:0.14105162024497986
epoch£º153	 i:9 	 global-step:3069	 l-p:0.17224322259426117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7396, 5.3093, 5.4700],
        [4.7396, 4.8730, 4.8210],
        [4.7396, 4.7610, 4.7442],
        [4.7396, 4.7435, 4.7400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.1414273977279663 
model_pd.l_d.mean(): -18.744365692138672 
model_pd.lagr.mean(): -18.602937698364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5893], device='cuda:0')), ('power', tensor([-19.5512], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.1414273977279663
epoch£º154	 i:1 	 global-step:3081	 l-p:0.1306270807981491
epoch£º154	 i:2 	 global-step:3082	 l-p:0.200474351644516
epoch£º154	 i:3 	 global-step:3083	 l-p:0.139981210231781
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12102977186441422
epoch£º154	 i:5 	 global-step:3085	 l-p:0.15692996978759766
epoch£º154	 i:6 	 global-step:3086	 l-p:0.13411164283752441
epoch£º154	 i:7 	 global-step:3087	 l-p:0.14187903702259064
epoch£º154	 i:8 	 global-step:3088	 l-p:0.3711106777191162
epoch£º154	 i:9 	 global-step:3089	 l-p:0.1302367001771927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0545, 5.6953, 5.8847],
        [5.0545, 5.6068, 5.7261],
        [5.0545, 5.0970, 5.0672],
        [5.0545, 5.3768, 5.3577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.12936179339885712 
model_pd.l_d.mean(): -19.950233459472656 
model_pd.lagr.mean(): -19.820871353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.6240], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.12936179339885712
epoch£º155	 i:1 	 global-step:3101	 l-p:0.24677878618240356
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13076251745224
epoch£º155	 i:3 	 global-step:3103	 l-p:0.12569089233875275
epoch£º155	 i:4 	 global-step:3104	 l-p:0.11777118593454361
epoch£º155	 i:5 	 global-step:3105	 l-p:0.1536199152469635
epoch£º155	 i:6 	 global-step:3106	 l-p:0.12743918597698212
epoch£º155	 i:7 	 global-step:3107	 l-p:0.1663736253976822
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12189826369285583
epoch£º155	 i:9 	 global-step:3109	 l-p:0.13681374490261078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8870, 5.9778, 6.6364],
        [4.8870, 6.0009, 6.6857],
        [4.8870, 5.0347, 4.9802],
        [4.8870, 5.9745, 6.6293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12619292736053467 
model_pd.l_d.mean(): -19.983089447021484 
model_pd.lagr.mean(): -19.856897354125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.7141], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12619292736053467
epoch£º156	 i:1 	 global-step:3121	 l-p:0.15135546028614044
epoch£º156	 i:2 	 global-step:3122	 l-p:0.1253344565629959
epoch£º156	 i:3 	 global-step:3123	 l-p:0.07849393039941788
epoch£º156	 i:4 	 global-step:3124	 l-p:0.14664842188358307
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13940589129924774
epoch£º156	 i:6 	 global-step:3126	 l-p:0.15457099676132202
epoch£º156	 i:7 	 global-step:3127	 l-p:0.13001005351543427
epoch£º156	 i:8 	 global-step:3128	 l-p:0.1219903901219368
epoch£º156	 i:9 	 global-step:3129	 l-p:0.1460290253162384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9445, 5.0063, 4.9679],
        [4.9445, 5.4315, 5.5134],
        [4.9445, 5.8217, 6.2434],
        [4.9445, 5.1717, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.07942767441272736 
model_pd.l_d.mean(): -20.169748306274414 
model_pd.lagr.mean(): -20.090320587158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.8787], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.07942767441272736
epoch£º157	 i:1 	 global-step:3141	 l-p:0.11508859694004059
epoch£º157	 i:2 	 global-step:3142	 l-p:0.16387370228767395
epoch£º157	 i:3 	 global-step:3143	 l-p:0.1262792944908142
epoch£º157	 i:4 	 global-step:3144	 l-p:0.14505961537361145
epoch£º157	 i:5 	 global-step:3145	 l-p:0.18906153738498688
epoch£º157	 i:6 	 global-step:3146	 l-p:0.13292498886585236
epoch£º157	 i:7 	 global-step:3147	 l-p:0.15368567407131195
epoch£º157	 i:8 	 global-step:3148	 l-p:0.1409192830324173
epoch£º157	 i:9 	 global-step:3149	 l-p:0.20758506655693054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8970, 4.8969],
        [4.8969, 4.9047, 4.8979],
        [4.8969, 4.8969, 4.8969],
        [4.8969, 5.2925, 5.3195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.14894519746303558 
model_pd.l_d.mean(): -19.583826065063477 
model_pd.lagr.mean(): -19.43488121032715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.14894519746303558
epoch£º158	 i:1 	 global-step:3161	 l-p:0.1346946656703949
epoch£º158	 i:2 	 global-step:3162	 l-p:0.121515192091465
epoch£º158	 i:3 	 global-step:3163	 l-p:0.1207379400730133
epoch£º158	 i:4 	 global-step:3164	 l-p:0.1266389787197113
epoch£º158	 i:5 	 global-step:3165	 l-p:0.13047602772712708
epoch£º158	 i:6 	 global-step:3166	 l-p:0.15539731085300446
epoch£º158	 i:7 	 global-step:3167	 l-p:0.14183993637561798
epoch£º158	 i:8 	 global-step:3168	 l-p:0.1379714161157608
epoch£º158	 i:9 	 global-step:3169	 l-p:0.27744749188423157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8206, 4.9537, 4.9008],
        [4.8206, 4.8785, 4.8421],
        [4.8206, 4.9201, 4.8713],
        [4.8206, 4.8523, 4.8290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.4207460582256317 
model_pd.l_d.mean(): -20.48779296875 
model_pd.lagr.mean(): -20.067047119140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4829], device='cuda:0')), ('power', tensor([-21.2049], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.4207460582256317
epoch£º159	 i:1 	 global-step:3181	 l-p:0.13949798047542572
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13686881959438324
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.43992722034454346
epoch£º159	 i:4 	 global-step:3184	 l-p:0.1544928401708603
epoch£º159	 i:5 	 global-step:3185	 l-p:0.1325020045042038
epoch£º159	 i:6 	 global-step:3186	 l-p:0.12135151773691177
epoch£º159	 i:7 	 global-step:3187	 l-p:0.1358427256345749
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12448211759328842
epoch£º159	 i:9 	 global-step:3189	 l-p:0.184564471244812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8676, 4.9223, 4.8871],
        [4.8676, 5.5552, 5.8065],
        [4.8676, 4.8676, 4.8676],
        [4.8676, 4.8681, 4.8676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.197274848818779 
model_pd.l_d.mean(): -20.888959884643555 
model_pd.lagr.mean(): -20.69168472290039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.5441], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.197274848818779
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11667167395353317
epoch£º160	 i:2 	 global-step:3202	 l-p:0.1261681318283081
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13175852596759796
epoch£º160	 i:4 	 global-step:3204	 l-p:0.1416419893503189
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.07889584451913834
epoch£º160	 i:6 	 global-step:3206	 l-p:0.1282547563314438
epoch£º160	 i:7 	 global-step:3207	 l-p:0.13265758752822876
epoch£º160	 i:8 	 global-step:3208	 l-p:0.13389474153518677
epoch£º160	 i:9 	 global-step:3209	 l-p:0.15662969648838043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9397, 5.0731, 5.0187],
        [4.9397, 5.0032, 4.9642],
        [4.9397, 5.5149, 5.6643],
        [4.9397, 5.5890, 5.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.16019679605960846 
model_pd.l_d.mean(): -20.868980407714844 
model_pd.lagr.mean(): -20.708784103393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4005], device='cuda:0')), ('power', tensor([-21.5061], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.16019679605960846
epoch£º161	 i:1 	 global-step:3221	 l-p:0.13114678859710693
epoch£º161	 i:2 	 global-step:3222	 l-p:0.28329649567604065
epoch£º161	 i:3 	 global-step:3223	 l-p:0.1170860081911087
epoch£º161	 i:4 	 global-step:3224	 l-p:0.03606116771697998
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19147300720214844
epoch£º161	 i:6 	 global-step:3226	 l-p:0.13080152869224548
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14572249352931976
epoch£º161	 i:8 	 global-step:3228	 l-p:0.1705089509487152
epoch£º161	 i:9 	 global-step:3229	 l-p:0.1671191304922104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6408, 5.1010, 5.1898],
        [4.6408, 4.6412, 4.6408],
        [4.6408, 4.7158, 4.6742],
        [4.6408, 5.2162, 5.3959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12155503779649734 
model_pd.l_d.mean(): -20.335569381713867 
model_pd.lagr.mean(): -20.214014053344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-21.1222], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12155503779649734
epoch£º162	 i:1 	 global-step:3241	 l-p:0.1546390950679779
epoch£º162	 i:2 	 global-step:3242	 l-p:0.1606350839138031
epoch£º162	 i:3 	 global-step:3243	 l-p:0.1591336578130722
epoch£º162	 i:4 	 global-step:3244	 l-p:0.0712476596236229
epoch£º162	 i:5 	 global-step:3245	 l-p:0.12858669459819794
epoch£º162	 i:6 	 global-step:3246	 l-p:0.19601508975028992
epoch£º162	 i:7 	 global-step:3247	 l-p:0.08601634204387665
epoch£º162	 i:8 	 global-step:3248	 l-p:0.5686887502670288
epoch£º162	 i:9 	 global-step:3249	 l-p:0.1252286285161972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9430, 5.8061, 6.2165],
        [4.9430, 4.9430, 4.9430],
        [4.9430, 5.2567, 5.2406],
        [4.9430, 4.9430, 4.9430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14484111964702606 
model_pd.l_d.mean(): -18.54429817199707 
model_pd.lagr.mean(): -18.399457931518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-19.3250], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14484111964702606
epoch£º163	 i:1 	 global-step:3261	 l-p:0.1173444613814354
epoch£º163	 i:2 	 global-step:3262	 l-p:0.11954861134290695
epoch£º163	 i:3 	 global-step:3263	 l-p:0.17113271355628967
epoch£º163	 i:4 	 global-step:3264	 l-p:0.1317518949508667
epoch£º163	 i:5 	 global-step:3265	 l-p:0.138223797082901
epoch£º163	 i:6 	 global-step:3266	 l-p:0.11682645231485367
epoch£º163	 i:7 	 global-step:3267	 l-p:0.14107759296894073
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12590812146663666
epoch£º163	 i:9 	 global-step:3269	 l-p:0.13194569945335388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9566, 5.1772, 5.1298],
        [4.9566, 4.9606, 4.9570],
        [4.9566, 4.9575, 4.9567],
        [4.9566, 4.9793, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1274663209915161 
model_pd.l_d.mean(): -20.163833618164062 
model_pd.lagr.mean(): -20.036367416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1274663209915161
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12813544273376465
epoch£º164	 i:2 	 global-step:3282	 l-p:0.14661964774131775
epoch£º164	 i:3 	 global-step:3283	 l-p:0.11124438047409058
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14926476776599884
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12970688939094543
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12972339987754822
epoch£º164	 i:7 	 global-step:3287	 l-p:0.14510852098464966
epoch£º164	 i:8 	 global-step:3288	 l-p:0.1502014696598053
epoch£º164	 i:9 	 global-step:3289	 l-p:0.14789089560508728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8951, 4.8951, 4.8951],
        [4.8951, 5.6363, 5.9356],
        [4.8951, 5.4144, 5.5263],
        [4.8951, 5.0772, 5.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13100241124629974 
model_pd.l_d.mean(): -20.2044677734375 
model_pd.lagr.mean(): -20.07346534729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.9135], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13100241124629974
epoch£º165	 i:1 	 global-step:3301	 l-p:0.13213133811950684
epoch£º165	 i:2 	 global-step:3302	 l-p:0.16264955699443817
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14334653317928314
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.016629204154014587
epoch£º165	 i:5 	 global-step:3305	 l-p:0.40678858757019043
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12461987882852554
epoch£º165	 i:7 	 global-step:3307	 l-p:0.14270293712615967
epoch£º165	 i:8 	 global-step:3308	 l-p:0.015076479874551296
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13115738332271576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8702, 4.8718, 4.8703],
        [4.8702, 4.9372, 4.8972],
        [4.8702, 4.9465, 4.9033],
        [4.8702, 4.8703, 4.8702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.1429571956396103 
model_pd.l_d.mean(): -18.978986740112305 
model_pd.lagr.mean(): -18.836029052734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-19.7437], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.1429571956396103
epoch£º166	 i:1 	 global-step:3321	 l-p:0.1321406364440918
epoch£º166	 i:2 	 global-step:3322	 l-p:0.1445213407278061
epoch£º166	 i:3 	 global-step:3323	 l-p:0.1430276483297348
epoch£º166	 i:4 	 global-step:3324	 l-p:0.12643131613731384
epoch£º166	 i:5 	 global-step:3325	 l-p:0.11149881035089493
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12366354465484619
epoch£º166	 i:7 	 global-step:3327	 l-p:0.1302681416273117
epoch£º166	 i:8 	 global-step:3328	 l-p:0.11892309039831161
epoch£º166	 i:9 	 global-step:3329	 l-p:0.8404843807220459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0056, 5.0056, 5.0056],
        [5.0056, 5.1580, 5.1023],
        [5.0056, 6.2000, 6.9611],
        [5.0056, 5.0174, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13492722809314728 
model_pd.l_d.mean(): -19.458099365234375 
model_pd.lagr.mean(): -19.323171615600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-20.1630], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13492722809314728
epoch£º167	 i:1 	 global-step:3341	 l-p:0.13003431260585785
epoch£º167	 i:2 	 global-step:3342	 l-p:0.1270926296710968
epoch£º167	 i:3 	 global-step:3343	 l-p:0.13898780941963196
epoch£º167	 i:4 	 global-step:3344	 l-p:0.13273777067661285
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12146522104740143
epoch£º167	 i:6 	 global-step:3346	 l-p:0.3702869415283203
epoch£º167	 i:7 	 global-step:3347	 l-p:0.07087680697441101
epoch£º167	 i:8 	 global-step:3348	 l-p:0.04802435636520386
epoch£º167	 i:9 	 global-step:3349	 l-p:0.12065773457288742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8241, 4.8246, 4.8241],
        [4.8241, 5.8396, 6.4302],
        [4.8241, 4.8241, 4.8241],
        [4.8241, 4.8241, 4.8241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.12831497192382812 
model_pd.l_d.mean(): -18.6581974029541 
model_pd.lagr.mean(): -18.529882431030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.4495], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.12831497192382812
epoch£º168	 i:1 	 global-step:3361	 l-p:0.1260140836238861
epoch£º168	 i:2 	 global-step:3362	 l-p:0.14647765457630157
epoch£º168	 i:3 	 global-step:3363	 l-p:0.14453965425491333
epoch£º168	 i:4 	 global-step:3364	 l-p:0.14144842326641083
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11997043341398239
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11653004586696625
epoch£º168	 i:7 	 global-step:3367	 l-p:0.14143241941928864
epoch£º168	 i:8 	 global-step:3368	 l-p:0.15155424177646637
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12819314002990723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8907, 4.8909, 4.8907],
        [4.8907, 4.9091, 4.8942],
        [4.8907, 5.0211, 4.9678],
        [4.8907, 4.9412, 4.9078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13750487565994263 
model_pd.l_d.mean(): -20.716171264648438 
model_pd.lagr.mean(): -20.57866668701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3809], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13750487565994263
epoch£º169	 i:1 	 global-step:3381	 l-p:0.1419396549463272
epoch£º169	 i:2 	 global-step:3382	 l-p:0.13369283080101013
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.10765478760004044
epoch£º169	 i:4 	 global-step:3384	 l-p:0.1405397206544876
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09747802466154099
epoch£º169	 i:6 	 global-step:3386	 l-p:0.051749058067798615
epoch£º169	 i:7 	 global-step:3387	 l-p:0.15242169797420502
epoch£º169	 i:8 	 global-step:3388	 l-p:0.1253916174173355
epoch£º169	 i:9 	 global-step:3389	 l-p:0.1487032175064087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9487, 5.1440, 5.0929],
        [4.9487, 4.9496, 4.9488],
        [4.9487, 4.9724, 4.9539],
        [4.9487, 4.9494, 4.9487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.15095782279968262 
model_pd.l_d.mean(): -18.79258155822754 
model_pd.lagr.mean(): -18.641624450683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5342], device='cuda:0')), ('power', tensor([-19.5436], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.15095782279968262
epoch£º170	 i:1 	 global-step:3401	 l-p:0.14094612002372742
epoch£º170	 i:2 	 global-step:3402	 l-p:0.13808777928352356
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12541766464710236
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12352342158555984
epoch£º170	 i:5 	 global-step:3405	 l-p:0.43121206760406494
epoch£º170	 i:6 	 global-step:3406	 l-p:0.12052588909864426
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12275054305791855
epoch£º170	 i:8 	 global-step:3408	 l-p:0.13010524213314056
epoch£º170	 i:9 	 global-step:3409	 l-p:0.15123941004276276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9569, 5.4455, 5.5327],
        [4.9569, 4.9583, 4.9570],
        [4.9569, 4.9820, 4.9626],
        [4.9569, 4.9569, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.1172390803694725 
model_pd.l_d.mean(): -18.685253143310547 
model_pd.lagr.mean(): -18.56801414489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.4504], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.1172390803694725
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483365833759308
epoch£º171	 i:2 	 global-step:3422	 l-p:0.16539521515369415
epoch£º171	 i:3 	 global-step:3423	 l-p:0.14427326619625092
epoch£º171	 i:4 	 global-step:3424	 l-p:0.07828911393880844
epoch£º171	 i:5 	 global-step:3425	 l-p:0.11680760979652405
epoch£º171	 i:6 	 global-step:3426	 l-p:0.13464239239692688
epoch£º171	 i:7 	 global-step:3427	 l-p:0.25303027033805847
epoch£º171	 i:8 	 global-step:3428	 l-p:0.138519287109375
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15589265525341034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8074, 4.8079, 4.8074],
        [4.8074, 4.8128, 4.8080],
        [4.8074, 5.0545, 5.0202],
        [4.8074, 4.8074, 4.8074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.1158832311630249 
model_pd.l_d.mean(): -20.791982650756836 
model_pd.lagr.mean(): -20.67609977722168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.4793], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.1158832311630249
epoch£º172	 i:1 	 global-step:3441	 l-p:0.19621409475803375
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09903901815414429
epoch£º172	 i:3 	 global-step:3443	 l-p:-0.1653682440519333
epoch£º172	 i:4 	 global-step:3444	 l-p:0.1268865466117859
epoch£º172	 i:5 	 global-step:3445	 l-p:0.1366317868232727
epoch£º172	 i:6 	 global-step:3446	 l-p:0.14092254638671875
epoch£º172	 i:7 	 global-step:3447	 l-p:0.1789015531539917
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12066928297281265
epoch£º172	 i:9 	 global-step:3449	 l-p:0.1297994703054428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1353, 6.2637, 6.9316],
        [5.1353, 5.2356, 5.1844],
        [5.1353, 5.8315, 6.0680],
        [5.1353, 5.2478, 5.1941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11810142546892166 
model_pd.l_d.mean(): -19.468385696411133 
model_pd.lagr.mean(): -19.350284576416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.1655], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11810142546892166
epoch£º173	 i:1 	 global-step:3461	 l-p:0.09931283444166183
epoch£º173	 i:2 	 global-step:3462	 l-p:0.12842674553394318
epoch£º173	 i:3 	 global-step:3463	 l-p:0.14312218129634857
epoch£º173	 i:4 	 global-step:3464	 l-p:0.12500867247581482
epoch£º173	 i:5 	 global-step:3465	 l-p:0.13909578323364258
epoch£º173	 i:6 	 global-step:3466	 l-p:0.1327439844608307
epoch£º173	 i:7 	 global-step:3467	 l-p:0.0915597528219223
epoch£º173	 i:8 	 global-step:3468	 l-p:0.09227403253316879
epoch£º173	 i:9 	 global-step:3469	 l-p:0.1417091339826584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6642, 5.4157, 5.7591],
        [4.6642, 4.6757, 4.6659],
        [4.6642, 4.7299, 4.6908],
        [4.6642, 4.7831, 4.7331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.1575206220149994 
model_pd.l_d.mean(): -20.258594512939453 
model_pd.lagr.mean(): -20.10107421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5425], device='cuda:0')), ('power', tensor([-21.0342], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.1575206220149994
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13991935551166534
epoch£º174	 i:2 	 global-step:3482	 l-p:0.1078774631023407
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15758226811885834
epoch£º174	 i:4 	 global-step:3484	 l-p:0.1577693372964859
epoch£º174	 i:5 	 global-step:3485	 l-p:-0.5043914914131165
epoch£º174	 i:6 	 global-step:3486	 l-p:0.1897546797990799
epoch£º174	 i:7 	 global-step:3487	 l-p:0.1360405832529068
epoch£º174	 i:8 	 global-step:3488	 l-p:0.12962831556797028
epoch£º174	 i:9 	 global-step:3489	 l-p:0.11856618523597717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 5.1444, 5.1419],
        [5.1417, 5.1532, 5.1434],
        [5.1417, 5.1958, 5.1601],
        [5.1417, 5.2253, 5.1784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12143197655677795 
model_pd.l_d.mean(): -20.6068172454834 
model_pd.lagr.mean(): -20.48538589477539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3579], device='cuda:0')), ('power', tensor([-21.1976], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12143197655677795
epoch£º175	 i:1 	 global-step:3501	 l-p:0.12045673280954361
epoch£º175	 i:2 	 global-step:3502	 l-p:0.15720538794994354
epoch£º175	 i:3 	 global-step:3503	 l-p:0.1958213448524475
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13158603012561798
epoch£º175	 i:5 	 global-step:3505	 l-p:0.11830044537782669
epoch£º175	 i:6 	 global-step:3506	 l-p:0.11533786356449127
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12438466399908066
epoch£º175	 i:8 	 global-step:3508	 l-p:0.12497123330831528
epoch£º175	 i:9 	 global-step:3509	 l-p:0.12434493750333786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9240, 4.9240, 4.9240],
        [4.9240, 4.9424, 4.9275],
        [4.9240, 5.4519, 5.5723],
        [4.9240, 5.1305, 5.0824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.09482494741678238 
model_pd.l_d.mean(): -19.311128616333008 
model_pd.lagr.mean(): -19.2163028717041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.0412], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.09482494741678238
epoch£º176	 i:1 	 global-step:3521	 l-p:0.17333196103572845
epoch£º176	 i:2 	 global-step:3522	 l-p:0.1369830071926117
epoch£º176	 i:3 	 global-step:3523	 l-p:0.13736097514629364
epoch£º176	 i:4 	 global-step:3524	 l-p:0.1009448990225792
epoch£º176	 i:5 	 global-step:3525	 l-p:0.5053024291992188
epoch£º176	 i:6 	 global-step:3526	 l-p:0.13887536525726318
epoch£º176	 i:7 	 global-step:3527	 l-p:0.13509631156921387
epoch£º176	 i:8 	 global-step:3528	 l-p:-0.3007488548755646
epoch£º176	 i:9 	 global-step:3529	 l-p:0.15770387649536133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8006, 5.8640, 6.5153],
        [4.8006, 4.8012, 4.8006],
        [4.8006, 4.8962, 4.8482],
        [4.8006, 4.8008, 4.8006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11440866440534592 
model_pd.l_d.mean(): -19.83009147644043 
model_pd.lagr.mean(): -19.715682983398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5278], device='cuda:0')), ('power', tensor([-20.5859], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.11440866440534592
epoch£º177	 i:1 	 global-step:3541	 l-p:0.13295909762382507
epoch£º177	 i:2 	 global-step:3542	 l-p:0.17259293794631958
epoch£º177	 i:3 	 global-step:3543	 l-p:0.11052850633859634
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13878953456878662
epoch£º177	 i:5 	 global-step:3545	 l-p:0.10793866962194443
epoch£º177	 i:6 	 global-step:3546	 l-p:0.13080719113349915
epoch£º177	 i:7 	 global-step:3547	 l-p:0.1254134327173233
epoch£º177	 i:8 	 global-step:3548	 l-p:0.14895567297935486
epoch£º177	 i:9 	 global-step:3549	 l-p:0.19637373089790344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9391, 4.9393, 4.9391],
        [4.9391, 4.9391, 4.9391],
        [4.9391, 4.9391, 4.9391],
        [4.9391, 5.0080, 4.9667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.19867651164531708 
model_pd.l_d.mean(): -18.319683074951172 
model_pd.lagr.mean(): -18.12100601196289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5402], device='cuda:0')), ('power', tensor([-19.0717], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.19867651164531708
epoch£º178	 i:1 	 global-step:3561	 l-p:0.14010430872440338
epoch£º178	 i:2 	 global-step:3562	 l-p:0.13761764764785767
epoch£º178	 i:3 	 global-step:3563	 l-p:0.1138107180595398
epoch£º178	 i:4 	 global-step:3564	 l-p:0.12942221760749817
epoch£º178	 i:5 	 global-step:3565	 l-p:0.15320874750614166
epoch£º178	 i:6 	 global-step:3566	 l-p:0.12530973553657532
epoch£º178	 i:7 	 global-step:3567	 l-p:0.28499841690063477
epoch£º178	 i:8 	 global-step:3568	 l-p:0.1228623166680336
epoch£º178	 i:9 	 global-step:3569	 l-p:0.07831879705190659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 4.8157, 4.8148],
        [4.8148, 4.8351, 4.8188],
        [4.8148, 4.8340, 4.8185],
        [4.8148, 4.8152, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.1744699478149414 
model_pd.l_d.mean(): -20.570770263671875 
model_pd.lagr.mean(): -20.39630126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.2748], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.1744699478149414
epoch£º179	 i:1 	 global-step:3581	 l-p:0.5437116026878357
epoch£º179	 i:2 	 global-step:3582	 l-p:0.1261375993490219
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12480690330266953
epoch£º179	 i:4 	 global-step:3584	 l-p:0.14997528493404388
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1307571530342102
epoch£º179	 i:6 	 global-step:3586	 l-p:0.16031217575073242
epoch£º179	 i:7 	 global-step:3587	 l-p:0.12630555033683777
epoch£º179	 i:8 	 global-step:3588	 l-p:0.10533714294433594
epoch£º179	 i:9 	 global-step:3589	 l-p:0.20111367106437683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8976, 4.8969],
        [4.8969, 4.8974, 4.8969],
        [4.8969, 5.0855, 5.0351],
        [4.8969, 4.8969, 4.8969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12802018225193024 
model_pd.l_d.mean(): -20.887514114379883 
model_pd.lagr.mean(): -20.75949478149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4122], device='cuda:0')), ('power', tensor([-21.5367], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12802018225193024
epoch£º180	 i:1 	 global-step:3601	 l-p:0.14906878769397736
epoch£º180	 i:2 	 global-step:3602	 l-p:-2.060403347015381
epoch£º180	 i:3 	 global-step:3603	 l-p:0.2070486843585968
epoch£º180	 i:4 	 global-step:3604	 l-p:-0.07794759422540665
epoch£º180	 i:5 	 global-step:3605	 l-p:0.1600050926208496
epoch£º180	 i:6 	 global-step:3606	 l-p:0.2657599151134491
epoch£º180	 i:7 	 global-step:3607	 l-p:0.12818185985088348
epoch£º180	 i:8 	 global-step:3608	 l-p:0.13517706096172333
epoch£º180	 i:9 	 global-step:3609	 l-p:0.13420340418815613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9307, 4.9373, 4.9314],
        [4.9307, 5.0032, 4.9606],
        [4.9307, 5.3230, 5.3525],
        [4.9307, 5.5854, 5.8101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.15092632174491882 
model_pd.l_d.mean(): -20.478111267089844 
model_pd.lagr.mean(): -20.327184677124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.15092632174491882
epoch£º181	 i:1 	 global-step:3621	 l-p:0.19971850514411926
epoch£º181	 i:2 	 global-step:3622	 l-p:0.1242745965719223
epoch£º181	 i:3 	 global-step:3623	 l-p:0.12849465012550354
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13530755043029785
epoch£º181	 i:5 	 global-step:3625	 l-p:0.1063191220164299
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13396789133548737
epoch£º181	 i:7 	 global-step:3627	 l-p:0.1266845464706421
epoch£º181	 i:8 	 global-step:3628	 l-p:0.1724073737859726
epoch£º181	 i:9 	 global-step:3629	 l-p:0.19614192843437195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9396, 4.9477, 4.9406],
        [4.9396, 4.9893, 4.9559],
        [4.9396, 5.0615, 5.0083],
        [4.9396, 4.9396, 4.9396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12426862120628357 
model_pd.l_d.mean(): -20.25748062133789 
model_pd.lagr.mean(): -20.133211135864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.9271], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12426862120628357
epoch£º182	 i:1 	 global-step:3641	 l-p:0.14273394644260406
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1283901184797287
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12142207473516464
epoch£º182	 i:4 	 global-step:3644	 l-p:0.2148592323064804
epoch£º182	 i:5 	 global-step:3645	 l-p:0.08479612320661545
epoch£º182	 i:6 	 global-step:3646	 l-p:0.1810024529695511
epoch£º182	 i:7 	 global-step:3647	 l-p:0.20536936819553375
epoch£º182	 i:8 	 global-step:3648	 l-p:0.14274300634860992
epoch£º182	 i:9 	 global-step:3649	 l-p:0.11360738426446915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0608, 5.3787, 5.3633],
        [5.0608, 5.2014, 5.1455],
        [5.0608, 5.0690, 5.0618],
        [5.0608, 5.7898, 6.0674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12625795602798462 
model_pd.l_d.mean(): -20.340900421142578 
model_pd.lagr.mean(): -20.214641571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-20.9890], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12625795602798462
epoch£º183	 i:1 	 global-step:3661	 l-p:0.12627685070037842
epoch£º183	 i:2 	 global-step:3662	 l-p:0.13033875823020935
epoch£º183	 i:3 	 global-step:3663	 l-p:0.12542401254177094
epoch£º183	 i:4 	 global-step:3664	 l-p:0.19431635737419128
epoch£º183	 i:5 	 global-step:3665	 l-p:0.4146577715873718
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14069174230098724
epoch£º183	 i:7 	 global-step:3667	 l-p:-1.4933382272720337
epoch£º183	 i:8 	 global-step:3668	 l-p:0.12895283102989197
epoch£º183	 i:9 	 global-step:3669	 l-p:0.1287694126367569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9053, 4.9743, 4.9328],
        [4.9053, 5.6722, 6.0029],
        [4.9053, 4.9373, 4.9133],
        [4.9053, 4.9053, 4.9053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.14094498753547668 
model_pd.l_d.mean(): -19.02195930480957 
model_pd.lagr.mean(): -18.881013870239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5307], device='cuda:0')), ('power', tensor([-19.7719], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.14094498753547668
epoch£º184	 i:1 	 global-step:3681	 l-p:0.16309992969036102
epoch£º184	 i:2 	 global-step:3682	 l-p:0.12965816259384155
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12198323756456375
epoch£º184	 i:4 	 global-step:3684	 l-p:0.13019898533821106
epoch£º184	 i:5 	 global-step:3685	 l-p:0.10056823492050171
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13618797063827515
epoch£º184	 i:7 	 global-step:3687	 l-p:0.12137757986783981
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11443649232387543
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09777259826660156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9300, 5.0471, 4.9943],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9445, 4.9323],
        [4.9300, 5.6585, 5.9507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12938018143177032 
model_pd.l_d.mean(): -19.371906280517578 
model_pd.lagr.mean(): -19.24252700805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.1168], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12938018143177032
epoch£º185	 i:1 	 global-step:3701	 l-p:0.23998847603797913
epoch£º185	 i:2 	 global-step:3702	 l-p:0.13698896765708923
epoch£º185	 i:3 	 global-step:3703	 l-p:0.14871284365653992
epoch£º185	 i:4 	 global-step:3704	 l-p:0.1451970636844635
epoch£º185	 i:5 	 global-step:3705	 l-p:0.01417587697505951
epoch£º185	 i:6 	 global-step:3706	 l-p:0.14427167177200317
epoch£º185	 i:7 	 global-step:3707	 l-p:0.1621992588043213
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11549343168735504
epoch£º185	 i:9 	 global-step:3709	 l-p:-2.4867587089538574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8943, 4.8943, 4.8943],
        [4.8943, 5.2418, 5.2494],
        [4.8943, 5.1997, 5.1866],
        [4.8943, 4.9970, 4.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13235993683338165 
model_pd.l_d.mean(): -20.221111297607422 
model_pd.lagr.mean(): -20.0887508392334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.9387], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13235993683338165
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13774102926254272
epoch£º186	 i:2 	 global-step:3722	 l-p:0.19763202965259552
epoch£º186	 i:3 	 global-step:3723	 l-p:0.1874023973941803
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10521413385868073
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11362472176551819
epoch£º186	 i:6 	 global-step:3726	 l-p:0.13410834968090057
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13956187665462494
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12081088870763779
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11620933562517166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1181, 5.1181],
        [5.1181, 5.2981, 5.2423],
        [5.1181, 5.4358, 5.4184],
        [5.1181, 5.1181, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.1926412135362625 
model_pd.l_d.mean(): -19.395360946655273 
model_pd.lagr.mean(): -19.202720642089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.0695], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.1926412135362625
epoch£º187	 i:1 	 global-step:3741	 l-p:0.14034870266914368
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11663993448019028
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13576185703277588
epoch£º187	 i:4 	 global-step:3744	 l-p:0.13870954513549805
epoch£º187	 i:5 	 global-step:3745	 l-p:0.12621422111988068
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05965939536690712
epoch£º187	 i:7 	 global-step:3747	 l-p:0.09832636266946793
epoch£º187	 i:8 	 global-step:3748	 l-p:0.10232286155223846
epoch£º187	 i:9 	 global-step:3749	 l-p:0.1438889056444168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6942, 5.3895, 5.6812],
        [4.6942, 4.7562, 4.7171],
        [4.6942, 4.6942, 4.6942],
        [4.6942, 5.3784, 5.6597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.14990957081317902 
model_pd.l_d.mean(): -20.689382553100586 
model_pd.lagr.mean(): -20.539472579956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-21.4257], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.14990957081317902
epoch£º188	 i:1 	 global-step:3761	 l-p:0.13251511752605438
epoch£º188	 i:2 	 global-step:3762	 l-p:0.1246630996465683
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14235030114650726
epoch£º188	 i:4 	 global-step:3764	 l-p:0.08902479708194733
epoch£º188	 i:5 	 global-step:3765	 l-p:0.18444010615348816
epoch£º188	 i:6 	 global-step:3766	 l-p:0.16651858389377594
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10966850072145462
epoch£º188	 i:8 	 global-step:3768	 l-p:0.1731521040201187
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11730719357728958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0383, 6.0355, 6.5817],
        [5.0383, 6.0516, 6.6148],
        [5.0383, 5.0383, 5.0383],
        [5.0383, 5.0383, 5.0383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10281164944171906 
model_pd.l_d.mean(): -19.674022674560547 
model_pd.lagr.mean(): -19.571210861206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.3667], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10281164944171906
epoch£º189	 i:1 	 global-step:3781	 l-p:0.21022745966911316
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11469417065382004
epoch£º189	 i:3 	 global-step:3783	 l-p:0.12662707269191742
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12223292142152786
epoch£º189	 i:5 	 global-step:3785	 l-p:0.10521005094051361
epoch£º189	 i:6 	 global-step:3786	 l-p:0.12324411422014236
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11035797744989395
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11788427084684372
epoch£º189	 i:9 	 global-step:3789	 l-p:0.13425390422344208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0345, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.13342022895812988 
model_pd.l_d.mean(): -20.220495223999023 
model_pd.lagr.mean(): -20.087074279785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.13342022895812988
epoch£º190	 i:1 	 global-step:3801	 l-p:0.13924366235733032
epoch£º190	 i:2 	 global-step:3802	 l-p:0.13302694261074066
epoch£º190	 i:3 	 global-step:3803	 l-p:0.07740402221679688
epoch£º190	 i:4 	 global-step:3804	 l-p:0.24377761781215668
epoch£º190	 i:5 	 global-step:3805	 l-p:0.1390097737312317
epoch£º190	 i:6 	 global-step:3806	 l-p:0.20353366434574127
epoch£º190	 i:7 	 global-step:3807	 l-p:0.15214160084724426
epoch£º190	 i:8 	 global-step:3808	 l-p:0.1509561538696289
epoch£º190	 i:9 	 global-step:3809	 l-p:0.15823575854301453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6779, 5.5109, 5.9397],
        [4.6779, 4.7165, 4.6879],
        [4.6779, 4.6779, 4.6779],
        [4.6779, 4.6779, 4.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.16520728170871735 
model_pd.l_d.mean(): -18.84347915649414 
model_pd.lagr.mean(): -18.678272247314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6520], device='cuda:0')), ('power', tensor([-19.7154], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.16520728170871735
epoch£º191	 i:1 	 global-step:3821	 l-p:0.20706824958324432
epoch£º191	 i:2 	 global-step:3822	 l-p:0.021375903859734535
epoch£º191	 i:3 	 global-step:3823	 l-p:0.13560549914836884
epoch£º191	 i:4 	 global-step:3824	 l-p:0.1608496755361557
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10943152010440826
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12196225672960281
epoch£º191	 i:7 	 global-step:3827	 l-p:0.14225706458091736
epoch£º191	 i:8 	 global-step:3828	 l-p:0.15518315136432648
epoch£º191	 i:9 	 global-step:3829	 l-p:0.1296723634004593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9334, 4.9493, 4.9359],
        [4.9334, 4.9430, 4.9346],
        [4.9334, 5.7662, 6.1599],
        [4.9334, 4.9561, 4.9378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19202692806720734 
model_pd.l_d.mean(): -20.20292091369629 
model_pd.lagr.mean(): -20.010894775390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9085], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19202692806720734
epoch£º192	 i:1 	 global-step:3841	 l-p:0.1470305621623993
epoch£º192	 i:2 	 global-step:3842	 l-p:0.4417550265789032
epoch£º192	 i:3 	 global-step:3843	 l-p:0.12885181605815887
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5392289757728577
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13068142533302307
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14514003694057465
epoch£º192	 i:7 	 global-step:3847	 l-p:0.1557130068540573
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12739810347557068
epoch£º192	 i:9 	 global-step:3849	 l-p:0.13173939287662506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9371, 5.7055, 6.0370],
        [4.9371, 4.9514, 4.9392],
        [4.9371, 4.9388, 4.9372],
        [4.9371, 4.9372, 4.9371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.136252760887146 
model_pd.l_d.mean(): -20.54520606994629 
model_pd.lagr.mean(): -20.408952713012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.136252760887146
epoch£º193	 i:1 	 global-step:3861	 l-p:0.13793157041072845
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05907554551959038
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.0893283560872078
epoch£º193	 i:4 	 global-step:3864	 l-p:0.1329251378774643
epoch£º193	 i:5 	 global-step:3865	 l-p:0.14665259420871735
epoch£º193	 i:6 	 global-step:3866	 l-p:0.133941188454628
epoch£º193	 i:7 	 global-step:3867	 l-p:0.10342980921268463
epoch£º193	 i:8 	 global-step:3868	 l-p:0.15017716586589813
epoch£º193	 i:9 	 global-step:3869	 l-p:0.11218217015266418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7177, 4.7177, 4.7177],
        [4.7177, 5.1899, 5.2912],
        [4.7177, 4.8766, 4.8255],
        [4.7177, 4.7416, 4.7220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.09614591300487518 
model_pd.l_d.mean(): -20.923112869262695 
model_pd.lagr.mean(): -20.826967239379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.6310], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.09614591300487518
epoch£º194	 i:1 	 global-step:3881	 l-p:0.14337241649627686
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.017651967704296112
epoch£º194	 i:3 	 global-step:3883	 l-p:0.13345329463481903
epoch£º194	 i:4 	 global-step:3884	 l-p:0.1358613520860672
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11681144684553146
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12126447260379791
epoch£º194	 i:7 	 global-step:3887	 l-p:0.14190514385700226
epoch£º194	 i:8 	 global-step:3888	 l-p:0.1304163634777069
epoch£º194	 i:9 	 global-step:3889	 l-p:1.3045940399169922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1023, 5.1715, 5.1289],
        [5.1023, 5.6085, 5.7044],
        [5.1023, 5.1047, 5.1025],
        [5.1023, 5.5111, 5.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11473318189382553 
model_pd.l_d.mean(): -20.305158615112305 
model_pd.lagr.mean(): -20.190425872802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4129], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11473318189382553
epoch£º195	 i:1 	 global-step:3901	 l-p:0.1191159188747406
epoch£º195	 i:2 	 global-step:3902	 l-p:0.12007210403680801
epoch£º195	 i:3 	 global-step:3903	 l-p:0.12040767073631287
epoch£º195	 i:4 	 global-step:3904	 l-p:0.16078300774097443
epoch£º195	 i:5 	 global-step:3905	 l-p:0.13427802920341492
epoch£º195	 i:6 	 global-step:3906	 l-p:0.142033189535141
epoch£º195	 i:7 	 global-step:3907	 l-p:0.2243405282497406
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11449882388114929
epoch£º195	 i:9 	 global-step:3909	 l-p:0.10723257064819336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7941, 4.7941, 4.7941],
        [4.7941, 4.7984, 4.7944],
        [4.7941, 4.7943, 4.7941],
        [4.7941, 4.7972, 4.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.1374264508485794 
model_pd.l_d.mean(): -19.624452590942383 
model_pd.lagr.mean(): -19.48702621459961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5740], device='cuda:0')), ('power', tensor([-20.4252], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.1374264508485794
epoch£º196	 i:1 	 global-step:3921	 l-p:0.09886612743139267
epoch£º196	 i:2 	 global-step:3922	 l-p:0.08999615162611008
epoch£º196	 i:3 	 global-step:3923	 l-p:0.09527364373207092
epoch£º196	 i:4 	 global-step:3924	 l-p:0.13066229224205017
epoch£º196	 i:5 	 global-step:3925	 l-p:0.1557716727256775
epoch£º196	 i:6 	 global-step:3926	 l-p:0.12409146875143051
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12654943764209747
epoch£º196	 i:8 	 global-step:3928	 l-p:0.13584665954113007
epoch£º196	 i:9 	 global-step:3929	 l-p:0.12398611009120941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9352, 4.9355, 4.9352],
        [4.9352, 5.3408, 5.3816],
        [4.9352, 4.9352, 4.9352],
        [4.9352, 4.9481, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.18534450232982635 
model_pd.l_d.mean(): -19.068618774414062 
model_pd.lagr.mean(): -18.88327407836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-19.8258], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.18534450232982635
epoch£º197	 i:1 	 global-step:3941	 l-p:0.19377785921096802
epoch£º197	 i:2 	 global-step:3942	 l-p:0.15182358026504517
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11366235464811325
epoch£º197	 i:4 	 global-step:3944	 l-p:0.0008014535997062922
epoch£º197	 i:5 	 global-step:3945	 l-p:0.13425399363040924
epoch£º197	 i:6 	 global-step:3946	 l-p:0.12888377904891968
epoch£º197	 i:7 	 global-step:3947	 l-p:0.13323043286800385
epoch£º197	 i:8 	 global-step:3948	 l-p:0.13210001587867737
epoch£º197	 i:9 	 global-step:3949	 l-p:0.12072791159152985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9713, 4.9908, 4.9746],
        [4.9713, 4.9714, 4.9713],
        [4.9713, 4.9713, 4.9713],
        [4.9713, 4.9721, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12789712846279144 
model_pd.l_d.mean(): -20.525915145874023 
model_pd.lagr.mean(): -20.39801788330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.1876], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.12789712846279144
epoch£º198	 i:1 	 global-step:3961	 l-p:0.13781161606311798
epoch£º198	 i:2 	 global-step:3962	 l-p:0.14969511330127716
epoch£º198	 i:3 	 global-step:3963	 l-p:0.12292961031198502
epoch£º198	 i:4 	 global-step:3964	 l-p:0.10889185965061188
epoch£º198	 i:5 	 global-step:3965	 l-p:0.12791500985622406
epoch£º198	 i:6 	 global-step:3966	 l-p:0.14037960767745972
epoch£º198	 i:7 	 global-step:3967	 l-p:0.2757185995578766
epoch£º198	 i:8 	 global-step:3968	 l-p:0.14134687185287476
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12410303205251694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9915, 4.9923, 4.9916],
        [4.9915, 5.0016, 4.9927],
        [4.9915, 4.9915, 4.9915],
        [4.9915, 5.1045, 5.0507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.16566939651966095 
model_pd.l_d.mean(): -19.046709060668945 
model_pd.lagr.mean(): -18.881040573120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-19.7598], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.16566939651966095
epoch£º199	 i:1 	 global-step:3981	 l-p:0.1175185963511467
epoch£º199	 i:2 	 global-step:3982	 l-p:0.12080635875463486
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12708412110805511
epoch£º199	 i:4 	 global-step:3984	 l-p:0.14026223123073578
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1737431436777115
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11406967043876648
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11667491495609283
epoch£º199	 i:8 	 global-step:3988	 l-p:0.11265507340431213
epoch£º199	 i:9 	 global-step:3989	 l-p:0.1562810242176056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0231, 5.0231, 5.0231],
        [5.0231, 5.0235, 5.0232],
        [5.0231, 5.0349, 5.0246],
        [5.0231, 5.3584, 5.3552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.14448291063308716 
model_pd.l_d.mean(): -20.52617835998535 
model_pd.lagr.mean(): -20.381694793701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4162], device='cuda:0')), ('power', tensor([-21.1756], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.14448291063308716
epoch£º200	 i:1 	 global-step:4001	 l-p:0.13946880400180817
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12589131295681
epoch£º200	 i:3 	 global-step:4003	 l-p:0.19323749840259552
epoch£º200	 i:4 	 global-step:4004	 l-p:0.13263216614723206
epoch£º200	 i:5 	 global-step:4005	 l-p:0.24510188400745392
epoch£º200	 i:6 	 global-step:4006	 l-p:0.1581099033355713
epoch£º200	 i:7 	 global-step:4007	 l-p:0.15053489804267883
epoch£º200	 i:8 	 global-step:4008	 l-p:0.11471450328826904
epoch£º200	 i:9 	 global-step:4009	 l-p:0.7183247804641724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9198, 4.9198, 4.9198],
        [4.9198, 5.3880, 5.4722],
        [4.9198, 4.9198, 4.9198],
        [4.9198, 5.0090, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.3611295223236084 
model_pd.l_d.mean(): -18.556961059570312 
model_pd.lagr.mean(): -18.195831298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5605], device='cuda:0')), ('power', tensor([-19.3323], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.3611295223236084
epoch£º201	 i:1 	 global-step:4021	 l-p:0.10966338962316513
epoch£º201	 i:2 	 global-step:4022	 l-p:0.08152065426111221
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13063600659370422
epoch£º201	 i:4 	 global-step:4024	 l-p:0.14910927414894104
epoch£º201	 i:5 	 global-step:4025	 l-p:0.13450902700424194
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12257007509469986
epoch£º201	 i:7 	 global-step:4027	 l-p:0.1351911574602127
epoch£º201	 i:8 	 global-step:4028	 l-p:0.12194009870290756
epoch£º201	 i:9 	 global-step:4029	 l-p:0.14950954914093018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0116, 5.0118, 5.0117],
        [5.0116, 5.0117, 5.0116],
        [5.0116, 5.0999, 5.0507],
        [5.0116, 5.0241, 5.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.13339120149612427 
model_pd.l_d.mean(): -20.06035041809082 
model_pd.lagr.mean(): -19.926959991455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.7404], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.13339120149612427
epoch£º202	 i:1 	 global-step:4041	 l-p:0.14932668209075928
epoch£º202	 i:2 	 global-step:4042	 l-p:0.13128036260604858
epoch£º202	 i:3 	 global-step:4043	 l-p:0.15026548504829407
epoch£º202	 i:4 	 global-step:4044	 l-p:0.19722948968410492
epoch£º202	 i:5 	 global-step:4045	 l-p:0.14595641195774078
epoch£º202	 i:6 	 global-step:4046	 l-p:0.13243110477924347
epoch£º202	 i:7 	 global-step:4047	 l-p:0.13370363414287567
epoch£º202	 i:8 	 global-step:4048	 l-p:0.038565099239349365
epoch£º202	 i:9 	 global-step:4049	 l-p:0.2104775607585907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.3476, 5.4806],
        [4.8243, 4.9077, 4.8599],
        [4.8243, 4.8752, 4.8393],
        [4.8243, 4.8244, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.06952997297048569 
model_pd.l_d.mean(): -18.929149627685547 
model_pd.lagr.mean(): -18.859619140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-19.6922], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.06952997297048569
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10468488186597824
epoch£º203	 i:2 	 global-step:4062	 l-p:0.10913484543561935
epoch£º203	 i:3 	 global-step:4063	 l-p:0.13938459753990173
epoch£º203	 i:4 	 global-step:4064	 l-p:-0.014026680029928684
epoch£º203	 i:5 	 global-step:4065	 l-p:0.1880025565624237
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13480183482170105
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13844142854213715
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12696555256843567
epoch£º203	 i:9 	 global-step:4069	 l-p:0.13477779924869537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9546, 5.4648, 5.5770],
        [4.9546, 4.9546, 4.9546],
        [4.9546, 5.4553, 5.5605],
        [4.9546, 5.0615, 5.0084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.1228175014257431 
model_pd.l_d.mean(): -20.372413635253906 
model_pd.lagr.mean(): -20.249595642089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0677], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.1228175014257431
epoch£º204	 i:1 	 global-step:4081	 l-p:0.15242363512516022
epoch£º204	 i:2 	 global-step:4082	 l-p:0.30761295557022095
epoch£º204	 i:3 	 global-step:4083	 l-p:0.3129518926143646
epoch£º204	 i:4 	 global-step:4084	 l-p:0.1439695507287979
epoch£º204	 i:5 	 global-step:4085	 l-p:0.10360919684171677
epoch£º204	 i:6 	 global-step:4086	 l-p:0.14278317987918854
epoch£º204	 i:7 	 global-step:4087	 l-p:0.14955469965934753
epoch£º204	 i:8 	 global-step:4088	 l-p:0.14276456832885742
epoch£º204	 i:9 	 global-step:4089	 l-p:0.13359691202640533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9503, 5.0170, 4.9744],
        [4.9503, 4.9503, 4.9503],
        [4.9503, 4.9504, 4.9503],
        [4.9503, 4.9503, 4.9503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.18841876089572906 
model_pd.l_d.mean(): -19.691707611083984 
model_pd.lagr.mean(): -19.50328826904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-20.4236], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.18841876089572906
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13660192489624023
epoch£º205	 i:2 	 global-step:4102	 l-p:0.12505197525024414
epoch£º205	 i:3 	 global-step:4103	 l-p:0.15809759497642517
epoch£º205	 i:4 	 global-step:4104	 l-p:0.15348497033119202
epoch£º205	 i:5 	 global-step:4105	 l-p:0.07311031967401505
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14254115521907806
epoch£º205	 i:7 	 global-step:4107	 l-p:0.17050375044345856
epoch£º205	 i:8 	 global-step:4108	 l-p:0.1122528612613678
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12313825637102127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 5.0670, 5.0377],
        [5.0280, 5.9752, 6.4747],
        [5.0280, 5.5954, 5.7456],
        [5.0280, 5.0280, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13938334584236145 
model_pd.l_d.mean(): -20.22357749938965 
model_pd.lagr.mean(): -20.08419418334961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-20.9178], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13938334584236145
epoch£º206	 i:1 	 global-step:4121	 l-p:0.10293334722518921
epoch£º206	 i:2 	 global-step:4122	 l-p:0.12199141085147858
epoch£º206	 i:3 	 global-step:4123	 l-p:0.15110501646995544
epoch£º206	 i:4 	 global-step:4124	 l-p:0.11495477706193924
epoch£º206	 i:5 	 global-step:4125	 l-p:0.16780278086662292
epoch£º206	 i:6 	 global-step:4126	 l-p:0.1360931694507599
epoch£º206	 i:7 	 global-step:4127	 l-p:0.1134067252278328
epoch£º206	 i:8 	 global-step:4128	 l-p:0.17041535675525665
epoch£º206	 i:9 	 global-step:4129	 l-p:0.12894999980926514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9724, 4.9937, 4.9758],
        [4.9724, 4.9724, 4.9724],
        [4.9724, 5.2571, 5.2333],
        [4.9724, 5.1428, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.13198745250701904 
model_pd.l_d.mean(): -19.841428756713867 
model_pd.lagr.mean(): -19.709442138671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5270], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.13198745250701904
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12597166001796722
epoch£º207	 i:2 	 global-step:4142	 l-p:0.20231077075004578
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16059161722660065
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16470733284950256
epoch£º207	 i:5 	 global-step:4145	 l-p:0.13590726256370544
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12252595275640488
epoch£º207	 i:7 	 global-step:4147	 l-p:0.5689247250556946
epoch£º207	 i:8 	 global-step:4148	 l-p:0.12738540768623352
epoch£º207	 i:9 	 global-step:4149	 l-p:0.1348656415939331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.8915, 4.8914],
        [4.8914, 5.3771, 5.4766],
        [4.8914, 5.0205, 4.9652],
        [4.8914, 4.8914, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.13262870907783508 
model_pd.l_d.mean(): -20.86741828918457 
model_pd.lagr.mean(): -20.73478889465332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.5234], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.13262870907783508
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.15369842946529388
epoch£º208	 i:2 	 global-step:4162	 l-p:0.125310480594635
epoch£º208	 i:3 	 global-step:4163	 l-p:0.1290934979915619
epoch£º208	 i:4 	 global-step:4164	 l-p:0.1365429311990738
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14291928708553314
epoch£º208	 i:6 	 global-step:4166	 l-p:0.18740153312683105
epoch£º208	 i:7 	 global-step:4167	 l-p:0.09754530340433121
epoch£º208	 i:8 	 global-step:4168	 l-p:0.210158109664917
epoch£º208	 i:9 	 global-step:4169	 l-p:0.15456314384937286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8772, 5.2160, 5.2225],
        [4.8772, 5.8038, 6.3035],
        [4.8772, 4.9276, 4.8916],
        [4.8772, 5.1284, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.0832662358880043 
model_pd.l_d.mean(): -20.242406845092773 
model_pd.lagr.mean(): -20.325672149658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.9817], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.0832662358880043
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13659238815307617
epoch£º209	 i:2 	 global-step:4182	 l-p:0.1285901665687561
epoch£º209	 i:3 	 global-step:4183	 l-p:0.14129284024238586
epoch£º209	 i:4 	 global-step:4184	 l-p:0.14280246198177338
epoch£º209	 i:5 	 global-step:4185	 l-p:0.1017032042145729
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13482816517353058
epoch£º209	 i:7 	 global-step:4187	 l-p:0.20048373937606812
epoch£º209	 i:8 	 global-step:4188	 l-p:0.15681232511997223
epoch£º209	 i:9 	 global-step:4189	 l-p:0.12428835779428482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0026, 5.0026, 5.0026],
        [5.0026, 5.0026, 5.0026],
        [5.0026, 5.1573, 5.1009],
        [5.0026, 5.0026, 5.0026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.17078746855258942 
model_pd.l_d.mean(): -20.625286102294922 
model_pd.lagr.mean(): -20.454498291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.2785], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.17078746855258942
epoch£º210	 i:1 	 global-step:4201	 l-p:0.13863706588745117
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14798419177532196
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12522029876708984
epoch£º210	 i:4 	 global-step:4204	 l-p:0.1350892186164856
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12279660999774933
epoch£º210	 i:6 	 global-step:4206	 l-p:0.11828761547803879
epoch£º210	 i:7 	 global-step:4207	 l-p:0.12348662316799164
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.329056054353714
epoch£º210	 i:9 	 global-step:4209	 l-p:0.12625069916248322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0662, 5.4719, 5.5062],
        [5.0662, 5.0662, 5.0662],
        [5.0662, 5.2085, 5.1511],
        [5.0662, 5.0662, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.13839510083198547 
model_pd.l_d.mean(): -19.56399154663086 
model_pd.lagr.mean(): -19.425596237182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4470], device='cuda:0')), ('power', tensor([-20.2343], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.13839510083198547
epoch£º211	 i:1 	 global-step:4221	 l-p:0.12709689140319824
epoch£º211	 i:2 	 global-step:4222	 l-p:0.1505294144153595
epoch£º211	 i:3 	 global-step:4223	 l-p:0.1718159317970276
epoch£º211	 i:4 	 global-step:4224	 l-p:0.1250777244567871
epoch£º211	 i:5 	 global-step:4225	 l-p:0.15360093116760254
epoch£º211	 i:6 	 global-step:4226	 l-p:0.10609645396471024
epoch£º211	 i:7 	 global-step:4227	 l-p:0.11786571890115738
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12420236319303513
epoch£º211	 i:9 	 global-step:4229	 l-p:0.1325768679380417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9751, 5.4480, 5.5324],
        [4.9751, 4.9752, 4.9751],
        [4.9751, 4.9751, 4.9751],
        [4.9751, 5.4175, 5.4810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.13759736716747284 
model_pd.l_d.mean(): -20.94319725036621 
model_pd.lagr.mean(): -20.805599212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3865], device='cuda:0')), ('power', tensor([-21.5668], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.13759736716747284
epoch£º212	 i:1 	 global-step:4241	 l-p:0.11819679290056229
epoch£º212	 i:2 	 global-step:4242	 l-p:0.1440632939338684
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12057163566350937
epoch£º212	 i:4 	 global-step:4244	 l-p:0.2461983561515808
epoch£º212	 i:5 	 global-step:4245	 l-p:0.15063723921775818
epoch£º212	 i:6 	 global-step:4246	 l-p:0.11993926763534546
epoch£º212	 i:7 	 global-step:4247	 l-p:0.1851338893175125
epoch£º212	 i:8 	 global-step:4248	 l-p:-1.0011160373687744
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10819871723651886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7771, 4.7775, 4.7771],
        [4.7771, 4.7771, 4.7771],
        [4.7771, 4.9229, 4.8684],
        [4.7771, 4.7839, 4.7771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12231504172086716 
model_pd.l_d.mean(): -19.526840209960938 
model_pd.lagr.mean(): -19.404525756835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5788], device='cuda:0')), ('power', tensor([-20.3314], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12231504172086716
epoch£º213	 i:1 	 global-step:4261	 l-p:0.1325872540473938
epoch£º213	 i:2 	 global-step:4262	 l-p:0.13917946815490723
epoch£º213	 i:3 	 global-step:4263	 l-p:0.1341514140367508
epoch£º213	 i:4 	 global-step:4264	 l-p:0.020589692518115044
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14378251135349274
epoch£º213	 i:6 	 global-step:4266	 l-p:0.1346350908279419
epoch£º213	 i:7 	 global-step:4267	 l-p:0.11960290372371674
epoch£º213	 i:8 	 global-step:4268	 l-p:0.4489350914955139
epoch£º213	 i:9 	 global-step:4269	 l-p:0.0965384840965271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8271, 4.8272, 4.8271],
        [4.8271, 4.8527, 4.8307],
        [4.8271, 4.9692, 4.9139],
        [4.8271, 4.8271, 4.8271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.14025652408599854 
model_pd.l_d.mean(): -20.610736846923828 
model_pd.lagr.mean(): -20.47047996520996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-21.3137], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.14025652408599854
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12440089881420135
epoch£º214	 i:2 	 global-step:4282	 l-p:0.09175185859203339
epoch£º214	 i:3 	 global-step:4283	 l-p:0.18618522584438324
epoch£º214	 i:4 	 global-step:4284	 l-p:0.1378278136253357
epoch£º214	 i:5 	 global-step:4285	 l-p:0.3250523805618286
epoch£º214	 i:6 	 global-step:4286	 l-p:0.1726359724998474
epoch£º214	 i:7 	 global-step:4287	 l-p:0.46143150329589844
epoch£º214	 i:8 	 global-step:4288	 l-p:0.14249372482299805
epoch£º214	 i:9 	 global-step:4289	 l-p:0.18543539941310883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9984, 5.1567, 5.1002],
        [4.9984, 4.9986, 4.9984],
        [4.9984, 5.0103, 4.9995],
        [4.9984, 4.9984, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1598052978515625 
model_pd.l_d.mean(): -20.52350616455078 
model_pd.lagr.mean(): -20.36370086669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.1926], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1598052978515625
epoch£º215	 i:1 	 global-step:4301	 l-p:0.1299809366464615
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14428956806659698
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11818574368953705
epoch£º215	 i:4 	 global-step:4304	 l-p:4.194358825683594
epoch£º215	 i:5 	 global-step:4305	 l-p:0.12550148367881775
epoch£º215	 i:6 	 global-step:4306	 l-p:0.13928623497486115
epoch£º215	 i:7 	 global-step:4307	 l-p:0.11865347623825073
epoch£º215	 i:8 	 global-step:4308	 l-p:0.12832888960838318
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11150714755058289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0822, 5.0830, 5.0822],
        [5.0822, 5.4354, 5.4393],
        [5.0822, 5.0987, 5.0843],
        [5.0822, 5.0822, 5.0822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.118524469435215 
model_pd.l_d.mean(): -19.11777114868164 
model_pd.lagr.mean(): -18.99924659729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-19.7967], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.118524469435215
epoch£º216	 i:1 	 global-step:4321	 l-p:0.11841670423746109
epoch£º216	 i:2 	 global-step:4322	 l-p:0.15726187825202942
epoch£º216	 i:3 	 global-step:4323	 l-p:0.15007002651691437
epoch£º216	 i:4 	 global-step:4324	 l-p:0.1514384001493454
epoch£º216	 i:5 	 global-step:4325	 l-p:0.18877805769443512
epoch£º216	 i:6 	 global-step:4326	 l-p:0.08566537499427795
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12580382823944092
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12295947968959808
epoch£º216	 i:9 	 global-step:4329	 l-p:0.1565476357936859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9541, 4.9626, 4.9545],
        [4.9541, 5.3954, 5.4601],
        [4.9541, 5.1410, 5.0883],
        [4.9541, 4.9541, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.13429293036460876 
model_pd.l_d.mean(): -20.62839126586914 
model_pd.lagr.mean(): -20.494098663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.2842], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.13429293036460876
epoch£º217	 i:1 	 global-step:4341	 l-p:0.103899285197258
epoch£º217	 i:2 	 global-step:4342	 l-p:0.24806331098079681
epoch£º217	 i:3 	 global-step:4343	 l-p:0.15983788669109344
epoch£º217	 i:4 	 global-step:4344	 l-p:0.1726672500371933
epoch£º217	 i:5 	 global-step:4345	 l-p:0.12978827953338623
epoch£º217	 i:6 	 global-step:4346	 l-p:0.13334894180297852
epoch£º217	 i:7 	 global-step:4347	 l-p:0.28734150528907776
epoch£º217	 i:8 	 global-step:4348	 l-p:0.21384172141551971
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13903780281543732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9595, 4.9619, 4.9595],
        [4.9595, 4.9595, 4.9595],
        [4.9595, 4.9596, 4.9595],
        [4.9595, 4.9595, 4.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12264920771121979 
model_pd.l_d.mean(): -20.660490036010742 
model_pd.lagr.mean(): -20.537839889526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.3169], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12264920771121979
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13353922963142395
epoch£º218	 i:2 	 global-step:4362	 l-p:0.1365964710712433
epoch£º218	 i:3 	 global-step:4363	 l-p:0.17739973962306976
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12672999501228333
epoch£º218	 i:5 	 global-step:4365	 l-p:0.25689926743507385
epoch£º218	 i:6 	 global-step:4366	 l-p:0.6218008399009705
epoch£º218	 i:7 	 global-step:4367	 l-p:0.125974640250206
epoch£º218	 i:8 	 global-step:4368	 l-p:0.19696879386901855
epoch£º218	 i:9 	 global-step:4369	 l-p:0.08987025916576385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0179, 5.0222, 5.0181],
        [5.0179, 5.0281, 5.0186],
        [5.0179, 5.1762, 5.1192],
        [5.0179, 6.0427, 6.6261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.1196451410651207 
model_pd.l_d.mean(): -18.85625648498535 
model_pd.lagr.mean(): -18.736610412597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-19.5765], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.1196451410651207
epoch£º219	 i:1 	 global-step:4381	 l-p:0.042847879230976105
epoch£º219	 i:2 	 global-step:4382	 l-p:0.13764940202236176
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11595026403665543
epoch£º219	 i:4 	 global-step:4384	 l-p:0.15602411329746246
epoch£º219	 i:5 	 global-step:4385	 l-p:0.11611196398735046
epoch£º219	 i:6 	 global-step:4386	 l-p:0.14327970147132874
epoch£º219	 i:7 	 global-step:4387	 l-p:0.12767767906188965
epoch£º219	 i:8 	 global-step:4388	 l-p:0.11719201505184174
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13013824820518494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0530, 5.0532, 5.0530],
        [5.0530, 5.1161, 5.0739],
        [5.0530, 5.0530, 5.0530],
        [5.0530, 5.0650, 5.0540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.14565831422805786 
model_pd.l_d.mean(): -20.346195220947266 
model_pd.lagr.mean(): -20.200536727905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.0001], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.14565831422805786
epoch£º220	 i:1 	 global-step:4401	 l-p:0.12677903473377228
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10777505487203598
epoch£º220	 i:3 	 global-step:4403	 l-p:0.13951660692691803
epoch£º220	 i:4 	 global-step:4404	 l-p:0.1490025371313095
epoch£º220	 i:5 	 global-step:4405	 l-p:0.15217120945453644
epoch£º220	 i:6 	 global-step:4406	 l-p:0.055697087198495865
epoch£º220	 i:7 	 global-step:4407	 l-p:0.14174994826316833
epoch£º220	 i:8 	 global-step:4408	 l-p:0.1582309752702713
epoch£º220	 i:9 	 global-step:4409	 l-p:0.16701632738113403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7709, 5.1912, 5.2546],
        [4.7709, 5.0291, 5.0021],
        [4.7709, 4.8660, 4.8135],
        [4.7709, 4.7709, 4.7709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.1612442135810852 
model_pd.l_d.mean(): -20.192264556884766 
model_pd.lagr.mean(): -20.031021118164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5210], device='cuda:0')), ('power', tensor([-20.9451], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.1612442135810852
epoch£º221	 i:1 	 global-step:4421	 l-p:0.11815713346004486
epoch£º221	 i:2 	 global-step:4422	 l-p:0.14224569499492645
epoch£º221	 i:3 	 global-step:4423	 l-p:0.15498389303684235
epoch£º221	 i:4 	 global-step:4424	 l-p:0.07716993242502213
epoch£º221	 i:5 	 global-step:4425	 l-p:0.06180812790989876
epoch£º221	 i:6 	 global-step:4426	 l-p:0.1276254653930664
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13835760951042175
epoch£º221	 i:8 	 global-step:4428	 l-p:0.17249737679958344
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13363149762153625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8986, 5.8886, 6.4529],
        [4.8986, 5.4194, 5.5461],
        [4.8986, 5.8448, 6.3632],
        [4.8986, 5.3769, 5.4718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.11896426975727081 
model_pd.l_d.mean(): -19.598283767700195 
model_pd.lagr.mean(): -19.479318618774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5572], device='cuda:0')), ('power', tensor([-20.3816], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.11896426975727081
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11687211692333221
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14638833701610565
epoch£º222	 i:3 	 global-step:4443	 l-p:0.14745256304740906
epoch£º222	 i:4 	 global-step:4444	 l-p:0.12878762185573578
epoch£º222	 i:5 	 global-step:4445	 l-p:0.1354316920042038
epoch£º222	 i:6 	 global-step:4446	 l-p:0.20721843838691711
epoch£º222	 i:7 	 global-step:4447	 l-p:0.0953398272395134
epoch£º222	 i:8 	 global-step:4448	 l-p:0.05855425074696541
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.10583758354187012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 4.8278, 4.8277],
        [4.8277, 4.9321, 4.8779],
        [4.8277, 5.0142, 4.9635],
        [4.8277, 4.8358, 4.8275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.25730693340301514 
model_pd.l_d.mean(): -20.742177963256836 
model_pd.lagr.mean(): -20.48487091064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.4427], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.25730693340301514
epoch£º223	 i:1 	 global-step:4461	 l-p:0.13924863934516907
epoch£º223	 i:2 	 global-step:4462	 l-p:0.13328255712985992
epoch£º223	 i:3 	 global-step:4463	 l-p:0.11807698011398315
epoch£º223	 i:4 	 global-step:4464	 l-p:0.12109315395355225
epoch£º223	 i:5 	 global-step:4465	 l-p:0.19609519839286804
epoch£º223	 i:6 	 global-step:4466	 l-p:0.2832772433757782
epoch£º223	 i:7 	 global-step:4467	 l-p:0.23470133543014526
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13591916859149933
epoch£º223	 i:9 	 global-step:4469	 l-p:0.1669154316186905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0357, 5.0357],
        [5.0357, 5.0357, 5.0357],
        [5.0357, 5.0659, 5.0409],
        [5.0357, 5.0363, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12212011218070984 
model_pd.l_d.mean(): -20.57383918762207 
model_pd.lagr.mean(): -20.451719284057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.2066], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12212011218070984
epoch£º224	 i:1 	 global-step:4481	 l-p:0.1236015036702156
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13439476490020752
epoch£º224	 i:3 	 global-step:4483	 l-p:0.14684507250785828
epoch£º224	 i:4 	 global-step:4484	 l-p:0.14201821386814117
epoch£º224	 i:5 	 global-step:4485	 l-p:0.07550372928380966
epoch£º224	 i:6 	 global-step:4486	 l-p:0.15511086583137512
epoch£º224	 i:7 	 global-step:4487	 l-p:0.15428411960601807
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10857049375772476
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12164738029241562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.2218, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.11657657474279404 
model_pd.l_d.mean(): -19.738582611083984 
model_pd.lagr.mean(): -19.622005462646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400], device='cuda:0')), ('power', tensor([-20.4037], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.11657657474279404
epoch£º225	 i:1 	 global-step:4501	 l-p:0.013084390200674534
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12361659854650497
epoch£º225	 i:3 	 global-step:4503	 l-p:0.17280131578445435
epoch£º225	 i:4 	 global-step:4504	 l-p:0.12017162889242172
epoch£º225	 i:5 	 global-step:4505	 l-p:0.17262248694896698
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11893902719020844
epoch£º225	 i:7 	 global-step:4507	 l-p:0.12538525462150574
epoch£º225	 i:8 	 global-step:4508	 l-p:0.1652202308177948
epoch£º225	 i:9 	 global-step:4509	 l-p:0.12731030583381653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9763, 5.1359, 5.0790],
        [4.9763, 5.5084, 5.6376],
        [4.9763, 5.0413, 4.9975],
        [4.9763, 4.9763, 4.9763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.1249021515250206 
model_pd.l_d.mean(): -20.594324111938477 
model_pd.lagr.mean(): -20.46942138671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.1249021515250206
epoch£º226	 i:1 	 global-step:4521	 l-p:0.21629886329174042
epoch£º226	 i:2 	 global-step:4522	 l-p:0.09313873201608658
epoch£º226	 i:3 	 global-step:4523	 l-p:0.23877215385437012
epoch£º226	 i:4 	 global-step:4524	 l-p:0.14849458634853363
epoch£º226	 i:5 	 global-step:4525	 l-p:0.17836354672908783
epoch£º226	 i:6 	 global-step:4526	 l-p:0.12457814812660217
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11364564299583435
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14239297807216644
epoch£º226	 i:9 	 global-step:4529	 l-p:0.19368910789489746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 5.1062, 5.0497],
        [4.9905, 4.9915, 4.9905],
        [4.9905, 5.0124, 4.9930],
        [4.9905, 4.9906, 4.9905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.14750269055366516 
model_pd.l_d.mean(): -19.997440338134766 
model_pd.lagr.mean(): -19.849937438964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-20.6807], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.14750269055366516
epoch£º227	 i:1 	 global-step:4541	 l-p:0.11991100013256073
epoch£º227	 i:2 	 global-step:4542	 l-p:0.12921665608882904
epoch£º227	 i:3 	 global-step:4543	 l-p:0.16551828384399414
epoch£º227	 i:4 	 global-step:4544	 l-p:0.21129649877548218
epoch£º227	 i:5 	 global-step:4545	 l-p:0.20452357828617096
epoch£º227	 i:6 	 global-step:4546	 l-p:0.1258838176727295
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12425288558006287
epoch£º227	 i:8 	 global-step:4548	 l-p:0.12692128121852875
epoch£º227	 i:9 	 global-step:4549	 l-p:0.17028862237930298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0181, 5.0181, 5.0181],
        [5.0181, 5.0181, 5.0181],
        [5.0181, 5.0183, 5.0181],
        [5.0181, 5.0480, 5.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.13167737424373627 
model_pd.l_d.mean(): -20.522972106933594 
model_pd.lagr.mean(): -20.391294479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.1845], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.13167737424373627
epoch£º228	 i:1 	 global-step:4561	 l-p:0.1289372444152832
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13833211362361908
epoch£º228	 i:3 	 global-step:4563	 l-p:0.12956452369689941
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09396062791347504
epoch£º228	 i:5 	 global-step:4565	 l-p:0.19356584548950195
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13425269722938538
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14209255576133728
epoch£º228	 i:8 	 global-step:4568	 l-p:0.1728033572435379
epoch£º228	 i:9 	 global-step:4569	 l-p:0.17880645394325256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9766, 4.9767, 4.9766],
        [4.9766, 5.5341, 5.6835],
        [4.9766, 4.9894, 4.9771],
        [4.9766, 4.9847, 4.9766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.1371258646249771 
model_pd.l_d.mean(): -18.64252471923828 
model_pd.lagr.mean(): -18.505399703979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5444], device='cuda:0')), ('power', tensor([-19.4024], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.1371258646249771
epoch£º229	 i:1 	 global-step:4581	 l-p:0.1404634267091751
epoch£º229	 i:2 	 global-step:4582	 l-p:0.15949992835521698
epoch£º229	 i:3 	 global-step:4583	 l-p:0.17146213352680206
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11762558668851852
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13368342816829681
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.06004999205470085
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12178647518157959
epoch£º229	 i:8 	 global-step:4588	 l-p:0.1123773381114006
epoch£º229	 i:9 	 global-step:4589	 l-p:0.1286177933216095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0868, 5.1082, 5.0893],
        [5.0868, 6.0698, 6.6006],
        [5.0868, 5.1167, 5.0917],
        [5.0868, 5.2927, 5.2404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.10759016871452332 
model_pd.l_d.mean(): -19.03960609436035 
model_pd.lagr.mean(): -18.932016372680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-19.7501], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.10759016871452332
epoch£º230	 i:1 	 global-step:4601	 l-p:0.1321985423564911
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12211954593658447
epoch£º230	 i:3 	 global-step:4603	 l-p:0.15398210287094116
epoch£º230	 i:4 	 global-step:4604	 l-p:0.16988959908485413
epoch£º230	 i:5 	 global-step:4605	 l-p:0.13059717416763306
epoch£º230	 i:6 	 global-step:4606	 l-p:0.23631730675697327
epoch£º230	 i:7 	 global-step:4607	 l-p:0.15237145125865936
epoch£º230	 i:8 	 global-step:4608	 l-p:0.1411057859659195
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.10140493512153625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8813, 4.8813, 4.8813],
        [4.8813, 4.8814, 4.8813],
        [4.8813, 5.7612, 6.2150],
        [4.8813, 4.9288, 4.8918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12915046513080597 
model_pd.l_d.mean(): -20.14002227783203 
model_pd.lagr.mean(): -20.01087188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.8803], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12915046513080597
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14236998558044434
epoch£º231	 i:2 	 global-step:4622	 l-p:0.12565530836582184
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13442814350128174
epoch£º231	 i:4 	 global-step:4624	 l-p:0.1384107917547226
epoch£º231	 i:5 	 global-step:4625	 l-p:0.272297739982605
epoch£º231	 i:6 	 global-step:4626	 l-p:0.1363983303308487
epoch£º231	 i:7 	 global-step:4627	 l-p:0.06167805567383766
epoch£º231	 i:8 	 global-step:4628	 l-p:0.04674893617630005
epoch£º231	 i:9 	 global-step:4629	 l-p:0.16914218664169312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7763, 4.7763, 4.7763],
        [4.7763, 4.7983, 4.7767],
        [4.7763, 5.1426, 5.1713],
        [4.7763, 4.7768, 4.7761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12486621737480164 
model_pd.l_d.mean(): -20.680591583251953 
model_pd.lagr.mean(): -20.55572509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4879], device='cuda:0')), ('power', tensor([-21.4049], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12486621737480164
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18945813179016113
epoch£º232	 i:2 	 global-step:4642	 l-p:0.0564962662756443
epoch£º232	 i:3 	 global-step:4643	 l-p:0.1407143771648407
epoch£º232	 i:4 	 global-step:4644	 l-p:0.14082014560699463
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13375650346279144
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14836937189102173
epoch£º232	 i:7 	 global-step:4647	 l-p:0.12391730397939682
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11937306821346283
epoch£º232	 i:9 	 global-step:4649	 l-p:0.1213809922337532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0946, 5.0614],
        [5.0523, 5.0704, 5.0538],
        [5.0523, 5.3566, 5.3386],
        [5.0523, 5.8850, 6.2702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.08485978096723557 
model_pd.l_d.mean(): -19.99919891357422 
model_pd.lagr.mean(): -19.914339065551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.7057], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.08485978096723557
epoch£º233	 i:1 	 global-step:4661	 l-p:0.1371009796857834
epoch£º233	 i:2 	 global-step:4662	 l-p:0.11049772053956985
epoch£º233	 i:3 	 global-step:4663	 l-p:0.1328967809677124
epoch£º233	 i:4 	 global-step:4664	 l-p:0.13812416791915894
epoch£º233	 i:5 	 global-step:4665	 l-p:0.15475548803806305
epoch£º233	 i:6 	 global-step:4666	 l-p:0.14768433570861816
epoch£º233	 i:7 	 global-step:4667	 l-p:0.13534031808376312
epoch£º233	 i:8 	 global-step:4668	 l-p:0.120579294860363
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12988729774951935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 4.9703, 4.9682],
        [4.9684, 4.9763, 4.9682],
        [4.9684, 4.9815, 4.9686],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1231609433889389 
model_pd.l_d.mean(): -20.02406120300293 
model_pd.lagr.mean(): -19.90089988708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.7472], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1231609433889389
epoch£º234	 i:1 	 global-step:4681	 l-p:0.5397364497184753
epoch£º234	 i:2 	 global-step:4682	 l-p:0.1247696653008461
epoch£º234	 i:3 	 global-step:4683	 l-p:0.12900124490261078
epoch£º234	 i:4 	 global-step:4684	 l-p:0.19838958978652954
epoch£º234	 i:5 	 global-step:4685	 l-p:0.03690885379910469
epoch£º234	 i:6 	 global-step:4686	 l-p:0.13044272363185883
epoch£º234	 i:7 	 global-step:4687	 l-p:0.14489759504795074
epoch£º234	 i:8 	 global-step:4688	 l-p:0.13984765112400055
epoch£º234	 i:9 	 global-step:4689	 l-p:0.07255438715219498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7859, 5.4235, 5.6598],
        [4.7859, 4.7859, 4.7859],
        [4.7859, 4.7870, 4.7854],
        [4.7859, 4.7859, 4.7859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13790154457092285 
model_pd.l_d.mean(): -20.54977798461914 
model_pd.lagr.mean(): -20.411876678466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.2660], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13790154457092285
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.3234064280986786
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13479302823543549
epoch£º235	 i:3 	 global-step:4703	 l-p:0.04905722662806511
epoch£º235	 i:4 	 global-step:4704	 l-p:0.16354964673519135
epoch£º235	 i:5 	 global-step:4705	 l-p:0.17326733469963074
epoch£º235	 i:6 	 global-step:4706	 l-p:0.13064491748809814
epoch£º235	 i:7 	 global-step:4707	 l-p:0.14047247171401978
epoch£º235	 i:8 	 global-step:4708	 l-p:0.12356273084878922
epoch£º235	 i:9 	 global-step:4709	 l-p:0.12258125096559525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 5.1054, 5.0573],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 5.0316, 4.9734],
        [4.8974, 4.9192, 4.8983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13090725243091583 
model_pd.l_d.mean(): -20.49442481994629 
model_pd.lagr.mean(): -20.36351776123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13090725243091583
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13499099016189575
epoch£º236	 i:2 	 global-step:4722	 l-p:0.13498491048812866
epoch£º236	 i:3 	 global-step:4723	 l-p:0.13846361637115479
epoch£º236	 i:4 	 global-step:4724	 l-p:0.33564409613609314
epoch£º236	 i:5 	 global-step:4725	 l-p:0.1120021641254425
epoch£º236	 i:6 	 global-step:4726	 l-p:0.1492382138967514
epoch£º236	 i:7 	 global-step:4727	 l-p:0.11484619975090027
epoch£º236	 i:8 	 global-step:4728	 l-p:0.15211395919322968
epoch£º236	 i:9 	 global-step:4729	 l-p:0.12765644490718842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.5559, 5.8734],
        [4.8243, 4.9016, 4.8511],
        [4.8243, 4.8243, 4.8243],
        [4.8243, 4.9505, 4.8927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.05067116767168045 
model_pd.l_d.mean(): -19.58149528503418 
model_pd.lagr.mean(): -19.530824661254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5423], device='cuda:0')), ('power', tensor([-20.3494], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.05067116767168045
epoch£º237	 i:1 	 global-step:4741	 l-p:0.13289490342140198
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.03772793337702751
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14688023924827576
epoch£º237	 i:4 	 global-step:4744	 l-p:0.12754687666893005
epoch£º237	 i:5 	 global-step:4745	 l-p:0.18750888109207153
epoch£º237	 i:6 	 global-step:4746	 l-p:0.12392671406269073
epoch£º237	 i:7 	 global-step:4747	 l-p:0.14507366716861725
epoch£º237	 i:8 	 global-step:4748	 l-p:0.13124535977840424
epoch£º237	 i:9 	 global-step:4749	 l-p:0.12022155523300171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9807, 5.8701, 6.3205],
        [4.9807, 5.0297, 4.9917],
        [4.9807, 5.4909, 5.6044],
        [4.9807, 4.9807, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.13202570378780365 
model_pd.l_d.mean(): -17.28903579711914 
model_pd.lagr.mean(): -17.157011032104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6337], device='cuda:0')), ('power', tensor([-18.1253], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.13202570378780365
epoch£º238	 i:1 	 global-step:4761	 l-p:0.19713832437992096
epoch£º238	 i:2 	 global-step:4762	 l-p:0.126944437623024
epoch£º238	 i:3 	 global-step:4763	 l-p:0.1331428736448288
epoch£º238	 i:4 	 global-step:4764	 l-p:0.15771760046482086
epoch£º238	 i:5 	 global-step:4765	 l-p:0.19939862191677094
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09317602962255478
epoch£º238	 i:7 	 global-step:4767	 l-p:0.18406528234481812
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12464822828769684
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12719912827014923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9628, 5.4570, 5.5601],
        [4.9628, 5.7587, 6.1201],
        [4.9628, 5.0487, 4.9961],
        [4.9628, 5.3190, 5.3318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.15833857655525208 
model_pd.l_d.mean(): -20.348304748535156 
model_pd.lagr.mean(): -20.189966201782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.0516], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.15833857655525208
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10930314660072327
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12825247645378113
epoch£º239	 i:3 	 global-step:4783	 l-p:0.13616053760051727
epoch£º239	 i:4 	 global-step:4784	 l-p:-0.09997871518135071
epoch£º239	 i:5 	 global-step:4785	 l-p:0.1320541948080063
epoch£º239	 i:6 	 global-step:4786	 l-p:0.12197934836149216
epoch£º239	 i:7 	 global-step:4787	 l-p:0.15792681276798248
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11743684858083725
epoch£º239	 i:9 	 global-step:4789	 l-p:0.10356587916612625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7702, 5.0885, 5.0902],
        [4.7702, 4.8018, 4.7717],
        [4.7702, 4.8309, 4.7852],
        [4.7702, 4.7700, 4.7702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.04387674853205681 
model_pd.l_d.mean(): -20.714492797851562 
model_pd.lagr.mean(): -20.75836944580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4213], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.04387674853205681
epoch£º240	 i:1 	 global-step:4801	 l-p:0.16233281791210175
epoch£º240	 i:2 	 global-step:4802	 l-p:0.1404424011707306
epoch£º240	 i:3 	 global-step:4803	 l-p:0.06483650207519531
epoch£º240	 i:4 	 global-step:4804	 l-p:0.13033081591129303
epoch£º240	 i:5 	 global-step:4805	 l-p:0.12709711492061615
epoch£º240	 i:6 	 global-step:4806	 l-p:0.14522701501846313
epoch£º240	 i:7 	 global-step:4807	 l-p:0.152892604470253
epoch£º240	 i:8 	 global-step:4808	 l-p:0.20119008421897888
epoch£º240	 i:9 	 global-step:4809	 l-p:0.008088121190667152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9235, 4.9732, 4.9339],
        [4.9235, 4.9235, 4.9235],
        [4.9235, 5.4266, 5.5395],
        [4.9235, 5.6785, 6.0061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.34436360001564026 
model_pd.l_d.mean(): -20.448394775390625 
model_pd.lagr.mean(): -20.10403060913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.1421], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.34436360001564026
epoch£º241	 i:1 	 global-step:4821	 l-p:0.3576104938983917
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11997737735509872
epoch£º241	 i:3 	 global-step:4823	 l-p:0.15541283786296844
epoch£º241	 i:4 	 global-step:4824	 l-p:0.13406729698181152
epoch£º241	 i:5 	 global-step:4825	 l-p:0.12761975824832916
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11834262311458588
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11930913478136063
epoch£º241	 i:8 	 global-step:4828	 l-p:0.1336791068315506
epoch£º241	 i:9 	 global-step:4829	 l-p:0.0709373727440834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0557, 5.3535, 5.3325],
        [5.0557, 5.0730, 5.0563],
        [5.0557, 5.0557, 5.0557],
        [5.0557, 5.7603, 6.0273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.14043200016021729 
model_pd.l_d.mean(): -20.276185989379883 
model_pd.lagr.mean(): -20.135753631591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-20.9462], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.14043200016021729
epoch£º242	 i:1 	 global-step:4841	 l-p:0.124775730073452
epoch£º242	 i:2 	 global-step:4842	 l-p:0.1574757993221283
epoch£º242	 i:3 	 global-step:4843	 l-p:0.12659244239330292
epoch£º242	 i:4 	 global-step:4844	 l-p:0.14923015236854553
epoch£º242	 i:5 	 global-step:4845	 l-p:0.1324823945760727
epoch£º242	 i:6 	 global-step:4846	 l-p:0.09521372616291046
epoch£º242	 i:7 	 global-step:4847	 l-p:0.13411657512187958
epoch£º242	 i:8 	 global-step:4848	 l-p:0.12463260442018509
epoch£º242	 i:9 	 global-step:4849	 l-p:0.2675166726112366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9689, 4.9940, 4.9703],
        [4.9689, 4.9688, 4.9689],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 5.5872, 5.7882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.15441419184207916 
model_pd.l_d.mean(): -20.06182861328125 
model_pd.lagr.mean(): -19.907413482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.15441419184207916
epoch£º243	 i:1 	 global-step:4861	 l-p:0.34999868273735046
epoch£º243	 i:2 	 global-step:4862	 l-p:0.1294494867324829
epoch£º243	 i:3 	 global-step:4863	 l-p:0.13986200094223022
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12119437009096146
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11844726651906967
epoch£º243	 i:6 	 global-step:4866	 l-p:0.1958795189857483
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.09587378054857254
epoch£º243	 i:8 	 global-step:4868	 l-p:0.1314113438129425
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13166095316410065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8765, 4.8887, 4.8750],
        [4.8765, 5.1567, 5.1348],
        [4.8765, 4.9078, 4.8785],
        [4.8765, 5.4180, 5.5648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.05519481375813484 
model_pd.l_d.mean(): -20.495203018188477 
model_pd.lagr.mean(): -20.44000816345215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.05519481375813484
epoch£º244	 i:1 	 global-step:4881	 l-p:0.13948331773281097
epoch£º244	 i:2 	 global-step:4882	 l-p:0.1229717805981636
epoch£º244	 i:3 	 global-step:4883	 l-p:0.1292712539434433
epoch£º244	 i:4 	 global-step:4884	 l-p:0.12551888823509216
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.060907457023859024
epoch£º244	 i:6 	 global-step:4886	 l-p:0.13509945571422577
epoch£º244	 i:7 	 global-step:4887	 l-p:0.22194303572177887
epoch£º244	 i:8 	 global-step:4888	 l-p:0.08170053362846375
epoch£º244	 i:9 	 global-step:4889	 l-p:0.15736490488052368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.0081, 4.9491],
        [4.8648, 5.1494, 5.1300],
        [4.8648, 4.8648, 4.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.08372485637664795 
model_pd.l_d.mean(): -20.004383087158203 
model_pd.lagr.mean(): -19.920658111572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-20.7625], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.08372485637664795
epoch£º245	 i:1 	 global-step:4901	 l-p:0.21301814913749695
epoch£º245	 i:2 	 global-step:4902	 l-p:-0.3032490611076355
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14887574315071106
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11591047048568726
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13196296989917755
epoch£º245	 i:6 	 global-step:4906	 l-p:0.08011158555746078
epoch£º245	 i:7 	 global-step:4907	 l-p:0.16839584708213806
epoch£º245	 i:8 	 global-step:4908	 l-p:0.12428327649831772
epoch£º245	 i:9 	 global-step:4909	 l-p:0.12341107428073883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 5.6221, 5.7100],
        [5.1325, 5.1326, 5.1325],
        [5.1325, 5.1848, 5.1450],
        [5.1325, 5.1325, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12045373767614365 
model_pd.l_d.mean(): -19.457027435302734 
model_pd.lagr.mean(): -19.33657455444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-20.1510], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12045373767614365
epoch£º246	 i:1 	 global-step:4921	 l-p:0.12988398969173431
epoch£º246	 i:2 	 global-step:4922	 l-p:0.1273346245288849
epoch£º246	 i:3 	 global-step:4923	 l-p:0.136764258146286
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13089236617088318
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12898039817810059
epoch£º246	 i:6 	 global-step:4926	 l-p:0.02778364159166813
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13892288506031036
epoch£º246	 i:8 	 global-step:4928	 l-p:0.15403766930103302
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12092168629169464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 5.0493, 5.0493],
        [5.0493, 5.0572, 5.0487],
        [5.0493, 5.1392, 5.0847],
        [5.0493, 5.0493, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.0759192705154419 
model_pd.l_d.mean(): -19.95372772216797 
model_pd.lagr.mean(): -19.8778076171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-20.6225], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.0759192705154419
epoch£º247	 i:1 	 global-step:4941	 l-p:0.13495303690433502
epoch£º247	 i:2 	 global-step:4942	 l-p:0.169984832406044
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13954584300518036
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14203234016895294
epoch£º247	 i:5 	 global-step:4945	 l-p:0.19284388422966003
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12907524406909943
epoch£º247	 i:7 	 global-step:4947	 l-p:0.13004092872142792
epoch£º247	 i:8 	 global-step:4948	 l-p:0.119932621717453
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11529280990362167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0459, 5.0459, 5.0459],
        [5.0459, 5.0459, 5.0459],
        [5.0459, 5.0530, 5.0452],
        [5.0459, 5.0459, 5.0459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.1097455769777298 
model_pd.l_d.mean(): -20.614587783813477 
model_pd.lagr.mean(): -20.50484275817871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133], device='cuda:0')), ('power', tensor([-21.2620], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.1097455769777298
epoch£º248	 i:1 	 global-step:4961	 l-p:0.1400812566280365
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12368986010551453
epoch£º248	 i:3 	 global-step:4963	 l-p:0.17744868993759155
epoch£º248	 i:4 	 global-step:4964	 l-p:0.1276199370622635
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13680586218833923
epoch£º248	 i:6 	 global-step:4966	 l-p:0.1328810155391693
epoch£º248	 i:7 	 global-step:4967	 l-p:0.14310447871685028
epoch£º248	 i:8 	 global-step:4968	 l-p:0.12229947745800018
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.15898820757865906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9005, 5.2959, 5.3360],
        [4.9005, 5.3443, 5.4166],
        [4.9005, 4.9669, 4.9183],
        [4.9005, 5.6387, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.006082405801862478 
model_pd.l_d.mean(): -20.677947998046875 
model_pd.lagr.mean(): -20.671865463256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.3645], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.006082405801862478
epoch£º249	 i:1 	 global-step:4981	 l-p:0.152459979057312
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12304068356752396
epoch£º249	 i:3 	 global-step:4983	 l-p:0.1817658245563507
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21217015385627747
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13354535400867462
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10747205466032028
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12473589926958084
epoch£º249	 i:8 	 global-step:4988	 l-p:0.1505402773618698
epoch£º249	 i:9 	 global-step:4989	 l-p:0.49460315704345703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 5.1167, 5.0600],
        [4.9395, 4.9395, 4.9395],
        [4.9395, 4.9498, 4.9377],
        [4.9395, 4.9395, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16068539023399353 
model_pd.l_d.mean(): -19.52203369140625 
model_pd.lagr.mean(): -19.36134910583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.2634], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16068539023399353
epoch£º250	 i:1 	 global-step:5001	 l-p:0.16915805637836456
epoch£º250	 i:2 	 global-step:5002	 l-p:0.1279459446668625
epoch£º250	 i:3 	 global-step:5003	 l-p:0.14027154445648193
epoch£º250	 i:4 	 global-step:5004	 l-p:0.12828125059604645
epoch£º250	 i:5 	 global-step:5005	 l-p:0.12652914226055145
epoch£º250	 i:6 	 global-step:5006	 l-p:0.1437731683254242
epoch£º250	 i:7 	 global-step:5007	 l-p:0.1243731752038002
epoch£º250	 i:8 	 global-step:5008	 l-p:0.1629408448934555
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08773292601108551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.2101, 5.1496],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 5.1302, 5.0783],
        [5.0521, 5.0521, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18398940563201904 
model_pd.l_d.mean(): -20.156991958618164 
model_pd.lagr.mean(): -19.973003387451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.8441], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18398940563201904
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13463865220546722
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12298578023910522
epoch£º251	 i:3 	 global-step:5023	 l-p:0.11959443986415863
epoch£º251	 i:4 	 global-step:5024	 l-p:0.1421973556280136
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11592794209718704
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07899322360754013
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12587961554527283
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12722140550613403
epoch£º251	 i:9 	 global-step:5029	 l-p:0.13055452704429626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0204, 5.0203, 5.0204],
        [5.0204, 5.9337, 6.4042],
        [5.0204, 5.0946, 5.0436],
        [5.0204, 5.1837, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12824617326259613 
model_pd.l_d.mean(): -20.0947265625 
model_pd.lagr.mean(): -19.966480255126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12824617326259613
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21431586146354675
epoch£º252	 i:2 	 global-step:5042	 l-p:0.12720656394958496
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10369554907083511
epoch£º252	 i:4 	 global-step:5044	 l-p:0.17751747369766235
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12636108696460724
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5342134237289429
epoch£º252	 i:7 	 global-step:5047	 l-p:0.1344224214553833
epoch£º252	 i:8 	 global-step:5048	 l-p:0.13038668036460876
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07688461989164352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8350, 5.6682, 6.0837],
        [4.8350, 4.9044, 4.8533],
        [4.8350, 4.8962, 4.8481],
        [4.8350, 5.2284, 5.2715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.13382482528686523 
model_pd.l_d.mean(): -20.601797103881836 
model_pd.lagr.mean(): -20.467971801757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.3105], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.13382482528686523
epoch£º253	 i:1 	 global-step:5061	 l-p:0.22805577516555786
epoch£º253	 i:2 	 global-step:5062	 l-p:0.11031195521354675
epoch£º253	 i:3 	 global-step:5063	 l-p:0.13835471868515015
epoch£º253	 i:4 	 global-step:5064	 l-p:0.07849235832691193
epoch£º253	 i:5 	 global-step:5065	 l-p:0.1416543424129486
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13150344789028168
epoch£º253	 i:7 	 global-step:5067	 l-p:0.23731616139411926
epoch£º253	 i:8 	 global-step:5068	 l-p:0.143331378698349
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12494724243879318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0128, 5.0220, 5.0113],
        [5.0128, 5.0533, 5.0177],
        [5.0128, 5.0128, 5.0128],
        [5.0128, 5.0128, 5.0128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.1571919023990631 
model_pd.l_d.mean(): -20.200695037841797 
model_pd.lagr.mean(): -20.043502807617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4580], device='cuda:0')), ('power', tensor([-20.8892], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.1571919023990631
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09935018420219421
epoch£º254	 i:2 	 global-step:5082	 l-p:0.1427006870508194
epoch£º254	 i:3 	 global-step:5083	 l-p:0.13175198435783386
epoch£º254	 i:4 	 global-step:5084	 l-p:0.1044013649225235
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11703619360923767
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11476942896842957
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14848574995994568
epoch£º254	 i:8 	 global-step:5088	 l-p:0.1506095826625824
epoch£º254	 i:9 	 global-step:5089	 l-p:0.1536850482225418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0550, 5.9603, 6.4183],
        [5.0550, 5.1889, 5.1273],
        [5.0550, 5.4994, 5.5621],
        [5.0550, 6.1157, 6.7342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.15715321898460388 
model_pd.l_d.mean(): -18.83354949951172 
model_pd.lagr.mean(): -18.676395416259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-19.5653], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.15715321898460388
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11566037684679031
epoch£º255	 i:2 	 global-step:5102	 l-p:0.0903046503663063
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12296565622091293
epoch£º255	 i:4 	 global-step:5104	 l-p:0.1728735715150833
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12643688917160034
epoch£º255	 i:6 	 global-step:5106	 l-p:0.117802694439888
epoch£º255	 i:7 	 global-step:5107	 l-p:0.22583232820034027
epoch£º255	 i:8 	 global-step:5108	 l-p:0.19006462395191193
epoch£º255	 i:9 	 global-step:5109	 l-p:0.1432124525308609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9524, 4.9802, 4.9521],
        [4.9524, 4.9530, 4.9516],
        [4.9524, 4.9524, 4.9524],
        [4.9524, 5.3126, 5.3285]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.1442895382642746 
model_pd.l_d.mean(): -20.630762100219727 
model_pd.lagr.mean(): -20.486473083496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-21.3014], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.1442895382642746
epoch£º256	 i:1 	 global-step:5121	 l-p:-1.12046217918396
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13404011726379395
epoch£º256	 i:3 	 global-step:5123	 l-p:0.1317869871854782
epoch£º256	 i:4 	 global-step:5124	 l-p:0.08614642173051834
epoch£º256	 i:5 	 global-step:5125	 l-p:0.13315124809741974
epoch£º256	 i:6 	 global-step:5126	 l-p:0.13757039606571198
epoch£º256	 i:7 	 global-step:5127	 l-p:0.26093053817749023
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12209311127662659
epoch£º256	 i:9 	 global-step:5129	 l-p:0.4238179624080658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8820, 4.8820, 4.8820],
        [4.8820, 4.8820, 4.8820],
        [4.8820, 5.0531, 4.9953],
        [4.8820, 4.8845, 4.8800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12436847388744354 
model_pd.l_d.mean(): -19.61056137084961 
model_pd.lagr.mean(): -19.48619270324707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-20.3304], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12436847388744354
epoch£º257	 i:1 	 global-step:5141	 l-p:0.1405889242887497
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12482389807701111
epoch£º257	 i:3 	 global-step:5143	 l-p:0.07230623066425323
epoch£º257	 i:4 	 global-step:5144	 l-p:0.17366589605808258
epoch£º257	 i:5 	 global-step:5145	 l-p:0.12843559682369232
epoch£º257	 i:6 	 global-step:5146	 l-p:0.13023346662521362
epoch£º257	 i:7 	 global-step:5147	 l-p:0.17065532505512238
epoch£º257	 i:8 	 global-step:5148	 l-p:0.2841341197490692
epoch£º257	 i:9 	 global-step:5149	 l-p:0.13181331753730774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0314, 5.0312, 5.0313],
        [5.0314, 5.0617, 5.0324],
        [5.0314, 5.0314, 5.0314],
        [5.0314, 5.4106, 5.4335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12389259040355682 
model_pd.l_d.mean(): -20.088342666625977 
model_pd.lagr.mean(): -19.96445083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-20.7781], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12389259040355682
epoch£º258	 i:1 	 global-step:5161	 l-p:0.12640592455863953
epoch£º258	 i:2 	 global-step:5162	 l-p:0.1826086938381195
epoch£º258	 i:3 	 global-step:5163	 l-p:0.1065203994512558
epoch£º258	 i:4 	 global-step:5164	 l-p:0.11889451742172241
epoch£º258	 i:5 	 global-step:5165	 l-p:0.16046425700187683
epoch£º258	 i:6 	 global-step:5166	 l-p:0.16654323041439056
epoch£º258	 i:7 	 global-step:5167	 l-p:0.12168828397989273
epoch£º258	 i:8 	 global-step:5168	 l-p:0.11846419423818588
epoch£º258	 i:9 	 global-step:5169	 l-p:0.13969974219799042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0140, 5.3885, 5.4095],
        [5.0140, 5.0140, 5.0140],
        [5.0140, 5.0140, 5.0140],
        [5.0140, 5.0819, 5.0319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11529939621686935 
model_pd.l_d.mean(): -18.97616195678711 
model_pd.lagr.mean(): -18.860862731933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4973], device='cuda:0')), ('power', tensor([-19.6915], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11529939621686935
epoch£º259	 i:1 	 global-step:5181	 l-p:0.10173976421356201
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15407446026802063
epoch£º259	 i:3 	 global-step:5183	 l-p:0.14728745818138123
epoch£º259	 i:4 	 global-step:5184	 l-p:-1.5414752960205078
epoch£º259	 i:5 	 global-step:5185	 l-p:-0.0066255186684429646
epoch£º259	 i:6 	 global-step:5186	 l-p:0.12284404784440994
epoch£º259	 i:7 	 global-step:5187	 l-p:0.12742339074611664
epoch£º259	 i:8 	 global-step:5188	 l-p:0.062424931675195694
epoch£º259	 i:9 	 global-step:5189	 l-p:0.151356503367424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8638, 5.3769, 5.5040],
        [4.8638, 5.1160, 5.0821],
        [4.8638, 4.8638, 4.8638],
        [4.8638, 4.9322, 4.8801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.19848741590976715 
model_pd.l_d.mean(): -20.15461540222168 
model_pd.lagr.mean(): -19.956127166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5213], device='cuda:0')), ('power', tensor([-20.9073], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.19848741590976715
epoch£º260	 i:1 	 global-step:5201	 l-p:0.07466737926006317
epoch£º260	 i:2 	 global-step:5202	 l-p:0.192795068025589
epoch£º260	 i:3 	 global-step:5203	 l-p:0.11093524843454361
epoch£º260	 i:4 	 global-step:5204	 l-p:0.025303324684500694
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13696959614753723
epoch£º260	 i:6 	 global-step:5206	 l-p:0.1338430494070053
epoch£º260	 i:7 	 global-step:5207	 l-p:0.155287504196167
epoch£º260	 i:8 	 global-step:5208	 l-p:0.3682045638561249
epoch£º260	 i:9 	 global-step:5209	 l-p:0.1302347034215927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9962, 4.9962, 4.9962],
        [4.9962, 5.3227, 5.3179],
        [4.9962, 5.3648, 5.3834],
        [4.9962, 5.0011, 4.9941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.1819303333759308 
model_pd.l_d.mean(): -20.067338943481445 
model_pd.lagr.mean(): -19.885408401489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-20.7443], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.1819303333759308
epoch£º261	 i:1 	 global-step:5221	 l-p:0.09598568826913834
epoch£º261	 i:2 	 global-step:5222	 l-p:0.15625344216823578
epoch£º261	 i:3 	 global-step:5223	 l-p:0.1169528067111969
epoch£º261	 i:4 	 global-step:5224	 l-p:0.15246912837028503
epoch£º261	 i:5 	 global-step:5225	 l-p:0.13656377792358398
epoch£º261	 i:6 	 global-step:5226	 l-p:0.12250018864870071
epoch£º261	 i:7 	 global-step:5227	 l-p:0.13416729867458344
epoch£º261	 i:8 	 global-step:5228	 l-p:0.1426437497138977
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13730141520500183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 5.0412, 5.0405],
        [5.0410, 5.0511, 5.0387],
        [5.0410, 5.0407, 5.0409],
        [5.0410, 5.0407, 5.0409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.12419139593839645 
model_pd.l_d.mean(): -19.544954299926758 
model_pd.lagr.mean(): -19.42076301574707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.2472], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.12419139593839645
epoch£º262	 i:1 	 global-step:5241	 l-p:0.13045254349708557
epoch£º262	 i:2 	 global-step:5242	 l-p:0.14176687598228455
epoch£º262	 i:3 	 global-step:5243	 l-p:-0.09330160915851593
epoch£º262	 i:4 	 global-step:5244	 l-p:0.07201921939849854
epoch£º262	 i:5 	 global-step:5245	 l-p:0.28412649035453796
epoch£º262	 i:6 	 global-step:5246	 l-p:0.18677955865859985
epoch£º262	 i:7 	 global-step:5247	 l-p:0.14947862923145294
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.013587560504674911
epoch£º262	 i:9 	 global-step:5249	 l-p:0.13480713963508606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9483, 5.4100, 5.4921],
        [4.9483, 5.2272, 5.2014],
        [4.9483, 4.9498, 4.9467],
        [4.9483, 5.3540, 5.3982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.13244862854480743 
model_pd.l_d.mean(): -19.346105575561523 
model_pd.lagr.mean(): -19.21365737915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-20.0728], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.13244862854480743
epoch£º263	 i:1 	 global-step:5261	 l-p:0.3356139361858368
epoch£º263	 i:2 	 global-step:5262	 l-p:0.13563193380832672
epoch£º263	 i:3 	 global-step:5263	 l-p:0.13561387360095978
epoch£º263	 i:4 	 global-step:5264	 l-p:0.1855953484773636
epoch£º263	 i:5 	 global-step:5265	 l-p:0.14000605046749115
epoch£º263	 i:6 	 global-step:5266	 l-p:0.16339026391506195
epoch£º263	 i:7 	 global-step:5267	 l-p:0.13209310173988342
epoch£º263	 i:8 	 global-step:5268	 l-p:0.08913344889879227
epoch£º263	 i:9 	 global-step:5269	 l-p:0.12508133053779602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0682, 5.0680],
        [5.0683, 5.5738, 5.6790],
        [5.0683, 5.8368, 6.1623],
        [5.0683, 5.4151, 5.4178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.15184225142002106 
model_pd.l_d.mean(): -20.497413635253906 
model_pd.lagr.mean(): -20.345571517944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.1517], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.15184225142002106
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1298166811466217
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10284465551376343
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12148106098175049
epoch£º264	 i:4 	 global-step:5284	 l-p:0.14276225864887238
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14111974835395813
epoch£º264	 i:6 	 global-step:5286	 l-p:0.1719241440296173
epoch£º264	 i:7 	 global-step:5287	 l-p:0.21114185452461243
epoch£º264	 i:8 	 global-step:5288	 l-p:0.14568418264389038
epoch£º264	 i:9 	 global-step:5289	 l-p:0.11967809498310089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.8771, 6.2761],
        [5.0351, 5.1887, 5.1264],
        [5.0351, 5.0351, 5.0351],
        [5.0351, 5.0351, 5.0351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11227447539567947 
model_pd.l_d.mean(): -19.797971725463867 
model_pd.lagr.mean(): -19.685697555541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.5225], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11227447539567947
epoch£º265	 i:1 	 global-step:5301	 l-p:0.14329463243484497
epoch£º265	 i:2 	 global-step:5302	 l-p:0.13343213498592377
epoch£º265	 i:3 	 global-step:5303	 l-p:0.20620717108249664
epoch£º265	 i:4 	 global-step:5304	 l-p:0.08757854998111725
epoch£º265	 i:5 	 global-step:5305	 l-p:0.15430113673210144
epoch£º265	 i:6 	 global-step:5306	 l-p:0.12970907986164093
epoch£º265	 i:7 	 global-step:5307	 l-p:0.13245807588100433
epoch£º265	 i:8 	 global-step:5308	 l-p:0.14639315009117126
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12137802690267563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0445, 5.0441, 5.0444],
        [5.0445, 5.0444, 5.0445],
        [5.0445, 5.0459, 5.0433],
        [5.0445, 5.0444, 5.0445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.1245107650756836 
model_pd.l_d.mean(): -20.35291290283203 
model_pd.lagr.mean(): -20.22840118408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.0259], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.1245107650756836
epoch£º266	 i:1 	 global-step:5321	 l-p:0.1314556896686554
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12626193463802338
epoch£º266	 i:3 	 global-step:5323	 l-p:0.5622094869613647
epoch£º266	 i:4 	 global-step:5324	 l-p:0.15950168669223785
epoch£º266	 i:5 	 global-step:5325	 l-p:10.580345153808594
epoch£º266	 i:6 	 global-step:5326	 l-p:0.14160017669200897
epoch£º266	 i:7 	 global-step:5327	 l-p:0.2128227800130844
epoch£º266	 i:8 	 global-step:5328	 l-p:0.1308491826057434
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13453389704227448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0244, 5.0244, 5.0244],
        [5.0244, 5.1135, 5.0554],
        [5.0244, 5.0244, 5.0244],
        [5.0244, 5.0244, 5.0236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.1321476846933365 
model_pd.l_d.mean(): -19.963226318359375 
model_pd.lagr.mean(): -19.831079483032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4891], device='cuda:0')), ('power', tensor([-20.6810], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.1321476846933365
epoch£º267	 i:1 	 global-step:5341	 l-p:0.14683733880519867
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11768755316734314
epoch£º267	 i:3 	 global-step:5343	 l-p:0.13550828397274017
epoch£º267	 i:4 	 global-step:5344	 l-p:0.12092448770999908
epoch£º267	 i:5 	 global-step:5345	 l-p:0.21909397840499878
epoch£º267	 i:6 	 global-step:5346	 l-p:0.5530768632888794
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12545321881771088
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13591431081295013
epoch£º267	 i:9 	 global-step:5349	 l-p:0.18190787732601166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9703, 4.9702, 4.9703],
        [4.9703, 5.1782, 5.1258],
        [4.9703, 4.9699, 4.9693],
        [4.9703, 4.9703, 4.9703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.1508447229862213 
model_pd.l_d.mean(): -20.23975372314453 
model_pd.lagr.mean(): -20.088909149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-20.9493], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.1508447229862213
epoch£º268	 i:1 	 global-step:5361	 l-p:0.13835939764976501
epoch£º268	 i:2 	 global-step:5362	 l-p:0.15181003510951996
epoch£º268	 i:3 	 global-step:5363	 l-p:0.12221954017877579
epoch£º268	 i:4 	 global-step:5364	 l-p:0.24422012269496918
epoch£º268	 i:5 	 global-step:5365	 l-p:0.2684919238090515
epoch£º268	 i:6 	 global-step:5366	 l-p:0.1783885657787323
epoch£º268	 i:7 	 global-step:5367	 l-p:0.1318623572587967
epoch£º268	 i:8 	 global-step:5368	 l-p:0.14727775752544403
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12736859917640686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0325, 5.0324, 5.0325],
        [5.0325, 5.0320, 5.0323],
        [5.0325, 5.0438, 5.0291],
        [5.0325, 5.0365, 5.0301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.18202559649944305 
model_pd.l_d.mean(): -20.42031478881836 
model_pd.lagr.mean(): -20.23828887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.1038], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.18202559649944305
epoch£º269	 i:1 	 global-step:5381	 l-p:0.21478061378002167
epoch£º269	 i:2 	 global-step:5382	 l-p:0.1398930549621582
epoch£º269	 i:3 	 global-step:5383	 l-p:0.11545627564191818
epoch£º269	 i:4 	 global-step:5384	 l-p:0.12399585545063019
epoch£º269	 i:5 	 global-step:5385	 l-p:0.1234065592288971
epoch£º269	 i:6 	 global-step:5386	 l-p:0.10815862566232681
epoch£º269	 i:7 	 global-step:5387	 l-p:0.18612152338027954
epoch£º269	 i:8 	 global-step:5388	 l-p:0.14222022891044617
epoch£º269	 i:9 	 global-step:5389	 l-p:0.10535905510187149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 4.9340, 4.9346],
        [4.9348, 5.1876, 5.1509],
        [4.9348, 4.9346, 4.9348],
        [4.9348, 5.0432, 4.9805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13600091636180878 
model_pd.l_d.mean(): -20.645374298095703 
model_pd.lagr.mean(): -20.50937271118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-21.3222], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.13600091636180878
epoch£º270	 i:1 	 global-step:5401	 l-p:0.018916597589850426
epoch£º270	 i:2 	 global-step:5402	 l-p:0.050698716193437576
epoch£º270	 i:3 	 global-step:5403	 l-p:0.15853452682495117
epoch£º270	 i:4 	 global-step:5404	 l-p:0.14541269838809967
epoch£º270	 i:5 	 global-step:5405	 l-p:0.023124026134610176
epoch£º270	 i:6 	 global-step:5406	 l-p:0.2564946413040161
epoch£º270	 i:7 	 global-step:5407	 l-p:0.13490232825279236
epoch£º270	 i:8 	 global-step:5408	 l-p:0.13464932143688202
epoch£º270	 i:9 	 global-step:5409	 l-p:0.18866302073001862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8825, 4.8825, 4.8825],
        [4.8825, 4.9690, 4.9093],
        [4.8825, 5.2627, 5.2947],
        [4.8825, 5.5864, 5.8744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.0990375503897667 
model_pd.l_d.mean(): -20.487178802490234 
model_pd.lagr.mean(): -20.388141632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-21.2048], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.0990375503897667
epoch£º271	 i:1 	 global-step:5421	 l-p:0.12662801146507263
epoch£º271	 i:2 	 global-step:5422	 l-p:0.166375532746315
epoch£º271	 i:3 	 global-step:5423	 l-p:0.1688845008611679
epoch£º271	 i:4 	 global-step:5424	 l-p:0.13838542997837067
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12122920900583267
epoch£º271	 i:6 	 global-step:5426	 l-p:-0.1192202940583229
epoch£º271	 i:7 	 global-step:5427	 l-p:0.12886680662631989
epoch£º271	 i:8 	 global-step:5428	 l-p:0.23407454788684845
epoch£º271	 i:9 	 global-step:5429	 l-p:0.13734170794487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9951, 4.9944, 4.9943],
        [4.9951, 5.4821, 5.5794],
        [4.9951, 4.9963, 4.9929],
        [4.9951, 4.9943, 4.9947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.29392746090888977 
model_pd.l_d.mean(): -19.21246337890625 
model_pd.lagr.mean(): -18.918535232543945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9314], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.29392746090888977
epoch£º272	 i:1 	 global-step:5441	 l-p:0.13660097122192383
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11329885572195053
epoch£º272	 i:3 	 global-step:5443	 l-p:0.08178815990686417
epoch£º272	 i:4 	 global-step:5444	 l-p:0.13547971844673157
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12132081389427185
epoch£º272	 i:6 	 global-step:5446	 l-p:0.16656070947647095
epoch£º272	 i:7 	 global-step:5447	 l-p:0.12424439936876297
epoch£º272	 i:8 	 global-step:5448	 l-p:0.15058007836341858
epoch£º272	 i:9 	 global-step:5449	 l-p:0.13227617740631104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0224, 5.0896, 5.0368],
        [5.0224, 5.0220, 5.0223],
        [5.0224, 5.0253, 5.0195],
        [5.0224, 5.0217, 5.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.1418539136648178 
model_pd.l_d.mean(): -18.519277572631836 
model_pd.lagr.mean(): -18.377424240112305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5394], device='cuda:0')), ('power', tensor([-19.2726], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.1418539136648178
epoch£º273	 i:1 	 global-step:5461	 l-p:0.1320270448923111
epoch£º273	 i:2 	 global-step:5462	 l-p:0.1362348198890686
epoch£º273	 i:3 	 global-step:5463	 l-p:0.34283384680747986
epoch£º273	 i:4 	 global-step:5464	 l-p:0.11952302604913712
epoch£º273	 i:5 	 global-step:5465	 l-p:0.08030033111572266
epoch£º273	 i:6 	 global-step:5466	 l-p:0.1439628005027771
epoch£º273	 i:7 	 global-step:5467	 l-p:0.1352677345275879
epoch£º273	 i:8 	 global-step:5468	 l-p:0.1356983780860901
epoch£º273	 i:9 	 global-step:5469	 l-p:0.29263779520988464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8274, 5.0227, 4.9690],
        [4.8274, 5.5095, 5.7844],
        [4.8274, 4.8266, 4.8273],
        [4.8274, 5.5077, 5.7810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.054797396063804626 
model_pd.l_d.mean(): -20.69412612915039 
model_pd.lagr.mean(): -20.639328002929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4008], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.054797396063804626
epoch£º274	 i:1 	 global-step:5481	 l-p:0.1388600915670395
epoch£º274	 i:2 	 global-step:5482	 l-p:0.2518712878227234
epoch£º274	 i:3 	 global-step:5483	 l-p:-0.24346844851970673
epoch£º274	 i:4 	 global-step:5484	 l-p:0.15412889420986176
epoch£º274	 i:5 	 global-step:5485	 l-p:0.143660768866539
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12708286941051483
epoch£º274	 i:7 	 global-step:5487	 l-p:0.1261511594057083
epoch£º274	 i:8 	 global-step:5488	 l-p:-0.6087188720703125
epoch£º274	 i:9 	 global-step:5489	 l-p:0.10900034010410309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 5.1849, 5.1569],
        [4.9125, 5.2080, 5.1908],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 4.9110, 4.9116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.16206665337085724 
model_pd.l_d.mean(): -20.778507232666016 
model_pd.lagr.mean(): -20.616439819335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.16206665337085724
epoch£º275	 i:1 	 global-step:5501	 l-p:0.1341918408870697
epoch£º275	 i:2 	 global-step:5502	 l-p:0.12437807023525238
epoch£º275	 i:3 	 global-step:5503	 l-p:0.21676038205623627
epoch£º275	 i:4 	 global-step:5504	 l-p:0.18662025034427643
epoch£º275	 i:5 	 global-step:5505	 l-p:0.17394275963306427
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11600840091705322
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10471270978450775
epoch£º275	 i:8 	 global-step:5508	 l-p:0.15012593567371368
epoch£º275	 i:9 	 global-step:5509	 l-p:0.12751123309135437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.8176, 6.0676],
        [5.1271, 5.1271, 5.1263],
        [5.1271, 5.1271, 5.1271],
        [5.1271, 5.1269, 5.1271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12381025403738022 
model_pd.l_d.mean(): -20.77425193786621 
model_pd.lagr.mean(): -20.650442123413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3680], device='cuda:0')), ('power', tensor([-21.3771], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12381025403738022
epoch£º276	 i:1 	 global-step:5521	 l-p:0.14502441883087158
epoch£º276	 i:2 	 global-step:5522	 l-p:0.14405931532382965
epoch£º276	 i:3 	 global-step:5523	 l-p:0.13329872488975525
epoch£º276	 i:4 	 global-step:5524	 l-p:0.15607841312885284
epoch£º276	 i:5 	 global-step:5525	 l-p:0.172522634267807
epoch£º276	 i:6 	 global-step:5526	 l-p:0.1378854662179947
epoch£º276	 i:7 	 global-step:5527	 l-p:0.11647168546915054
epoch£º276	 i:8 	 global-step:5528	 l-p:0.11094474792480469
epoch£º276	 i:9 	 global-step:5529	 l-p:0.1590568572282791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9983, 5.4807, 5.5746],
        [4.9983, 5.5675, 5.7278],
        [4.9983, 4.9983, 4.9983],
        [4.9983, 5.3711, 5.3921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.14515937864780426 
model_pd.l_d.mean(): -20.20163917541504 
model_pd.lagr.mean(): -20.056480407714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-20.9101], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.14515937864780426
epoch£º277	 i:1 	 global-step:5541	 l-p:0.12425222247838974
epoch£º277	 i:2 	 global-step:5542	 l-p:0.12559282779693604
epoch£º277	 i:3 	 global-step:5543	 l-p:0.14565809071063995
epoch£º277	 i:4 	 global-step:5544	 l-p:-0.20654113590717316
epoch£º277	 i:5 	 global-step:5545	 l-p:0.10350755602121353
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14581124484539032
epoch£º277	 i:7 	 global-step:5547	 l-p:0.03493689000606537
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13850831985473633
epoch£º277	 i:9 	 global-step:5549	 l-p:0.14383110404014587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8266, 4.8249, 4.8262],
        [4.8266, 4.8242, 4.8237],
        [4.8266, 4.8369, 4.8173],
        [4.8266, 5.3123, 5.4225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.13573378324508667 
model_pd.l_d.mean(): -19.17479705810547 
model_pd.lagr.mean(): -19.0390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.9367], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.13573378324508667
epoch£º278	 i:1 	 global-step:5561	 l-p:0.17826779186725616
epoch£º278	 i:2 	 global-step:5562	 l-p:0.07297706604003906
epoch£º278	 i:3 	 global-step:5563	 l-p:0.12317540496587753
epoch£º278	 i:4 	 global-step:5564	 l-p:0.17787374556064606
epoch£º278	 i:5 	 global-step:5565	 l-p:0.12192581593990326
epoch£º278	 i:6 	 global-step:5566	 l-p:0.15087872743606567
epoch£º278	 i:7 	 global-step:5567	 l-p:0.12226220965385437
epoch£º278	 i:8 	 global-step:5568	 l-p:0.18435077369213104
epoch£º278	 i:9 	 global-step:5569	 l-p:0.12893079221248627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0364, 5.0798, 5.0373],
        [5.0364, 5.0378, 5.0335],
        [5.0364, 5.5247, 5.6203],
        [5.0364, 5.0704, 5.0341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.13452628254890442 
model_pd.l_d.mean(): -20.48410987854004 
model_pd.lagr.mean(): -20.34958267211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.1462], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.13452628254890442
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21253058314323425
epoch£º279	 i:2 	 global-step:5582	 l-p:0.10726560652256012
epoch£º279	 i:3 	 global-step:5583	 l-p:0.09274353086948395
epoch£º279	 i:4 	 global-step:5584	 l-p:0.13641797006130219
epoch£º279	 i:5 	 global-step:5585	 l-p:0.15300874412059784
epoch£º279	 i:6 	 global-step:5586	 l-p:0.15079371631145477
epoch£º279	 i:7 	 global-step:5587	 l-p:0.12356444448232651
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13840951025485992
epoch£º279	 i:9 	 global-step:5589	 l-p:0.13606366515159607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0596, 5.0598],
        [5.0598, 5.0612, 5.0571],
        [5.0598, 5.0715, 5.0546],
        [5.0598, 5.0969, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.125022754073143 
model_pd.l_d.mean(): -20.04180145263672 
model_pd.lagr.mean(): -19.916778564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-20.7016], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.125022754073143
epoch£º280	 i:1 	 global-step:5601	 l-p:0.1488078385591507
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14774945378303528
epoch£º280	 i:3 	 global-step:5603	 l-p:0.13087959587574005
epoch£º280	 i:4 	 global-step:5604	 l-p:0.13280673325061798
epoch£º280	 i:5 	 global-step:5605	 l-p:0.1055200919508934
epoch£º280	 i:6 	 global-step:5606	 l-p:0.09654664993286133
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12395431101322174
epoch£º280	 i:8 	 global-step:5608	 l-p:0.1327357292175293
epoch£º280	 i:9 	 global-step:5609	 l-p:0.09747444838285446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8522, 4.9434, 4.8800],
        [4.8522, 4.8521, 4.8522],
        [4.8522, 4.8499, 4.8513],
        [4.8522, 4.8558, 4.8442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.09650756418704987 
model_pd.l_d.mean(): -20.619237899780273 
model_pd.lagr.mean(): -20.522729873657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.3306], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.09650756418704987
epoch£º281	 i:1 	 global-step:5621	 l-p:0.09925185889005661
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15712276101112366
epoch£º281	 i:3 	 global-step:5623	 l-p:0.3319324851036072
epoch£º281	 i:4 	 global-step:5624	 l-p:0.10362677276134491
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11883685737848282
epoch£º281	 i:6 	 global-step:5626	 l-p:0.16376762092113495
epoch£º281	 i:7 	 global-step:5627	 l-p:0.13793961703777313
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13264892995357513
epoch£º281	 i:9 	 global-step:5629	 l-p:0.12105046957731247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9773, 4.9855, 4.9704],
        [4.9773, 5.1079, 5.0417],
        [4.9773, 4.9895, 4.9699],
        [4.9773, 5.6460, 5.8926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.0996880903840065 
model_pd.l_d.mean(): -18.578153610229492 
model_pd.lagr.mean(): -18.478466033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5939], device='cuda:0')), ('power', tensor([-19.3878], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.0996880903840065
epoch£º282	 i:1 	 global-step:5641	 l-p:0.13556982576847076
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18354511260986328
epoch£º282	 i:3 	 global-step:5643	 l-p:0.21972782909870148
epoch£º282	 i:4 	 global-step:5644	 l-p:0.17696182429790497
epoch£º282	 i:5 	 global-step:5645	 l-p:0.14839376509189606
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11336620151996613
epoch£º282	 i:7 	 global-step:5647	 l-p:0.1495586633682251
epoch£º282	 i:8 	 global-step:5648	 l-p:0.13100385665893555
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12773826718330383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0741, 5.3294, 5.2885],
        [5.0741, 5.7698, 6.0300],
        [5.0741, 5.3293, 5.2884],
        [5.0741, 5.0734, 5.0740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.08443090319633484 
model_pd.l_d.mean(): -20.070384979248047 
model_pd.lagr.mean(): -19.98595428466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-20.7644], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.08443090319633484
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12172145396471024
epoch£º283	 i:2 	 global-step:5662	 l-p:0.1263924241065979
epoch£º283	 i:3 	 global-step:5663	 l-p:0.14276880025863647
epoch£º283	 i:4 	 global-step:5664	 l-p:0.19752365350723267
epoch£º283	 i:5 	 global-step:5665	 l-p:0.1336686760187149
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12401481717824936
epoch£º283	 i:7 	 global-step:5667	 l-p:0.36352309584617615
epoch£º283	 i:8 	 global-step:5668	 l-p:0.1782326102256775
epoch£º283	 i:9 	 global-step:5669	 l-p:0.3429180383682251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0047, 5.0030, 5.0038],
        [5.0047, 5.0034, 5.0043],
        [5.0047, 5.0046, 5.0047],
        [5.0047, 5.1477, 5.0815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.18250449001789093 
model_pd.l_d.mean(): -20.757949829101562 
model_pd.lagr.mean(): -20.5754451751709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.4068], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.18250449001789093
epoch£º284	 i:1 	 global-step:5681	 l-p:0.1748882681131363
epoch£º284	 i:2 	 global-step:5682	 l-p:0.13659778237342834
epoch£º284	 i:3 	 global-step:5683	 l-p:0.1529029756784439
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12272147834300995
epoch£º284	 i:5 	 global-step:5685	 l-p:0.030080599710345268
epoch£º284	 i:6 	 global-step:5686	 l-p:0.12787045538425446
epoch£º284	 i:7 	 global-step:5687	 l-p:0.1386014074087143
epoch£º284	 i:8 	 global-step:5688	 l-p:0.11706172674894333
epoch£º284	 i:9 	 global-step:5689	 l-p:0.10153354704380035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 5.1175, 5.1175],
        [5.1175, 5.1165, 5.1170],
        [5.1175, 5.1295, 5.1121],
        [5.1175, 5.3605, 5.3138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11175960302352905 
model_pd.l_d.mean(): -20.45635223388672 
model_pd.lagr.mean(): -20.344593048095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4089], device='cuda:0')), ('power', tensor([-21.0975], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11175960302352905
epoch£º285	 i:1 	 global-step:5701	 l-p:0.12621858716011047
epoch£º285	 i:2 	 global-step:5702	 l-p:0.1350879818201065
epoch£º285	 i:3 	 global-step:5703	 l-p:0.1220233142375946
epoch£º285	 i:4 	 global-step:5704	 l-p:0.12809474766254425
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.39904335141181946
epoch£º285	 i:6 	 global-step:5706	 l-p:0.03740672394633293
epoch£º285	 i:7 	 global-step:5707	 l-p:-0.03991705924272537
epoch£º285	 i:8 	 global-step:5708	 l-p:0.19223123788833618
epoch£º285	 i:9 	 global-step:5709	 l-p:0.1269574761390686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9446, 4.9422, 4.9430],
        [4.9446, 4.9446, 4.9446],
        [4.9446, 4.9446, 4.9446],
        [4.9446, 4.9440, 4.9446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.13077782094478607 
model_pd.l_d.mean(): -20.625341415405273 
model_pd.lagr.mean(): -20.494564056396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3010], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.13077782094478607
epoch£º286	 i:1 	 global-step:5721	 l-p:0.13180039823055267
epoch£º286	 i:2 	 global-step:5722	 l-p:0.2586715519428253
epoch£º286	 i:3 	 global-step:5723	 l-p:0.10380332916975021
epoch£º286	 i:4 	 global-step:5724	 l-p:0.07639551907777786
epoch£º286	 i:5 	 global-step:5725	 l-p:0.007916583679616451
epoch£º286	 i:6 	 global-step:5726	 l-p:0.13086622953414917
epoch£º286	 i:7 	 global-step:5727	 l-p:0.13495032489299774
epoch£º286	 i:8 	 global-step:5728	 l-p:0.1406726837158203
epoch£º286	 i:9 	 global-step:5729	 l-p:0.13850031793117523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0042, 5.2002, 5.1418],
        [5.0042, 5.2785, 5.2473],
        [5.0042, 5.0297, 4.9968],
        [5.0042, 5.0030, 5.0011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.13990211486816406 
model_pd.l_d.mean(): -20.42766571044922 
model_pd.lagr.mean(): -20.287763595581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.1089], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.13990211486816406
epoch£º287	 i:1 	 global-step:5741	 l-p:0.12817642092704773
epoch£º287	 i:2 	 global-step:5742	 l-p:0.18279491364955902
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.025099677965044975
epoch£º287	 i:4 	 global-step:5744	 l-p:0.13319024443626404
epoch£º287	 i:5 	 global-step:5745	 l-p:0.14534398913383484
epoch£º287	 i:6 	 global-step:5746	 l-p:0.13909867405891418
epoch£º287	 i:7 	 global-step:5747	 l-p:0.14264661073684692
epoch£º287	 i:8 	 global-step:5748	 l-p:0.1432102918624878
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.03933243080973625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9343, 4.9316, 4.9329],
        [4.9343, 4.9343, 4.9343],
        [4.9343, 4.9339, 4.9343],
        [4.9343, 5.0732, 5.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.19524849951267242 
model_pd.l_d.mean(): -20.571918487548828 
model_pd.lagr.mean(): -20.376670837402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.19524849951267242
epoch£º288	 i:1 	 global-step:5761	 l-p:0.14523424208164215
epoch£º288	 i:2 	 global-step:5762	 l-p:0.1476513296365738
epoch£º288	 i:3 	 global-step:5763	 l-p:0.5841099619865417
epoch£º288	 i:4 	 global-step:5764	 l-p:0.1408250778913498
epoch£º288	 i:5 	 global-step:5765	 l-p:0.13252440094947815
epoch£º288	 i:6 	 global-step:5766	 l-p:0.14808149635791779
epoch£º288	 i:7 	 global-step:5767	 l-p:0.12429973483085632
epoch£º288	 i:8 	 global-step:5768	 l-p:0.0998845174908638
epoch£º288	 i:9 	 global-step:5769	 l-p:0.40090736746788025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9869, 5.3255, 5.3270],
        [4.9869, 4.9866, 4.9869],
        [4.9869, 4.9867, 4.9869],
        [4.9869, 4.9846, 4.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11736764758825302 
model_pd.l_d.mean(): -19.024179458618164 
model_pd.lagr.mean(): -18.90681266784668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5396], device='cuda:0')), ('power', tensor([-19.7833], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11736764758825302
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09225147217512131
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8266642093658447
epoch£º289	 i:3 	 global-step:5783	 l-p:0.13661058247089386
epoch£º289	 i:4 	 global-step:5784	 l-p:0.15133723616600037
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7834663391113281
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17849861085414886
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14371682703495026
epoch£º289	 i:8 	 global-step:5788	 l-p:0.12316882610321045
epoch£º289	 i:9 	 global-step:5789	 l-p:0.1420350819826126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 4.9905, 4.9905],
        [4.9905, 4.9894, 4.9903],
        [4.9905, 4.9905, 4.9905],
        [4.9905, 5.0120, 4.9811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.15303762257099152 
model_pd.l_d.mean(): -20.612634658813477 
model_pd.lagr.mean(): -20.459596633911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.2681], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.15303762257099152
epoch£º290	 i:1 	 global-step:5801	 l-p:0.12861022353172302
epoch£º290	 i:2 	 global-step:5802	 l-p:0.12778334319591522
epoch£º290	 i:3 	 global-step:5803	 l-p:0.9654569625854492
epoch£º290	 i:4 	 global-step:5804	 l-p:0.12943334877490997
epoch£º290	 i:5 	 global-step:5805	 l-p:0.12769177556037903
epoch£º290	 i:6 	 global-step:5806	 l-p:0.1409863829612732
epoch£º290	 i:7 	 global-step:5807	 l-p:0.21686124801635742
epoch£º290	 i:8 	 global-step:5808	 l-p:0.1510617733001709
epoch£º290	 i:9 	 global-step:5809	 l-p:0.148172065615654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8546, 4.8820, 4.8421],
        [4.8546, 5.0853, 5.0404],
        [4.8546, 4.8509, 4.8529],
        [4.8546, 4.8545, 4.8546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.09717415273189545 
model_pd.l_d.mean(): -20.9140625 
model_pd.lagr.mean(): -20.8168888092041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.5845], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.09717415273189545
epoch£º291	 i:1 	 global-step:5821	 l-p:0.11708302050828934
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12082500010728836
epoch£º291	 i:3 	 global-step:5823	 l-p:0.16658362746238708
epoch£º291	 i:4 	 global-step:5824	 l-p:0.12981483340263367
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.1363748461008072
epoch£º291	 i:6 	 global-step:5826	 l-p:0.1780763566493988
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16347575187683105
epoch£º291	 i:8 	 global-step:5828	 l-p:0.09888298064470291
epoch£º291	 i:9 	 global-step:5829	 l-p:0.13254079222679138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1124, 5.1124, 5.1124],
        [5.1124, 5.1111, 5.1121],
        [5.1124, 5.1122, 5.1124],
        [5.1124, 5.1124, 5.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.12873582541942596 
model_pd.l_d.mean(): -20.04471778869629 
model_pd.lagr.mean(): -19.91598129272461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-20.7065], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.12873582541942596
epoch£º292	 i:1 	 global-step:5841	 l-p:0.12402617931365967
epoch£º292	 i:2 	 global-step:5842	 l-p:0.11811492592096329
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11644324660301208
epoch£º292	 i:4 	 global-step:5844	 l-p:0.10088463872671127
epoch£º292	 i:5 	 global-step:5845	 l-p:0.1557774543762207
epoch£º292	 i:6 	 global-step:5846	 l-p:0.11527559161186218
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18496543169021606
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12896744906902313
epoch£º292	 i:9 	 global-step:5849	 l-p:0.12693820893764496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0555, 5.1136, 5.0594],
        [5.0555, 5.1108, 5.0579],
        [5.0555, 5.0623, 5.0474],
        [5.0555, 5.0580, 5.0489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12848716974258423 
model_pd.l_d.mean(): -20.611299514770508 
model_pd.lagr.mean(): -20.482812881469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4025], device='cuda:0')), ('power', tensor([-21.2477], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12848716974258423
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11644923686981201
epoch£º293	 i:2 	 global-step:5862	 l-p:0.28982919454574585
epoch£º293	 i:3 	 global-step:5863	 l-p:-7.456951141357422
epoch£º293	 i:4 	 global-step:5864	 l-p:0.13267503678798676
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12385108321905136
epoch£º293	 i:6 	 global-step:5866	 l-p:-0.14646287262439728
epoch£º293	 i:7 	 global-step:5867	 l-p:0.1284342110157013
epoch£º293	 i:8 	 global-step:5868	 l-p:0.1489388644695282
epoch£º293	 i:9 	 global-step:5869	 l-p:0.26383358240127563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9377, 4.9451, 4.9260],
        [4.9377, 5.2143, 5.1859],
        [4.9377, 5.2856, 5.2947],
        [4.9377, 5.0054, 4.9450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.16289162635803223 
model_pd.l_d.mean(): -19.485294342041016 
model_pd.lagr.mean(): -19.322402954101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.2075], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.16289162635803223
epoch£º294	 i:1 	 global-step:5881	 l-p:0.14387229084968567
epoch£º294	 i:2 	 global-step:5882	 l-p:-4.590037822723389
epoch£º294	 i:3 	 global-step:5883	 l-p:0.1289467066526413
epoch£º294	 i:4 	 global-step:5884	 l-p:0.11628022789955139
epoch£º294	 i:5 	 global-step:5885	 l-p:0.5156528949737549
epoch£º294	 i:6 	 global-step:5886	 l-p:0.15591466426849365
epoch£º294	 i:7 	 global-step:5887	 l-p:0.11370231211185455
epoch£º294	 i:8 	 global-step:5888	 l-p:0.12960296869277954
epoch£º294	 i:9 	 global-step:5889	 l-p:-0.2387557178735733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9660, 4.9917, 4.9547],
        [4.9660, 4.9659, 4.9660],
        [4.9660, 4.9653, 4.9659],
        [4.9660, 4.9660, 4.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.14080700278282166 
model_pd.l_d.mean(): -20.867040634155273 
model_pd.lagr.mean(): -20.726234436035156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-21.5156], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.14080700278282166
epoch£º295	 i:1 	 global-step:5901	 l-p:0.1256810873746872
epoch£º295	 i:2 	 global-step:5902	 l-p:0.027347583323717117
epoch£º295	 i:3 	 global-step:5903	 l-p:-0.02690490148961544
epoch£º295	 i:4 	 global-step:5904	 l-p:0.05192974582314491
epoch£º295	 i:5 	 global-step:5905	 l-p:0.1387174427509308
epoch£º295	 i:6 	 global-step:5906	 l-p:0.15448787808418274
epoch£º295	 i:7 	 global-step:5907	 l-p:0.1252393275499344
epoch£º295	 i:8 	 global-step:5908	 l-p:0.1386452317237854
epoch£º295	 i:9 	 global-step:5909	 l-p:0.14043563604354858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0516, 5.0952, 5.0475],
        [5.0516, 5.0498, 5.0511],
        [5.0516, 5.0513, 5.0515],
        [5.0516, 5.0516, 5.0516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.1629960536956787 
model_pd.l_d.mean(): -20.47294044494629 
model_pd.lagr.mean(): -20.30994415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4274], device='cuda:0')), ('power', tensor([-21.1333], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.1629960536956787
epoch£º296	 i:1 	 global-step:5921	 l-p:0.13325949013233185
epoch£º296	 i:2 	 global-step:5922	 l-p:0.1368561089038849
epoch£º296	 i:3 	 global-step:5923	 l-p:0.19992253184318542
epoch£º296	 i:4 	 global-step:5924	 l-p:0.17711299657821655
epoch£º296	 i:5 	 global-step:5925	 l-p:0.10654151439666748
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12544982135295868
epoch£º296	 i:7 	 global-step:5927	 l-p:0.1370757520198822
epoch£º296	 i:8 	 global-step:5928	 l-p:0.1336103230714798
epoch£º296	 i:9 	 global-step:5929	 l-p:0.10468468070030212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9698, 5.6610, 5.9290],
        [4.9698, 4.9875, 4.9569],
        [4.9698, 4.9664, 4.9682],
        [4.9698, 4.9695, 4.9698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.5466778874397278 
model_pd.l_d.mean(): -19.515117645263672 
model_pd.lagr.mean(): -18.96843910217285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.2615], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.5466778874397278
epoch£º297	 i:1 	 global-step:5941	 l-p:0.12502413988113403
epoch£º297	 i:2 	 global-step:5942	 l-p:0.13434459269046783
epoch£º297	 i:3 	 global-step:5943	 l-p:0.1110370010137558
epoch£º297	 i:4 	 global-step:5944	 l-p:0.13559013605117798
epoch£º297	 i:5 	 global-step:5945	 l-p:0.14317816495895386
epoch£º297	 i:6 	 global-step:5946	 l-p:-0.8871768116950989
epoch£º297	 i:7 	 global-step:5947	 l-p:0.09995853155851364
epoch£º297	 i:8 	 global-step:5948	 l-p:0.12287228554487228
epoch£º297	 i:9 	 global-step:5949	 l-p:0.158148393034935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7991, 4.7991, 4.7991],
        [4.7991, 4.7934, 4.7899],
        [4.7991, 4.7990, 4.7991],
        [4.7991, 5.6078, 6.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.14042901992797852 
model_pd.l_d.mean(): -19.8391170501709 
model_pd.lagr.mean(): -19.698688507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5305], device='cuda:0')), ('power', tensor([-20.5978], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.14042901992797852
epoch£º298	 i:1 	 global-step:5961	 l-p:0.6376861929893494
epoch£º298	 i:2 	 global-step:5962	 l-p:0.1538885086774826
epoch£º298	 i:3 	 global-step:5963	 l-p:0.1298641413450241
epoch£º298	 i:4 	 global-step:5964	 l-p:0.197122260928154
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12739470601081848
epoch£º298	 i:6 	 global-step:5966	 l-p:-0.24716763198375702
epoch£º298	 i:7 	 global-step:5967	 l-p:0.15202882885932922
epoch£º298	 i:8 	 global-step:5968	 l-p:0.12999427318572998
epoch£º298	 i:9 	 global-step:5969	 l-p:0.298583060503006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0139, 5.0110, 5.0129],
        [5.0139, 5.0220, 5.0022],
        [5.0139, 5.0642, 5.0108],
        [5.0139, 5.1473, 5.0771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12206871807575226 
model_pd.l_d.mean(): -20.472902297973633 
model_pd.lagr.mean(): -20.350833892822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-21.1511], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12206871807575226
epoch£º299	 i:1 	 global-step:5981	 l-p:0.17432287335395813
epoch£º299	 i:2 	 global-step:5982	 l-p:0.24807541072368622
epoch£º299	 i:3 	 global-step:5983	 l-p:0.1285732090473175
epoch£º299	 i:4 	 global-step:5984	 l-p:0.1328391134738922
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14669214189052582
epoch£º299	 i:6 	 global-step:5986	 l-p:0.13293661177158356
epoch£º299	 i:7 	 global-step:5987	 l-p:0.2901350259780884
epoch£º299	 i:8 	 global-step:5988	 l-p:0.0950533077120781
epoch£º299	 i:9 	 global-step:5989	 l-p:0.20691227912902832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0229, 5.7162, 5.9805],
        [5.0229, 5.0198, 5.0181],
        [5.0229, 5.0229, 5.0229],
        [5.0229, 5.0196, 5.0184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.16598013043403625 
model_pd.l_d.mean(): -20.656349182128906 
model_pd.lagr.mean(): -20.49036979675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.3102], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.16598013043403625
epoch£º300	 i:1 	 global-step:6001	 l-p:0.21646660566329956
epoch£º300	 i:2 	 global-step:6002	 l-p:0.13340653479099274
epoch£º300	 i:3 	 global-step:6003	 l-p:0.1336330622434616
epoch£º300	 i:4 	 global-step:6004	 l-p:0.13411583006381989
epoch£º300	 i:5 	 global-step:6005	 l-p:0.12811492383480072
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11988570541143417
epoch£º300	 i:7 	 global-step:6007	 l-p:0.30963173508644104
epoch£º300	 i:8 	 global-step:6008	 l-p:0.12937070429325104
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14313581585884094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9377, 4.9406, 4.9246],
        [4.9377, 4.9377, 4.9377],
        [4.9377, 5.0852, 5.0158],
        [4.9377, 4.9377, 4.9377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.05841155722737312 
model_pd.l_d.mean(): -19.231918334960938 
model_pd.lagr.mean(): -19.173507690429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5223], device='cuda:0')), ('power', tensor([-19.9756], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.05841155722737312
epoch£º301	 i:1 	 global-step:6021	 l-p:0.1272803395986557
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11923841387033463
epoch£º301	 i:3 	 global-step:6023	 l-p:0.14934282004833221
epoch£º301	 i:4 	 global-step:6024	 l-p:0.10254188627004623
epoch£º301	 i:5 	 global-step:6025	 l-p:0.1414441466331482
epoch£º301	 i:6 	 global-step:6026	 l-p:0.14478884637355804
epoch£º301	 i:7 	 global-step:6027	 l-p:0.017577532678842545
epoch£º301	 i:8 	 global-step:6028	 l-p:0.7028133869171143
epoch£º301	 i:9 	 global-step:6029	 l-p:0.09091383963823318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8460, 4.8512, 4.8279],
        [4.8460, 4.8454, 4.8460],
        [4.8460, 5.4194, 5.5990],
        [4.8460, 4.8455, 4.8460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.14680984616279602 
model_pd.l_d.mean(): -20.7423095703125 
model_pd.lagr.mean(): -20.59549903869629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.4385], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.14680984616279602
epoch£º302	 i:1 	 global-step:6041	 l-p:0.1870880126953125
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12256299704313278
epoch£º302	 i:3 	 global-step:6043	 l-p:0.0778997391462326
epoch£º302	 i:4 	 global-step:6044	 l-p:0.1493689864873886
epoch£º302	 i:5 	 global-step:6045	 l-p:0.13530991971492767
epoch£º302	 i:6 	 global-step:6046	 l-p:0.24831098318099976
epoch£º302	 i:7 	 global-step:6047	 l-p:0.1464022994041443
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14350590109825134
epoch£º302	 i:9 	 global-step:6049	 l-p:-0.11987753957509995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9055, 4.9055, 4.9055],
        [4.9055, 4.9093, 4.8898],
        [4.9055, 5.7123, 6.0948],
        [4.9055, 4.9384, 4.8908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.13937529921531677 
model_pd.l_d.mean(): -19.162534713745117 
model_pd.lagr.mean(): -19.02315902709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.13937529921531677
epoch£º303	 i:1 	 global-step:6061	 l-p:0.1671847552061081
epoch£º303	 i:2 	 global-step:6062	 l-p:0.1381276547908783
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.6766167283058167
epoch£º303	 i:4 	 global-step:6064	 l-p:0.20239390432834625
epoch£º303	 i:5 	 global-step:6065	 l-p:0.09147121012210846
epoch£º303	 i:6 	 global-step:6066	 l-p:0.13700352609157562
epoch£º303	 i:7 	 global-step:6067	 l-p:0.13158302009105682
epoch£º303	 i:8 	 global-step:6068	 l-p:0.1390584409236908
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09800279140472412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1354, 5.1354, 5.1354],
        [5.1354, 5.1354, 5.1354],
        [5.1354, 5.1350, 5.1354],
        [5.1354, 5.1353, 5.1354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12618020176887512 
model_pd.l_d.mean(): -19.06873893737793 
model_pd.lagr.mean(): -18.94255828857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-19.7427], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12618020176887512
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11137152463197708
epoch£º304	 i:2 	 global-step:6082	 l-p:0.1276189535856247
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14276915788650513
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15873406827449799
epoch£º304	 i:5 	 global-step:6085	 l-p:0.16865624487400055
epoch£º304	 i:6 	 global-step:6086	 l-p:0.07513173669576645
epoch£º304	 i:7 	 global-step:6087	 l-p:0.14923983812332153
epoch£º304	 i:8 	 global-step:6088	 l-p:0.1258062869310379
epoch£º304	 i:9 	 global-step:6089	 l-p:0.13142414391040802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1760, 5.1221],
        [5.1249, 5.1248, 5.1249],
        [5.1249, 5.8272, 6.0890],
        [5.1249, 5.1287, 5.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.13360720872879028 
model_pd.l_d.mean(): -20.151151657104492 
model_pd.lagr.mean(): -20.01754379272461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-20.8263], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.13360720872879028
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12808434665203094
epoch£º305	 i:2 	 global-step:6102	 l-p:0.0928245335817337
epoch£º305	 i:3 	 global-step:6103	 l-p:0.18331077694892883
epoch£º305	 i:4 	 global-step:6104	 l-p:0.18399165570735931
epoch£º305	 i:5 	 global-step:6105	 l-p:0.12673629820346832
epoch£º305	 i:6 	 global-step:6106	 l-p:0.1318400800228119
epoch£º305	 i:7 	 global-step:6107	 l-p:0.17740203440189362
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11271679401397705
epoch£º305	 i:9 	 global-step:6109	 l-p:0.1128099337220192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1130, 5.1130, 5.1130],
        [5.1130, 5.7847, 6.0211],
        [5.1130, 5.1130, 5.1130],
        [5.1130, 5.2188, 5.1480]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.1572442650794983 
model_pd.l_d.mean(): -19.032512664794922 
model_pd.lagr.mean(): -18.875268936157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5016], device='cuda:0')), ('power', tensor([-19.7528], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.1572442650794983
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11841073632240295
epoch£º306	 i:2 	 global-step:6122	 l-p:0.1255340427160263
epoch£º306	 i:3 	 global-step:6123	 l-p:0.09408538788557053
epoch£º306	 i:4 	 global-step:6124	 l-p:0.13849109411239624
epoch£º306	 i:5 	 global-step:6125	 l-p:0.13392159342765808
epoch£º306	 i:6 	 global-step:6126	 l-p:0.14441421627998352
epoch£º306	 i:7 	 global-step:6127	 l-p:0.11816264688968658
epoch£º306	 i:8 	 global-step:6128	 l-p:-0.09166019409894943
epoch£º306	 i:9 	 global-step:6129	 l-p:0.03482561558485031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9538, 4.9538, 4.9538],
        [4.9538, 4.9481, 4.9459],
        [4.9538, 5.3715, 5.4233],
        [4.9538, 4.9478, 4.9472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14339375495910645 
model_pd.l_d.mean(): -20.59876251220703 
model_pd.lagr.mean(): -20.455368041992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4503], device='cuda:0')), ('power', tensor([-21.2838], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14339375495910645
epoch£º307	 i:1 	 global-step:6141	 l-p:0.13460570573806763
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12127532809972763
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14150698482990265
epoch£º307	 i:4 	 global-step:6144	 l-p:0.14103983342647552
epoch£º307	 i:5 	 global-step:6145	 l-p:0.11212269216775894
epoch£º307	 i:6 	 global-step:6146	 l-p:-0.050262171775102615
epoch£º307	 i:7 	 global-step:6147	 l-p:0.2740418612957001
epoch£º307	 i:8 	 global-step:6148	 l-p:0.15613070130348206
epoch£º307	 i:9 	 global-step:6149	 l-p:0.12964323163032532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8487, 4.8445, 4.8477],
        [4.8487, 4.8432, 4.8468],
        [4.8487, 4.8409, 4.8375],
        [4.8487, 4.9192, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.13226597011089325 
model_pd.l_d.mean(): -20.99280548095703 
model_pd.lagr.mean(): -20.86054039001465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.6594], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.13226597011089325
epoch£º308	 i:1 	 global-step:6161	 l-p:0.09652727097272873
epoch£º308	 i:2 	 global-step:6162	 l-p:0.142488032579422
epoch£º308	 i:3 	 global-step:6163	 l-p:0.1582508683204651
epoch£º308	 i:4 	 global-step:6164	 l-p:0.15516521036624908
epoch£º308	 i:5 	 global-step:6165	 l-p:0.8308119773864746
epoch£º308	 i:6 	 global-step:6166	 l-p:0.061570193618535995
epoch£º308	 i:7 	 global-step:6167	 l-p:0.2929163873195648
epoch£º308	 i:8 	 global-step:6168	 l-p:0.1291390061378479
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13179533183574677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9778, 4.9487],
        [4.9677, 5.2054, 5.1576],
        [4.9677, 4.9676, 4.9677],
        [4.9677, 5.3406, 5.3623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.17574244737625122 
model_pd.l_d.mean(): -19.057754516601562 
model_pd.lagr.mean(): -18.88201141357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5267], device='cuda:0')), ('power', tensor([-19.8040], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.17574244737625122
epoch£º309	 i:1 	 global-step:6181	 l-p:0.3630465269088745
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14014802873134613
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12194124609231949
epoch£º309	 i:4 	 global-step:6184	 l-p:0.11998184025287628
epoch£º309	 i:5 	 global-step:6185	 l-p:0.13153190910816193
epoch£º309	 i:6 	 global-step:6186	 l-p:0.13492317497730255
epoch£º309	 i:7 	 global-step:6187	 l-p:0.16581273078918457
epoch£º309	 i:8 	 global-step:6188	 l-p:0.15375308692455292
epoch£º309	 i:9 	 global-step:6189	 l-p:0.12939737737178802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.1584, 5.1603],
        [5.1609, 5.1608, 5.1609],
        [5.1609, 5.4175, 5.3716],
        [5.1609, 5.7041, 5.8320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.1538127064704895 
model_pd.l_d.mean(): -20.7089786529541 
model_pd.lagr.mean(): -20.555166244506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3758], device='cuda:0')), ('power', tensor([-21.3191], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.1538127064704895
epoch£º310	 i:1 	 global-step:6201	 l-p:-0.006447190884500742
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12053053826093674
epoch£º310	 i:3 	 global-step:6203	 l-p:0.11358565837144852
epoch£º310	 i:4 	 global-step:6204	 l-p:0.1446937918663025
epoch£º310	 i:5 	 global-step:6205	 l-p:0.1363854706287384
epoch£º310	 i:6 	 global-step:6206	 l-p:0.15229947865009308
epoch£º310	 i:7 	 global-step:6207	 l-p:0.13299474120140076
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10510016977787018
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13693946599960327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0439, 5.0877, 5.0332],
        [5.0439, 5.0841, 5.0317],
        [5.0439, 5.0526, 5.0272],
        [5.0439, 5.1119, 5.0466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11715158075094223 
model_pd.l_d.mean(): -19.06842041015625 
model_pd.lagr.mean(): -18.951269149780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4955], device='cuda:0')), ('power', tensor([-19.7829], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11715158075094223
epoch£º311	 i:1 	 global-step:6221	 l-p:0.28406351804733276
epoch£º311	 i:2 	 global-step:6222	 l-p:0.22388996183872223
epoch£º311	 i:3 	 global-step:6223	 l-p:0.13373856246471405
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12389415502548218
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11508016288280487
epoch£º311	 i:6 	 global-step:6226	 l-p:0.18341197073459625
epoch£º311	 i:7 	 global-step:6227	 l-p:-0.020467571914196014
epoch£º311	 i:8 	 global-step:6228	 l-p:0.1472891867160797
epoch£º311	 i:9 	 global-step:6229	 l-p:0.13762161135673523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9730, 5.0137, 4.9584],
        [4.9730, 4.9701, 4.9725],
        [4.9730, 4.9730, 4.9730],
        [4.9730, 5.4439, 5.5317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): -0.01904342696070671 
model_pd.l_d.mean(): -20.76611328125 
model_pd.lagr.mean(): -20.78515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4259], device='cuda:0')), ('power', tensor([-21.4281], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:-0.01904342696070671
epoch£º312	 i:1 	 global-step:6241	 l-p:0.11115378886461258
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10846517235040665
epoch£º312	 i:3 	 global-step:6243	 l-p:0.27357280254364014
epoch£º312	 i:4 	 global-step:6244	 l-p:0.147670716047287
epoch£º312	 i:5 	 global-step:6245	 l-p:0.08407825231552124
epoch£º312	 i:6 	 global-step:6246	 l-p:0.16262519359588623
epoch£º312	 i:7 	 global-step:6247	 l-p:0.1330573558807373
epoch£º312	 i:8 	 global-step:6248	 l-p:0.13755014538764954
epoch£º312	 i:9 	 global-step:6249	 l-p:-0.038615137338638306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9665, 4.9592, 4.9588],
        [4.9665, 5.5546, 5.7348],
        [4.9665, 5.0059, 4.9507],
        [4.9665, 4.9620, 4.9651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.15063993632793427 
model_pd.l_d.mean(): -20.679067611694336 
model_pd.lagr.mean(): -20.528427124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.3518], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.15063993632793427
epoch£º313	 i:1 	 global-step:6261	 l-p:0.13287752866744995
epoch£º313	 i:2 	 global-step:6262	 l-p:0.25586092472076416
epoch£º313	 i:3 	 global-step:6263	 l-p:0.19049502909183502
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14570555090904236
epoch£º313	 i:5 	 global-step:6265	 l-p:0.11844883859157562
epoch£º313	 i:6 	 global-step:6266	 l-p:0.1698896437883377
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13224488496780396
epoch£º313	 i:8 	 global-step:6268	 l-p:0.07408192753791809
epoch£º313	 i:9 	 global-step:6269	 l-p:0.01819382607936859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9392, 4.9392],
        [4.9392, 4.9335, 4.9371],
        [4.9392, 4.9334, 4.9249],
        [4.9392, 4.9398, 4.9194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.037624187767505646 
model_pd.l_d.mean(): -19.995223999023438 
model_pd.lagr.mean(): -19.957599639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5268], device='cuda:0')), ('power', tensor([-20.7518], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.037624187767505646
epoch£º314	 i:1 	 global-step:6281	 l-p:0.14022500813007355
epoch£º314	 i:2 	 global-step:6282	 l-p:0.15122190117835999
epoch£º314	 i:3 	 global-step:6283	 l-p:0.1257931888103485
epoch£º314	 i:4 	 global-step:6284	 l-p:1.1124299764633179
epoch£º314	 i:5 	 global-step:6285	 l-p:0.24550212919712067
epoch£º314	 i:6 	 global-step:6286	 l-p:0.13788436353206635
epoch£º314	 i:7 	 global-step:6287	 l-p:0.12837199866771698
epoch£º314	 i:8 	 global-step:6288	 l-p:0.09822317212820053
epoch£º314	 i:9 	 global-step:6289	 l-p:0.13888725638389587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 5.3409, 5.2833],
        [5.1184, 5.1562, 5.1058],
        [5.1184, 5.1184, 5.1184],
        [5.1184, 5.5534, 5.6064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.12841463088989258 
model_pd.l_d.mean(): -20.201927185058594 
model_pd.lagr.mean(): -20.07351303100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4268], device='cuda:0')), ('power', tensor([-20.8586], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.12841463088989258
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11613897234201431
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11049407720565796
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12834064662456512
epoch£º315	 i:4 	 global-step:6304	 l-p:0.1318928599357605
epoch£º315	 i:5 	 global-step:6305	 l-p:0.1897667795419693
epoch£º315	 i:6 	 global-step:6306	 l-p:0.2027144432067871
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15266743302345276
epoch£º315	 i:8 	 global-step:6308	 l-p:0.11981002241373062
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13605351746082306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9858, 4.9817, 4.9847],
        [4.9858, 4.9850, 4.9857],
        [4.9858, 5.0405, 4.9771],
        [4.9858, 4.9784, 4.9771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.13898132741451263 
model_pd.l_d.mean(): -20.9676513671875 
model_pd.lagr.mean(): -20.828670501708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3907], device='cuda:0')), ('power', tensor([-21.5959], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.13898132741451263
epoch£º316	 i:1 	 global-step:6321	 l-p:0.07710149139165878
epoch£º316	 i:2 	 global-step:6322	 l-p:0.4809998869895935
epoch£º316	 i:3 	 global-step:6323	 l-p:0.1478513926267624
epoch£º316	 i:4 	 global-step:6324	 l-p:0.12107264995574951
epoch£º316	 i:5 	 global-step:6325	 l-p:0.14791810512542725
epoch£º316	 i:6 	 global-step:6326	 l-p:0.10582423210144043
epoch£º316	 i:7 	 global-step:6327	 l-p:0.040410012006759644
epoch£º316	 i:8 	 global-step:6328	 l-p:0.13349446654319763
epoch£º316	 i:9 	 global-step:6329	 l-p:0.11983772367238998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7130, 4.6993, 4.7002],
        [4.7130, 4.7123, 4.7129],
        [4.7130, 4.7062, 4.6832],
        [4.7130, 4.7025, 4.6872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.12889333069324493 
model_pd.l_d.mean(): -20.399311065673828 
model_pd.lagr.mean(): -20.270418167114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5464], device='cuda:0')), ('power', tensor([-21.1803], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.12889333069324493
epoch£º317	 i:1 	 global-step:6341	 l-p:0.18020740151405334
epoch£º317	 i:2 	 global-step:6342	 l-p:0.19573451578617096
epoch£º317	 i:3 	 global-step:6343	 l-p:0.13777953386306763
epoch£º317	 i:4 	 global-step:6344	 l-p:0.24512004852294922
epoch£º317	 i:5 	 global-step:6345	 l-p:0.1361301839351654
epoch£º317	 i:6 	 global-step:6346	 l-p:0.13265106081962585
epoch£º317	 i:7 	 global-step:6347	 l-p:0.1307353973388672
epoch£º317	 i:8 	 global-step:6348	 l-p:0.15439240634441376
epoch£º317	 i:9 	 global-step:6349	 l-p:0.1408374011516571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8655, 4.8553, 4.8532],
        [4.8655, 4.8654, 4.8655],
        [4.8655, 4.8605, 4.8642],
        [4.8655, 4.8651, 4.8655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.06811942160129547 
model_pd.l_d.mean(): -20.253562927246094 
model_pd.lagr.mean(): -20.185443878173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5076], device='cuda:0')), ('power', tensor([-20.9933], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.06811942160129547
epoch£º318	 i:1 	 global-step:6361	 l-p:0.14379817247390747
epoch£º318	 i:2 	 global-step:6362	 l-p:0.12967710196971893
epoch£º318	 i:3 	 global-step:6363	 l-p:0.20035718381404877
epoch£º318	 i:4 	 global-step:6364	 l-p:0.1322173774242401
epoch£º318	 i:5 	 global-step:6365	 l-p:0.016979141160845757
epoch£º318	 i:6 	 global-step:6366	 l-p:-0.2371077537536621
epoch£º318	 i:7 	 global-step:6367	 l-p:0.13401935994625092
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12125087529420853
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12355697154998779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1095, 5.1155, 5.0925],
        [5.1095, 5.1405, 5.0932],
        [5.1095, 5.1095, 5.1095],
        [5.1095, 5.1095, 5.1095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.1621595323085785 
model_pd.l_d.mean(): -20.502567291259766 
model_pd.lagr.mean(): -20.340408325195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4167], device='cuda:0')), ('power', tensor([-21.1522], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.1621595323085785
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11460386216640472
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12514686584472656
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13227218389511108
epoch£º319	 i:4 	 global-step:6384	 l-p:0.13703185319900513
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12372327595949173
epoch£º319	 i:6 	 global-step:6386	 l-p:0.5059741139411926
epoch£º319	 i:7 	 global-step:6387	 l-p:-0.043678224086761475
epoch£º319	 i:8 	 global-step:6388	 l-p:0.1666126251220703
epoch£º319	 i:9 	 global-step:6389	 l-p:0.18408368527889252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9959, 5.0163, 4.9729],
        [4.9959, 5.0647, 4.9949],
        [4.9959, 5.0512, 4.9862],
        [4.9959, 4.9938, 4.9957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.16119861602783203 
model_pd.l_d.mean(): -20.164119720458984 
model_pd.lagr.mean(): -20.00292205810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4424], device='cuda:0')), ('power', tensor([-20.8363], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.16119861602783203
epoch£º320	 i:1 	 global-step:6401	 l-p:0.14414319396018982
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10229721665382385
epoch£º320	 i:3 	 global-step:6403	 l-p:0.18266822397708893
epoch£º320	 i:4 	 global-step:6404	 l-p:0.11615903675556183
epoch£º320	 i:5 	 global-step:6405	 l-p:0.14326350390911102
epoch£º320	 i:6 	 global-step:6406	 l-p:0.127651646733284
epoch£º320	 i:7 	 global-step:6407	 l-p:0.19685278832912445
epoch£º320	 i:8 	 global-step:6408	 l-p:0.20542627573013306
epoch£º320	 i:9 	 global-step:6409	 l-p:0.1419544816017151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0656, 5.0590, 5.0617],
        [5.0656, 5.4023, 5.3972],
        [5.0656, 5.0629, 5.0652],
        [5.0656, 5.0655, 5.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.11695034801959991 
model_pd.l_d.mean(): -19.57416343688965 
model_pd.lagr.mean(): -19.457212448120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4785], device='cuda:0')), ('power', tensor([-20.2769], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.11695034801959991
epoch£º321	 i:1 	 global-step:6421	 l-p:0.27554675936698914
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11346916109323502
epoch£º321	 i:3 	 global-step:6423	 l-p:0.12781134247779846
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12439706176519394
epoch£º321	 i:5 	 global-step:6425	 l-p:1.5621204376220703
epoch£º321	 i:6 	 global-step:6426	 l-p:0.1256352961063385
epoch£º321	 i:7 	 global-step:6427	 l-p:0.22548915445804596
epoch£º321	 i:8 	 global-step:6428	 l-p:0.14110273122787476
epoch£º321	 i:9 	 global-step:6429	 l-p:0.15167446434497833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9834, 5.8169, 6.2151],
        [4.9834, 4.9833, 4.9834],
        [4.9834, 4.9831, 4.9834],
        [4.9834, 4.9834, 4.9834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.17570368945598602 
model_pd.l_d.mean(): -20.642290115356445 
model_pd.lagr.mean(): -20.46658706665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.3103], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.17570368945598602
epoch£º322	 i:1 	 global-step:6441	 l-p:0.14351926743984222
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11668052524328232
epoch£º322	 i:3 	 global-step:6443	 l-p:0.18844477832317352
epoch£º322	 i:4 	 global-step:6444	 l-p:0.10390546917915344
epoch£º322	 i:5 	 global-step:6445	 l-p:0.1354413628578186
epoch£º322	 i:6 	 global-step:6446	 l-p:0.12766166031360626
epoch£º322	 i:7 	 global-step:6447	 l-p:0.03842688351869583
epoch£º322	 i:8 	 global-step:6448	 l-p:-0.39684081077575684
epoch£º322	 i:9 	 global-step:6449	 l-p:-0.3360244333744049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0228, 5.0228, 5.0228],
        [5.0228, 5.0199, 5.0223],
        [5.0228, 5.1803, 5.1062],
        [5.0228, 5.0147, 5.0174]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.5342251658439636 
model_pd.l_d.mean(): -20.37462615966797 
model_pd.lagr.mean(): -19.84040069580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4573], device='cuda:0')), ('power', tensor([-21.0644], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.5342251658439636
epoch£º323	 i:1 	 global-step:6461	 l-p:0.14290158450603485
epoch£º323	 i:2 	 global-step:6462	 l-p:0.10051587224006653
epoch£º323	 i:3 	 global-step:6463	 l-p:0.1651681363582611
epoch£º323	 i:4 	 global-step:6464	 l-p:0.1286367028951645
epoch£º323	 i:5 	 global-step:6465	 l-p:0.12047001719474792
epoch£º323	 i:6 	 global-step:6466	 l-p:0.3799732029438019
epoch£º323	 i:7 	 global-step:6467	 l-p:0.11658868193626404
epoch£º323	 i:8 	 global-step:6468	 l-p:0.11367274820804596
epoch£º323	 i:9 	 global-step:6469	 l-p:0.12618768215179443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2333, 5.2333, 5.2333],
        [5.2333, 5.2333, 5.2333],
        [5.2333, 5.2292, 5.2318],
        [5.2333, 5.2430, 5.2170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.1329433172941208 
model_pd.l_d.mean(): -19.667932510375977 
model_pd.lagr.mean(): -19.534988403320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.3708], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.1329433172941208
epoch£º324	 i:1 	 global-step:6481	 l-p:0.1195397824048996
epoch£º324	 i:2 	 global-step:6482	 l-p:0.14989973604679108
epoch£º324	 i:3 	 global-step:6483	 l-p:0.13084381818771362
epoch£º324	 i:4 	 global-step:6484	 l-p:0.09787485003471375
epoch£º324	 i:5 	 global-step:6485	 l-p:0.16906428337097168
epoch£º324	 i:6 	 global-step:6486	 l-p:0.13241809606552124
epoch£º324	 i:7 	 global-step:6487	 l-p:0.13421575725078583
epoch£º324	 i:8 	 global-step:6488	 l-p:0.10358408093452454
epoch£º324	 i:9 	 global-step:6489	 l-p:0.28846827149391174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8380, 4.8527, 4.8038],
        [4.8380, 4.8312, 4.8361],
        [4.8380, 4.8379, 4.8380],
        [4.8380, 4.9205, 4.8419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.1368769407272339 
model_pd.l_d.mean(): -19.28652000427246 
model_pd.lagr.mean(): -19.149642944335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5369], device='cuda:0')), ('power', tensor([-20.0457], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.1368769407272339
epoch£º325	 i:1 	 global-step:6501	 l-p:0.12105029821395874
epoch£º325	 i:2 	 global-step:6502	 l-p:0.009904298931360245
epoch£º325	 i:3 	 global-step:6503	 l-p:0.14274686574935913
epoch£º325	 i:4 	 global-step:6504	 l-p:0.13055875897407532
epoch£º325	 i:5 	 global-step:6505	 l-p:0.1419266015291214
epoch£º325	 i:6 	 global-step:6506	 l-p:0.15695813298225403
epoch£º325	 i:7 	 global-step:6507	 l-p:0.12430579960346222
epoch£º325	 i:8 	 global-step:6508	 l-p:0.11079712957143784
epoch£º325	 i:9 	 global-step:6509	 l-p:0.1298605352640152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8922, 4.9681, 4.8911],
        [4.8922, 4.8914, 4.8922],
        [4.8922, 5.6505, 5.9884],
        [4.8922, 4.9551, 4.8813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.13309301435947418 
model_pd.l_d.mean(): -20.32103157043457 
model_pd.lagr.mean(): -20.187938690185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4927], device='cuda:0')), ('power', tensor([-21.0463], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.13309301435947418
epoch£º326	 i:1 	 global-step:6521	 l-p:0.13150905072689056
epoch£º326	 i:2 	 global-step:6522	 l-p:0.16220256686210632
epoch£º326	 i:3 	 global-step:6523	 l-p:0.10476773977279663
epoch£º326	 i:4 	 global-step:6524	 l-p:0.1652144193649292
epoch£º326	 i:5 	 global-step:6525	 l-p:0.11505737155675888
epoch£º326	 i:6 	 global-step:6526	 l-p:0.12574830651283264
epoch£º326	 i:7 	 global-step:6527	 l-p:0.1374678611755371
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11464664340019226
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13393352925777435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8889, 5.6612, 6.0124],
        [4.8889, 4.8888, 4.8889],
        [4.8889, 4.8766, 4.8707],
        [4.8889, 4.8881, 4.8889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.13359934091567993 
model_pd.l_d.mean(): -20.16056251525879 
model_pd.lagr.mean(): -20.026962280273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4997], device='cuda:0')), ('power', tensor([-20.8912], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.13359934091567993
epoch£º327	 i:1 	 global-step:6541	 l-p:-0.12413636595010757
epoch£º327	 i:2 	 global-step:6542	 l-p:0.11729323863983154
epoch£º327	 i:3 	 global-step:6543	 l-p:0.11462590843439102
epoch£º327	 i:4 	 global-step:6544	 l-p:0.03588809818029404
epoch£º327	 i:5 	 global-step:6545	 l-p:0.15863588452339172
epoch£º327	 i:6 	 global-step:6546	 l-p:0.1547856479883194
epoch£º327	 i:7 	 global-step:6547	 l-p:0.09022890031337738
epoch£º327	 i:8 	 global-step:6548	 l-p:0.12970291078090668
epoch£º327	 i:9 	 global-step:6549	 l-p:0.12727031111717224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0248, 5.0205, 5.0238],
        [5.0248, 5.0229, 5.0246],
        [5.0248, 5.1043, 5.0284],
        [5.0248, 5.5237, 5.6281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.2557147145271301 
model_pd.l_d.mean(): -20.097944259643555 
model_pd.lagr.mean(): -19.84222984313965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-20.8102], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.2557147145271301
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12412253767251968
epoch£º328	 i:2 	 global-step:6562	 l-p:0.10228385776281357
epoch£º328	 i:3 	 global-step:6563	 l-p:0.3596550226211548
epoch£º328	 i:4 	 global-step:6564	 l-p:0.16413673758506775
epoch£º328	 i:5 	 global-step:6565	 l-p:0.1116969957947731
epoch£º328	 i:6 	 global-step:6566	 l-p:0.21335774660110474
epoch£º328	 i:7 	 global-step:6567	 l-p:0.16784201562404633
epoch£º328	 i:8 	 global-step:6568	 l-p:0.12924562394618988
epoch£º328	 i:9 	 global-step:6569	 l-p:0.10599158704280853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0894, 5.0894, 5.0894],
        [5.0894, 5.3459, 5.2994],
        [5.0894, 5.0887, 5.0893],
        [5.0894, 5.0808, 5.0774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.13061319291591644 
model_pd.l_d.mean(): -19.03495979309082 
model_pd.lagr.mean(): -18.904346466064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-19.7739], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.13061319291591644
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11389634758234024
epoch£º329	 i:2 	 global-step:6582	 l-p:0.11682099103927612
epoch£º329	 i:3 	 global-step:6583	 l-p:0.14454980194568634
epoch£º329	 i:4 	 global-step:6584	 l-p:0.11823821812868118
epoch£º329	 i:5 	 global-step:6585	 l-p:0.16643868386745453
epoch£º329	 i:6 	 global-step:6586	 l-p:0.12646828591823578
epoch£º329	 i:7 	 global-step:6587	 l-p:0.6593034863471985
epoch£º329	 i:8 	 global-step:6588	 l-p:0.11223877966403961
epoch£º329	 i:9 	 global-step:6589	 l-p:-0.0722273513674736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9608, 4.9602, 4.9608],
        [4.9608, 4.9600, 4.9608],
        [4.9608, 4.9603, 4.9608],
        [4.9608, 4.9608, 4.9608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.1294221132993698 
model_pd.l_d.mean(): -20.53544807434082 
model_pd.lagr.mean(): -20.40602684020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.2178], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.1294221132993698
epoch£º330	 i:1 	 global-step:6601	 l-p:0.15837055444717407
epoch£º330	 i:2 	 global-step:6602	 l-p:0.14854516088962555
epoch£º330	 i:3 	 global-step:6603	 l-p:0.012737607583403587
epoch£º330	 i:4 	 global-step:6604	 l-p:0.15590600669384003
epoch£º330	 i:5 	 global-step:6605	 l-p:-0.20365756750106812
epoch£º330	 i:6 	 global-step:6606	 l-p:0.1354830116033554
epoch£º330	 i:7 	 global-step:6607	 l-p:0.11986743658781052
epoch£º330	 i:8 	 global-step:6608	 l-p:0.12380682677030563
epoch£º330	 i:9 	 global-step:6609	 l-p:0.1486268937587738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7965, 4.8334, 4.7632],
        [4.7965, 5.4742, 5.7490],
        [4.7965, 4.7936, 4.7961],
        [4.7965, 5.3715, 5.5573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.09668472409248352 
model_pd.l_d.mean(): -20.292043685913086 
model_pd.lagr.mean(): -20.195358276367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5335], device='cuda:0')), ('power', tensor([-21.0587], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.09668472409248352
epoch£º331	 i:1 	 global-step:6621	 l-p:0.13501541316509247
epoch£º331	 i:2 	 global-step:6622	 l-p:0.15582503378391266
epoch£º331	 i:3 	 global-step:6623	 l-p:0.12335032969713211
epoch£º331	 i:4 	 global-step:6624	 l-p:0.1577560007572174
epoch£º331	 i:5 	 global-step:6625	 l-p:0.17507487535476685
epoch£º331	 i:6 	 global-step:6626	 l-p:0.13280580937862396
epoch£º331	 i:7 	 global-step:6627	 l-p:-0.22133909165859222
epoch£º331	 i:8 	 global-step:6628	 l-p:0.12703068554401398
epoch£º331	 i:9 	 global-step:6629	 l-p:0.11885691434144974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0863, 5.1037, 5.0587],
        [5.0863, 5.2601, 5.1859],
        [5.0863, 5.1513, 5.0786],
        [5.0863, 5.0843, 5.0861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.1270797997713089 
model_pd.l_d.mean(): -20.289194107055664 
model_pd.lagr.mean(): -20.1621150970459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-20.9752], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.1270797997713089
epoch£º332	 i:1 	 global-step:6641	 l-p:0.1129414513707161
epoch£º332	 i:2 	 global-step:6642	 l-p:0.20890361070632935
epoch£º332	 i:3 	 global-step:6643	 l-p:0.10795683413743973
epoch£º332	 i:4 	 global-step:6644	 l-p:0.19575630128383636
epoch£º332	 i:5 	 global-step:6645	 l-p:0.13943101465702057
epoch£º332	 i:6 	 global-step:6646	 l-p:0.13061076402664185
epoch£º332	 i:7 	 global-step:6647	 l-p:0.14521744847297668
epoch£º332	 i:8 	 global-step:6648	 l-p:0.17054356634616852
epoch£º332	 i:9 	 global-step:6649	 l-p:0.11910327523946762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0757, 5.0757, 5.0757],
        [5.0757, 5.3192, 5.2668],
        [5.0757, 5.0757, 5.0757],
        [5.0757, 5.0757, 5.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.15121126174926758 
model_pd.l_d.mean(): -20.369548797607422 
model_pd.lagr.mean(): -20.218338012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.0346], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.15121126174926758
epoch£º333	 i:1 	 global-step:6661	 l-p:0.11862592399120331
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12080203741788864
epoch£º333	 i:3 	 global-step:6663	 l-p:0.28765562176704407
epoch£º333	 i:4 	 global-step:6664	 l-p:0.17229300737380981
epoch£º333	 i:5 	 global-step:6665	 l-p:0.1424061357975006
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12362617254257202
epoch£º333	 i:7 	 global-step:6667	 l-p:-0.10452739149332047
epoch£º333	 i:8 	 global-step:6668	 l-p:0.13861437141895294
epoch£º333	 i:9 	 global-step:6669	 l-p:0.00461672293022275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0034, 5.0030, 5.0034],
        [5.0034, 5.0013, 5.0032],
        [5.0034, 5.0351, 4.9740],
        [5.0034, 5.0002, 4.9739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.1487087607383728 
model_pd.l_d.mean(): -20.886892318725586 
model_pd.lagr.mean(): -20.738183975219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4013], device='cuda:0')), ('power', tensor([-21.5250], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.1487087607383728
epoch£º334	 i:1 	 global-step:6681	 l-p:0.18878823518753052
epoch£º334	 i:2 	 global-step:6682	 l-p:0.14345403015613556
epoch£º334	 i:3 	 global-step:6683	 l-p:0.0039023635908961296
epoch£º334	 i:4 	 global-step:6684	 l-p:0.14574111998081207
epoch£º334	 i:5 	 global-step:6685	 l-p:0.4010069668292999
epoch£º334	 i:6 	 global-step:6686	 l-p:0.13445547223091125
epoch£º334	 i:7 	 global-step:6687	 l-p:0.24959367513656616
epoch£º334	 i:8 	 global-step:6688	 l-p:0.12692494690418243
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11247431486845016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1088, 5.0996, 5.0920],
        [5.1088, 5.8670, 6.1800],
        [5.1088, 5.1088, 5.1088],
        [5.1088, 5.9477, 6.3354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.1020832434296608 
model_pd.l_d.mean(): -19.247982025146484 
model_pd.lagr.mean(): -19.145898818969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5074], device='cuda:0')), ('power', tensor([-19.9766], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.1020832434296608
epoch£º335	 i:1 	 global-step:6701	 l-p:0.1313599795103073
epoch£º335	 i:2 	 global-step:6702	 l-p:0.17704768478870392
epoch£º335	 i:3 	 global-step:6703	 l-p:0.16216523945331573
epoch£º335	 i:4 	 global-step:6704	 l-p:0.1360008716583252
epoch£º335	 i:5 	 global-step:6705	 l-p:0.14257597923278809
epoch£º335	 i:6 	 global-step:6706	 l-p:1.0158714056015015
epoch£º335	 i:7 	 global-step:6707	 l-p:0.1211659386754036
epoch£º335	 i:8 	 global-step:6708	 l-p:0.13795697689056396
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12445986270904541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0183, 5.0054, 5.0011],
        [5.0183, 5.0170, 4.9868],
        [5.0183, 5.2532, 5.1981],
        [5.0183, 5.0065, 4.9976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11897037923336029 
model_pd.l_d.mean(): -19.3824462890625 
model_pd.lagr.mean(): -19.26347541809082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-20.0703], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11897037923336029
epoch£º336	 i:1 	 global-step:6721	 l-p:0.04892909526824951
epoch£º336	 i:2 	 global-step:6722	 l-p:0.1307069957256317
epoch£º336	 i:3 	 global-step:6723	 l-p:0.07099640369415283
epoch£º336	 i:4 	 global-step:6724	 l-p:0.14177080988883972
epoch£º336	 i:5 	 global-step:6725	 l-p:0.13367967307567596
epoch£º336	 i:6 	 global-step:6726	 l-p:0.5491452813148499
epoch£º336	 i:7 	 global-step:6727	 l-p:0.12054122984409332
epoch£º336	 i:8 	 global-step:6728	 l-p:0.14253288507461548
epoch£º336	 i:9 	 global-step:6729	 l-p:-1.155604600906372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8784, 5.7472, 6.1907],
        [4.8784, 4.8618, 4.8648],
        [4.8784, 4.9541, 4.8707],
        [4.8784, 5.3062, 5.3693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13571524620056152 
model_pd.l_d.mean(): -18.46298599243164 
model_pd.lagr.mean(): -18.3272705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5680], device='cuda:0')), ('power', tensor([-19.2450], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13571524620056152
epoch£º337	 i:1 	 global-step:6741	 l-p:0.2625412940979004
epoch£º337	 i:2 	 global-step:6742	 l-p:0.024957820773124695
epoch£º337	 i:3 	 global-step:6743	 l-p:0.1312379688024521
epoch£º337	 i:4 	 global-step:6744	 l-p:0.10315589606761932
epoch£º337	 i:5 	 global-step:6745	 l-p:0.13031382858753204
epoch£º337	 i:6 	 global-step:6746	 l-p:0.12934386730194092
epoch£º337	 i:7 	 global-step:6747	 l-p:0.14855322241783142
epoch£º337	 i:8 	 global-step:6748	 l-p:0.14658303558826447
epoch£º337	 i:9 	 global-step:6749	 l-p:0.17284606397151947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9283, 4.9133, 4.9021],
        [4.9283, 4.9146, 4.9203],
        [4.9283, 4.9246, 4.9278],
        [4.9283, 4.9283, 4.9283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.12933871150016785 
model_pd.l_d.mean(): -20.280750274658203 
model_pd.lagr.mean(): -20.151411056518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-21.0055], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.12933871150016785
epoch£º338	 i:1 	 global-step:6761	 l-p:0.1228199154138565
epoch£º338	 i:2 	 global-step:6762	 l-p:0.09505286812782288
epoch£º338	 i:3 	 global-step:6763	 l-p:0.13132759928703308
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.23154041171073914
epoch£º338	 i:5 	 global-step:6765	 l-p:0.1553068310022354
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10291597247123718
epoch£º338	 i:7 	 global-step:6767	 l-p:0.17296941578388214
epoch£º338	 i:8 	 global-step:6768	 l-p:0.10941573232412338
epoch£º338	 i:9 	 global-step:6769	 l-p:0.1497797966003418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.6033, 5.7588],
        [5.0369, 5.0229, 5.0217],
        [5.0369, 5.0244, 5.0288],
        [5.0369, 5.0369, 5.0369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.1255655735731125 
model_pd.l_d.mean(): -20.413959503173828 
model_pd.lagr.mean(): -20.288393020629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.0979], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.1255655735731125
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13723726570606232
epoch£º339	 i:2 	 global-step:6782	 l-p:0.13032448291778564
epoch£º339	 i:3 	 global-step:6783	 l-p:220.15249633789062
epoch£º339	 i:4 	 global-step:6784	 l-p:0.158128023147583
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.04037502780556679
epoch£º339	 i:6 	 global-step:6786	 l-p:0.14574365317821503
epoch£º339	 i:7 	 global-step:6787	 l-p:0.1259182244539261
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15274345874786377
epoch£º339	 i:9 	 global-step:6789	 l-p:-2.850623607635498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0634, 5.4480, 5.4697],
        [5.0634, 5.0616, 5.0632],
        [5.0634, 5.0632, 5.0634],
        [5.0634, 5.0516, 5.0406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.12169868499040604 
model_pd.l_d.mean(): -19.937898635864258 
model_pd.lagr.mean(): -19.816200256347656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-20.6470], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.12169868499040604
epoch£º340	 i:1 	 global-step:6801	 l-p:0.13442496955394745
epoch£º340	 i:2 	 global-step:6802	 l-p:0.2610134184360504
epoch£º340	 i:3 	 global-step:6803	 l-p:0.13992463052272797
epoch£º340	 i:4 	 global-step:6804	 l-p:0.1268145889043808
epoch£º340	 i:5 	 global-step:6805	 l-p:0.11107762157917023
epoch£º340	 i:6 	 global-step:6806	 l-p:0.21061144769191742
epoch£º340	 i:7 	 global-step:6807	 l-p:0.1235126256942749
epoch£º340	 i:8 	 global-step:6808	 l-p:0.17768938839435577
epoch£º340	 i:9 	 global-step:6809	 l-p:0.11716371029615402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1200, 5.1194, 5.1199],
        [5.1200, 5.1191, 5.0892],
        [5.1200, 5.6024, 5.6870],
        [5.1200, 5.6200, 5.7175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.12924842536449432 
model_pd.l_d.mean(): -20.100399017333984 
model_pd.lagr.mean(): -19.97115135192871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4620], device='cuda:0')), ('power', tensor([-20.7920], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.12924842536449432
epoch£º341	 i:1 	 global-step:6821	 l-p:0.144670769572258
epoch£º341	 i:2 	 global-step:6822	 l-p:0.13638637959957123
epoch£º341	 i:3 	 global-step:6823	 l-p:0.11940746009349823
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12977062165737152
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12103313207626343
epoch£º341	 i:6 	 global-step:6826	 l-p:-0.007010087836533785
epoch£º341	 i:7 	 global-step:6827	 l-p:0.1472521424293518
epoch£º341	 i:8 	 global-step:6828	 l-p:0.09310075640678406
epoch£º341	 i:9 	 global-step:6829	 l-p:0.30611467361450195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9310, 4.9310, 4.9310],
        [4.9310, 5.4605, 5.5958],
        [4.9310, 5.6558, 5.9573],
        [4.9310, 5.4614, 5.5974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.1481131762266159 
model_pd.l_d.mean(): -20.723745346069336 
model_pd.lagr.mean(): -20.575632095336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.4083], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.1481131762266159
epoch£º342	 i:1 	 global-step:6841	 l-p:0.34871286153793335
epoch£º342	 i:2 	 global-step:6842	 l-p:0.1423967182636261
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12585273385047913
epoch£º342	 i:4 	 global-step:6844	 l-p:0.13283534348011017
epoch£º342	 i:5 	 global-step:6845	 l-p:0.29140546917915344
epoch£º342	 i:6 	 global-step:6846	 l-p:0.03779395669698715
epoch£º342	 i:7 	 global-step:6847	 l-p:0.08110485225915909
epoch£º342	 i:8 	 global-step:6848	 l-p:0.06741232424974442
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13756446540355682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0457, 5.6074, 5.7581],
        [5.0457, 5.0457, 5.0457],
        [5.0457, 5.7973, 6.1105],
        [5.0457, 5.0340, 5.0400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13654875755310059 
model_pd.l_d.mean(): -20.753154754638672 
model_pd.lagr.mean(): -20.616605758666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4058], device='cuda:0')), ('power', tensor([-21.3944], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13654875755310059
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13247236609458923
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13336771726608276
epoch£º343	 i:3 	 global-step:6863	 l-p:0.12316014617681503
epoch£º343	 i:4 	 global-step:6864	 l-p:0.1328408569097519
epoch£º343	 i:5 	 global-step:6865	 l-p:-0.08442015200853348
epoch£º343	 i:6 	 global-step:6866	 l-p:0.1308652013540268
epoch£º343	 i:7 	 global-step:6867	 l-p:-1.1190356016159058
epoch£º343	 i:8 	 global-step:6868	 l-p:0.1278214454650879
epoch£º343	 i:9 	 global-step:6869	 l-p:0.130499929189682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[4.9884, 5.7196, 6.0206],
        [4.9884, 5.5516, 5.7086],
        [4.9884, 4.9769, 4.9540],
        [4.9884, 4.9754, 4.9558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14535517990589142 
model_pd.l_d.mean(): -20.780109405517578 
model_pd.lagr.mean(): -20.634754180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4198], device='cuda:0')), ('power', tensor([-21.4360], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14535517990589142
epoch£º344	 i:1 	 global-step:6881	 l-p:0.10978677868843079
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13265489041805267
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12794248759746552
epoch£º344	 i:4 	 global-step:6884	 l-p:0.10176635533571243
epoch£º344	 i:5 	 global-step:6885	 l-p:0.15081581473350525
epoch£º344	 i:6 	 global-step:6886	 l-p:0.13331876695156097
epoch£º344	 i:7 	 global-step:6887	 l-p:0.08917918056249619
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14826235175132751
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.21300113201141357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8916, 4.8811, 4.8882],
        [4.8916, 5.5995, 5.8900],
        [4.8916, 4.8789, 4.8865],
        [4.8916, 4.9550, 4.8696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12395492196083069 
model_pd.l_d.mean(): -19.50413703918457 
model_pd.lagr.mean(): -19.38018226623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.2266], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12395492196083069
epoch£º345	 i:1 	 global-step:6901	 l-p:0.13999010622501373
epoch£º345	 i:2 	 global-step:6902	 l-p:0.11973673850297928
epoch£º345	 i:3 	 global-step:6903	 l-p:0.1458873301744461
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11851348727941513
epoch£º345	 i:5 	 global-step:6905	 l-p:-4.935512065887451
epoch£º345	 i:6 	 global-step:6906	 l-p:0.08433060348033905
epoch£º345	 i:7 	 global-step:6907	 l-p:0.18395663797855377
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13274696469306946
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12390134483575821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0416, 5.0355, 5.0403],
        [5.0416, 5.0415, 5.0416],
        [5.0416, 5.0278, 5.0337],
        [5.0416, 5.1634, 5.0765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1455235332250595 
model_pd.l_d.mean(): -19.89682960510254 
model_pd.lagr.mean(): -19.751306533813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5094], device='cuda:0')), ('power', tensor([-20.6345], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1455235332250595
epoch£º346	 i:1 	 global-step:6921	 l-p:0.1311987340450287
epoch£º346	 i:2 	 global-step:6922	 l-p:0.14081549644470215
epoch£º346	 i:3 	 global-step:6923	 l-p:0.2090817093849182
epoch£º346	 i:4 	 global-step:6924	 l-p:0.129987433552742
epoch£º346	 i:5 	 global-step:6925	 l-p:0.10428714752197266
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12162743508815765
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12872323393821716
epoch£º346	 i:8 	 global-step:6928	 l-p:0.1349584311246872
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.26453787088394165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0070, 5.0070, 5.0070],
        [5.0070, 5.0854, 4.9989],
        [5.0070, 5.8019, 6.1587],
        [5.0070, 5.0041, 5.0067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.1205277219414711 
model_pd.l_d.mean(): -19.966083526611328 
model_pd.lagr.mean(): -19.845556259155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5060], device='cuda:0')), ('power', tensor([-20.7011], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.1205277219414711
epoch£º347	 i:1 	 global-step:6941	 l-p:0.0748169794678688
epoch£º347	 i:2 	 global-step:6942	 l-p:0.08258029818534851
epoch£º347	 i:3 	 global-step:6943	 l-p:0.07383476197719574
epoch£º347	 i:4 	 global-step:6944	 l-p:0.12695027887821198
epoch£º347	 i:5 	 global-step:6945	 l-p:0.1683472990989685
epoch£º347	 i:6 	 global-step:6946	 l-p:0.24781255424022675
epoch£º347	 i:7 	 global-step:6947	 l-p:0.1253657341003418
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11907090991735458
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13609939813613892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0407, 5.0231, 5.0221],
        [5.0407, 5.0269, 5.0334],
        [5.0407, 5.0234, 5.0245],
        [5.0407, 5.0407, 5.0407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.13611336052417755 
model_pd.l_d.mean(): -20.422195434570312 
model_pd.lagr.mean(): -20.286081314086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350], device='cuda:0')), ('power', tensor([-21.0897], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.13611336052417755
epoch£º348	 i:1 	 global-step:6961	 l-p:-0.005616655107587576
epoch£º348	 i:2 	 global-step:6962	 l-p:0.16106590628623962
epoch£º348	 i:3 	 global-step:6963	 l-p:0.1940886229276657
epoch£º348	 i:4 	 global-step:6964	 l-p:0.12876276671886444
epoch£º348	 i:5 	 global-step:6965	 l-p:0.12001203000545502
epoch£º348	 i:6 	 global-step:6966	 l-p:0.14944545924663544
epoch£º348	 i:7 	 global-step:6967	 l-p:-0.013008508831262589
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13968545198440552
epoch£º348	 i:9 	 global-step:6969	 l-p:0.05767689645290375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9721, 4.9721, 4.9721],
        [4.9721, 4.9711, 4.9720],
        [4.9721, 4.9708, 4.9720],
        [4.9721, 4.9573, 4.9646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.13935594260692596 
model_pd.l_d.mean(): -18.90040397644043 
model_pd.lagr.mean(): -18.76104736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5466], device='cuda:0')), ('power', tensor([-19.6652], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.13935594260692596
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12774300575256348
epoch£º349	 i:2 	 global-step:6982	 l-p:0.134329691529274
epoch£º349	 i:3 	 global-step:6983	 l-p:0.15668272972106934
epoch£º349	 i:4 	 global-step:6984	 l-p:0.1241961270570755
epoch£º349	 i:5 	 global-step:6985	 l-p:0.1450091302394867
epoch£º349	 i:6 	 global-step:6986	 l-p:0.15773822367191315
epoch£º349	 i:7 	 global-step:6987	 l-p:-0.02276742458343506
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13805769383907318
epoch£º349	 i:9 	 global-step:6989	 l-p:0.08031967282295227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8171, 4.8157, 4.8170],
        [4.8171, 5.2625, 5.3423],
        [4.8171, 5.1233, 5.1088],
        [4.8171, 5.2712, 5.3575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.21543116867542267 
model_pd.l_d.mean(): -20.493749618530273 
model_pd.lagr.mean(): -20.278318405151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-21.2414], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.21543116867542267
epoch£º350	 i:1 	 global-step:7001	 l-p:0.12807349860668182
epoch£º350	 i:2 	 global-step:7002	 l-p:0.06588616222143173
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11877495795488358
epoch£º350	 i:4 	 global-step:7004	 l-p:0.255660742521286
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13355053961277008
epoch£º350	 i:6 	 global-step:7006	 l-p:0.12155963480472565
epoch£º350	 i:7 	 global-step:7007	 l-p:0.044268038123846054
epoch£º350	 i:8 	 global-step:7008	 l-p:0.1359419822692871
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12309429049491882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0296, 5.0113, 5.0141],
        [5.0296, 5.0107, 5.0047],
        [5.0296, 5.1875, 5.1039],
        [5.0296, 5.2064, 5.1274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): -0.05699538439512253 
model_pd.l_d.mean(): -20.172433853149414 
model_pd.lagr.mean(): -20.229429244995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4853], device='cuda:0')), ('power', tensor([-20.8885], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:-0.05699538439512253
epoch£º351	 i:1 	 global-step:7021	 l-p:0.5754042863845825
epoch£º351	 i:2 	 global-step:7022	 l-p:0.10866645723581314
epoch£º351	 i:3 	 global-step:7023	 l-p:0.1387416273355484
epoch£º351	 i:4 	 global-step:7024	 l-p:0.12197788804769516
epoch£º351	 i:5 	 global-step:7025	 l-p:0.1322789490222931
epoch£º351	 i:6 	 global-step:7026	 l-p:0.15238825976848602
epoch£º351	 i:7 	 global-step:7027	 l-p:0.11980575323104858
epoch£º351	 i:8 	 global-step:7028	 l-p:0.06209515035152435
epoch£º351	 i:9 	 global-step:7029	 l-p:0.13359561562538147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9646, 4.9454, 4.9509],
        [4.9646, 5.0442, 4.9542],
        [4.9646, 4.9442, 4.9311],
        [4.9646, 5.1699, 5.1006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.09115854650735855 
model_pd.l_d.mean(): -20.983610153198242 
model_pd.lagr.mean(): -20.892452239990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4017], device='cuda:0')), ('power', tensor([-21.6231], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.09115854650735855
epoch£º352	 i:1 	 global-step:7041	 l-p:0.186640664935112
epoch£º352	 i:2 	 global-step:7042	 l-p:0.14162233471870422
epoch£º352	 i:3 	 global-step:7043	 l-p:0.19013060629367828
epoch£º352	 i:4 	 global-step:7044	 l-p:0.14538447558879852
epoch£º352	 i:5 	 global-step:7045	 l-p:0.6845382452011108
epoch£º352	 i:6 	 global-step:7046	 l-p:0.4806833267211914
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11341981589794159
epoch£º352	 i:8 	 global-step:7048	 l-p:0.13721060752868652
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11855697631835938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1262, 5.1214, 5.1254],
        [5.1262, 5.1244, 5.1261],
        [5.1262, 5.1246, 5.1261],
        [5.1262, 5.1262, 5.1262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12594808638095856 
model_pd.l_d.mean(): -20.453218460083008 
model_pd.lagr.mean(): -20.3272705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12594808638095856
epoch£º353	 i:1 	 global-step:7061	 l-p:0.13183626532554626
epoch£º353	 i:2 	 global-step:7062	 l-p:0.17492075264453888
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13929533958435059
epoch£º353	 i:4 	 global-step:7064	 l-p:0.11266685277223587
epoch£º353	 i:5 	 global-step:7065	 l-p:0.16632705926895142
epoch£º353	 i:6 	 global-step:7066	 l-p:0.14816682040691376
epoch£º353	 i:7 	 global-step:7067	 l-p:0.1412719190120697
epoch£º353	 i:8 	 global-step:7068	 l-p:-0.19569988548755646
epoch£º353	 i:9 	 global-step:7069	 l-p:0.11865708231925964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0083, 4.9910, 4.9979],
        [5.0083, 5.2755, 5.2312],
        [5.0083, 5.8298, 6.2105],
        [5.0083, 5.0031, 5.0075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.1272812783718109 
model_pd.l_d.mean(): -20.750085830688477 
model_pd.lagr.mean(): -20.622804641723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4039], device='cuda:0')), ('power', tensor([-21.3894], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.1272812783718109
epoch£º354	 i:1 	 global-step:7081	 l-p:0.2833724915981293
epoch£º354	 i:2 	 global-step:7082	 l-p:0.0813479945063591
epoch£º354	 i:3 	 global-step:7083	 l-p:0.18355990946292877
epoch£º354	 i:4 	 global-step:7084	 l-p:0.1414317935705185
epoch£º354	 i:5 	 global-step:7085	 l-p:0.13545773923397064
epoch£º354	 i:6 	 global-step:7086	 l-p:0.14375120401382446
epoch£º354	 i:7 	 global-step:7087	 l-p:0.1388082206249237
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12468671798706055
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.032443221658468246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9846, 5.8045, 6.1863],
        [4.9846, 4.9718, 4.9798],
        [4.9846, 4.9816, 4.9843],
        [4.9846, 4.9835, 4.9846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.1310834437608719 
model_pd.l_d.mean(): -20.144502639770508 
model_pd.lagr.mean(): -20.01342010498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4749], device='cuda:0')), ('power', tensor([-20.8497], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.1310834437608719
epoch£º355	 i:1 	 global-step:7101	 l-p:0.12837763130664825
epoch£º355	 i:2 	 global-step:7102	 l-p:0.1748187243938446
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12443328648805618
epoch£º355	 i:4 	 global-step:7104	 l-p:0.14784473180770874
epoch£º355	 i:5 	 global-step:7105	 l-p:0.1679212599992752
epoch£º355	 i:6 	 global-step:7106	 l-p:0.15005822479724884
epoch£º355	 i:7 	 global-step:7107	 l-p:0.0829145535826683
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14033910632133484
epoch£º355	 i:9 	 global-step:7109	 l-p:0.08271375298500061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8221, 4.7936, 4.7920],
        [4.8221, 5.2313, 5.2837],
        [4.8221, 4.8221, 4.8221],
        [4.8221, 4.7934, 4.7897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16282348334789276 
model_pd.l_d.mean(): -19.89008903503418 
model_pd.lagr.mean(): -19.727266311645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5183], device='cuda:0')), ('power', tensor([-20.6368], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16282348334789276
epoch£º356	 i:1 	 global-step:7121	 l-p:0.15065638720989227
epoch£º356	 i:2 	 global-step:7122	 l-p:0.14772063493728638
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19722697138786316
epoch£º356	 i:4 	 global-step:7124	 l-p:0.07875671982765198
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10898599028587341
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18199703097343445
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06876669824123383
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08801425993442535
epoch£º356	 i:9 	 global-step:7129	 l-p:0.1481126993894577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9222, 5.6704, 5.9923],
        [4.9222, 4.9204, 4.9221],
        [4.9222, 4.9143, 4.9205],
        [4.9222, 4.9007, 4.8748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 2.4554026126861572 
model_pd.l_d.mean(): -20.00990867614746 
model_pd.lagr.mean(): -17.554506301879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-20.7814], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:2.4554026126861572
epoch£º357	 i:1 	 global-step:7141	 l-p:0.15439856052398682
epoch£º357	 i:2 	 global-step:7142	 l-p:0.13712255656719208
epoch£º357	 i:3 	 global-step:7143	 l-p:0.9068470597267151
epoch£º357	 i:4 	 global-step:7144	 l-p:0.06605471670627594
epoch£º357	 i:5 	 global-step:7145	 l-p:0.13775597512722015
epoch£º357	 i:6 	 global-step:7146	 l-p:0.1319810450077057
epoch£º357	 i:7 	 global-step:7147	 l-p:0.12689360976219177
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11654959619045258
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12824949622154236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9399, 4.9391, 4.9399],
        [4.9399, 4.9399, 4.9399],
        [4.9399, 4.9424, 4.8808],
        [4.9399, 5.5224, 5.6984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.07922926545143127 
model_pd.l_d.mean(): -20.620887756347656 
model_pd.lagr.mean(): -20.541658401489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.3118], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.07922926545143127
epoch£º358	 i:1 	 global-step:7161	 l-p:0.03208083659410477
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15523909032344818
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13316427171230316
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09350761026144028
epoch£º358	 i:5 	 global-step:7165	 l-p:0.1295957863330841
epoch£º358	 i:6 	 global-step:7166	 l-p:0.14747300744056702
epoch£º358	 i:7 	 global-step:7167	 l-p:0.1339009702205658
epoch£º358	 i:8 	 global-step:7168	 l-p:0.27352482080459595
epoch£º358	 i:9 	 global-step:7169	 l-p:0.19057631492614746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9708, 4.9537, 4.9625],
        [4.9708, 4.9708, 4.9708],
        [4.9708, 4.9599, 4.9153],
        [4.9708, 4.9708, 4.9708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.14102803170681 
model_pd.l_d.mean(): -20.287635803222656 
model_pd.lagr.mean(): -20.146608352661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-20.9963], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.14102803170681
epoch£º359	 i:1 	 global-step:7181	 l-p:0.1273803561925888
epoch£º359	 i:2 	 global-step:7182	 l-p:0.1319613754749298
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12426343560218811
epoch£º359	 i:4 	 global-step:7184	 l-p:0.5166422724723816
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14467968046665192
epoch£º359	 i:6 	 global-step:7186	 l-p:0.24176554381847382
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09545142948627472
epoch£º359	 i:8 	 global-step:7188	 l-p:0.1705818921327591
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14188764989376068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9012, 4.8755, 4.8525],
        [4.9012, 4.8762, 4.8509],
        [4.9012, 4.8730, 4.8682],
        [4.9012, 4.9012, 4.9012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.13123458623886108 
model_pd.l_d.mean(): -20.131200790405273 
model_pd.lagr.mean(): -19.99996566772461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5240], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.13123458623886108
epoch£º360	 i:1 	 global-step:7201	 l-p:0.1132899820804596
epoch£º360	 i:2 	 global-step:7202	 l-p:0.18716861307621002
epoch£º360	 i:3 	 global-step:7203	 l-p:0.018494490534067154
epoch£º360	 i:4 	 global-step:7204	 l-p:0.16252870857715607
epoch£º360	 i:5 	 global-step:7205	 l-p:0.1501164436340332
epoch£º360	 i:6 	 global-step:7206	 l-p:0.1466897577047348
epoch£º360	 i:7 	 global-step:7207	 l-p:0.08180432766675949
epoch£º360	 i:8 	 global-step:7208	 l-p:0.1310863047838211
epoch£º360	 i:9 	 global-step:7209	 l-p:0.14104151725769043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8488, 4.8488],
        [4.8488, 4.8487, 4.8488],
        [4.8488, 4.8486, 4.8487],
        [4.8488, 4.8472, 4.8487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.17711329460144043 
model_pd.l_d.mean(): -20.642576217651367 
model_pd.lagr.mean(): -20.465463638305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.3706], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.17711329460144043
epoch£º361	 i:1 	 global-step:7221	 l-p:0.19014379382133484
epoch£º361	 i:2 	 global-step:7222	 l-p:-0.15260504186153412
epoch£º361	 i:3 	 global-step:7223	 l-p:0.0926855057477951
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11244652420282364
epoch£º361	 i:5 	 global-step:7225	 l-p:0.1155354455113411
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12881945073604584
epoch£º361	 i:7 	 global-step:7227	 l-p:0.1329789161682129
epoch£º361	 i:8 	 global-step:7228	 l-p:-0.23795615136623383
epoch£º361	 i:9 	 global-step:7229	 l-p:0.3406422734260559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0872, 5.0736, 5.0391],
        [5.0872, 5.0872, 5.0872],
        [5.0872, 5.0872, 5.0872],
        [5.0872, 5.5303, 5.5869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.1309884488582611 
model_pd.l_d.mean(): -20.35894203186035 
model_pd.lagr.mean(): -20.22795295715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-21.0340], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.1309884488582611
epoch£º362	 i:1 	 global-step:7241	 l-p:0.19505877792835236
epoch£º362	 i:2 	 global-step:7242	 l-p:0.13581451773643494
epoch£º362	 i:3 	 global-step:7243	 l-p:0.13783620297908783
epoch£º362	 i:4 	 global-step:7244	 l-p:0.17902123928070068
epoch£º362	 i:5 	 global-step:7245	 l-p:0.13181017339229584
epoch£º362	 i:6 	 global-step:7246	 l-p:0.14042386412620544
epoch£º362	 i:7 	 global-step:7247	 l-p:0.13359403610229492
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12310106307268143
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13740089535713196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1223, 5.1002, 5.0977],
        [5.1223, 5.1217, 5.1223],
        [5.1223, 5.1210, 5.1222],
        [5.1223, 5.1225, 5.0699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.19488318264484406 
model_pd.l_d.mean(): -20.695037841796875 
model_pd.lagr.mean(): -20.500154495239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3926], device='cuda:0')), ('power', tensor([-21.3222], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.19488318264484406
epoch£º363	 i:1 	 global-step:7261	 l-p:0.15029413998126984
epoch£º363	 i:2 	 global-step:7262	 l-p:0.08306741714477539
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11465802043676376
epoch£º363	 i:4 	 global-step:7264	 l-p:0.12735621631145477
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14190569519996643
epoch£º363	 i:6 	 global-step:7266	 l-p:0.11657322943210602
epoch£º363	 i:7 	 global-step:7267	 l-p:0.15546436607837677
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12031839042901993
epoch£º363	 i:9 	 global-step:7269	 l-p:0.12349198758602142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1266, 6.0415, 6.4958],
        [5.1266, 5.1042, 5.1018],
        [5.1266, 5.1265, 5.1266],
        [5.1266, 5.1373, 5.0735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14771926403045654 
model_pd.l_d.mean(): -19.332773208618164 
model_pd.lagr.mean(): -19.185054779052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.0205], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14771926403045654
epoch£º364	 i:1 	 global-step:7281	 l-p:0.12149365991353989
epoch£º364	 i:2 	 global-step:7282	 l-p:-1.003956913948059
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11938038468360901
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13849034905433655
epoch£º364	 i:5 	 global-step:7285	 l-p:0.08953221142292023
epoch£º364	 i:6 	 global-step:7286	 l-p:0.05495680868625641
epoch£º364	 i:7 	 global-step:7287	 l-p:0.17638100683689117
epoch£º364	 i:8 	 global-step:7288	 l-p:0.13297267258167267
epoch£º364	 i:9 	 global-step:7289	 l-p:0.04473385587334633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8606, 4.8606, 4.8606],
        [4.8606, 4.8396, 4.7913],
        [4.8606, 4.8294, 4.8058],
        [4.8606, 4.8545, 4.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.09822484850883484 
model_pd.l_d.mean(): -20.775676727294922 
model_pd.lagr.mean(): -20.677452087402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.4775], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.09822484850883484
epoch£º365	 i:1 	 global-step:7301	 l-p:0.17630627751350403
epoch£º365	 i:2 	 global-step:7302	 l-p:0.15016686916351318
epoch£º365	 i:3 	 global-step:7303	 l-p:0.110816091299057
epoch£º365	 i:4 	 global-step:7304	 l-p:0.14577120542526245
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13734377920627594
epoch£º365	 i:6 	 global-step:7306	 l-p:-0.42172372341156006
epoch£º365	 i:7 	 global-step:7307	 l-p:0.11336736381053925
epoch£º365	 i:8 	 global-step:7308	 l-p:0.12613220512866974
epoch£º365	 i:9 	 global-step:7309	 l-p:0.13455289602279663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9393, 4.9356, 4.9389],
        [4.9393, 5.6251, 5.8879],
        [4.9393, 5.7131, 6.0551],
        [4.9393, 4.9364, 4.9391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11646819859743118 
model_pd.l_d.mean(): -18.692138671875 
model_pd.lagr.mean(): -18.57567024230957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6124], device='cuda:0')), ('power', tensor([-19.5220], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11646819859743118
epoch£º366	 i:1 	 global-step:7321	 l-p:0.1369708776473999
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13697504997253418
epoch£º366	 i:3 	 global-step:7323	 l-p:0.1376609355211258
epoch£º366	 i:4 	 global-step:7324	 l-p:-0.7654343843460083
epoch£º366	 i:5 	 global-step:7325	 l-p:0.1326819807291031
epoch£º366	 i:6 	 global-step:7326	 l-p:0.09337369352579117
epoch£º366	 i:7 	 global-step:7327	 l-p:0.1408395767211914
epoch£º366	 i:8 	 global-step:7328	 l-p:0.06650272011756897
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12536348402500153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.0808, 5.8288, 6.1321],
        [5.0808, 5.0681, 5.0243],
        [5.0808, 5.0564, 5.0397],
        [5.0808, 5.1695, 5.0720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.22796958684921265 
model_pd.l_d.mean(): -19.335205078125 
model_pd.lagr.mean(): -19.107234954833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.0590], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.22796958684921265
epoch£º367	 i:1 	 global-step:7341	 l-p:0.1430528461933136
epoch£º367	 i:2 	 global-step:7342	 l-p:0.14228391647338867
epoch£º367	 i:3 	 global-step:7343	 l-p:0.14376945793628693
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13073225319385529
epoch£º367	 i:5 	 global-step:7345	 l-p:0.13005371391773224
epoch£º367	 i:6 	 global-step:7346	 l-p:0.1430736929178238
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12055507302284241
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10063939541578293
epoch£º367	 i:9 	 global-step:7349	 l-p:0.12954209744930267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2091, 5.2091, 5.2091],
        [5.2091, 5.2091, 5.2091],
        [5.2091, 5.2070, 5.2090],
        [5.2091, 5.2607, 5.1732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15633997321128845 
model_pd.l_d.mean(): -20.297834396362305 
model_pd.lagr.mean(): -20.141494750976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-20.9505], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15633997321128845
epoch£º368	 i:1 	 global-step:7361	 l-p:0.08508509397506714
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12416168302297592
epoch£º368	 i:3 	 global-step:7363	 l-p:0.20341354608535767
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11638547480106354
epoch£º368	 i:5 	 global-step:7365	 l-p:0.13571195304393768
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15771536529064178
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11758903414011002
epoch£º368	 i:8 	 global-step:7368	 l-p:0.10905593633651733
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18325752019882202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0182, 5.1302, 5.0308],
        [5.0182, 5.0164, 5.0181],
        [5.0182, 5.0182, 5.0182],
        [5.0182, 5.7466, 6.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.14615905284881592 
model_pd.l_d.mean(): -19.734041213989258 
model_pd.lagr.mean(): -19.58788299560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-20.5017], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.14615905284881592
epoch£º369	 i:1 	 global-step:7381	 l-p:0.1236361488699913
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08354643732309341
epoch£º369	 i:3 	 global-step:7383	 l-p:0.17491622269153595
epoch£º369	 i:4 	 global-step:7384	 l-p:0.12767814099788666
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12351202964782715
epoch£º369	 i:6 	 global-step:7386	 l-p:0.1688416600227356
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09320233017206192
epoch£º369	 i:8 	 global-step:7388	 l-p:-2.865882396697998
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12788958847522736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 5.0603, 4.9666],
        [5.0183, 5.0159, 5.0181],
        [5.0183, 5.5356, 5.6510],
        [5.0183, 4.9884, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.13192754983901978 
model_pd.l_d.mean(): -17.95644187927246 
model_pd.lagr.mean(): -17.824514389038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5624], device='cuda:0')), ('power', tensor([-18.7271], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.13192754983901978
epoch£º370	 i:1 	 global-step:7401	 l-p:0.1366669088602066
epoch£º370	 i:2 	 global-step:7402	 l-p:0.07121075689792633
epoch£º370	 i:3 	 global-step:7403	 l-p:0.1378764510154724
epoch£º370	 i:4 	 global-step:7404	 l-p:0.1335754692554474
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13490036129951477
epoch£º370	 i:6 	 global-step:7406	 l-p:0.1522257775068283
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11684213578701019
epoch£º370	 i:8 	 global-step:7408	 l-p:-6.192322254180908
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12220094352960587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0345, 5.0345, 5.0345],
        [5.0345, 5.0333, 5.0344],
        [5.0345, 5.7566, 6.0409],
        [5.0345, 5.0235, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.0984870195388794 
model_pd.l_d.mean(): -20.28420639038086 
model_pd.lagr.mean(): -20.382694244384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.9851], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.0984870195388794
epoch£º371	 i:1 	 global-step:7421	 l-p:0.12921077013015747
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12136974185705185
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14552275836467743
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11169148981571198
epoch£º371	 i:5 	 global-step:7425	 l-p:-0.09229190647602081
epoch£º371	 i:6 	 global-step:7426	 l-p:0.11760597676038742
epoch£º371	 i:7 	 global-step:7427	 l-p:0.19878077507019043
epoch£º371	 i:8 	 global-step:7428	 l-p:0.12984000146389008
epoch£º371	 i:9 	 global-step:7429	 l-p:0.1218990907073021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 5.6347, 5.9973],
        [4.8474, 4.8474, 4.8474],
        [4.8474, 4.8460, 4.8474],
        [4.8474, 4.8540, 4.7660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1301591396331787 
model_pd.l_d.mean(): -19.425920486450195 
model_pd.lagr.mean(): -19.295761108398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.2042], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1301591396331787
epoch£º372	 i:1 	 global-step:7441	 l-p:0.06885293871164322
epoch£º372	 i:2 	 global-step:7442	 l-p:0.1345345824956894
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15794295072555542
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18106304109096527
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13206341862678528
epoch£º372	 i:6 	 global-step:7446	 l-p:0.204919695854187
epoch£º372	 i:7 	 global-step:7447	 l-p:0.1645973175764084
epoch£º372	 i:8 	 global-step:7448	 l-p:0.14185862243175507
epoch£º372	 i:9 	 global-step:7449	 l-p:0.0855853259563446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8497, 5.5047, 5.7482],
        [4.8497, 4.8309, 4.8430],
        [4.8497, 4.8497, 4.8497],
        [4.8497, 4.8895, 4.7865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14398890733718872 
model_pd.l_d.mean(): -19.023653030395508 
model_pd.lagr.mean(): -18.879663467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5752], device='cuda:0')), ('power', tensor([-19.8190], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14398890733718872
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.04252201318740845
epoch£º373	 i:2 	 global-step:7462	 l-p:0.08079437166452408
epoch£º373	 i:3 	 global-step:7463	 l-p:0.15704859793186188
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11838029325008392
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13327787816524506
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14464910328388214
epoch£º373	 i:7 	 global-step:7467	 l-p:0.15977130830287933
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14680252969264984
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10244353860616684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9572, 4.9543, 4.8781],
        [4.9572, 4.9325, 4.9440],
        [4.9572, 4.9379, 4.9495],
        [4.9572, 5.7144, 6.0373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09907515347003937 
model_pd.l_d.mean(): -20.610050201416016 
model_pd.lagr.mean(): -20.510974884033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.2972], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09907515347003937
epoch£º374	 i:1 	 global-step:7481	 l-p:0.2340102195739746
epoch£º374	 i:2 	 global-step:7482	 l-p:0.06192095950245857
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12595245242118835
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13862036168575287
epoch£º374	 i:5 	 global-step:7485	 l-p:0.13526557385921478
epoch£º374	 i:6 	 global-step:7486	 l-p:0.09787824749946594
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10803035646677017
epoch£º374	 i:8 	 global-step:7488	 l-p:0.14086604118347168
epoch£º374	 i:9 	 global-step:7489	 l-p:0.06710463017225266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9055, 4.9016, 4.9051],
        [4.9055, 4.9055, 4.9055],
        [4.9055, 4.9009, 4.9050],
        [4.9055, 4.9054, 4.9055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.07003331929445267 
model_pd.l_d.mean(): -20.4699764251709 
model_pd.lagr.mean(): -20.39994239807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.1667], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.07003331929445267
epoch£º375	 i:1 	 global-step:7501	 l-p:0.36975619196891785
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14866939187049866
epoch£º375	 i:3 	 global-step:7503	 l-p:0.12171222269535065
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12726691365242004
epoch£º375	 i:5 	 global-step:7505	 l-p:0.1332465261220932
epoch£º375	 i:6 	 global-step:7506	 l-p:0.18696247041225433
epoch£º375	 i:7 	 global-step:7507	 l-p:0.13226564228534698
epoch£º375	 i:8 	 global-step:7508	 l-p:0.1460997611284256
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11688625067472458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0618, 5.0332, 5.0401],
        [5.0618, 5.1067, 5.0089],
        [5.0618, 5.0617, 5.0618],
        [5.0618, 5.0405, 4.9942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11038725823163986 
model_pd.l_d.mean(): -19.67599868774414 
model_pd.lagr.mean(): -19.565610885620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.3995], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11038725823163986
epoch£º376	 i:1 	 global-step:7521	 l-p:0.1677733212709427
epoch£º376	 i:2 	 global-step:7522	 l-p:0.1669313609600067
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13312606513500214
epoch£º376	 i:4 	 global-step:7524	 l-p:0.3263390362262726
epoch£º376	 i:5 	 global-step:7525	 l-p:0.1361895501613617
epoch£º376	 i:6 	 global-step:7526	 l-p:0.13497447967529297
epoch£º376	 i:7 	 global-step:7527	 l-p:0.11020820587873459
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13254404067993164
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.0927683413028717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 5.0373, 4.9816],
        [5.0543, 5.0256, 5.0333],
        [5.0543, 5.0288, 5.0390],
        [5.0543, 5.7432, 5.9953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.017343157902359962 
model_pd.l_d.mean(): -20.28009605407715 
model_pd.lagr.mean(): -20.297439575195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.9574], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.017343157902359962
epoch£º377	 i:1 	 global-step:7541	 l-p:0.02506779320538044
epoch£º377	 i:2 	 global-step:7542	 l-p:0.16613724827766418
epoch£º377	 i:3 	 global-step:7543	 l-p:0.23462997376918793
epoch£º377	 i:4 	 global-step:7544	 l-p:0.11495134979486465
epoch£º377	 i:5 	 global-step:7545	 l-p:0.1291048228740692
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14147786796092987
epoch£º377	 i:7 	 global-step:7547	 l-p:0.13147060573101044
epoch£º377	 i:8 	 global-step:7548	 l-p:0.0732671245932579
epoch£º377	 i:9 	 global-step:7549	 l-p:0.121330127120018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2203, 5.2535, 5.1657],
        [5.2203, 5.2203, 5.2203],
        [5.2203, 5.1928, 5.1844],
        [5.2203, 5.1984, 5.2070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.13631221652030945 
model_pd.l_d.mean(): -19.89691162109375 
model_pd.lagr.mean(): -19.76059913635254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5831], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.13631221652030945
epoch£º378	 i:1 	 global-step:7561	 l-p:0.11157431453466415
epoch£º378	 i:2 	 global-step:7562	 l-p:0.1318344622850418
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13728183507919312
epoch£º378	 i:4 	 global-step:7564	 l-p:0.1058085635304451
epoch£º378	 i:5 	 global-step:7565	 l-p:0.15564042329788208
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14393281936645508
epoch£º378	 i:7 	 global-step:7567	 l-p:0.1241987943649292
epoch£º378	 i:8 	 global-step:7568	 l-p:0.12449014186859131
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13486051559448242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 4.9663, 4.9322],
        [4.9992, 4.9992, 4.9992],
        [4.9992, 4.9899, 4.9974],
        [4.9992, 4.9761, 4.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.1465788036584854 
model_pd.l_d.mean(): -19.10871696472168 
model_pd.lagr.mean(): -18.96213722229004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.8637], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.1465788036584854
epoch£º379	 i:1 	 global-step:7581	 l-p:0.09322991967201233
epoch£º379	 i:2 	 global-step:7582	 l-p:0.12443503737449646
epoch£º379	 i:3 	 global-step:7583	 l-p:0.09104306250810623
epoch£º379	 i:4 	 global-step:7584	 l-p:0.14154501259326935
epoch£º379	 i:5 	 global-step:7585	 l-p:0.14033089578151703
epoch£º379	 i:6 	 global-step:7586	 l-p:0.076173797249794
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13409048318862915
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14801891148090363
epoch£º379	 i:9 	 global-step:7589	 l-p:0.12385450303554535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7711, 4.9184, 4.8203],
        [4.7711, 4.7570, 4.7677],
        [4.7711, 4.7708, 4.7711],
        [4.7711, 4.7681, 4.7708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.1576586216688156 
model_pd.l_d.mean(): -20.415176391601562 
model_pd.lagr.mean(): -20.257516860961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5207], device='cuda:0')), ('power', tensor([-21.1701], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.1576586216688156
epoch£º380	 i:1 	 global-step:7601	 l-p:0.1860167235136032
epoch£º380	 i:2 	 global-step:7602	 l-p:0.12984366714954376
epoch£º380	 i:3 	 global-step:7603	 l-p:0.1295318603515625
epoch£º380	 i:4 	 global-step:7604	 l-p:0.12225168198347092
epoch£º380	 i:5 	 global-step:7605	 l-p:0.16148924827575684
epoch£º380	 i:6 	 global-step:7606	 l-p:0.10793069750070572
epoch£º380	 i:7 	 global-step:7607	 l-p:0.0283106230199337
epoch£º380	 i:8 	 global-step:7608	 l-p:-0.06406217068433762
epoch£º380	 i:9 	 global-step:7609	 l-p:0.709274411201477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7287, 4.7286, 4.7287],
        [4.7287, 5.1256, 5.1717],
        [4.7287, 4.6918, 4.7062],
        [4.7287, 4.6760, 4.6576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.35440826416015625 
model_pd.l_d.mean(): -20.108707427978516 
model_pd.lagr.mean(): -19.75429916381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5971], device='cuda:0')), ('power', tensor([-20.9384], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.35440826416015625
epoch£º381	 i:1 	 global-step:7621	 l-p:0.2138168215751648
epoch£º381	 i:2 	 global-step:7622	 l-p:0.10451392829418182
epoch£º381	 i:3 	 global-step:7623	 l-p:0.08677700906991959
epoch£º381	 i:4 	 global-step:7624	 l-p:0.31555432081222534
epoch£º381	 i:5 	 global-step:7625	 l-p:0.1267632097005844
epoch£º381	 i:6 	 global-step:7626	 l-p:0.12901929020881653
epoch£º381	 i:7 	 global-step:7627	 l-p:0.17957374453544617
epoch£º381	 i:8 	 global-step:7628	 l-p:0.11085307598114014
epoch£º381	 i:9 	 global-step:7629	 l-p:0.13886196911334991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1345, 5.1009, 5.0970],
        [5.1345, 5.1343, 5.1345],
        [5.1345, 5.1338, 5.1345],
        [5.1345, 5.1024, 5.0789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.12880045175552368 
model_pd.l_d.mean(): -20.78360366821289 
model_pd.lagr.mean(): -20.654802322387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3700], device='cuda:0')), ('power', tensor([-21.3886], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.12880045175552368
epoch£º382	 i:1 	 global-step:7641	 l-p:0.14446459710597992
epoch£º382	 i:2 	 global-step:7642	 l-p:0.15217678248882294
epoch£º382	 i:3 	 global-step:7643	 l-p:0.13500942289829254
epoch£º382	 i:4 	 global-step:7644	 l-p:-0.07651563733816147
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11545087397098541
epoch£º382	 i:6 	 global-step:7646	 l-p:0.0649857446551323
epoch£º382	 i:7 	 global-step:7647	 l-p:0.18180760741233826
epoch£º382	 i:8 	 global-step:7648	 l-p:0.008671252988278866
epoch£º382	 i:9 	 global-step:7649	 l-p:0.10953206568956375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1018, 5.0990, 5.1016],
        [5.1018, 5.1013, 5.0223],
        [5.1018, 5.1018, 5.1018],
        [5.1018, 5.0726, 5.0821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.12763498723506927 
model_pd.l_d.mean(): -20.191762924194336 
model_pd.lagr.mean(): -20.064128875732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-20.8728], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.12763498723506927
epoch£º383	 i:1 	 global-step:7661	 l-p:0.32552438974380493
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12995444238185883
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12176724523305893
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08543498069047928
epoch£º383	 i:5 	 global-step:7665	 l-p:0.13231392204761505
epoch£º383	 i:6 	 global-step:7666	 l-p:0.14375634491443634
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13562630116939545
epoch£º383	 i:8 	 global-step:7668	 l-p:0.18008650839328766
epoch£º383	 i:9 	 global-step:7669	 l-p:-0.07146187871694565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9721, 5.4922, 5.6109],
        [4.9721, 4.9349, 4.8942],
        [4.9721, 5.2182, 5.1559],
        [4.9721, 4.9482, 4.9616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.5011501312255859 
model_pd.l_d.mean(): -20.77112579345703 
model_pd.lagr.mean(): -20.269975662231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4426], device='cuda:0')), ('power', tensor([-21.4502], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.5011501312255859
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15983985364437103
epoch£º384	 i:2 	 global-step:7682	 l-p:0.13446101546287537
epoch£º384	 i:3 	 global-step:7683	 l-p:0.1867428719997406
epoch£º384	 i:4 	 global-step:7684	 l-p:0.1395941823720932
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10933800786733627
epoch£º384	 i:6 	 global-step:7686	 l-p:0.10957593470811844
epoch£º384	 i:7 	 global-step:7687	 l-p:0.01794406771659851
epoch£º384	 i:8 	 global-step:7688	 l-p:0.11022758483886719
epoch£º384	 i:9 	 global-step:7689	 l-p:0.13202539086341858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0036, 4.9965, 4.9134],
        [5.0036, 5.0016, 5.0034],
        [5.0036, 5.0035, 5.0036],
        [5.0036, 5.4929, 5.5852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.1378791779279709 
model_pd.l_d.mean(): -20.23219871520996 
model_pd.lagr.mean(): -20.09432029724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.9486], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.1378791779279709
epoch£º385	 i:1 	 global-step:7701	 l-p:0.12359718233346939
epoch£º385	 i:2 	 global-step:7702	 l-p:0.14491821825504303
epoch£º385	 i:3 	 global-step:7703	 l-p:0.11618341505527496
epoch£º385	 i:4 	 global-step:7704	 l-p:0.12130607664585114
epoch£º385	 i:5 	 global-step:7705	 l-p:0.14754711091518402
epoch£º385	 i:6 	 global-step:7706	 l-p:0.10448694229125977
epoch£º385	 i:7 	 global-step:7707	 l-p:0.12865014374256134
epoch£º385	 i:8 	 global-step:7708	 l-p:0.2435910701751709
epoch£º385	 i:9 	 global-step:7709	 l-p:0.3356931507587433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8121, 4.7725, 4.7856],
        [4.8121, 4.9951, 4.9069],
        [4.8121, 4.8121, 4.8121],
        [4.8121, 4.7807, 4.7963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.17001549899578094 
model_pd.l_d.mean(): -20.179439544677734 
model_pd.lagr.mean(): -20.009424209594727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5616], device='cuda:0')), ('power', tensor([-20.9736], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.17001549899578094
epoch£º386	 i:1 	 global-step:7721	 l-p:0.15769663453102112
epoch£º386	 i:2 	 global-step:7722	 l-p:0.16043338179588318
epoch£º386	 i:3 	 global-step:7723	 l-p:0.6635375022888184
epoch£º386	 i:4 	 global-step:7724	 l-p:0.04256729036569595
epoch£º386	 i:5 	 global-step:7725	 l-p:0.08470596373081207
epoch£º386	 i:6 	 global-step:7726	 l-p:0.1120363175868988
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12179578840732574
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16281607747077942
epoch£º386	 i:9 	 global-step:7729	 l-p:0.1269242763519287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2627, 5.2432, 5.2542],
        [5.2627, 5.7622, 5.8438],
        [5.2627, 5.2623, 5.2627],
        [5.2627, 5.2335, 5.2394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1255064606666565 
model_pd.l_d.mean(): -20.59641456604004 
model_pd.lagr.mean(): -20.47090721130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3675], device='cuda:0')), ('power', tensor([-21.1968], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1255064606666565
epoch£º387	 i:1 	 global-step:7741	 l-p:0.12677983939647675
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12484772503376007
epoch£º387	 i:3 	 global-step:7743	 l-p:0.005629644263535738
epoch£º387	 i:4 	 global-step:7744	 l-p:0.11803630739450455
epoch£º387	 i:5 	 global-step:7745	 l-p:0.13426820933818817
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10921098291873932
epoch£º387	 i:7 	 global-step:7747	 l-p:0.1487099677324295
epoch£º387	 i:8 	 global-step:7748	 l-p:0.17214401066303253
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11774882674217224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1222, 5.1193, 5.1220],
        [5.1222, 5.1000, 5.1127],
        [5.1222, 5.1187, 5.1219],
        [5.1222, 5.1105, 5.0361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.1138332411646843 
model_pd.l_d.mean(): -19.43572998046875 
model_pd.lagr.mean(): -19.321897506713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5478], device='cuda:0')), ('power', tensor([-20.2077], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.1138332411646843
epoch£º388	 i:1 	 global-step:7761	 l-p:-0.04308459162712097
epoch£º388	 i:2 	 global-step:7762	 l-p:0.01273809839040041
epoch£º388	 i:3 	 global-step:7763	 l-p:0.18440109491348267
epoch£º388	 i:4 	 global-step:7764	 l-p:0.1333433985710144
epoch£º388	 i:5 	 global-step:7765	 l-p:0.15482541918754578
epoch£º388	 i:6 	 global-step:7766	 l-p:0.4215569496154785
epoch£º388	 i:7 	 global-step:7767	 l-p:0.1527281403541565
epoch£º388	 i:8 	 global-step:7768	 l-p:0.13162319362163544
epoch£º388	 i:9 	 global-step:7769	 l-p:0.12964320182800293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9261, 4.9797, 4.8630],
        [4.9261, 4.8893, 4.8263],
        [4.9261, 4.9259, 4.9261],
        [4.9261, 4.8884, 4.9011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.12538012862205505 
model_pd.l_d.mean(): -19.402233123779297 
model_pd.lagr.mean(): -19.276853561401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5473], device='cuda:0')), ('power', tensor([-20.1733], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.12538012862205505
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11600523442029953
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15434633195400238
epoch£º389	 i:3 	 global-step:7783	 l-p:0.14555510878562927
epoch£º389	 i:4 	 global-step:7784	 l-p:0.06489839404821396
epoch£º389	 i:5 	 global-step:7785	 l-p:0.1490083634853363
epoch£º389	 i:6 	 global-step:7786	 l-p:0.15292304754257202
epoch£º389	 i:7 	 global-step:7787	 l-p:0.1446191817522049
epoch£º389	 i:8 	 global-step:7788	 l-p:0.09089353680610657
epoch£º389	 i:9 	 global-step:7789	 l-p:0.08175588399171829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0095, 5.0092, 5.0095],
        [5.0095, 4.9634, 4.9436],
        [5.0095, 5.5431, 5.6678],
        [5.0095, 5.0092, 5.0095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.0641474798321724 
model_pd.l_d.mean(): -20.672189712524414 
model_pd.lagr.mean(): -20.608041763305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-21.3465], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.0641474798321724
epoch£º390	 i:1 	 global-step:7801	 l-p:0.11770885437726974
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12410641461610794
epoch£º390	 i:3 	 global-step:7803	 l-p:0.1469772905111313
epoch£º390	 i:4 	 global-step:7804	 l-p:0.12184955179691315
epoch£º390	 i:5 	 global-step:7805	 l-p:0.1507224291563034
epoch£º390	 i:6 	 global-step:7806	 l-p:0.11787373572587967
epoch£º390	 i:7 	 global-step:7807	 l-p:0.12889796495437622
epoch£º390	 i:8 	 global-step:7808	 l-p:0.14183412492275238
epoch£º390	 i:9 	 global-step:7809	 l-p:0.1332111954689026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1617, 5.1541, 5.1605],
        [5.1617, 5.1610, 5.0753],
        [5.1617, 5.1420, 5.0759],
        [5.1617, 5.1581, 5.1613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.14925333857536316 
model_pd.l_d.mean(): -20.662158966064453 
model_pd.lagr.mean(): -20.51290512084961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2862], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.14925333857536316
epoch£º391	 i:1 	 global-step:7821	 l-p:0.29051336646080017
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13071584701538086
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12014871090650558
epoch£º391	 i:4 	 global-step:7824	 l-p:0.11100897192955017
epoch£º391	 i:5 	 global-step:7825	 l-p:0.1233903095126152
epoch£º391	 i:6 	 global-step:7826	 l-p:-0.002589864656329155
epoch£º391	 i:7 	 global-step:7827	 l-p:0.1200057789683342
epoch£º391	 i:8 	 global-step:7828	 l-p:0.12464435398578644
epoch£º391	 i:9 	 global-step:7829	 l-p:0.18092843890190125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8740, 4.8366, 4.8519],
        [4.8740, 4.8737, 4.8740],
        [4.8740, 5.6220, 5.9395],
        [4.8740, 4.8740, 4.8740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.12373358756303787 
model_pd.l_d.mean(): -19.91590118408203 
model_pd.lagr.mean(): -19.79216766357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5042], device='cuda:0')), ('power', tensor([-20.6485], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.12373358756303787
epoch£º392	 i:1 	 global-step:7841	 l-p:0.13165612518787384
epoch£º392	 i:2 	 global-step:7842	 l-p:0.14975649118423462
epoch£º392	 i:3 	 global-step:7843	 l-p:0.16405624151229858
epoch£º392	 i:4 	 global-step:7844	 l-p:0.16975991427898407
epoch£º392	 i:5 	 global-step:7845	 l-p:-0.595064103603363
epoch£º392	 i:6 	 global-step:7846	 l-p:0.15034013986587524
epoch£º392	 i:7 	 global-step:7847	 l-p:-0.014865340664982796
epoch£º392	 i:8 	 global-step:7848	 l-p:0.13164030015468597
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11695893853902817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9573, 5.5653, 5.7537],
        [4.9573, 4.9573, 4.9573],
        [4.9573, 5.1578, 5.0712],
        [4.9573, 4.9573, 4.9573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): -0.05909836292266846 
model_pd.l_d.mean(): -20.339202880859375 
model_pd.lagr.mean(): -20.39830207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5023], device='cuda:0')), ('power', tensor([-21.0745], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:-0.05909836292266846
epoch£º393	 i:1 	 global-step:7861	 l-p:0.2225153148174286
epoch£º393	 i:2 	 global-step:7862	 l-p:0.1639912724494934
epoch£º393	 i:3 	 global-step:7863	 l-p:0.12131766229867935
epoch£º393	 i:4 	 global-step:7864	 l-p:-0.15751221776008606
epoch£º393	 i:5 	 global-step:7865	 l-p:0.12789322435855865
epoch£º393	 i:6 	 global-step:7866	 l-p:-0.03987186774611473
epoch£º393	 i:7 	 global-step:7867	 l-p:0.14373734593391418
epoch£º393	 i:8 	 global-step:7868	 l-p:0.13583247363567352
epoch£º393	 i:9 	 global-step:7869	 l-p:0.1268150508403778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0755, 5.0398, 5.0517],
        [5.0755, 5.0751, 5.0755],
        [5.0755, 5.0412, 4.9831],
        [5.0755, 5.8004, 6.0774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.1870308220386505 
model_pd.l_d.mean(): -20.559175491333008 
model_pd.lagr.mean(): -20.37214469909668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4265], device='cuda:0')), ('power', tensor([-21.2195], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.1870308220386505
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11792545020580292
epoch£º394	 i:2 	 global-step:7882	 l-p:0.1116446852684021
epoch£º394	 i:3 	 global-step:7883	 l-p:0.0983491912484169
epoch£º394	 i:4 	 global-step:7884	 l-p:0.09784872084856033
epoch£º394	 i:5 	 global-step:7885	 l-p:0.1139669194817543
epoch£º394	 i:6 	 global-step:7886	 l-p:0.1497804820537567
epoch£º394	 i:7 	 global-step:7887	 l-p:0.1342623084783554
epoch£º394	 i:8 	 global-step:7888	 l-p:0.16883158683776855
epoch£º394	 i:9 	 global-step:7889	 l-p:0.14531616866588593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0477, 5.0461, 5.0476],
        [5.0477, 5.0036, 5.0072],
        [5.0477, 5.0477, 5.0477],
        [5.0477, 5.0114, 5.0242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.11626311391592026 
model_pd.l_d.mean(): -20.101057052612305 
model_pd.lagr.mean(): -19.98479461669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.8216], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.11626311391592026
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12118207663297653
epoch£º395	 i:2 	 global-step:7902	 l-p:0.13724076747894287
epoch£º395	 i:3 	 global-step:7903	 l-p:0.152871236205101
epoch£º395	 i:4 	 global-step:7904	 l-p:0.06532195955514908
epoch£º395	 i:5 	 global-step:7905	 l-p:0.048723120242357254
epoch£º395	 i:6 	 global-step:7906	 l-p:0.141896054148674
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11750511080026627
epoch£º395	 i:8 	 global-step:7908	 l-p:0.16843494772911072
epoch£º395	 i:9 	 global-step:7909	 l-p:0.16493210196495056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9047, 5.0876, 4.9933],
        [4.9047, 5.1468, 5.0802],
        [4.9047, 4.8861, 4.8995],
        [4.9047, 4.8932, 4.9026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.13941144943237305 
model_pd.l_d.mean(): -21.063066482543945 
model_pd.lagr.mean(): -20.923654556274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.7190], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.13941144943237305
epoch£º396	 i:1 	 global-step:7921	 l-p:0.04274451732635498
epoch£º396	 i:2 	 global-step:7922	 l-p:0.1256963312625885
epoch£º396	 i:3 	 global-step:7923	 l-p:0.14202037453651428
epoch£º396	 i:4 	 global-step:7924	 l-p:0.09372680634260178
epoch£º396	 i:5 	 global-step:7925	 l-p:0.1667485237121582
epoch£º396	 i:6 	 global-step:7926	 l-p:0.12219449132680893
epoch£º396	 i:7 	 global-step:7927	 l-p:0.05304383113980293
epoch£º396	 i:8 	 global-step:7928	 l-p:0.15658847987651825
epoch£º396	 i:9 	 global-step:7929	 l-p:0.10942729562520981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9434, 5.6159, 5.8589],
        [4.9434, 4.9379, 4.9428],
        [4.9434, 4.8964, 4.9051],
        [4.9434, 4.8994, 4.9112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11599446088075638 
model_pd.l_d.mean(): -19.302425384521484 
model_pd.lagr.mean(): -19.186429977416992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5977], device='cuda:0')), ('power', tensor([-20.1239], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11599446088075638
epoch£º397	 i:1 	 global-step:7941	 l-p:0.04799240455031395
epoch£º397	 i:2 	 global-step:7942	 l-p:0.17046847939491272
epoch£º397	 i:3 	 global-step:7943	 l-p:0.13238497078418732
epoch£º397	 i:4 	 global-step:7944	 l-p:0.08178135007619858
epoch£º397	 i:5 	 global-step:7945	 l-p:0.25466516613960266
epoch£º397	 i:6 	 global-step:7946	 l-p:0.06722012162208557
epoch£º397	 i:7 	 global-step:7947	 l-p:0.1401633620262146
epoch£º397	 i:8 	 global-step:7948	 l-p:0.1318047046661377
epoch£º397	 i:9 	 global-step:7949	 l-p:0.055301256477832794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0899, 5.3376, 5.2677],
        [5.0899, 5.0899, 5.0899],
        [5.0899, 5.0884, 5.0899],
        [5.0899, 5.2682, 5.1690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): -0.02414063923060894 
model_pd.l_d.mean(): -19.316871643066406 
model_pd.lagr.mean(): -19.341012954711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-20.0524], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:-0.02414063923060894
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10897437483072281
epoch£º398	 i:2 	 global-step:7962	 l-p:0.2501743733882904
epoch£º398	 i:3 	 global-step:7963	 l-p:0.1455828845500946
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14307187497615814
epoch£º398	 i:5 	 global-step:7965	 l-p:0.11768359690904617
epoch£º398	 i:6 	 global-step:7966	 l-p:0.13514256477355957
epoch£º398	 i:7 	 global-step:7967	 l-p:0.12557782232761383
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12461241334676743
epoch£º398	 i:9 	 global-step:7969	 l-p:0.11458449065685272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0857, 5.0852, 5.0856],
        [5.0857, 5.0437, 5.0530],
        [5.0857, 5.1784, 5.0587],
        [5.0857, 5.0576, 4.9809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11490390449762344 
model_pd.l_d.mean(): -20.03514862060547 
model_pd.lagr.mean(): -19.920244216918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4728], device='cuda:0')), ('power', tensor([-20.7370], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11490390449762344
epoch£º399	 i:1 	 global-step:7981	 l-p:0.08653262257575989
epoch£º399	 i:2 	 global-step:7982	 l-p:0.12265188992023468
epoch£º399	 i:3 	 global-step:7983	 l-p:-1.0510482788085938
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13186874985694885
epoch£º399	 i:5 	 global-step:7985	 l-p:0.143907830119133
epoch£º399	 i:6 	 global-step:7986	 l-p:0.1372803896665573
epoch£º399	 i:7 	 global-step:7987	 l-p:0.1493152230978012
epoch£º399	 i:8 	 global-step:7988	 l-p:0.17034894227981567
epoch£º399	 i:9 	 global-step:7989	 l-p:-0.051373548805713654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9437, 4.9312, 4.8248],
        [4.9437, 5.0562, 4.9368],
        [4.9437, 4.9273, 4.9396],
        [4.9437, 4.9218, 4.8224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.14432820677757263 
model_pd.l_d.mean(): -19.655366897583008 
model_pd.lagr.mean(): -19.511037826538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5057], device='cuda:0')), ('power', tensor([-20.3867], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.14432820677757263
epoch£º400	 i:1 	 global-step:8001	 l-p:0.10628549009561539
epoch£º400	 i:2 	 global-step:8002	 l-p:0.11032998561859131
epoch£º400	 i:3 	 global-step:8003	 l-p:0.14121079444885254
epoch£º400	 i:4 	 global-step:8004	 l-p:0.14310036599636078
epoch£º400	 i:5 	 global-step:8005	 l-p:0.517288863658905
epoch£º400	 i:6 	 global-step:8006	 l-p:0.1431320160627365
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.05368729308247566
epoch£º400	 i:8 	 global-step:8008	 l-p:0.10952741652727127
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13167759776115417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9851, 5.1501, 5.0457],
        [4.9851, 4.9849, 4.9851],
        [4.9851, 5.0973, 4.9778],
        [4.9851, 5.2911, 5.2553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14807561039924622 
model_pd.l_d.mean(): -19.49081802368164 
model_pd.lagr.mean(): -19.342742919921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5827], device='cuda:0')), ('power', tensor([-20.2990], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14807561039924622
epoch£º401	 i:1 	 global-step:8021	 l-p:0.1516999900341034
epoch£º401	 i:2 	 global-step:8022	 l-p:0.1477704644203186
epoch£º401	 i:3 	 global-step:8023	 l-p:0.3920629024505615
epoch£º401	 i:4 	 global-step:8024	 l-p:0.12007024139165878
epoch£º401	 i:5 	 global-step:8025	 l-p:0.02416636236011982
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14840680360794067
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11627396941184998
epoch£º401	 i:8 	 global-step:8028	 l-p:0.09440927952528
epoch£º401	 i:9 	 global-step:8029	 l-p:0.15021494030952454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9777, 4.9777, 4.9777],
        [4.9777, 4.9773, 4.9777],
        [4.9777, 4.9311, 4.9424],
        [4.9777, 4.9777, 4.9777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13729079067707062 
model_pd.l_d.mean(): -20.754552841186523 
model_pd.lagr.mean(): -20.61726188659668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.4172], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13729079067707062
epoch£º402	 i:1 	 global-step:8041	 l-p:0.13996697962284088
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13704851269721985
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09790698438882828
epoch£º402	 i:4 	 global-step:8044	 l-p:0.12091966718435287
epoch£º402	 i:5 	 global-step:8045	 l-p:0.07134585827589035
epoch£º402	 i:6 	 global-step:8046	 l-p:0.14313866198062897
epoch£º402	 i:7 	 global-step:8047	 l-p:0.19090835750102997
epoch£º402	 i:8 	 global-step:8048	 l-p:0.42371001839637756
epoch£º402	 i:9 	 global-step:8049	 l-p:0.16061744093894958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0163, 4.9762, 4.9911],
        [5.0163, 5.0163, 5.0163],
        [5.0163, 4.9790, 4.9950],
        [5.0163, 5.0007, 5.0125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.17499060928821564 
model_pd.l_d.mean(): -20.52252197265625 
model_pd.lagr.mean(): -20.347532272338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4391], device='cuda:0')), ('power', tensor([-21.1953], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.17499060928821564
epoch£º403	 i:1 	 global-step:8061	 l-p:0.12858012318611145
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13669782876968384
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12839755415916443
epoch£º403	 i:4 	 global-step:8064	 l-p:0.07694707810878754
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13881416618824005
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12427954375743866
epoch£º403	 i:7 	 global-step:8067	 l-p:0.2724584937095642
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12394538521766663
epoch£º403	 i:9 	 global-step:8069	 l-p:0.07814498990774155
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0723, 5.1972, 5.0803],
        [5.0723, 5.5466, 5.6185],
        [5.0723, 5.0444, 5.0604],
        [5.0723, 5.0675, 5.0718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.15457607805728912 
model_pd.l_d.mean(): -19.174142837524414 
model_pd.lagr.mean(): -19.019567489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5358], device='cuda:0')), ('power', tensor([-19.9309], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.15457607805728912
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11418909579515457
epoch£º404	 i:2 	 global-step:8082	 l-p:0.12169123440980911
epoch£º404	 i:3 	 global-step:8083	 l-p:0.129108265042305
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12101201713085175
epoch£º404	 i:5 	 global-step:8085	 l-p:0.03863474354147911
epoch£º404	 i:6 	 global-step:8086	 l-p:0.16129043698310852
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12119405716657639
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.14961141347885132
epoch£º404	 i:9 	 global-step:8089	 l-p:0.1484658420085907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1009, 5.0893, 4.9911],
        [5.1009, 5.1009, 5.1009],
        [5.1009, 5.1010, 5.1009],
        [5.1009, 5.0514, 5.0121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10051009804010391 
model_pd.l_d.mean(): -20.3660888671875 
model_pd.lagr.mean(): -20.265579223632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4511], device='cuda:0')), ('power', tensor([-21.0494], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10051009804010391
epoch£º405	 i:1 	 global-step:8101	 l-p:0.16914397478103638
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10521664470434189
epoch£º405	 i:3 	 global-step:8103	 l-p:0.010954060591757298
epoch£º405	 i:4 	 global-step:8104	 l-p:-0.04313867539167404
epoch£º405	 i:5 	 global-step:8105	 l-p:0.03559762239456177
epoch£º405	 i:6 	 global-step:8106	 l-p:0.15712882578372955
epoch£º405	 i:7 	 global-step:8107	 l-p:0.1501959264278412
epoch£º405	 i:8 	 global-step:8108	 l-p:0.12562905251979828
epoch£º405	 i:9 	 global-step:8109	 l-p:0.1330288052558899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0969, 5.0524, 5.0619],
        [5.0969, 5.0786, 5.0917],
        [5.0969, 5.0464, 5.0084],
        [5.0969, 5.2660, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.007567090913653374 
model_pd.l_d.mean(): -19.683921813964844 
model_pd.lagr.mean(): -19.676355361938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.3994], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.007567090913653374
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09879221022129059
epoch£º406	 i:2 	 global-step:8122	 l-p:0.13006184995174408
epoch£º406	 i:3 	 global-step:8123	 l-p:0.16744503378868103
epoch£º406	 i:4 	 global-step:8124	 l-p:0.15070337057113647
epoch£º406	 i:5 	 global-step:8125	 l-p:0.4382491111755371
epoch£º406	 i:6 	 global-step:8126	 l-p:0.1060756966471672
epoch£º406	 i:7 	 global-step:8127	 l-p:0.07442040741443634
epoch£º406	 i:8 	 global-step:8128	 l-p:0.1269966959953308
epoch£º406	 i:9 	 global-step:8129	 l-p:0.1287948340177536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0604, 5.0544, 5.0597],
        [5.0604, 5.0551, 5.0599],
        [5.0604, 5.1495, 5.0256],
        [5.0604, 5.0495, 4.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.16455796360969543 
model_pd.l_d.mean(): -20.644203186035156 
model_pd.lagr.mean(): -20.479644775390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.3020], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.16455796360969543
epoch£º407	 i:1 	 global-step:8141	 l-p:0.1264902651309967
epoch£º407	 i:2 	 global-step:8142	 l-p:0.1522170603275299
epoch£º407	 i:3 	 global-step:8143	 l-p:0.11983330547809601
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.28872352838516235
epoch£º407	 i:5 	 global-step:8145	 l-p:0.14640812575817108
epoch£º407	 i:6 	 global-step:8146	 l-p:0.14062853157520294
epoch£º407	 i:7 	 global-step:8147	 l-p:0.3786606788635254
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11570330709218979
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12228187173604965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1472, 5.1182, 5.1339],
        [5.1472, 5.1216, 5.1369],
        [5.1472, 5.0993, 5.0594],
        [5.1472, 5.2292, 5.1063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.3775981366634369 
model_pd.l_d.mean(): -20.58096694946289 
model_pd.lagr.mean(): -20.203369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4183], device='cuda:0')), ('power', tensor([-21.2332], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.3775981366634369
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13967636227607727
epoch£º408	 i:2 	 global-step:8162	 l-p:0.1462026685476303
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13924990594387054
epoch£º408	 i:4 	 global-step:8164	 l-p:0.2695782482624054
epoch£º408	 i:5 	 global-step:8165	 l-p:0.20593973994255066
epoch£º408	 i:6 	 global-step:8166	 l-p:0.12017473578453064
epoch£º408	 i:7 	 global-step:8167	 l-p:0.12177378684282303
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12907391786575317
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09386340528726578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1560, 5.1367, 5.1501],
        [5.1560, 5.1347, 5.1489],
        [5.1560, 5.1544, 5.1559],
        [5.1560, 5.1549, 5.1560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.12831659615039825 
model_pd.l_d.mean(): -20.60171127319336 
model_pd.lagr.mean(): -20.4733943939209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3869], device='cuda:0')), ('power', tensor([-21.2220], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.12831659615039825
epoch£º409	 i:1 	 global-step:8181	 l-p:0.16524375975131989
epoch£º409	 i:2 	 global-step:8182	 l-p:0.30988508462905884
epoch£º409	 i:3 	 global-step:8183	 l-p:0.1437860131263733
epoch£º409	 i:4 	 global-step:8184	 l-p:0.140533447265625
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12382626533508301
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.02637859247624874
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.16004984080791473
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14304964244365692
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11665660887956619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1138, 5.0838, 5.1000],
        [5.1138, 5.0654, 5.0700],
        [5.1138, 5.1081, 5.1132],
        [5.1138, 5.1138, 5.1138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15385833382606506 
model_pd.l_d.mean(): -20.465360641479492 
model_pd.lagr.mean(): -20.31150245666504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4363], device='cuda:0')), ('power', tensor([-21.1346], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15385833382606506
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13299907743930817
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09656854718923569
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12879090011119843
epoch£º410	 i:4 	 global-step:8204	 l-p:0.32563114166259766
epoch£º410	 i:5 	 global-step:8205	 l-p:0.1315717101097107
epoch£º410	 i:6 	 global-step:8206	 l-p:0.0884673222899437
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12980259954929352
epoch£º410	 i:8 	 global-step:8208	 l-p:0.1456897258758545
epoch£º410	 i:9 	 global-step:8209	 l-p:0.08257406204938889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0058, 5.0058, 5.0058],
        [5.0058, 5.0052, 5.0058],
        [5.0058, 5.0057, 5.0058],
        [5.0058, 5.0046, 5.0058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.47643232345581055 
model_pd.l_d.mean(): -20.367380142211914 
model_pd.lagr.mean(): -19.890947341918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.0789], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.47643232345581055
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13547241687774658
epoch£º411	 i:2 	 global-step:8222	 l-p:0.15124474465847015
epoch£º411	 i:3 	 global-step:8223	 l-p:0.1730310469865799
epoch£º411	 i:4 	 global-step:8224	 l-p:0.08818946033716202
epoch£º411	 i:5 	 global-step:8225	 l-p:0.3596194088459015
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07578395307064056
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12247373163700104
epoch£º411	 i:8 	 global-step:8228	 l-p:0.1353379189968109
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14213936030864716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9784, 4.9725, 4.9777],
        [4.9784, 5.7478, 6.0695],
        [4.9784, 4.9784, 4.9784],
        [4.9784, 4.9253, 4.9332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.28572380542755127 
model_pd.l_d.mean(): -19.183713912963867 
model_pd.lagr.mean(): -18.89798927307129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5364], device='cuda:0')), ('power', tensor([-19.9412], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.28572380542755127
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10843103379011154
epoch£º412	 i:2 	 global-step:8242	 l-p:0.13692373037338257
epoch£º412	 i:3 	 global-step:8243	 l-p:0.14640837907791138
epoch£º412	 i:4 	 global-step:8244	 l-p:0.12286627292633057
epoch£º412	 i:5 	 global-step:8245	 l-p:0.15713679790496826
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09304793924093246
epoch£º412	 i:7 	 global-step:8247	 l-p:0.07820381224155426
epoch£º412	 i:8 	 global-step:8248	 l-p:0.16446787118911743
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11923103034496307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9198, 4.8743, 4.8911],
        [4.9198, 4.9198, 4.9198],
        [4.9198, 5.6083, 5.8640],
        [4.9198, 4.9198, 4.9198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.15011872351169586 
model_pd.l_d.mean(): -19.94144630432129 
model_pd.lagr.mean(): -19.79132843017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5258], device='cuda:0')), ('power', tensor([-20.6964], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.15011872351169586
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14782990515232086
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13548079133033752
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07736416906118393
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11972148716449738
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18305553495883942
epoch£º413	 i:6 	 global-step:8266	 l-p:0.1700959950685501
epoch£º413	 i:7 	 global-step:8267	 l-p:0.15546393394470215
epoch£º413	 i:8 	 global-step:8268	 l-p:0.026918182149529457
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13065388798713684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8873, 4.8836, 4.8870],
        [4.8873, 4.9999, 4.8760],
        [4.8873, 4.8873, 4.8873],
        [4.8873, 4.8629, 4.8795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08890245109796524 
model_pd.l_d.mean(): -19.561071395874023 
model_pd.lagr.mean(): -19.472169876098633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5489], device='cuda:0')), ('power', tensor([-20.3355], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.08890245109796524
epoch£º414	 i:1 	 global-step:8281	 l-p:0.14454206824302673
epoch£º414	 i:2 	 global-step:8282	 l-p:0.1725989133119583
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12552174925804138
epoch£º414	 i:4 	 global-step:8284	 l-p:0.15083231031894684
epoch£º414	 i:5 	 global-step:8285	 l-p:0.17381660640239716
epoch£º414	 i:6 	 global-step:8286	 l-p:0.08907243609428406
epoch£º414	 i:7 	 global-step:8287	 l-p:0.10466142743825912
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13418136537075043
epoch£º414	 i:9 	 global-step:8289	 l-p:0.16002704203128815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9338, 5.0215, 4.8926],
        [4.9338, 5.3978, 5.4706],
        [4.9338, 4.9704, 4.8392],
        [4.9338, 4.9199, 4.9310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10663395375013351 
model_pd.l_d.mean(): -19.901334762573242 
model_pd.lagr.mean(): -19.794700622558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5673], device='cuda:0')), ('power', tensor([-20.6983], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.10663395375013351
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12467126548290253
epoch£º415	 i:2 	 global-step:8302	 l-p:0.15851910412311554
epoch£º415	 i:3 	 global-step:8303	 l-p:0.12977012991905212
epoch£º415	 i:4 	 global-step:8304	 l-p:0.14163115620613098
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.13622602820396423
epoch£º415	 i:6 	 global-step:8306	 l-p:0.1343541443347931
epoch£º415	 i:7 	 global-step:8307	 l-p:0.13329903781414032
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05337153375148773
epoch£º415	 i:9 	 global-step:8309	 l-p:0.128143772482872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9580, 4.9271, 4.9452],
        [4.9580, 4.9579, 4.9580],
        [4.9580, 5.0332, 4.9028],
        [4.9580, 4.9580, 4.9580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.16990698873996735 
model_pd.l_d.mean(): -20.660364151000977 
model_pd.lagr.mean(): -20.49045753479004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-21.3620], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.16990698873996735
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.4800390601158142
epoch£º416	 i:2 	 global-step:8322	 l-p:0.07484350353479385
epoch£º416	 i:3 	 global-step:8323	 l-p:0.11427462100982666
epoch£º416	 i:4 	 global-step:8324	 l-p:0.1242498978972435
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08350537717342377
epoch£º416	 i:6 	 global-step:8326	 l-p:0.11068373173475266
epoch£º416	 i:7 	 global-step:8327	 l-p:0.12579858303070068
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11941216886043549
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12774915993213654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0225, 5.0143, 5.0214],
        [5.0225, 5.0055, 5.0183],
        [5.0225, 4.9786, 4.9943],
        [5.0225, 5.0225, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.3505712151527405 
model_pd.l_d.mean(): -20.42921257019043 
model_pd.lagr.mean(): -20.078641891479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-21.1247], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.3505712151527405
epoch£º417	 i:1 	 global-step:8341	 l-p:0.14094428718090057
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12919963896274567
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12083838880062103
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12459660321474075
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14461062848567963
epoch£º417	 i:6 	 global-step:8346	 l-p:0.04825759306550026
epoch£º417	 i:7 	 global-step:8347	 l-p:0.10599282383918762
epoch£º417	 i:8 	 global-step:8348	 l-p:0.14287085831165314
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14643529057502747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9362, 4.9360, 4.9362],
        [4.9362, 4.9361, 4.9362],
        [4.9362, 5.1246, 5.0262],
        [4.9362, 4.8727, 4.8670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.08085706830024719 
model_pd.l_d.mean(): -19.642803192138672 
model_pd.lagr.mean(): -19.72365951538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5216], device='cuda:0')), ('power', tensor([-20.3902], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.08085706830024719
epoch£º418	 i:1 	 global-step:8361	 l-p:0.15137387812137604
epoch£º418	 i:2 	 global-step:8362	 l-p:0.10981413722038269
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13782554864883423
epoch£º418	 i:4 	 global-step:8364	 l-p:0.14892743527889252
epoch£º418	 i:5 	 global-step:8365	 l-p:0.11990620195865631
epoch£º418	 i:6 	 global-step:8366	 l-p:0.12079469114542007
epoch£º418	 i:7 	 global-step:8367	 l-p:0.18232837319374084
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11711753904819489
epoch£º418	 i:9 	 global-step:8369	 l-p:0.13255909085273743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9462, 4.8829, 4.8775],
        [4.9462, 4.9777, 4.8459],
        [4.9462, 4.9147, 4.8097],
        [4.9462, 4.9372, 4.9449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.15542401373386383 
model_pd.l_d.mean(): -20.319652557373047 
model_pd.lagr.mean(): -20.164228439331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-21.0608], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.15542401373386383
epoch£º419	 i:1 	 global-step:8381	 l-p:0.13348142802715302
epoch£º419	 i:2 	 global-step:8382	 l-p:0.11639772355556488
epoch£º419	 i:3 	 global-step:8383	 l-p:0.13318361341953278
epoch£º419	 i:4 	 global-step:8384	 l-p:0.14062367379665375
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08383818715810776
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.026171555742621422
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12787605822086334
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13941502571105957
epoch£º419	 i:9 	 global-step:8389	 l-p:0.2874203026294708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9825, 5.1300, 5.0149],
        [4.9825, 4.9240, 4.9275],
        [4.9825, 4.9185, 4.8820],
        [4.9825, 4.9821, 4.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.07006870210170746 
model_pd.l_d.mean(): -19.638324737548828 
model_pd.lagr.mean(): -19.568256378173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5047], device='cuda:0')), ('power', tensor([-20.3685], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.07006870210170746
epoch£º420	 i:1 	 global-step:8401	 l-p:0.12930655479431152
epoch£º420	 i:2 	 global-step:8402	 l-p:0.1345633715391159
epoch£º420	 i:3 	 global-step:8403	 l-p:0.13207945227622986
epoch£º420	 i:4 	 global-step:8404	 l-p:0.13058291375637054
epoch£º420	 i:5 	 global-step:8405	 l-p:0.24232497811317444
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07488027215003967
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12493018805980682
epoch£º420	 i:8 	 global-step:8408	 l-p:0.10993480682373047
epoch£º420	 i:9 	 global-step:8409	 l-p:0.22989676892757416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0569, 5.0007, 4.9477],
        [5.0569, 5.6755, 5.8590],
        [5.0569, 5.0226, 5.0405],
        [5.0569, 5.0509, 5.0563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09883329272270203 
model_pd.l_d.mean(): -20.31360626220703 
model_pd.lagr.mean(): -20.214773178100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-21.0084], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09883329272270203
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12569743394851685
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11559613794088364
epoch£º421	 i:3 	 global-step:8423	 l-p:0.1370665282011032
epoch£º421	 i:4 	 global-step:8424	 l-p:0.17564497888088226
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06598568707704544
epoch£º421	 i:6 	 global-step:8426	 l-p:0.9558770060539246
epoch£º421	 i:7 	 global-step:8427	 l-p:0.14213712513446808
epoch£º421	 i:8 	 global-step:8428	 l-p:0.1349376142024994
epoch£º421	 i:9 	 global-step:8429	 l-p:0.127243310213089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0732, 5.0587, 5.0701],
        [5.0732, 5.0732, 5.0732],
        [5.0732, 5.0725, 5.0732],
        [5.0732, 5.0976, 4.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.26591750979423523 
model_pd.l_d.mean(): -20.276090621948242 
model_pd.lagr.mean(): -20.010173797607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.9773], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.26591750979423523
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12950687110424042
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11799327284097672
epoch£º422	 i:3 	 global-step:8443	 l-p:0.1231631264090538
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16174370050430298
epoch£º422	 i:5 	 global-step:8445	 l-p:0.09191358089447021
epoch£º422	 i:6 	 global-step:8446	 l-p:0.08558487892150879
epoch£º422	 i:7 	 global-step:8447	 l-p:0.010745124891400337
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15885069966316223
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12417173385620117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1364, 5.1364, 5.1364],
        [5.1364, 5.2183, 5.0900],
        [5.1364, 5.1197, 5.1323],
        [5.1364, 5.1177, 5.0164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10818898677825928 
model_pd.l_d.mean(): -20.104412078857422 
model_pd.lagr.mean(): -19.99622344970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-20.8017], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10818898677825928
epoch£º423	 i:1 	 global-step:8461	 l-p:-1.8364107608795166
epoch£º423	 i:2 	 global-step:8462	 l-p:0.32186752557754517
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1413254290819168
epoch£º423	 i:4 	 global-step:8464	 l-p:0.19341476261615753
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11404317617416382
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13491728901863098
epoch£º423	 i:7 	 global-step:8467	 l-p:0.13224713504314423
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12404391914606094
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12108208984136581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1198, 5.1143],
        [5.1739, 5.1717, 5.1737],
        [5.1739, 5.4554, 5.3955],
        [5.1739, 5.1739, 5.1739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.12906113266944885 
model_pd.l_d.mean(): -20.15003204345703 
model_pd.lagr.mean(): -20.020971298217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-20.8435], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.12906113266944885
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13376981019973755
epoch£º424	 i:2 	 global-step:8482	 l-p:-0.5435327887535095
epoch£º424	 i:3 	 global-step:8483	 l-p:0.10334554314613342
epoch£º424	 i:4 	 global-step:8484	 l-p:0.049737222492694855
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14418862760066986
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13359592854976654
epoch£º424	 i:7 	 global-step:8487	 l-p:0.09081558138132095
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.013039207085967064
epoch£º424	 i:9 	 global-step:8489	 l-p:0.2911190390586853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 4.9712, 4.9771],
        [5.0280, 4.9692, 4.9113],
        [5.0280, 5.1801, 5.0652],
        [5.0280, 5.0280, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1692904233932495 
model_pd.l_d.mean(): -19.516820907592773 
model_pd.lagr.mean(): -19.347530364990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-20.2263], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1692904233932495
epoch£º425	 i:1 	 global-step:8501	 l-p:0.1428244709968567
epoch£º425	 i:2 	 global-step:8502	 l-p:0.2547151446342468
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.2960510849952698
epoch£º425	 i:4 	 global-step:8504	 l-p:0.0908992737531662
epoch£º425	 i:5 	 global-step:8505	 l-p:0.11863919347524643
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11912354826927185
epoch£º425	 i:7 	 global-step:8507	 l-p:0.014653889462351799
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07552504539489746
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12304998934268951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1143, 5.1062, 5.1132],
        [5.1143, 5.1136, 5.1143],
        [5.1143, 5.1118, 5.1141],
        [5.1143, 5.4757, 5.4644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.6601040363311768 
model_pd.l_d.mean(): -19.04492950439453 
model_pd.lagr.mean(): -19.705034255981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-19.7945], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.6601040363311768
epoch£º426	 i:1 	 global-step:8521	 l-p:0.18667496740818024
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13610538840293884
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14554943144321442
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12301605194807053
epoch£º426	 i:5 	 global-step:8525	 l-p:0.12663882970809937
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12463591992855072
epoch£º426	 i:7 	 global-step:8527	 l-p:0.09945828467607498
epoch£º426	 i:8 	 global-step:8528	 l-p:0.1527579128742218
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11399379372596741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 5.0530, 5.0582],
        [5.0588, 5.0562, 5.0586],
        [5.0588, 5.0137, 5.0299],
        [5.0588, 5.0624, 4.9397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.16184227168560028 
model_pd.l_d.mean(): -19.81321907043457 
model_pd.lagr.mean(): -19.651376724243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-20.4881], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.16184227168560028
epoch£º427	 i:1 	 global-step:8541	 l-p:0.10331760346889496
epoch£º427	 i:2 	 global-step:8542	 l-p:0.264328271150589
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11587238311767578
epoch£º427	 i:4 	 global-step:8544	 l-p:0.1202472671866417
epoch£º427	 i:5 	 global-step:8545	 l-p:0.005135702900588512
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17393504083156586
epoch£º427	 i:7 	 global-step:8547	 l-p:0.07563682645559311
epoch£º427	 i:8 	 global-step:8548	 l-p:0.15454278886318207
epoch£º427	 i:9 	 global-step:8549	 l-p:0.1412012279033661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[4.9935, 5.4507, 5.5123],
        [4.9935, 4.9305, 4.8732],
        [4.9935, 5.3169, 5.2872],
        [4.9935, 4.9388, 4.8610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.2586963474750519 
model_pd.l_d.mean(): -20.606443405151367 
model_pd.lagr.mean(): -20.347747802734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-21.2964], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.2586963474750519
epoch£º428	 i:1 	 global-step:8561	 l-p:0.11968132108449936
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10668712109327316
epoch£º428	 i:3 	 global-step:8563	 l-p:0.1341155469417572
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12287683039903641
epoch£º428	 i:5 	 global-step:8565	 l-p:0.0069673871621489525
epoch£º428	 i:6 	 global-step:8566	 l-p:0.16049742698669434
epoch£º428	 i:7 	 global-step:8567	 l-p:0.13192519545555115
epoch£º428	 i:8 	 global-step:8568	 l-p:0.17214243113994598
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11810212582349777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9044, 4.8037],
        [4.9485, 4.9484, 4.9485],
        [4.9485, 5.1254, 5.0193],
        [4.9485, 5.6233, 5.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11054731160402298 
model_pd.l_d.mean(): -19.406116485595703 
model_pd.lagr.mean(): -19.295568466186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5921], device='cuda:0')), ('power', tensor([-20.2229], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11054731160402298
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10606612265110016
epoch£º429	 i:2 	 global-step:8582	 l-p:0.15056496858596802
epoch£º429	 i:3 	 global-step:8583	 l-p:0.15707065165042877
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16859827935695648
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.0034078359603881836
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11074448376893997
epoch£º429	 i:7 	 global-step:8587	 l-p:0.1124999150633812
epoch£º429	 i:8 	 global-step:8588	 l-p:0.0794711634516716
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11280633509159088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9807, 4.9503, 4.9690],
        [4.9807, 5.6612, 5.9016],
        [4.9807, 4.9223, 4.9319],
        [4.9807, 4.9794, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.31435564160346985 
model_pd.l_d.mean(): -19.349191665649414 
model_pd.lagr.mean(): -19.034835815429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5953], device='cuda:0')), ('power', tensor([-20.1687], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.31435564160346985
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14897413551807404
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13904812932014465
epoch£º430	 i:3 	 global-step:8603	 l-p:0.1495581418275833
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13181445002555847
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12202104926109314
epoch£º430	 i:6 	 global-step:8606	 l-p:0.12016205489635468
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12328483164310455
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11301055550575256
epoch£º430	 i:9 	 global-step:8609	 l-p:0.14888790249824524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9228, 5.0312, 4.9013],
        [4.9228, 5.5934, 5.8300],
        [4.9228, 4.9108, 4.9207],
        [4.9228, 4.8660, 4.7764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.03990897536277771 
model_pd.l_d.mean(): -20.118698120117188 
model_pd.lagr.mean(): -20.07878875732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5264], device='cuda:0')), ('power', tensor([-20.8763], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.03990897536277771
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10815805941820145
epoch£º431	 i:2 	 global-step:8622	 l-p:0.13771820068359375
epoch£º431	 i:3 	 global-step:8623	 l-p:0.13269750773906708
epoch£º431	 i:4 	 global-step:8624	 l-p:0.16535823047161102
epoch£º431	 i:5 	 global-step:8625	 l-p:0.08488939702510834
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12768621742725372
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13923455774784088
epoch£º431	 i:8 	 global-step:8628	 l-p:0.14143134653568268
epoch£º431	 i:9 	 global-step:8629	 l-p:0.17456921935081482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9281, 5.5273, 5.7037],
        [4.9281, 4.9280, 4.9281],
        [4.9281, 4.9100, 4.9239],
        [4.9281, 4.8954, 4.9151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1453678011894226 
model_pd.l_d.mean(): -20.183744430541992 
model_pd.lagr.mean(): -20.038375854492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5215], device='cuda:0')), ('power', tensor([-20.9370], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1453678011894226
epoch£º432	 i:1 	 global-step:8641	 l-p:0.15429659187793732
epoch£º432	 i:2 	 global-step:8642	 l-p:0.09835271537303925
epoch£º432	 i:3 	 global-step:8643	 l-p:0.12303688377141953
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11283449828624725
epoch£º432	 i:5 	 global-step:8645	 l-p:0.1326199322938919
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13747625052928925
epoch£º432	 i:7 	 global-step:8647	 l-p:0.0476083904504776
epoch£º432	 i:8 	 global-step:8648	 l-p:0.28883346915245056
epoch£º432	 i:9 	 global-step:8649	 l-p:0.1462046205997467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0194, 4.9539, 4.9013],
        [5.0194, 5.2105, 5.1085],
        [5.0194, 5.0194, 5.0194],
        [5.0194, 4.9944, 5.0114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.14007453620433807 
model_pd.l_d.mean(): -20.101041793823242 
model_pd.lagr.mean(): -19.960968017578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.8324], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.14007453620433807
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1452941745519638
epoch£º433	 i:2 	 global-step:8662	 l-p:0.1320282518863678
epoch£º433	 i:3 	 global-step:8663	 l-p:0.3872058093547821
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11383157223463058
epoch£º433	 i:5 	 global-step:8665	 l-p:0.050641708076000214
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.025167951360344887
epoch£º433	 i:7 	 global-step:8667	 l-p:0.1886792778968811
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14634762704372406
epoch£º433	 i:9 	 global-step:8669	 l-p:0.1392023116350174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0585, 5.0329, 4.9217],
        [5.0585, 5.3095, 5.2349],
        [5.0585, 4.9931, 4.9807],
        [5.0585, 4.9946, 4.9443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.145636647939682 
model_pd.l_d.mean(): -20.890439987182617 
model_pd.lagr.mean(): -20.744802474975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4040], device='cuda:0')), ('power', tensor([-21.5313], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.145636647939682
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14674220979213715
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11819513887166977
epoch£º434	 i:3 	 global-step:8683	 l-p:0.5582093000411987
epoch£º434	 i:4 	 global-step:8684	 l-p:0.04715634882450104
epoch£º434	 i:5 	 global-step:8685	 l-p:0.1983748972415924
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1222003847360611
epoch£º434	 i:7 	 global-step:8687	 l-p:0.1514664739370346
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14046119153499603
epoch£º434	 i:9 	 global-step:8689	 l-p:0.09897281229496002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9970, 4.9969, 4.9970],
        [4.9970, 4.9970, 4.9970],
        [4.9970, 4.9970, 4.9970],
        [4.9970, 5.5929, 5.7606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.15412916243076324 
model_pd.l_d.mean(): -20.823253631591797 
model_pd.lagr.mean(): -20.669124603271484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4219], device='cuda:0')), ('power', tensor([-21.4818], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.15412916243076324
epoch£º435	 i:1 	 global-step:8701	 l-p:0.09397312253713608
epoch£º435	 i:2 	 global-step:8702	 l-p:0.1089792400598526
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12624143064022064
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13677096366882324
epoch£º435	 i:5 	 global-step:8705	 l-p:0.10050647705793381
epoch£º435	 i:6 	 global-step:8706	 l-p:0.08207955211400986
epoch£º435	 i:7 	 global-step:8707	 l-p:0.17997851967811584
epoch£º435	 i:8 	 global-step:8708	 l-p:0.17090286314487457
epoch£º435	 i:9 	 global-step:8709	 l-p:0.13270366191864014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9153, 5.4615, 5.5956],
        [4.9153, 5.2139, 5.1713],
        [4.9153, 4.9073, 4.9143],
        [4.9153, 4.9904, 4.8518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10692890733480453 
model_pd.l_d.mean(): -20.488380432128906 
model_pd.lagr.mean(): -20.381450653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5133], device='cuda:0')), ('power', tensor([-21.2366], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10692890733480453
epoch£º436	 i:1 	 global-step:8721	 l-p:0.10430633276700974
epoch£º436	 i:2 	 global-step:8722	 l-p:0.0974716767668724
epoch£º436	 i:3 	 global-step:8723	 l-p:-0.1289607137441635
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12226323038339615
epoch£º436	 i:5 	 global-step:8725	 l-p:0.1394941508769989
epoch£º436	 i:6 	 global-step:8726	 l-p:0.14948883652687073
epoch£º436	 i:7 	 global-step:8727	 l-p:0.1637018620967865
epoch£º436	 i:8 	 global-step:8728	 l-p:0.14470337331295013
epoch£º436	 i:9 	 global-step:8729	 l-p:0.11616967618465424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0054, 5.0015, 5.0051],
        [5.0054, 5.7602, 6.0615],
        [5.0054, 5.1941, 5.0903],
        [5.0054, 5.2255, 5.1361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.072151780128479 
model_pd.l_d.mean(): -19.474382400512695 
model_pd.lagr.mean(): -19.402231216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-20.1879], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.072151780128479
epoch£º437	 i:1 	 global-step:8741	 l-p:3.3052356243133545
epoch£º437	 i:2 	 global-step:8742	 l-p:0.1857524961233139
epoch£º437	 i:3 	 global-step:8743	 l-p:0.08885756880044937
epoch£º437	 i:4 	 global-step:8744	 l-p:0.08133074641227722
epoch£º437	 i:5 	 global-step:8745	 l-p:0.11053991317749023
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13978596031665802
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12732098996639252
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1862974315881729
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11135906726121902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2192, 5.1652, 5.1124],
        [5.2192, 5.2880, 5.1559],
        [5.2192, 5.1958, 5.2115],
        [5.2192, 5.2192, 5.2192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12467033416032791 
model_pd.l_d.mean(): -19.38204574584961 
model_pd.lagr.mean(): -19.257375717163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4275], device='cuda:0')), ('power', tensor([-20.0305], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12467033416032791
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13350042700767517
epoch£º438	 i:2 	 global-step:8762	 l-p:0.19789038598537445
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10656282305717468
epoch£º438	 i:4 	 global-step:8764	 l-p:0.08925799280405045
epoch£º438	 i:5 	 global-step:8765	 l-p:0.13486619293689728
epoch£º438	 i:6 	 global-step:8766	 l-p:0.23147188127040863
epoch£º438	 i:7 	 global-step:8767	 l-p:0.12156408280134201
epoch£º438	 i:8 	 global-step:8768	 l-p:0.14950044453144073
epoch£º438	 i:9 	 global-step:8769	 l-p:0.12045107781887054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1792, 5.1497, 5.0501],
        [5.1792, 5.1699, 5.0546],
        [5.1792, 5.1736, 5.0561],
        [5.1792, 5.1790, 5.1792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.10093789547681808 
model_pd.l_d.mean(): -20.776004791259766 
model_pd.lagr.mean(): -20.675065994262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3730], device='cuda:0')), ('power', tensor([-21.3840], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.10093789547681808
epoch£º439	 i:1 	 global-step:8781	 l-p:0.120521679520607
epoch£º439	 i:2 	 global-step:8782	 l-p:-0.868231475353241
epoch£º439	 i:3 	 global-step:8783	 l-p:0.11949881166219711
epoch£º439	 i:4 	 global-step:8784	 l-p:0.1318889707326889
epoch£º439	 i:5 	 global-step:8785	 l-p:-0.21207614243030548
epoch£º439	 i:6 	 global-step:8786	 l-p:0.13608109951019287
epoch£º439	 i:7 	 global-step:8787	 l-p:0.24426619708538055
epoch£º439	 i:8 	 global-step:8788	 l-p:0.15240351855754852
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12648360431194305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0830, 5.0580, 5.0749],
        [5.0830, 5.0629, 5.0776],
        [5.0830, 5.0160, 5.0000],
        [5.0830, 5.2577, 5.1468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.0962659940123558 
model_pd.l_d.mean(): -19.379419326782227 
model_pd.lagr.mean(): -19.283153533935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4827], device='cuda:0')), ('power', tensor([-20.0843], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.0962659940123558
epoch£º440	 i:1 	 global-step:8801	 l-p:0.1284816563129425
epoch£º440	 i:2 	 global-step:8802	 l-p:0.11258254945278168
epoch£º440	 i:3 	 global-step:8803	 l-p:0.1795797348022461
epoch£º440	 i:4 	 global-step:8804	 l-p:0.1442706137895584
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13560377061367035
epoch£º440	 i:6 	 global-step:8806	 l-p:-0.036994874477386475
epoch£º440	 i:7 	 global-step:8807	 l-p:0.10049337148666382
epoch£º440	 i:8 	 global-step:8808	 l-p:0.1493520438671112
epoch£º440	 i:9 	 global-step:8809	 l-p:0.16580019891262054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0275, 5.0197, 5.0266],
        [5.0275, 5.1891, 5.0735],
        [5.0275, 5.0267, 5.0275],
        [5.0275, 4.9696, 4.8872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.05915142223238945 
model_pd.l_d.mean(): -19.1633243560791 
model_pd.lagr.mean(): -19.10417366027832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5469], device='cuda:0')), ('power', tensor([-19.9314], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.05915142223238945
epoch£º441	 i:1 	 global-step:8821	 l-p:0.14190374314785004
epoch£º441	 i:2 	 global-step:8822	 l-p:-0.1157650351524353
epoch£º441	 i:3 	 global-step:8823	 l-p:0.1544971615076065
epoch£º441	 i:4 	 global-step:8824	 l-p:0.052930813282728195
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13416731357574463
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12662583589553833
epoch£º441	 i:7 	 global-step:8827	 l-p:0.16670718789100647
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11471673846244812
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12217450886964798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1706, 5.1691, 5.1705],
        [5.1706, 5.5999, 5.6286],
        [5.1706, 5.2218, 5.0872],
        [5.1706, 5.1519, 5.1658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12351959198713303 
model_pd.l_d.mean(): -20.27863121032715 
model_pd.lagr.mean(): -20.15511131286621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145], device='cuda:0')), ('power', tensor([-20.9236], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12351959198713303
epoch£º442	 i:1 	 global-step:8841	 l-p:0.13025623559951782
epoch£º442	 i:2 	 global-step:8842	 l-p:0.34529799222946167
epoch£º442	 i:3 	 global-step:8843	 l-p:0.14741267263889313
epoch£º442	 i:4 	 global-step:8844	 l-p:0.14803454279899597
epoch£º442	 i:5 	 global-step:8845	 l-p:-2.048966407775879
epoch£º442	 i:6 	 global-step:8846	 l-p:0.13354197144508362
epoch£º442	 i:7 	 global-step:8847	 l-p:0.13710856437683105
epoch£º442	 i:8 	 global-step:8848	 l-p:0.11608253419399261
epoch£º442	 i:9 	 global-step:8849	 l-p:1.2162067890167236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.1331, 5.1518],
        [5.1677, 5.1670, 5.1677],
        [5.1677, 5.1667, 5.1677],
        [5.1677, 5.1231, 5.1401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.11282625049352646 
model_pd.l_d.mean(): -20.044950485229492 
model_pd.lagr.mean(): -19.932125091552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4227], device='cuda:0')), ('power', tensor([-20.6957], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.11282625049352646
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13288085162639618
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12804760038852692
epoch£º443	 i:3 	 global-step:8863	 l-p:-4.911503314971924
epoch£º443	 i:4 	 global-step:8864	 l-p:0.15797029435634613
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13742797076702118
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11666359007358551
epoch£º443	 i:7 	 global-step:8867	 l-p:0.03650754317641258
epoch£º443	 i:8 	 global-step:8868	 l-p:0.1280006319284439
epoch£º443	 i:9 	 global-step:8869	 l-p:1.2581795454025269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 5.2107, 5.0907],
        [5.0588, 5.0123, 5.0313],
        [5.0588, 4.9888, 4.9466],
        [5.0588, 5.1537, 5.0185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): -0.5182635188102722 
model_pd.l_d.mean(): -18.972829818725586 
model_pd.lagr.mean(): -19.491092681884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5369], device='cuda:0')), ('power', tensor([-19.7286], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:-0.5182635188102722
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14262931048870087
epoch£º444	 i:2 	 global-step:8882	 l-p:0.1344297230243683
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12790437042713165
epoch£º444	 i:4 	 global-step:8884	 l-p:-0.727689266204834
epoch£º444	 i:5 	 global-step:8885	 l-p:0.1292313188314438
epoch£º444	 i:6 	 global-step:8886	 l-p:0.1487545669078827
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15981411933898926
epoch£º444	 i:8 	 global-step:8888	 l-p:0.16866646707057953
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.044150907546281815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9558, 4.9554, 4.9558],
        [4.9558, 4.8852, 4.8124],
        [4.9558, 4.9454, 4.9543],
        [4.9558, 4.8859, 4.8881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.035681091248989105 
model_pd.l_d.mean(): -20.643932342529297 
model_pd.lagr.mean(): -20.67961311340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-21.3477], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.035681091248989105
epoch£º445	 i:1 	 global-step:8901	 l-p:0.14216427505016327
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10501385480165482
epoch£º445	 i:3 	 global-step:8903	 l-p:0.03891676664352417
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13784921169281006
epoch£º445	 i:5 	 global-step:8905	 l-p:0.17003504931926727
epoch£º445	 i:6 	 global-step:8906	 l-p:0.1128268763422966
epoch£º445	 i:7 	 global-step:8907	 l-p:0.12028919160366058
epoch£º445	 i:8 	 global-step:8908	 l-p:0.13517579436302185
epoch£º445	 i:9 	 global-step:8909	 l-p:0.13009344041347504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.9772, 4.9076, 4.8352],
        [4.9772, 5.4333, 5.4926],
        [4.9772, 4.9391, 4.8216],
        [4.9772, 5.4591, 5.5376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.11794305592775345 
model_pd.l_d.mean(): -20.20750617980957 
model_pd.lagr.mean(): -20.089563369750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5053], device='cuda:0')), ('power', tensor([-20.9445], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.11794305592775345
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14406663179397583
epoch£º446	 i:2 	 global-step:8922	 l-p:0.13301630318164825
epoch£º446	 i:3 	 global-step:8923	 l-p:0.1054101288318634
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14260202646255493
epoch£º446	 i:5 	 global-step:8925	 l-p:0.12032826244831085
epoch£º446	 i:6 	 global-step:8926	 l-p:0.14263030886650085
epoch£º446	 i:7 	 global-step:8927	 l-p:0.149639293551445
epoch£º446	 i:8 	 global-step:8928	 l-p:0.05034821853041649
epoch£º446	 i:9 	 global-step:8929	 l-p:0.07642820477485657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9647, 4.9598, 4.9642],
        [4.9647, 4.9266, 4.9479],
        [4.9647, 4.9021, 4.8105],
        [4.9647, 4.9045, 4.8090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10340598970651627 
model_pd.l_d.mean(): -20.391504287719727 
model_pd.lagr.mean(): -20.288097381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4921], device='cuda:0')), ('power', tensor([-21.1170], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10340598970651627
epoch£º447	 i:1 	 global-step:8941	 l-p:0.14410175383090973
epoch£º447	 i:2 	 global-step:8942	 l-p:0.14907477796077728
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13639116287231445
epoch£º447	 i:4 	 global-step:8944	 l-p:0.13219717144966125
epoch£º447	 i:5 	 global-step:8945	 l-p:-0.09898952394723892
epoch£º447	 i:6 	 global-step:8946	 l-p:0.14641809463500977
epoch£º447	 i:7 	 global-step:8947	 l-p:0.09086165577173233
epoch£º447	 i:8 	 global-step:8948	 l-p:0.0955769494175911
epoch£º447	 i:9 	 global-step:8949	 l-p:0.11770626157522202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9796, 5.5572, 5.7094],
        [4.9796, 5.3743, 5.3892],
        [4.9796, 4.9512, 4.9701],
        [4.9796, 4.9795, 4.9796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13603642582893372 
model_pd.l_d.mean(): -19.819856643676758 
model_pd.lagr.mean(): -19.683820724487305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5319], device='cuda:0')), ('power', tensor([-20.5797], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13603642582893372
epoch£º448	 i:1 	 global-step:8961	 l-p:0.09842416644096375
epoch£º448	 i:2 	 global-step:8962	 l-p:0.1470511257648468
epoch£º448	 i:3 	 global-step:8963	 l-p:-0.023315999656915665
epoch£º448	 i:4 	 global-step:8964	 l-p:0.09554864466190338
epoch£º448	 i:5 	 global-step:8965	 l-p:0.07886945456266403
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13621333241462708
epoch£º448	 i:7 	 global-step:8967	 l-p:0.13270042836666107
epoch£º448	 i:8 	 global-step:8968	 l-p:0.2150854468345642
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13706517219543457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9342, 4.9321, 4.9341],
        [4.9342, 4.8548, 4.8392],
        [4.9342, 4.8947, 4.9166],
        [4.9342, 4.8862, 4.9081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.10677441209554672 
model_pd.l_d.mean(): -20.770404815673828 
model_pd.lagr.mean(): -20.66362953186035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.4705], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.10677441209554672
epoch£º449	 i:1 	 global-step:8981	 l-p:0.10769714415073395
epoch£º449	 i:2 	 global-step:8982	 l-p:0.16888420283794403
epoch£º449	 i:3 	 global-step:8983	 l-p:0.17056669294834137
epoch£º449	 i:4 	 global-step:8984	 l-p:0.1572585552930832
epoch£º449	 i:5 	 global-step:8985	 l-p:0.05771712586283684
epoch£º449	 i:6 	 global-step:8986	 l-p:0.1401243805885315
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12217877805233002
epoch£º449	 i:8 	 global-step:8988	 l-p:0.1748901903629303
epoch£º449	 i:9 	 global-step:8989	 l-p:0.10978470742702484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9688, 4.9684, 4.9688],
        [4.9688, 4.9687, 4.9688],
        [4.9688, 4.9657, 4.9686],
        [4.9688, 5.2227, 5.1490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.08381122350692749 
model_pd.l_d.mean(): -20.680509567260742 
model_pd.lagr.mean(): -20.596698760986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.3813], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.08381122350692749
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15577518939971924
epoch£º450	 i:2 	 global-step:9002	 l-p:0.11989648640155792
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12757891416549683
epoch£º450	 i:4 	 global-step:9004	 l-p:0.07370428740978241
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.8753096461296082
epoch£º450	 i:6 	 global-step:9006	 l-p:0.17469415068626404
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14837363362312317
epoch£º450	 i:8 	 global-step:9008	 l-p:0.093412384390831
epoch£º450	 i:9 	 global-step:9009	 l-p:0.1208706945180893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0160, 4.9936, 5.0099],
        [5.0160, 4.9642, 4.9840],
        [5.0160, 5.0159, 5.0160],
        [5.0160, 5.0157, 5.0160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13383691012859344 
model_pd.l_d.mean(): -18.772106170654297 
model_pd.lagr.mean(): -18.638269424438477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5839], device='cuda:0')), ('power', tensor([-19.5736], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13383691012859344
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10502605885267258
epoch£º451	 i:2 	 global-step:9022	 l-p:0.1400800347328186
epoch£º451	 i:3 	 global-step:9023	 l-p:0.15454882383346558
epoch£º451	 i:4 	 global-step:9024	 l-p:0.06723536550998688
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11895942687988281
epoch£º451	 i:6 	 global-step:9026	 l-p:0.5083121657371521
epoch£º451	 i:7 	 global-step:9027	 l-p:0.1469651460647583
epoch£º451	 i:8 	 global-step:9028	 l-p:0.1339997798204422
epoch£º451	 i:9 	 global-step:9029	 l-p:0.019345540553331375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0344, 4.9729, 4.9864],
        [5.0344, 5.0331, 5.0344],
        [5.0344, 5.0317, 5.0343],
        [5.0344, 5.0344, 5.0344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.20380157232284546 
model_pd.l_d.mean(): -20.091144561767578 
model_pd.lagr.mean(): -19.88734245300293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4997], device='cuda:0')), ('power', tensor([-20.8211], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.20380157232284546
epoch£º452	 i:1 	 global-step:9041	 l-p:0.40278732776641846
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10787951946258545
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12549297511577606
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12595410645008087
epoch£º452	 i:5 	 global-step:9045	 l-p:0.06297238916158676
epoch£º452	 i:6 	 global-step:9046	 l-p:0.1188104897737503
epoch£º452	 i:7 	 global-step:9047	 l-p:-0.13573290407657623
epoch£º452	 i:8 	 global-step:9048	 l-p:0.13323931396007538
epoch£º452	 i:9 	 global-step:9049	 l-p:0.13500642776489258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0783, 5.0770, 5.0783],
        [5.0783, 5.0783, 5.0783],
        [5.0783, 5.0763, 5.0782],
        [5.0783, 5.0418, 5.0625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.21332614123821259 
model_pd.l_d.mean(): -20.68779182434082 
model_pd.lagr.mean(): -20.47446632385254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4269], device='cuda:0')), ('power', tensor([-21.3498], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.21332614123821259
epoch£º453	 i:1 	 global-step:9061	 l-p:0.1415383368730545
epoch£º453	 i:2 	 global-step:9062	 l-p:0.05471364036202431
epoch£º453	 i:3 	 global-step:9063	 l-p:0.06925405561923981
epoch£º453	 i:4 	 global-step:9064	 l-p:0.12632261216640472
epoch£º453	 i:5 	 global-step:9065	 l-p:0.12915368378162384
epoch£º453	 i:6 	 global-step:9066	 l-p:0.23789408802986145
epoch£º453	 i:7 	 global-step:9067	 l-p:0.06763175129890442
epoch£º453	 i:8 	 global-step:9068	 l-p:0.1098824068903923
epoch£º453	 i:9 	 global-step:9069	 l-p:0.1391524225473404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1274, 5.0603, 5.0599],
        [5.1274, 5.2705, 5.1441],
        [5.1274, 5.1247, 5.1273],
        [5.1274, 5.1158, 5.1255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1423492431640625 
model_pd.l_d.mean(): -20.398273468017578 
model_pd.lagr.mean(): -20.255924224853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4143], device='cuda:0')), ('power', tensor([-21.0443], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1423492431640625
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.16680629551410675
epoch£º454	 i:2 	 global-step:9082	 l-p:0.1690722405910492
epoch£º454	 i:3 	 global-step:9083	 l-p:0.1468353569507599
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15676946938037872
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12273408472537994
epoch£º454	 i:6 	 global-step:9086	 l-p:0.1166125014424324
epoch£º454	 i:7 	 global-step:9087	 l-p:0.0990639477968216
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.019645581021904945
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13344499468803406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0194, 4.9474, 4.8714],
        [5.0194, 5.0189, 5.0194],
        [5.0194, 5.0193, 5.0194],
        [5.0194, 5.0194, 5.0194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.130426287651062 
model_pd.l_d.mean(): -20.213237762451172 
model_pd.lagr.mean(): -20.08281135559082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.9370], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.130426287651062
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12102880328893661
epoch£º455	 i:2 	 global-step:9102	 l-p:0.15214796364307404
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12666332721710205
epoch£º455	 i:4 	 global-step:9104	 l-p:0.09841296821832657
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11400522291660309
epoch£º455	 i:6 	 global-step:9106	 l-p:0.12827953696250916
epoch£º455	 i:7 	 global-step:9107	 l-p:0.280799001455307
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1719622015953064
epoch£º455	 i:9 	 global-step:9109	 l-p:0.13020752370357513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8370, 4.8370, 4.8370],
        [4.8370, 4.7686, 4.7864],
        [4.8370, 4.8364, 4.8370],
        [4.8370, 4.8341, 4.8368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.115913525223732 
model_pd.l_d.mean(): -20.448566436767578 
model_pd.lagr.mean(): -20.332653045654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5111], device='cuda:0')), ('power', tensor([-21.1941], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.115913525223732
epoch£º456	 i:1 	 global-step:9121	 l-p:0.165268212556839
epoch£º456	 i:2 	 global-step:9122	 l-p:0.15444496273994446
epoch£º456	 i:3 	 global-step:9123	 l-p:0.7581577301025391
epoch£º456	 i:4 	 global-step:9124	 l-p:0.1361258178949356
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18572230637073517
epoch£º456	 i:6 	 global-step:9126	 l-p:0.14391982555389404
epoch£º456	 i:7 	 global-step:9127	 l-p:0.22350165247917175
epoch£º456	 i:8 	 global-step:9128	 l-p:0.15166105329990387
epoch£º456	 i:9 	 global-step:9129	 l-p:0.09920839220285416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9022, 5.0725, 4.9568],
        [4.9022, 4.8303, 4.8423],
        [4.9022, 4.9052, 4.7551],
        [4.9022, 4.9022, 4.9022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.20559345185756683 
model_pd.l_d.mean(): -20.88719940185547 
model_pd.lagr.mean(): -20.68160629272461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.5835], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.20559345185756683
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08784394711256027
epoch£º457	 i:2 	 global-step:9142	 l-p:0.14694073796272278
epoch£º457	 i:3 	 global-step:9143	 l-p:0.1409558206796646
epoch£º457	 i:4 	 global-step:9144	 l-p:0.1082194596529007
epoch£º457	 i:5 	 global-step:9145	 l-p:0.15494373440742493
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12614579498767853
epoch£º457	 i:7 	 global-step:9147	 l-p:0.11802373826503754
epoch£º457	 i:8 	 global-step:9148	 l-p:0.0439869649708271
epoch£º457	 i:9 	 global-step:9149	 l-p:0.1298791617155075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0645, 5.0645, 5.0645],
        [5.0645, 5.0231, 5.0448],
        [5.0645, 5.0645, 5.0645],
        [5.0645, 5.0644, 5.0645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.14964738488197327 
model_pd.l_d.mean(): -20.72591209411621 
model_pd.lagr.mean(): -20.576265335083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-21.3881], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.14964738488197327
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11873982846736908
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13271261751651764
epoch£º458	 i:3 	 global-step:9163	 l-p:0.18834654986858368
epoch£º458	 i:4 	 global-step:9164	 l-p:0.11505942791700363
epoch£º458	 i:5 	 global-step:9165	 l-p:0.12610693275928497
epoch£º458	 i:6 	 global-step:9166	 l-p:-0.012474984861910343
epoch£º458	 i:7 	 global-step:9167	 l-p:0.1327141672372818
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12299440056085587
epoch£º458	 i:9 	 global-step:9169	 l-p:0.23464928567409515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1103, 5.1103, 5.1103],
        [5.1103, 5.2472, 5.1175],
        [5.1103, 5.8640, 6.1505],
        [5.1103, 5.1102, 5.1103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.13939914107322693 
model_pd.l_d.mean(): -20.220640182495117 
model_pd.lagr.mean(): -20.081241607666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-20.8724], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.13939914107322693
epoch£º459	 i:1 	 global-step:9181	 l-p:0.14375068247318268
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11573486775159836
epoch£º459	 i:3 	 global-step:9183	 l-p:0.24697761237621307
epoch£º459	 i:4 	 global-step:9184	 l-p:0.07324567437171936
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05747436359524727
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13611659407615662
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11322511732578278
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15146298706531525
epoch£º459	 i:9 	 global-step:9189	 l-p:0.03749828785657883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0386, 5.0292, 5.0373],
        [5.0386, 4.9648, 4.8901],
        [5.0386, 5.7087, 5.9296],
        [5.0386, 5.0714, 4.9237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.12318497151136398 
model_pd.l_d.mean(): -18.784805297851562 
model_pd.lagr.mean(): -18.66162109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5687], device='cuda:0')), ('power', tensor([-19.5709], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.12318497151136398
epoch£º460	 i:1 	 global-step:9201	 l-p:0.1306314468383789
epoch£º460	 i:2 	 global-step:9202	 l-p:0.126093789935112
epoch£º460	 i:3 	 global-step:9203	 l-p:0.24191536009311676
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10855920612812042
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14537367224693298
epoch£º460	 i:6 	 global-step:9206	 l-p:0.1439271718263626
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13795946538448334
epoch£º460	 i:8 	 global-step:9208	 l-p:0.10087850689888
epoch£º460	 i:9 	 global-step:9209	 l-p:0.10901577025651932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9436, 4.9416, 4.9435],
        [4.9436, 4.8588, 4.7928],
        [4.9436, 4.8977, 4.9211],
        [4.9436, 5.0271, 4.8811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1681414693593979 
model_pd.l_d.mean(): -18.578815460205078 
model_pd.lagr.mean(): -18.410673141479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5816], device='cuda:0')), ('power', tensor([-19.3759], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.1681414693593979
epoch£º461	 i:1 	 global-step:9221	 l-p:0.03690554201602936
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11172422021627426
epoch£º461	 i:3 	 global-step:9223	 l-p:0.15948019921779633
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13895371556282043
epoch£º461	 i:5 	 global-step:9225	 l-p:0.12972547113895416
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15966472029685974
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07796373218297958
epoch£º461	 i:8 	 global-step:9228	 l-p:0.09774471819400787
epoch£º461	 i:9 	 global-step:9229	 l-p:0.11707787960767746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 4.9669, 4.8390],
        [5.0072, 4.9409, 4.8410],
        [5.0072, 4.9666, 4.9892],
        [5.0072, 4.9671, 4.9896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09242450445890427 
model_pd.l_d.mean(): -20.467527389526367 
model_pd.lagr.mean(): -20.375102996826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.1745], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09242450445890427
epoch£º462	 i:1 	 global-step:9241	 l-p:0.30307093262672424
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14604820311069489
epoch£º462	 i:3 	 global-step:9243	 l-p:0.0972614586353302
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.1307458132505417
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11978313326835632
epoch£º462	 i:6 	 global-step:9246	 l-p:0.15194717049598694
epoch£º462	 i:7 	 global-step:9247	 l-p:0.0636860802769661
epoch£º462	 i:8 	 global-step:9248	 l-p:0.09363847225904465
epoch£º462	 i:9 	 global-step:9249	 l-p:0.14133954048156738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0647, 5.0632, 5.0646],
        [5.0647, 5.0539, 5.0631],
        [5.0647, 5.0551, 5.0634],
        [5.0647, 5.0646, 5.0647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.2161877453327179 
model_pd.l_d.mean(): -18.22557258605957 
model_pd.lagr.mean(): -18.009384155273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5988], device='cuda:0')), ('power', tensor([-19.0364], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.2161877453327179
epoch£º463	 i:1 	 global-step:9261	 l-p:-1.265692949295044
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14631325006484985
epoch£º463	 i:3 	 global-step:9263	 l-p:0.060230255126953125
epoch£º463	 i:4 	 global-step:9264	 l-p:0.12676085531711578
epoch£º463	 i:5 	 global-step:9265	 l-p:0.11838453263044357
epoch£º463	 i:6 	 global-step:9266	 l-p:0.12355111539363861
epoch£º463	 i:7 	 global-step:9267	 l-p:0.13043838739395142
epoch£º463	 i:8 	 global-step:9268	 l-p:0.08066335320472717
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11847078055143356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1002, 5.0966, 5.0999],
        [5.1002, 5.1878, 5.0442],
        [5.1002, 5.3023, 5.1963],
        [5.1002, 5.1002, 5.1002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): 0.00045806882553733885 
model_pd.l_d.mean(): -20.58644676208496 
model_pd.lagr.mean(): -20.585988998413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.2494], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:0.00045806882553733885
epoch£º464	 i:1 	 global-step:9281	 l-p:0.19069549441337585
epoch£º464	 i:2 	 global-step:9282	 l-p:0.07166323065757751
epoch£º464	 i:3 	 global-step:9283	 l-p:0.13351120054721832
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11364217847585678
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12199777364730835
epoch£º464	 i:6 	 global-step:9286	 l-p:0.3123287856578827
epoch£º464	 i:7 	 global-step:9287	 l-p:0.14457125961780548
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17764998972415924
epoch£º464	 i:9 	 global-step:9289	 l-p:0.1332160383462906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0933, 5.0933, 5.0933],
        [5.0933, 5.0919, 5.0933],
        [5.0933, 5.0733, 5.0885],
        [5.0933, 5.0141, 4.9698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12983545660972595 
model_pd.l_d.mean(): -20.12667465209961 
model_pd.lagr.mean(): -19.99683952331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.8475], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12983545660972595
epoch£º465	 i:1 	 global-step:9301	 l-p:0.5156251192092896
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12349545955657959
epoch£º465	 i:3 	 global-step:9303	 l-p:0.1343034952878952
epoch£º465	 i:4 	 global-step:9304	 l-p:0.1203741729259491
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12985271215438843
epoch£º465	 i:6 	 global-step:9306	 l-p:0.15150725841522217
epoch£º465	 i:7 	 global-step:9307	 l-p:0.15768657624721527
epoch£º465	 i:8 	 global-step:9308	 l-p:0.14286817610263824
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09687948226928711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9997, 4.9173, 4.9068],
        [4.9997, 4.9134, 4.8707],
        [4.9997, 4.9841, 4.9967],
        [4.9997, 4.9949, 4.8474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.13938800990581512 
model_pd.l_d.mean(): -18.977487564086914 
model_pd.lagr.mean(): -18.83810043334961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5478], device='cuda:0')), ('power', tensor([-19.7444], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.13938800990581512
epoch£º466	 i:1 	 global-step:9321	 l-p:0.1267486810684204
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13768930733203888
epoch£º466	 i:3 	 global-step:9323	 l-p:0.12991108000278473
epoch£º466	 i:4 	 global-step:9324	 l-p:2.564795732498169
epoch£º466	 i:5 	 global-step:9325	 l-p:0.06464089453220367
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11465903371572495
epoch£º466	 i:7 	 global-step:9327	 l-p:0.08296091854572296
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12748698890209198
epoch£º466	 i:9 	 global-step:9329	 l-p:0.11130183190107346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0482, 5.0482, 5.0482],
        [5.0482, 5.1807, 5.0470],
        [5.0482, 5.0250, 5.0420],
        [5.0482, 4.9648, 4.9362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12551723420619965 
model_pd.l_d.mean(): -20.328947067260742 
model_pd.lagr.mean(): -20.20343017578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4679], device='cuda:0')), ('power', tensor([-21.0290], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12551723420619965
epoch£º467	 i:1 	 global-step:9341	 l-p:0.19451594352722168
epoch£º467	 i:2 	 global-step:9342	 l-p:0.3711501955986023
epoch£º467	 i:3 	 global-step:9343	 l-p:0.12192828953266144
epoch£º467	 i:4 	 global-step:9344	 l-p:0.1444147825241089
epoch£º467	 i:5 	 global-step:9345	 l-p:1.248752474784851
epoch£º467	 i:6 	 global-step:9346	 l-p:0.05626533925533295
epoch£º467	 i:7 	 global-step:9347	 l-p:0.056681565940380096
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11678577959537506
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11389164626598358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1819, 5.1491, 5.1693],
        [5.1819, 5.3476, 5.2251],
        [5.1819, 5.1303, 5.1497],
        [5.1819, 5.8645, 6.0829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.4468539357185364 
model_pd.l_d.mean(): -20.519502639770508 
model_pd.lagr.mean(): -20.072649002075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-21.1702], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.4468539357185364
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10660148411989212
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12661674618721008
epoch£º468	 i:3 	 global-step:9363	 l-p:0.20718303322792053
epoch£º468	 i:4 	 global-step:9364	 l-p:0.14537189900875092
epoch£º468	 i:5 	 global-step:9365	 l-p:0.14408595860004425
epoch£º468	 i:6 	 global-step:9366	 l-p:0.12966041266918182
epoch£º468	 i:7 	 global-step:9367	 l-p:0.11136351525783539
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.18221795558929443
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11334867030382156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1402, 5.0983, 5.1202],
        [5.1402, 5.1402, 5.1402],
        [5.1402, 5.1374, 5.1400],
        [5.1402, 5.1402, 5.1402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09367215633392334 
model_pd.l_d.mean(): -20.47349739074707 
model_pd.lagr.mean(): -20.379825592041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.1180], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09367215633392334
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08364007622003555
epoch£º469	 i:2 	 global-step:9382	 l-p:0.12909163534641266
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13533185422420502
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.02830963023006916
epoch£º469	 i:5 	 global-step:9385	 l-p:0.510184109210968
epoch£º469	 i:6 	 global-step:9386	 l-p:0.1798207312822342
epoch£º469	 i:7 	 global-step:9387	 l-p:0.1386619508266449
epoch£º469	 i:8 	 global-step:9388	 l-p:0.10637634247541428
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11703811585903168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0398, 5.0395, 5.0398],
        [5.0398, 4.9567, 4.9418],
        [5.0398, 4.9860, 5.0082],
        [5.0398, 5.0396, 5.0398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12225840985774994 
model_pd.l_d.mean(): -20.59147834777832 
model_pd.lagr.mean(): -20.469219207763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-21.2649], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12225840985774994
epoch£º470	 i:1 	 global-step:9401	 l-p:0.10384836047887802
epoch£º470	 i:2 	 global-step:9402	 l-p:0.13236038386821747
epoch£º470	 i:3 	 global-step:9403	 l-p:0.05615895986557007
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15257909893989563
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09331724047660828
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16482485830783844
epoch£º470	 i:7 	 global-step:9407	 l-p:0.1506328582763672
epoch£º470	 i:8 	 global-step:9408	 l-p:0.08944736421108246
epoch£º470	 i:9 	 global-step:9409	 l-p:-4.29488468170166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0124, 5.0124, 5.0124],
        [5.0124, 5.0124, 5.0124],
        [5.0124, 5.0124, 5.0124],
        [5.0124, 5.0124, 5.0124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.3894314467906952 
model_pd.l_d.mean(): -20.06432342529297 
model_pd.lagr.mean(): -19.67489242553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5129], device='cuda:0')), ('power', tensor([-20.8074], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.3894314467906952
epoch£º471	 i:1 	 global-step:9421	 l-p:0.14256100356578827
epoch£º471	 i:2 	 global-step:9422	 l-p:-10.721051216125488
epoch£º471	 i:3 	 global-step:9423	 l-p:0.13912059366703033
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.09653600305318832
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13835254311561584
epoch£º471	 i:6 	 global-step:9426	 l-p:0.1270374208688736
epoch£º471	 i:7 	 global-step:9427	 l-p:0.1330842524766922
epoch£º471	 i:8 	 global-step:9428	 l-p:0.0675152987241745
epoch£º471	 i:9 	 global-step:9429	 l-p:0.13747231662273407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0741, 5.0102, 4.9062],
        [5.0741, 5.0535, 5.0692],
        [5.0741, 5.4234, 5.3983],
        [5.0741, 5.0741, 5.0741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12852853536605835 
model_pd.l_d.mean(): -20.618337631225586 
model_pd.lagr.mean(): -20.489809036254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4314], device='cuda:0')), ('power', tensor([-21.2843], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12852853536605835
epoch£º472	 i:1 	 global-step:9441	 l-p:0.27928411960601807
epoch£º472	 i:2 	 global-step:9442	 l-p:0.12673084437847137
epoch£º472	 i:3 	 global-step:9443	 l-p:0.1369839757680893
epoch£º472	 i:4 	 global-step:9444	 l-p:0.14304593205451965
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12299373000860214
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08214191347360611
epoch£º472	 i:7 	 global-step:9447	 l-p:0.11389388889074326
epoch£º472	 i:8 	 global-step:9448	 l-p:0.19349156320095062
epoch£º472	 i:9 	 global-step:9449	 l-p:0.11974181234836578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9257, 4.8311, 4.7801],
        [4.9257, 4.9257, 4.9257],
        [4.9257, 4.9257, 4.9257],
        [4.9257, 4.9029, 4.9201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.12429583072662354 
model_pd.l_d.mean(): -19.67681884765625 
model_pd.lagr.mean(): -19.552522659301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5986], device='cuda:0')), ('power', tensor([-20.5032], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.12429583072662354
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12684963643550873
epoch£º473	 i:2 	 global-step:9462	 l-p:0.1838393360376358
epoch£º473	 i:3 	 global-step:9463	 l-p:0.11706634610891342
epoch£º473	 i:4 	 global-step:9464	 l-p:0.1338391900062561
epoch£º473	 i:5 	 global-step:9465	 l-p:0.146332249045372
epoch£º473	 i:6 	 global-step:9466	 l-p:0.17347033321857452
epoch£º473	 i:7 	 global-step:9467	 l-p:0.1480192095041275
epoch£º473	 i:8 	 global-step:9468	 l-p:0.11640471965074539
epoch£º473	 i:9 	 global-step:9469	 l-p:0.15620878338813782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9010, 4.8989, 4.9009],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.8377, 4.8606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.12734045088291168 
model_pd.l_d.mean(): -20.272218704223633 
model_pd.lagr.mean(): -20.144878387451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-21.0323], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.12734045088291168
epoch£º474	 i:1 	 global-step:9481	 l-p:0.15031829476356506
epoch£º474	 i:2 	 global-step:9482	 l-p:0.1522902399301529
epoch£º474	 i:3 	 global-step:9483	 l-p:0.1655937284231186
epoch£º474	 i:4 	 global-step:9484	 l-p:0.14958827197551727
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12211538106203079
epoch£º474	 i:6 	 global-step:9486	 l-p:0.1288970559835434
epoch£º474	 i:7 	 global-step:9487	 l-p:0.17396020889282227
epoch£º474	 i:8 	 global-step:9488	 l-p:0.11265868693590164
epoch£º474	 i:9 	 global-step:9489	 l-p:0.027146276086568832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9932, 4.9191, 4.9315],
        [4.9932, 4.9053, 4.8907],
        [4.9932, 4.9932, 4.9932],
        [4.9932, 4.9932, 4.9932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1426147073507309 
model_pd.l_d.mean(): -20.04828643798828 
model_pd.lagr.mean(): -19.905672073364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5259], device='cuda:0')), ('power', tensor([-20.8046], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1426147073507309
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15201152861118317
epoch£º475	 i:2 	 global-step:9502	 l-p:0.1372053474187851
epoch£º475	 i:3 	 global-step:9503	 l-p:0.09311700612306595
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10518456995487213
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18528397381305695
epoch£º475	 i:6 	 global-step:9506	 l-p:0.14978329837322235
epoch£º475	 i:7 	 global-step:9507	 l-p:0.1951534003019333
epoch£º475	 i:8 	 global-step:9508	 l-p:0.3013063073158264
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09890206903219223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 5.5381, 5.5330],
        [5.1512, 5.1512, 5.1512],
        [5.1512, 5.4700, 5.4217],
        [5.1512, 5.1023, 5.1244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12023639678955078 
model_pd.l_d.mean(): -20.456933975219727 
model_pd.lagr.mean(): -20.33669662475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([-21.1146], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12023639678955078
epoch£º476	 i:1 	 global-step:9521	 l-p:0.12865141034126282
epoch£º476	 i:2 	 global-step:9522	 l-p:0.21229316294193268
epoch£º476	 i:3 	 global-step:9523	 l-p:0.13398076593875885
epoch£º476	 i:4 	 global-step:9524	 l-p:0.148127481341362
epoch£º476	 i:5 	 global-step:9525	 l-p:0.13738830387592316
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.17206336557865143
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12243768572807312
epoch£º476	 i:8 	 global-step:9528	 l-p:0.06836866587400436
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12445208430290222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1143, 5.1134, 5.1143],
        [5.1143, 5.2202, 5.0761],
        [5.1143, 5.0709, 5.0939],
        [5.1143, 5.0306, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12913255393505096 
model_pd.l_d.mean(): -19.334665298461914 
model_pd.lagr.mean(): -19.20553207397461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.0603], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12913255393505096
epoch£º477	 i:1 	 global-step:9541	 l-p:0.13490436971187592
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11051124334335327
epoch£º477	 i:3 	 global-step:9543	 l-p:0.1353757381439209
epoch£º477	 i:4 	 global-step:9544	 l-p:0.1119823232293129
epoch£º477	 i:5 	 global-step:9545	 l-p:0.03386829420924187
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.05553958937525749
epoch£º477	 i:7 	 global-step:9547	 l-p:0.13108055293560028
epoch£º477	 i:8 	 global-step:9548	 l-p:0.1600005328655243
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16208671033382416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9411, 4.9411, 4.9411],
        [4.9411, 5.3502, 5.3717],
        [4.9411, 4.9411, 4.9411],
        [4.9411, 4.8651, 4.7491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09370779991149902 
model_pd.l_d.mean(): -18.751955032348633 
model_pd.lagr.mean(): -18.658246994018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5953], device='cuda:0')), ('power', tensor([-19.5650], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.09370779991149902
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16554725170135498
epoch£º478	 i:2 	 global-step:9562	 l-p:0.11536296457052231
epoch£º478	 i:3 	 global-step:9563	 l-p:0.18761740624904633
epoch£º478	 i:4 	 global-step:9564	 l-p:0.13446013629436493
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13976004719734192
epoch£º478	 i:6 	 global-step:9566	 l-p:0.13192926347255707
epoch£º478	 i:7 	 global-step:9567	 l-p:0.160419762134552
epoch£º478	 i:8 	 global-step:9568	 l-p:0.12926261126995087
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15450838208198547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8936, 5.2779, 5.2846],
        [4.8936, 4.8767, 4.8904],
        [4.8936, 4.8110, 4.6952],
        [4.8936, 4.8216, 4.8422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.1258755773305893 
model_pd.l_d.mean(): -19.892562866210938 
model_pd.lagr.mean(): -19.766687393188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5562], device='cuda:0')), ('power', tensor([-20.6781], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.1258755773305893
epoch£º479	 i:1 	 global-step:9581	 l-p:0.12330985814332962
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1557377576828003
epoch£º479	 i:3 	 global-step:9583	 l-p:0.10197796672582626
epoch£º479	 i:4 	 global-step:9584	 l-p:0.15678906440734863
epoch£º479	 i:5 	 global-step:9585	 l-p:0.1973341554403305
epoch£º479	 i:6 	 global-step:9586	 l-p:0.1752176284790039
epoch£º479	 i:7 	 global-step:9587	 l-p:0.2078939825296402
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.015998877584934235
epoch£º479	 i:9 	 global-step:9589	 l-p:0.24263469874858856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8578, 4.8578, 4.8578],
        [4.8578, 4.7615, 4.7539],
        [4.8578, 4.8578, 4.8578],
        [4.8578, 4.7941, 4.8194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12722627818584442 
model_pd.l_d.mean(): -19.347124099731445 
model_pd.lagr.mean(): -19.219898223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5474], device='cuda:0')), ('power', tensor([-20.1176], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12722627818584442
epoch£º480	 i:1 	 global-step:9601	 l-p:0.13594959676265717
epoch£º480	 i:2 	 global-step:9602	 l-p:0.1794186234474182
epoch£º480	 i:3 	 global-step:9603	 l-p:0.1545122116804123
epoch£º480	 i:4 	 global-step:9604	 l-p:0.09147658944129944
epoch£º480	 i:5 	 global-step:9605	 l-p:0.17632915079593658
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15448422729969025
epoch£º480	 i:7 	 global-step:9607	 l-p:0.1452966183423996
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10975375026464462
epoch£º480	 i:9 	 global-step:9609	 l-p:0.09918037056922913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0321, 4.9976, 5.0198],
        [5.0321, 5.0321, 5.0321],
        [5.0321, 5.0279, 5.0318],
        [5.0321, 5.0321, 5.0321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.13657429814338684 
model_pd.l_d.mean(): -20.824806213378906 
model_pd.lagr.mean(): -20.688232421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4286], device='cuda:0')), ('power', tensor([-21.4901], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.13657429814338684
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13669346272945404
epoch£º481	 i:2 	 global-step:9622	 l-p:0.14125366508960724
epoch£º481	 i:3 	 global-step:9623	 l-p:0.16114817559719086
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.09968452155590057
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14088107645511627
epoch£º481	 i:6 	 global-step:9626	 l-p:0.140157550573349
epoch£º481	 i:7 	 global-step:9627	 l-p:0.13500958681106567
epoch£º481	 i:8 	 global-step:9628	 l-p:0.06876539438962936
epoch£º481	 i:9 	 global-step:9629	 l-p:0.1391318142414093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9653, 4.9425, 4.9598],
        [4.9653, 4.9159, 4.9414],
        [4.9653, 4.8770, 4.7822],
        [4.9653, 5.0323, 4.8752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.07800044119358063 
model_pd.l_d.mean(): -20.909345626831055 
model_pd.lagr.mean(): -20.831344604492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.5779], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.07800044119358063
epoch£º482	 i:1 	 global-step:9641	 l-p:0.11365241557359695
epoch£º482	 i:2 	 global-step:9642	 l-p:0.10096395015716553
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08960261195898056
epoch£º482	 i:4 	 global-step:9644	 l-p:0.17867600917816162
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11746516823768616
epoch£º482	 i:6 	 global-step:9646	 l-p:0.14203611016273499
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12900982797145844
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12235911935567856
epoch£º482	 i:9 	 global-step:9649	 l-p:0.09163673222064972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0729, 5.0729, 5.0729],
        [5.0729, 5.1601, 5.0089],
        [5.0729, 5.0724, 5.0729],
        [5.0729, 5.0729, 5.0729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.02909720316529274 
model_pd.l_d.mean(): -18.721887588500977 
model_pd.lagr.mean(): -18.75098419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5601], device='cuda:0')), ('power', tensor([-19.4986], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.02909720316529274
epoch£º483	 i:1 	 global-step:9661	 l-p:0.23024402558803558
epoch£º483	 i:2 	 global-step:9662	 l-p:0.15074849128723145
epoch£º483	 i:3 	 global-step:9663	 l-p:0.12121228873729706
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13646627962589264
epoch£º483	 i:5 	 global-step:9665	 l-p:0.013779125176370144
epoch£º483	 i:6 	 global-step:9666	 l-p:0.12226694822311401
epoch£º483	 i:7 	 global-step:9667	 l-p:0.10997569561004639
epoch£º483	 i:8 	 global-step:9668	 l-p:0.1513051837682724
epoch£º483	 i:9 	 global-step:9669	 l-p:0.13490119576454163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.0799, 5.3042, 5.2041],
        [5.0799, 4.9991, 4.9094],
        [5.0799, 5.0397, 4.9011],
        [5.0799, 5.3295, 5.2425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.1314135193824768 
model_pd.l_d.mean(): -19.638086318969727 
model_pd.lagr.mean(): -19.506671905517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5476], device='cuda:0')), ('power', tensor([-20.4121], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.1314135193824768
epoch£º484	 i:1 	 global-step:9681	 l-p:0.14075925946235657
epoch£º484	 i:2 	 global-step:9682	 l-p:0.23757031559944153
epoch£º484	 i:3 	 global-step:9683	 l-p:0.05438481271266937
epoch£º484	 i:4 	 global-step:9684	 l-p:0.07743652909994125
epoch£º484	 i:5 	 global-step:9685	 l-p:-0.10402114689350128
epoch£º484	 i:6 	 global-step:9686	 l-p:0.1339045912027359
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12884831428527832
epoch£º484	 i:8 	 global-step:9688	 l-p:0.15020182728767395
epoch£º484	 i:9 	 global-step:9689	 l-p:0.1249990239739418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0217, 4.9896, 5.0112],
        [5.0217, 4.9431, 4.9535],
        [5.0217, 5.0217, 5.0217],
        [5.0217, 5.0217, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.1305304914712906 
model_pd.l_d.mean(): -21.121726989746094 
model_pd.lagr.mean(): -20.991195678710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3740], device='cuda:0')), ('power', tensor([-21.7345], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.1305304914712906
epoch£º485	 i:1 	 global-step:9701	 l-p:0.09104930609464645
epoch£º485	 i:2 	 global-step:9702	 l-p:0.12021760642528534
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12877361476421356
epoch£º485	 i:4 	 global-step:9704	 l-p:0.06190318986773491
epoch£º485	 i:5 	 global-step:9705	 l-p:0.12732186913490295
epoch£º485	 i:6 	 global-step:9706	 l-p:0.1651909053325653
epoch£º485	 i:7 	 global-step:9707	 l-p:0.24331678450107574
epoch£º485	 i:8 	 global-step:9708	 l-p:0.16862422227859497
epoch£º485	 i:9 	 global-step:9709	 l-p:0.4790818393230438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8663, 4.8082, 4.6586],
        [4.8663, 4.7614, 4.7297],
        [4.8663, 5.0201, 4.8901],
        [4.8663, 4.8059, 4.8326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.16814632713794708 
model_pd.l_d.mean(): -20.036893844604492 
model_pd.lagr.mean(): -19.86874771118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5684], device='cuda:0')), ('power', tensor([-20.8364], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.16814632713794708
epoch£º486	 i:1 	 global-step:9721	 l-p:0.12539570033550262
epoch£º486	 i:2 	 global-step:9722	 l-p:0.1456233412027359
epoch£º486	 i:3 	 global-step:9723	 l-p:0.22361792623996735
epoch£º486	 i:4 	 global-step:9724	 l-p:0.13091176748275757
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12329844385385513
epoch£º486	 i:6 	 global-step:9726	 l-p:0.169471874833107
epoch£º486	 i:7 	 global-step:9727	 l-p:0.2486855685710907
epoch£º486	 i:8 	 global-step:9728	 l-p:0.09958213567733765
epoch£º486	 i:9 	 global-step:9729	 l-p:0.13470211625099182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9109, 4.8085, 4.7754],
        [4.9109, 4.9107, 4.9109],
        [4.9109, 4.9109, 4.9109],
        [4.9109, 4.9289, 4.7634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.12059596925973892 
model_pd.l_d.mean(): -20.162137985229492 
model_pd.lagr.mean(): -20.041542053222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5222], device='cuda:0')), ('power', tensor([-20.9159], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.12059596925973892
epoch£º487	 i:1 	 global-step:9741	 l-p:0.19093383848667145
epoch£º487	 i:2 	 global-step:9742	 l-p:0.13030292093753815
epoch£º487	 i:3 	 global-step:9743	 l-p:0.027667516842484474
epoch£º487	 i:4 	 global-step:9744	 l-p:0.16923293471336365
epoch£º487	 i:5 	 global-step:9745	 l-p:0.11415578424930573
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13933488726615906
epoch£º487	 i:7 	 global-step:9747	 l-p:0.1468561291694641
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13291147351264954
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13256977498531342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0246, 5.0234, 5.0245],
        [5.0246, 4.9623, 4.8325],
        [5.0246, 5.0025, 5.0193],
        [5.0246, 5.3755, 5.3507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.1374725103378296 
model_pd.l_d.mean(): -20.40378189086914 
model_pd.lagr.mean(): -20.26630973815918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4802], device='cuda:0')), ('power', tensor([-21.1172], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.1374725103378296
epoch£º488	 i:1 	 global-step:9761	 l-p:0.09165515750646591
epoch£º488	 i:2 	 global-step:9762	 l-p:0.16055256128311157
epoch£º488	 i:3 	 global-step:9763	 l-p:0.11610196530818939
epoch£º488	 i:4 	 global-step:9764	 l-p:0.15376944839954376
epoch£º488	 i:5 	 global-step:9765	 l-p:0.0423152856528759
epoch£º488	 i:6 	 global-step:9766	 l-p:0.10434284061193466
epoch£º488	 i:7 	 global-step:9767	 l-p:0.10218235105276108
epoch£º488	 i:8 	 global-step:9768	 l-p:0.1249682605266571
epoch£º488	 i:9 	 global-step:9769	 l-p:0.16900552809238434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9662, 4.9331, 4.9554],
        [4.9662, 4.9659, 4.9662],
        [4.9662, 4.9300, 4.9535],
        [4.9662, 4.9662, 4.9662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.11878804117441177 
model_pd.l_d.mean(): -20.744346618652344 
model_pd.lagr.mean(): -20.625558853149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4434], device='cuda:0')), ('power', tensor([-21.4239], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.11878804117441177
epoch£º489	 i:1 	 global-step:9781	 l-p:0.12328197807073593
epoch£º489	 i:2 	 global-step:9782	 l-p:0.04358886182308197
epoch£º489	 i:3 	 global-step:9783	 l-p:0.12833192944526672
epoch£º489	 i:4 	 global-step:9784	 l-p:0.18009954690933228
epoch£º489	 i:5 	 global-step:9785	 l-p:0.17604541778564453
epoch£º489	 i:6 	 global-step:9786	 l-p:0.14028088748455048
epoch£º489	 i:7 	 global-step:9787	 l-p:0.14149372279644012
epoch£º489	 i:8 	 global-step:9788	 l-p:0.11367005854845047
epoch£º489	 i:9 	 global-step:9789	 l-p:0.13429592549800873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0075, 5.0075, 5.0075],
        [5.0075, 5.0071, 5.0075],
        [5.0075, 5.0068, 5.0075],
        [5.0075, 5.0075, 5.0075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1393815129995346 
model_pd.l_d.mean(): -20.840185165405273 
model_pd.lagr.mean(): -20.700803756713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4256], device='cuda:0')), ('power', tensor([-21.5027], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1393815129995346
epoch£º490	 i:1 	 global-step:9801	 l-p:0.11371692270040512
epoch£º490	 i:2 	 global-step:9802	 l-p:0.16689662635326385
epoch£º490	 i:3 	 global-step:9803	 l-p:0.11052045226097107
epoch£º490	 i:4 	 global-step:9804	 l-p:0.09352526813745499
epoch£º490	 i:5 	 global-step:9805	 l-p:0.11022146791219711
epoch£º490	 i:6 	 global-step:9806	 l-p:0.10190354287624359
epoch£º490	 i:7 	 global-step:9807	 l-p:0.11855409294366837
epoch£º490	 i:8 	 global-step:9808	 l-p:0.3289947807788849
epoch£º490	 i:9 	 global-step:9809	 l-p:0.13075828552246094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0660, 5.4848, 5.5033],
        [5.0660, 4.9728, 4.9490],
        [5.0660, 5.0281, 5.0516],
        [5.0660, 5.2455, 5.1230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.5080270171165466 
model_pd.l_d.mean(): -19.42618179321289 
model_pd.lagr.mean(): -18.918155670166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4932], device='cuda:0')), ('power', tensor([-20.1422], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.5080270171165466
epoch£º491	 i:1 	 global-step:9821	 l-p:0.1276598870754242
epoch£º491	 i:2 	 global-step:9822	 l-p:0.13426099717617035
epoch£º491	 i:3 	 global-step:9823	 l-p:0.13906250894069672
epoch£º491	 i:4 	 global-step:9824	 l-p:0.08870337903499603
epoch£º491	 i:5 	 global-step:9825	 l-p:0.1127636581659317
epoch£º491	 i:6 	 global-step:9826	 l-p:0.1367286890745163
epoch£º491	 i:7 	 global-step:9827	 l-p:0.016015004366636276
epoch£º491	 i:8 	 global-step:9828	 l-p:0.12225984036922455
epoch£º491	 i:9 	 global-step:9829	 l-p:0.12417938560247421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.1419, 5.1419],
        [5.1419, 5.0826, 4.9599],
        [5.1419, 5.1346, 5.1411],
        [5.1419, 5.1365, 5.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.08426837623119354 
model_pd.l_d.mean(): -20.668895721435547 
model_pd.lagr.mean(): -20.584627151489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4211], device='cuda:0')), ('power', tensor([-21.3249], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.08426837623119354
epoch£º492	 i:1 	 global-step:9841	 l-p:0.1312459409236908
epoch£º492	 i:2 	 global-step:9842	 l-p:0.09795159846544266
epoch£º492	 i:3 	 global-step:9843	 l-p:0.08515214920043945
epoch£º492	 i:4 	 global-step:9844	 l-p:0.132708340883255
epoch£º492	 i:5 	 global-step:9845	 l-p:0.048364315181970596
epoch£º492	 i:6 	 global-step:9846	 l-p:0.1453338861465454
epoch£º492	 i:7 	 global-step:9847	 l-p:0.12996171414852142
epoch£º492	 i:8 	 global-step:9848	 l-p:3.055314302444458
epoch£º492	 i:9 	 global-step:9849	 l-p:0.20869570970535278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1036, 5.8325, 6.0909],
        [5.1036, 5.1033, 5.1036],
        [5.1036, 5.0145, 5.0025],
        [5.1036, 5.6627, 5.7823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.10630826652050018 
model_pd.l_d.mean(): -17.595623016357422 
model_pd.lagr.mean(): -17.489315032958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6328], device='cuda:0')), ('power', tensor([-18.4343], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.10630826652050018
epoch£º493	 i:1 	 global-step:9861	 l-p:0.11660756170749664
epoch£º493	 i:2 	 global-step:9862	 l-p:0.13511578738689423
epoch£º493	 i:3 	 global-step:9863	 l-p:0.20338279008865356
epoch£º493	 i:4 	 global-step:9864	 l-p:0.13385693728923798
epoch£º493	 i:5 	 global-step:9865	 l-p:0.01893777772784233
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11655198037624359
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13527217507362366
epoch£º493	 i:8 	 global-step:9868	 l-p:-0.17011214792728424
epoch£º493	 i:9 	 global-step:9869	 l-p:0.18065275251865387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0943, 5.0280, 4.9048],
        [5.0943, 5.7494, 5.9463],
        [5.0943, 5.0155, 5.0256],
        [5.0943, 5.0771, 5.0909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.11804074794054031 
model_pd.l_d.mean(): -18.22035789489746 
model_pd.lagr.mean(): -18.102317810058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6014], device='cuda:0')), ('power', tensor([-19.0338], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.11804074794054031
epoch£º494	 i:1 	 global-step:9881	 l-p:0.14699502289295197
epoch£º494	 i:2 	 global-step:9882	 l-p:1.1534810066223145
epoch£º494	 i:3 	 global-step:9883	 l-p:0.06479518860578537
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1383107602596283
epoch£º494	 i:5 	 global-step:9885	 l-p:0.1660437136888504
epoch£º494	 i:6 	 global-step:9886	 l-p:0.025106258690357208
epoch£º494	 i:7 	 global-step:9887	 l-p:0.13394686579704285
epoch£º494	 i:8 	 global-step:9888	 l-p:-0.6034566164016724
epoch£º494	 i:9 	 global-step:9889	 l-p:0.13832402229309082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2091, 5.1266, 5.1188],
        [5.2091, 5.2086, 5.2091],
        [5.2091, 5.5636, 5.5314],
        [5.2091, 5.1984, 5.2076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.1256679743528366 
model_pd.l_d.mean(): -20.40424346923828 
model_pd.lagr.mean(): -20.278575897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.0473], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.1256679743528366
epoch£º495	 i:1 	 global-step:9901	 l-p:0.12163293361663818
epoch£º495	 i:2 	 global-step:9902	 l-p:0.13014478981494904
epoch£º495	 i:3 	 global-step:9903	 l-p:0.12188436090946198
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.0800645649433136
epoch£º495	 i:5 	 global-step:9905	 l-p:-0.0791541114449501
epoch£º495	 i:6 	 global-step:9906	 l-p:0.16131626069545746
epoch£º495	 i:7 	 global-step:9907	 l-p:0.1189703419804573
epoch£º495	 i:8 	 global-step:9908	 l-p:0.13337388634681702
epoch£º495	 i:9 	 global-step:9909	 l-p:0.10144540667533875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1355, 5.1355, 5.1355],
        [5.1355, 5.0776, 5.1011],
        [5.1355, 5.1355, 5.1355],
        [5.1355, 5.4694, 5.4270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.13019470870494843 
model_pd.l_d.mean(): -20.562040328979492 
model_pd.lagr.mean(): -20.43184471130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4206], device='cuda:0')), ('power', tensor([-21.2164], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.13019470870494843
epoch£º496	 i:1 	 global-step:9921	 l-p:0.12152253836393356
epoch£º496	 i:2 	 global-step:9922	 l-p:0.07010108977556229
epoch£º496	 i:3 	 global-step:9923	 l-p:0.16443157196044922
epoch£º496	 i:4 	 global-step:9924	 l-p:0.2615435719490051
epoch£º496	 i:5 	 global-step:9925	 l-p:0.10079745203256607
epoch£º496	 i:6 	 global-step:9926	 l-p:0.12693901360034943
epoch£º496	 i:7 	 global-step:9927	 l-p:0.5520612597465515
epoch£º496	 i:8 	 global-step:9928	 l-p:0.15510760247707367
epoch£º496	 i:9 	 global-step:9929	 l-p:0.11629735678434372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1218, 5.0454, 4.9376],
        [5.1218, 5.1218, 5.1218],
        [5.1218, 5.1218, 5.1218],
        [5.1218, 5.1203, 5.1218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.13159924745559692 
model_pd.l_d.mean(): -20.282350540161133 
model_pd.lagr.mean(): -20.1507511138916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4672], device='cuda:0')), ('power', tensor([-20.9812], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.13159924745559692
epoch£º497	 i:1 	 global-step:9941	 l-p:0.255550354719162
epoch£º497	 i:2 	 global-step:9942	 l-p:21.726720809936523
epoch£º497	 i:3 	 global-step:9943	 l-p:0.08318538218736649
epoch£º497	 i:4 	 global-step:9944	 l-p:0.12103906273841858
epoch£º497	 i:5 	 global-step:9945	 l-p:0.1326732188463211
epoch£º497	 i:6 	 global-step:9946	 l-p:0.08078654855489731
epoch£º497	 i:7 	 global-step:9947	 l-p:0.13280275464057922
epoch£º497	 i:8 	 global-step:9948	 l-p:0.05001230537891388
epoch£º497	 i:9 	 global-step:9949	 l-p:0.12678830325603485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1317, 5.0886, 5.1132],
        [5.1317, 5.5640, 5.5870],
        [5.1317, 5.3084, 5.1825],
        [5.1317, 5.1245, 5.1310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.18710163235664368 
model_pd.l_d.mean(): -20.334766387939453 
model_pd.lagr.mean(): -20.14766502380371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.0173], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.18710163235664368
epoch£º498	 i:1 	 global-step:9961	 l-p:0.047224387526512146
epoch£º498	 i:2 	 global-step:9962	 l-p:0.11355183273553848
epoch£º498	 i:3 	 global-step:9963	 l-p:0.1285315752029419
epoch£º498	 i:4 	 global-step:9964	 l-p:0.11728159338235855
epoch£º498	 i:5 	 global-step:9965	 l-p:0.11365390568971634
epoch£º498	 i:6 	 global-step:9966	 l-p:0.13262839615345
epoch£º498	 i:7 	 global-step:9967	 l-p:0.15995146334171295
epoch£º498	 i:8 	 global-step:9968	 l-p:0.17120009660720825
epoch£º498	 i:9 	 global-step:9969	 l-p:0.047623634338378906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1312, 5.1304, 5.1312],
        [5.1312, 5.5313, 5.5320],
        [5.1312, 5.1312, 5.1312],
        [5.1312, 5.1312, 5.1312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.14326030015945435 
model_pd.l_d.mean(): -20.955347061157227 
model_pd.lagr.mean(): -20.81208610534668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3660], device='cuda:0')), ('power', tensor([-21.5581], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.14326030015945435
epoch£º499	 i:1 	 global-step:9981	 l-p:0.09668796509504318
epoch£º499	 i:2 	 global-step:9982	 l-p:0.3411042392253876
epoch£º499	 i:3 	 global-step:9983	 l-p:0.13555556535720825
epoch£º499	 i:4 	 global-step:9984	 l-p:0.179255411028862
epoch£º499	 i:5 	 global-step:9985	 l-p:-0.004421348683536053
epoch£º499	 i:6 	 global-step:9986	 l-p:0.11942841857671738
epoch£º499	 i:7 	 global-step:9987	 l-p:0.0417446605861187
epoch£º499	 i:8 	 global-step:9988	 l-p:0.12798164784908295
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12192437797784805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1700, 5.0976, 5.1125],
        [5.1700, 5.0956, 4.9901],
        [5.1700, 5.1274, 5.1516],
        [5.1700, 5.2906, 5.1442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.024107493460178375 
model_pd.l_d.mean(): -20.779491424560547 
model_pd.lagr.mean(): -20.75538444519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3938], device='cuda:0')), ('power', tensor([-21.4087], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.024107493460178375
epoch£º500	 i:1 	 global-step:10001	 l-p:0.12496636807918549
epoch£º500	 i:2 	 global-step:10002	 l-p:0.0020318173337727785
epoch£º500	 i:3 	 global-step:10003	 l-p:0.11665406078100204
epoch£º500	 i:4 	 global-step:10004	 l-p:0.11347745358943939
epoch£º500	 i:5 	 global-step:10005	 l-p:0.12031007558107376
epoch£º500	 i:6 	 global-step:10006	 l-p:0.1156201884150505
epoch£º500	 i:7 	 global-step:10007	 l-p:0.13765761256217957
epoch£º500	 i:8 	 global-step:10008	 l-p:0.10099399089813232
epoch£º500	 i:9 	 global-step:10009	 l-p:0.13090780377388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.0492, 5.0486],
        [5.1350, 5.0603, 4.9478],
        [5.1350, 5.0769, 4.9449],
        [5.1350, 5.1095, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.10627236217260361 
model_pd.l_d.mean(): -19.960119247436523 
model_pd.lagr.mean(): -19.85384750366211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.6669], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.10627236217260361
epoch£º501	 i:1 	 global-step:10021	 l-p:0.1524285227060318
epoch£º501	 i:2 	 global-step:10022	 l-p:0.11075317859649658
epoch£º501	 i:3 	 global-step:10023	 l-p:0.1078861802816391
epoch£º501	 i:4 	 global-step:10024	 l-p:0.13445349037647247
epoch£º501	 i:5 	 global-step:10025	 l-p:0.1379023641347885
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11377562582492828
epoch£º501	 i:7 	 global-step:10027	 l-p:0.13265104591846466
epoch£º501	 i:8 	 global-step:10028	 l-p:0.0608522891998291
epoch£º501	 i:9 	 global-step:10029	 l-p:0.13206692039966583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0313, 4.9299, 4.8614],
        [5.0313, 5.0313, 5.0313],
        [5.0313, 5.3891, 5.3659],
        [5.0313, 5.0311, 5.0313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.14187105000019073 
model_pd.l_d.mean(): -19.596006393432617 
model_pd.lagr.mean(): -19.45413589477539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5189], device='cuda:0')), ('power', tensor([-20.3402], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.14187105000019073
epoch£º502	 i:1 	 global-step:10041	 l-p:0.0016915464075282216
epoch£º502	 i:2 	 global-step:10042	 l-p:0.17194488644599915
epoch£º502	 i:3 	 global-step:10043	 l-p:0.15233495831489563
epoch£º502	 i:4 	 global-step:10044	 l-p:0.12557464838027954
epoch£º502	 i:5 	 global-step:10045	 l-p:0.15693756937980652
epoch£º502	 i:6 	 global-step:10046	 l-p:0.04362980276346207
epoch£º502	 i:7 	 global-step:10047	 l-p:0.13467852771282196
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13454395532608032
epoch£º502	 i:9 	 global-step:10049	 l-p:0.11968880146741867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9697, 4.9696, 4.9697],
        [4.9697, 5.2899, 5.2450],
        [4.9697, 4.9697, 4.9697],
        [4.9697, 4.9461, 4.9641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.1540866494178772 
model_pd.l_d.mean(): -19.812328338623047 
model_pd.lagr.mean(): -19.658241271972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5076], device='cuda:0')), ('power', tensor([-20.5473], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.1540866494178772
epoch£º503	 i:1 	 global-step:10061	 l-p:0.10944908112287521
epoch£º503	 i:2 	 global-step:10062	 l-p:0.11483148485422134
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13477039337158203
epoch£º503	 i:4 	 global-step:10064	 l-p:0.0905461311340332
epoch£º503	 i:5 	 global-step:10065	 l-p:0.17334304749965668
epoch£º503	 i:6 	 global-step:10066	 l-p:0.1691962033510208
epoch£º503	 i:7 	 global-step:10067	 l-p:0.1956985592842102
epoch£º503	 i:8 	 global-step:10068	 l-p:0.12683996558189392
epoch£º503	 i:9 	 global-step:10069	 l-p:0.009098672308027744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0041, 4.9931, 5.0027],
        [5.0041, 4.9126, 4.7998],
        [5.0041, 4.9746, 4.9957],
        [5.0041, 4.9119, 4.9159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.07167232036590576 
model_pd.l_d.mean(): -20.41912078857422 
model_pd.lagr.mean(): -20.347448348999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-21.1440], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.07167232036590576
epoch£º504	 i:1 	 global-step:10081	 l-p:0.10955777764320374
epoch£º504	 i:2 	 global-step:10082	 l-p:0.14885492622852325
epoch£º504	 i:3 	 global-step:10083	 l-p:0.11444028466939926
epoch£º504	 i:4 	 global-step:10084	 l-p:0.14657293260097504
epoch£º504	 i:5 	 global-step:10085	 l-p:-0.06676409393548965
epoch£º504	 i:6 	 global-step:10086	 l-p:0.12535810470581055
epoch£º504	 i:7 	 global-step:10087	 l-p:0.23911772668361664
epoch£º504	 i:8 	 global-step:10088	 l-p:0.14164644479751587
epoch£º504	 i:9 	 global-step:10089	 l-p:0.1331188827753067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0758, 4.9781, 4.9603],
        [5.0758, 5.0281, 5.0544],
        [5.0758, 5.0714, 5.0755],
        [5.0758, 5.0118, 5.0366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.15933942794799805 
model_pd.l_d.mean(): -20.602087020874023 
model_pd.lagr.mean(): -20.442747116088867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4393], device='cuda:0')), ('power', tensor([-21.2759], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.15933942794799805
epoch£º505	 i:1 	 global-step:10101	 l-p:0.11983401328325272
epoch£º505	 i:2 	 global-step:10102	 l-p:0.09340556710958481
epoch£º505	 i:3 	 global-step:10103	 l-p:0.14441373944282532
epoch£º505	 i:4 	 global-step:10104	 l-p:0.03455204516649246
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13104045391082764
epoch£º505	 i:6 	 global-step:10106	 l-p:-0.7995014786720276
epoch£º505	 i:7 	 global-step:10107	 l-p:0.142982617020607
epoch£º505	 i:8 	 global-step:10108	 l-p:0.1140889897942543
epoch£º505	 i:9 	 global-step:10109	 l-p:0.09408046305179596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0823, 5.0714, 5.0808],
        [5.0823, 5.7572, 5.9689],
        [5.0823, 5.6112, 5.7064],
        [5.0823, 5.0101, 4.8794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.09140298515558243 
model_pd.l_d.mean(): -19.721277236938477 
model_pd.lagr.mean(): -19.62987518310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.4677], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.09140298515558243
epoch£º506	 i:1 	 global-step:10121	 l-p:0.10088281333446503
epoch£º506	 i:2 	 global-step:10122	 l-p:0.23529736697673798
epoch£º506	 i:3 	 global-step:10123	 l-p:0.11846734583377838
epoch£º506	 i:4 	 global-step:10124	 l-p:0.05956762284040451
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11892779171466827
epoch£º506	 i:6 	 global-step:10126	 l-p:0.18414190411567688
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10777672380208969
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11983410269021988
epoch£º506	 i:9 	 global-step:10129	 l-p:0.14018920063972473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1871, 5.1446, 5.1693],
        [5.1871, 5.0924, 5.0469],
        [5.1871, 5.0988, 5.0147],
        [5.1871, 5.8323, 6.0118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.14324019849300385 
model_pd.l_d.mean(): -20.367151260375977 
model_pd.lagr.mean(): -20.22391128540039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4397], device='cuda:0')), ('power', tensor([-21.0389], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.14324019849300385
epoch£º507	 i:1 	 global-step:10141	 l-p:0.13520917296409607
epoch£º507	 i:2 	 global-step:10142	 l-p:0.1232510432600975
epoch£º507	 i:3 	 global-step:10143	 l-p:0.23295395076274872
epoch£º507	 i:4 	 global-step:10144	 l-p:0.12630786001682281
epoch£º507	 i:5 	 global-step:10145	 l-p:0.08050988614559174
epoch£º507	 i:6 	 global-step:10146	 l-p:0.06389502435922623
epoch£º507	 i:7 	 global-step:10147	 l-p:0.1757451593875885
epoch£º507	 i:8 	 global-step:10148	 l-p:-0.05592921003699303
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12045814841985703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1998, 5.1773, 5.1944],
        [5.1998, 5.1976, 5.1997],
        [5.1998, 5.1992, 5.1998],
        [5.1998, 5.2229, 5.0594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09480065852403641 
model_pd.l_d.mean(): -19.458696365356445 
model_pd.lagr.mean(): -19.363895416259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4877], device='cuda:0')), ('power', tensor([-20.1695], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09480065852403641
epoch£º508	 i:1 	 global-step:10161	 l-p:0.13064922392368317
epoch£º508	 i:2 	 global-step:10162	 l-p:0.17093883454799652
epoch£º508	 i:3 	 global-step:10163	 l-p:0.31187203526496887
epoch£º508	 i:4 	 global-step:10164	 l-p:0.3483964204788208
epoch£º508	 i:5 	 global-step:10165	 l-p:0.1334337741136551
epoch£º508	 i:6 	 global-step:10166	 l-p:0.23281455039978027
epoch£º508	 i:7 	 global-step:10167	 l-p:0.10806657373905182
epoch£º508	 i:8 	 global-step:10168	 l-p:0.14083810150623322
epoch£º508	 i:9 	 global-step:10169	 l-p:0.12697821855545044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2516, 5.5154, 5.4264],
        [5.2516, 5.2512, 5.2516],
        [5.2516, 6.0078, 6.2746],
        [5.2516, 5.2145, 5.2377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.19880114495754242 
model_pd.l_d.mean(): -19.868854522705078 
model_pd.lagr.mean(): -19.670053482055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.5343], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.19880114495754242
epoch£º509	 i:1 	 global-step:10181	 l-p:0.2014002501964569
epoch£º509	 i:2 	 global-step:10182	 l-p:0.117665134370327
epoch£º509	 i:3 	 global-step:10183	 l-p:0.13515330851078033
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09234807640314102
epoch£º509	 i:5 	 global-step:10185	 l-p:0.1231023445725441
epoch£º509	 i:6 	 global-step:10186	 l-p:0.1148073673248291
epoch£º509	 i:7 	 global-step:10187	 l-p:-0.006943731103092432
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10874795913696289
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14255571365356445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1262, 5.1249, 5.1262],
        [5.1262, 5.2616, 5.1161],
        [5.1262, 5.2984, 5.1671],
        [5.1262, 5.1430, 4.9756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11734843254089355 
model_pd.l_d.mean(): -18.69815444946289 
model_pd.lagr.mean(): -18.580806732177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4750], device='cuda:0')), ('power', tensor([-19.3876], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11734843254089355
epoch£º510	 i:1 	 global-step:10201	 l-p:0.16818971931934357
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13187485933303833
epoch£º510	 i:3 	 global-step:10203	 l-p:0.050268225371837616
epoch£º510	 i:4 	 global-step:10204	 l-p:0.1281309276819229
epoch£º510	 i:5 	 global-step:10205	 l-p:0.13685567677021027
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14899705350399017
epoch£º510	 i:7 	 global-step:10207	 l-p:0.13391058146953583
epoch£º510	 i:8 	 global-step:10208	 l-p:0.16353565454483032
epoch£º510	 i:9 	 global-step:10209	 l-p:0.16673484444618225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9308, 4.8583, 4.8850],
        [4.9308, 4.9821, 4.8101],
        [4.9308, 4.8968, 4.9204],
        [4.9308, 4.9308, 4.9308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.18394266068935394 
model_pd.l_d.mean(): -20.096359252929688 
model_pd.lagr.mean(): -19.912416458129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5489], device='cuda:0')), ('power', tensor([-20.8766], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.18394266068935394
epoch£º511	 i:1 	 global-step:10221	 l-p:0.14697612822055817
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12662941217422485
epoch£º511	 i:3 	 global-step:10223	 l-p:0.18182583153247833
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13499173521995544
epoch£º511	 i:5 	 global-step:10225	 l-p:0.17977091670036316
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12792766094207764
epoch£º511	 i:7 	 global-step:10227	 l-p:0.1608741730451584
epoch£º511	 i:8 	 global-step:10228	 l-p:0.06633561849594116
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14761106669902802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9494, 4.9378, 4.9479],
        [4.9494, 4.9080, 4.9343],
        [4.9494, 4.9863, 4.8121],
        [4.9494, 4.8509, 4.8556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.16214604675769806 
model_pd.l_d.mean(): -20.652889251708984 
model_pd.lagr.mean(): -20.49074363708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-21.3490], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.16214604675769806
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10120926797389984
epoch£º512	 i:2 	 global-step:10242	 l-p:0.19940124452114105
epoch£º512	 i:3 	 global-step:10243	 l-p:0.14023913443088531
epoch£º512	 i:4 	 global-step:10244	 l-p:0.10864036530256271
epoch£º512	 i:5 	 global-step:10245	 l-p:0.108207106590271
epoch£º512	 i:6 	 global-step:10246	 l-p:0.1295129805803299
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14390167593955994
epoch£º512	 i:8 	 global-step:10248	 l-p:0.1568954586982727
epoch£º512	 i:9 	 global-step:10249	 l-p:0.21195533871650696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9235, 4.8080, 4.7733],
        [4.9235, 5.2399, 5.1927],
        [4.9235, 4.8134, 4.7991],
        [4.9235, 4.9213, 4.9234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.1503324806690216 
model_pd.l_d.mean(): -20.485246658325195 
model_pd.lagr.mean(): -20.33491325378418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5065], device='cuda:0')), ('power', tensor([-21.2264], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.1503324806690216
epoch£º513	 i:1 	 global-step:10261	 l-p:0.1660016030073166
epoch£º513	 i:2 	 global-step:10262	 l-p:0.13886131346225739
epoch£º513	 i:3 	 global-step:10263	 l-p:0.14902940392494202
epoch£º513	 i:4 	 global-step:10264	 l-p:0.18767523765563965
epoch£º513	 i:5 	 global-step:10265	 l-p:0.13270194828510284
epoch£º513	 i:6 	 global-step:10266	 l-p:0.1262611448764801
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12828563153743744
epoch£º513	 i:8 	 global-step:10268	 l-p:0.06924881041049957
epoch£º513	 i:9 	 global-step:10269	 l-p:0.09862229973077774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0385, 5.5855, 5.6959],
        [5.0385, 5.3662, 5.3205],
        [5.0385, 4.9400, 4.9372],
        [5.0385, 5.0383, 5.0385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.1463557332754135 
model_pd.l_d.mean(): -20.451791763305664 
model_pd.lagr.mean(): -20.305435180664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-21.1445], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.1463557332754135
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14681918919086456
epoch£º514	 i:2 	 global-step:10282	 l-p:0.161793053150177
epoch£º514	 i:3 	 global-step:10283	 l-p:0.4377906322479248
epoch£º514	 i:4 	 global-step:10284	 l-p:0.08074388653039932
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13203245401382446
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11561500281095505
epoch£º514	 i:7 	 global-step:10287	 l-p:0.14100632071495056
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12363962084054947
epoch£º514	 i:9 	 global-step:10289	 l-p:0.13431981205940247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0269, 5.3187, 5.2510],
        [5.0269, 5.2694, 5.1729],
        [5.0269, 4.9757, 5.0037],
        [5.0269, 4.9998, 4.8298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.028930306434631348 
model_pd.l_d.mean(): -19.059396743774414 
model_pd.lagr.mean(): -19.030466079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5622], device='cuda:0')), ('power', tensor([-19.8419], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.028930306434631348
epoch£º515	 i:1 	 global-step:10301	 l-p:0.1050829067826271
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11647647619247437
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15916961431503296
epoch£º515	 i:4 	 global-step:10304	 l-p:0.1337990164756775
epoch£º515	 i:5 	 global-step:10305	 l-p:0.1346672773361206
epoch£º515	 i:6 	 global-step:10306	 l-p:0.10106325149536133
epoch£º515	 i:7 	 global-step:10307	 l-p:0.2845821976661682
epoch£º515	 i:8 	 global-step:10308	 l-p:0.2675943374633789
epoch£º515	 i:9 	 global-step:10309	 l-p:0.15289409458637238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8956, 4.8861, 4.8945],
        [4.8956, 4.7842, 4.7748],
        [4.8956, 5.1472, 5.0604],
        [4.8956, 4.8782, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.3891568183898926 
model_pd.l_d.mean(): -20.00943946838379 
model_pd.lagr.mean(): -19.620283126831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5654], device='cuda:0')), ('power', tensor([-20.8056], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.3891568183898926
epoch£º516	 i:1 	 global-step:10321	 l-p:0.20417815446853638
epoch£º516	 i:2 	 global-step:10322	 l-p:0.14058034121990204
epoch£º516	 i:3 	 global-step:10323	 l-p:0.1308789849281311
epoch£º516	 i:4 	 global-step:10324	 l-p:0.09782427549362183
epoch£º516	 i:5 	 global-step:10325	 l-p:0.17839719355106354
epoch£º516	 i:6 	 global-step:10326	 l-p:0.10832390189170837
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15347318351268768
epoch£º516	 i:8 	 global-step:10328	 l-p:0.13240604102611542
epoch£º516	 i:9 	 global-step:10329	 l-p:0.14127634465694427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9523, 4.9450, 4.9516],
        [4.9523, 4.8475, 4.7282],
        [4.9523, 4.8400, 4.7402],
        [4.9523, 4.9523, 4.9523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.10290020704269409 
model_pd.l_d.mean(): -20.053150177001953 
model_pd.lagr.mean(): -19.95025062561035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.7857], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.10290020704269409
epoch£º517	 i:1 	 global-step:10341	 l-p:0.13110233843326569
epoch£º517	 i:2 	 global-step:10342	 l-p:0.1778443157672882
epoch£º517	 i:3 	 global-step:10343	 l-p:0.1491536796092987
epoch£º517	 i:4 	 global-step:10344	 l-p:0.12298949062824249
epoch£º517	 i:5 	 global-step:10345	 l-p:0.129583939909935
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10160709917545319
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.3880138397216797
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08200210332870483
epoch£º517	 i:9 	 global-step:10349	 l-p:0.13895905017852783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1498, 5.3186, 5.1833],
        [5.1498, 5.1055, 4.9496],
        [5.1498, 5.0748, 5.0954],
        [5.1498, 5.0463, 5.0023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11549558490514755 
model_pd.l_d.mean(): -20.29888916015625 
model_pd.lagr.mean(): -20.183393478393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4617], device='cuda:0')), ('power', tensor([-20.9923], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11549558490514755
epoch£º518	 i:1 	 global-step:10361	 l-p:0.041877202689647675
epoch£º518	 i:2 	 global-step:10362	 l-p:0.03030632995069027
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12205938249826431
epoch£º518	 i:4 	 global-step:10364	 l-p:0.19288024306297302
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13591131567955017
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09207107126712799
epoch£º518	 i:7 	 global-step:10367	 l-p:0.12529127299785614
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12062840163707733
epoch£º518	 i:9 	 global-step:10369	 l-p:0.2428758144378662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2277, 5.2250, 5.2276],
        [5.2277, 5.9098, 6.1139],
        [5.2277, 5.2248, 5.2276],
        [5.2277, 5.1657, 5.1904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.12684187293052673 
model_pd.l_d.mean(): -20.41441535949707 
model_pd.lagr.mean(): -20.287572860717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4074], device='cuda:0')), ('power', tensor([-21.0536], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.12684187293052673
epoch£º519	 i:1 	 global-step:10381	 l-p:0.1738719344139099
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11095122992992401
epoch£º519	 i:3 	 global-step:10383	 l-p:0.14951619505882263
epoch£º519	 i:4 	 global-step:10384	 l-p:-0.053805895149707794
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.1889531910419464
epoch£º519	 i:6 	 global-step:10386	 l-p:0.11988794058561325
epoch£º519	 i:7 	 global-step:10387	 l-p:0.06507249921560287
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11347777396440506
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11591186374425888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1623, 5.1623, 5.1623],
        [5.1623, 5.6995, 5.7930],
        [5.1623, 5.1623, 5.1623],
        [5.1623, 5.1553, 5.1616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13374321162700653 
model_pd.l_d.mean(): -19.564735412597656 
model_pd.lagr.mean(): -19.430992126464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4445], device='cuda:0')), ('power', tensor([-20.2326], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13374321162700653
epoch£º520	 i:1 	 global-step:10401	 l-p:0.06667112559080124
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12096022069454193
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.4677288234233856
epoch£º520	 i:4 	 global-step:10404	 l-p:0.15235275030136108
epoch£º520	 i:5 	 global-step:10405	 l-p:0.15685811638832092
epoch£º520	 i:6 	 global-step:10406	 l-p:0.13229651749134064
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12877802550792694
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13899435102939606
epoch£º520	 i:9 	 global-step:10409	 l-p:0.16484317183494568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0827, 4.9739, 4.9031],
        [5.0827, 5.0560, 5.0758],
        [5.0827, 5.6769, 5.8197],
        [5.0827, 4.9745, 4.9418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.1344558149576187 
model_pd.l_d.mean(): -20.72676658630371 
model_pd.lagr.mean(): -20.592309951782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.3781], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.1344558149576187
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12359775602817535
epoch£º521	 i:2 	 global-step:10422	 l-p:-0.2512337863445282
epoch£º521	 i:3 	 global-step:10423	 l-p:0.1523355096578598
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11191459745168686
epoch£º521	 i:5 	 global-step:10425	 l-p:0.14523814618587494
epoch£º521	 i:6 	 global-step:10426	 l-p:0.1404794305562973
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14395223557949066
epoch£º521	 i:8 	 global-step:10428	 l-p:0.11287711560726166
epoch£º521	 i:9 	 global-step:10429	 l-p:0.21733839809894562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9293, 4.9256, 4.9291],
        [4.9293, 4.9293, 4.9293],
        [4.9293, 4.9208, 4.9284],
        [4.9293, 4.9198, 4.9282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.11057452857494354 
model_pd.l_d.mean(): -20.749034881591797 
model_pd.lagr.mean(): -20.638460159301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4757], device='cuda:0')), ('power', tensor([-21.4616], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.11057452857494354
epoch£º522	 i:1 	 global-step:10441	 l-p:0.1896267533302307
epoch£º522	 i:2 	 global-step:10442	 l-p:0.1923637092113495
epoch£º522	 i:3 	 global-step:10443	 l-p:0.1448419690132141
epoch£º522	 i:4 	 global-step:10444	 l-p:0.17121349275112152
epoch£º522	 i:5 	 global-step:10445	 l-p:0.16408149898052216
epoch£º522	 i:6 	 global-step:10446	 l-p:0.06327221542596817
epoch£º522	 i:7 	 global-step:10447	 l-p:0.10880783945322037
epoch£º522	 i:8 	 global-step:10448	 l-p:0.13245190680027008
epoch£º522	 i:9 	 global-step:10449	 l-p:0.11050357669591904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0384, 5.0208, 5.0352],
        [5.0384, 5.0384, 5.0384],
        [5.0384, 5.6145, 5.7457],
        [5.0384, 5.0382, 5.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.1046924963593483 
model_pd.l_d.mean(): -18.284536361694336 
model_pd.lagr.mean(): -18.17984390258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6156], device='cuda:0')), ('power', tensor([-19.1131], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.1046924963593483
epoch£º523	 i:1 	 global-step:10461	 l-p:0.09292773902416229
epoch£º523	 i:2 	 global-step:10462	 l-p:0.8345587849617004
epoch£º523	 i:3 	 global-step:10463	 l-p:0.1499394178390503
epoch£º523	 i:4 	 global-step:10464	 l-p:0.1356034278869629
epoch£º523	 i:5 	 global-step:10465	 l-p:0.009709048084914684
epoch£º523	 i:6 	 global-step:10466	 l-p:0.16356170177459717
epoch£º523	 i:7 	 global-step:10467	 l-p:0.15005522966384888
epoch£º523	 i:8 	 global-step:10468	 l-p:0.1430569291114807
epoch£º523	 i:9 	 global-step:10469	 l-p:0.14229165017604828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9886, 4.9777, 4.9872],
        [4.9886, 5.2574, 5.1752],
        [4.9886, 4.9883, 4.9886],
        [4.9886, 4.9782, 4.9873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.20004241168498993 
model_pd.l_d.mean(): -20.3266544342041 
model_pd.lagr.mean(): -20.126611709594727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-21.0476], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.20004241168498993
epoch£º524	 i:1 	 global-step:10481	 l-p:0.13141021132469177
epoch£º524	 i:2 	 global-step:10482	 l-p:0.05018939822912216
epoch£º524	 i:3 	 global-step:10483	 l-p:0.12374040484428406
epoch£º524	 i:4 	 global-step:10484	 l-p:0.13631026446819305
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13415928184986115
epoch£º524	 i:6 	 global-step:10486	 l-p:0.13630810379981995
epoch£º524	 i:7 	 global-step:10487	 l-p:0.12570926547050476
epoch£º524	 i:8 	 global-step:10488	 l-p:0.1544763594865799
epoch£º524	 i:9 	 global-step:10489	 l-p:0.11530213803052902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9794, 4.9771, 4.9793],
        [4.9794, 4.9793, 4.9794],
        [4.9794, 4.9762, 4.9792],
        [4.9794, 4.8593, 4.8109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16578689217567444 
model_pd.l_d.mean(): -19.725080490112305 
model_pd.lagr.mean(): -19.559293746948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5200], device='cuda:0')), ('power', tensor([-20.4718], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16578689217567444
epoch£º525	 i:1 	 global-step:10501	 l-p:0.07813482731580734
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12330428510904312
epoch£º525	 i:3 	 global-step:10503	 l-p:0.11749311536550522
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12300467491149902
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12364570051431656
epoch£º525	 i:6 	 global-step:10506	 l-p:0.17833922803401947
epoch£º525	 i:7 	 global-step:10507	 l-p:0.16458548605442047
epoch£º525	 i:8 	 global-step:10508	 l-p:0.13019441068172455
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10040552914142609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0054, 4.9947, 5.0041],
        [5.0054, 4.8956, 4.8860],
        [5.0054, 5.0781, 4.9061],
        [5.0054, 5.1503, 5.0030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.14796461164951324 
model_pd.l_d.mean(): -20.656681060791016 
model_pd.lagr.mean(): -20.508716583251953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-21.3610], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.14796461164951324
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14419202506542206
epoch£º526	 i:2 	 global-step:10522	 l-p:0.06353794038295746
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12740421295166016
epoch£º526	 i:4 	 global-step:10524	 l-p:0.12977594137191772
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13876181840896606
epoch£º526	 i:6 	 global-step:10526	 l-p:0.22280147671699524
epoch£º526	 i:7 	 global-step:10527	 l-p:0.14089462161064148
epoch£º526	 i:8 	 global-step:10528	 l-p:-0.08857668936252594
epoch£º526	 i:9 	 global-step:10529	 l-p:0.1369076371192932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1290, 5.1290, 5.1290],
        [5.1290, 5.0310, 4.9186],
        [5.1290, 5.1290, 5.1290],
        [5.1290, 5.1284, 5.1290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12973296642303467 
model_pd.l_d.mean(): -20.513839721679688 
model_pd.lagr.mean(): -20.38410758972168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4253], device='cuda:0')), ('power', tensor([-21.1724], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12973296642303467
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13674817979335785
epoch£º527	 i:2 	 global-step:10542	 l-p:0.17105424404144287
epoch£º527	 i:3 	 global-step:10543	 l-p:0.14386369287967682
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.03148748353123665
epoch£º527	 i:5 	 global-step:10545	 l-p:0.13332512974739075
epoch£º527	 i:6 	 global-step:10546	 l-p:0.13474637269973755
epoch£º527	 i:7 	 global-step:10547	 l-p:3.4764392375946045
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12544865906238556
epoch£º527	 i:9 	 global-step:10549	 l-p:0.13710491359233856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0165, 5.0436],
        [5.0598, 5.0534, 5.0592],
        [5.0598, 5.3774, 5.3210],
        [5.0598, 5.3776, 5.3212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.02036467008292675 
model_pd.l_d.mean(): -19.777877807617188 
model_pd.lagr.mean(): -19.798242568969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.5250], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.02036467008292675
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12161081284284592
epoch£º528	 i:2 	 global-step:10562	 l-p:0.12961812317371368
epoch£º528	 i:3 	 global-step:10563	 l-p:0.141926109790802
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12263696640729904
epoch£º528	 i:5 	 global-step:10565	 l-p:0.08414449542760849
epoch£º528	 i:6 	 global-step:10566	 l-p:0.1518465280532837
epoch£º528	 i:7 	 global-step:10567	 l-p:0.1401471346616745
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13258296251296997
epoch£º528	 i:9 	 global-step:10569	 l-p:0.15587414801120758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9516, 4.8388, 4.7131],
        [4.9516, 4.9073, 4.7257],
        [4.9516, 4.8756, 4.9047],
        [4.9516, 5.1623, 5.0469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.1050962507724762 
model_pd.l_d.mean(): -20.69199562072754 
model_pd.lagr.mean(): -20.586898803710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-21.4080], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.1050962507724762
epoch£º529	 i:1 	 global-step:10581	 l-p:0.10110124200582504
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14545167982578278
epoch£º529	 i:3 	 global-step:10583	 l-p:0.14438411593437195
epoch£º529	 i:4 	 global-step:10584	 l-p:0.21937908232212067
epoch£º529	 i:5 	 global-step:10585	 l-p:0.09213008731603622
epoch£º529	 i:6 	 global-step:10586	 l-p:0.16980308294296265
epoch£º529	 i:7 	 global-step:10587	 l-p:0.1655605286359787
epoch£º529	 i:8 	 global-step:10588	 l-p:0.17411987483501434
epoch£º529	 i:9 	 global-step:10589	 l-p:0.15016765892505646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9914, 4.9914, 4.9914],
        [4.9914, 4.8680, 4.7979],
        [4.9914, 4.9914, 4.9914],
        [4.9914, 5.1035, 4.9418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13645879924297333 
model_pd.l_d.mean(): -20.338499069213867 
model_pd.lagr.mean(): -20.20203971862793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-21.0696], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13645879924297333
epoch£º530	 i:1 	 global-step:10601	 l-p:0.13272744417190552
epoch£º530	 i:2 	 global-step:10602	 l-p:0.12416554242372513
epoch£º530	 i:3 	 global-step:10603	 l-p:0.09334970265626907
epoch£º530	 i:4 	 global-step:10604	 l-p:-1.8822027444839478
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13207674026489258
epoch£º530	 i:6 	 global-step:10606	 l-p:0.16105011105537415
epoch£º530	 i:7 	 global-step:10607	 l-p:0.10408633202314377
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13214367628097534
epoch£º530	 i:9 	 global-step:10609	 l-p:0.020319318398833275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1206, 5.1157, 5.1203],
        [5.1206, 5.1206, 5.1206],
        [5.1206, 5.1206, 5.1206],
        [5.1206, 5.1206, 5.1206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14021794497966766 
model_pd.l_d.mean(): -19.414567947387695 
model_pd.lagr.mean(): -19.274349212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.1064], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14021794497966766
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12468843907117844
epoch£º531	 i:2 	 global-step:10622	 l-p:0.11568207293748856
epoch£º531	 i:3 	 global-step:10623	 l-p:0.1327533721923828
epoch£º531	 i:4 	 global-step:10624	 l-p:0.06531386077404022
epoch£º531	 i:5 	 global-step:10625	 l-p:0.11884430795907974
epoch£º531	 i:6 	 global-step:10626	 l-p:0.25420308113098145
epoch£º531	 i:7 	 global-step:10627	 l-p:0.1323697417974472
epoch£º531	 i:8 	 global-step:10628	 l-p:0.1326182335615158
epoch£º531	 i:9 	 global-step:10629	 l-p:0.1315353363752365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1939, 5.2209, 5.0439],
        [5.1939, 5.1500, 5.1768],
        [5.1939, 5.2909, 5.1254],
        [5.1939, 5.1912, 5.1938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): -0.1477101445198059 
model_pd.l_d.mean(): -19.410921096801758 
model_pd.lagr.mean(): -19.558631896972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4679], device='cuda:0')), ('power', tensor([-20.1009], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:-0.1477101445198059
epoch£º532	 i:1 	 global-step:10641	 l-p:0.12751248478889465
epoch£º532	 i:2 	 global-step:10642	 l-p:0.19489887356758118
epoch£º532	 i:3 	 global-step:10643	 l-p:0.1239762157201767
epoch£º532	 i:4 	 global-step:10644	 l-p:3.22087025642395
epoch£º532	 i:5 	 global-step:10645	 l-p:0.14043304324150085
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11574582755565643
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11374572664499283
epoch£º532	 i:8 	 global-step:10648	 l-p:0.1013074442744255
epoch£º532	 i:9 	 global-step:10649	 l-p:0.1870858371257782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2776, 5.6132, 5.5580],
        [5.2776, 5.2746, 5.2775],
        [5.2776, 5.2547, 5.2724],
        [5.2776, 5.1735, 5.1284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12236983329057693 
model_pd.l_d.mean(): -19.707763671875 
model_pd.lagr.mean(): -19.58539390563965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4215], device='cuda:0')), ('power', tensor([-20.3537], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12236983329057693
epoch£º533	 i:1 	 global-step:10661	 l-p:0.1143542230129242
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11532457172870636
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14027845859527588
epoch£º533	 i:4 	 global-step:10664	 l-p:0.4508645534515381
epoch£º533	 i:5 	 global-step:10665	 l-p:0.13460280001163483
epoch£º533	 i:6 	 global-step:10666	 l-p:0.028267493471503258
epoch£º533	 i:7 	 global-step:10667	 l-p:0.07100316137075424
epoch£º533	 i:8 	 global-step:10668	 l-p:0.13606956601142883
epoch£º533	 i:9 	 global-step:10669	 l-p:0.2652587592601776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.0510, 5.0161],
        [5.1625, 5.0970, 4.9402],
        [5.1625, 5.1548, 5.1617],
        [5.1625, 5.0514, 4.9718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.06807554513216019 
model_pd.l_d.mean(): -19.95216178894043 
model_pd.lagr.mean(): -19.88408660888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.6753], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.06807554513216019
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13530968129634857
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12482760846614838
epoch£º534	 i:3 	 global-step:10683	 l-p:0.1271345168352127
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11112678050994873
epoch£º534	 i:5 	 global-step:10685	 l-p:0.21375659108161926
epoch£º534	 i:6 	 global-step:10686	 l-p:0.035449858754873276
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11535005271434784
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14369724690914154
epoch£º534	 i:9 	 global-step:10689	 l-p:0.1510927528142929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0641, 5.0639, 5.0641],
        [5.0641, 5.0440, 5.0602],
        [5.0641, 5.0640, 5.0641],
        [5.0641, 5.0208, 5.0483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.3859585225582123 
model_pd.l_d.mean(): -20.322769165039062 
model_pd.lagr.mean(): -19.936811447143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4574], device='cuda:0')), ('power', tensor([-21.0120], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.3859585225582123
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14196227490901947
epoch£º535	 i:2 	 global-step:10702	 l-p:0.1514301598072052
epoch£º535	 i:3 	 global-step:10703	 l-p:0.1603923738002777
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12412507086992264
epoch£º535	 i:5 	 global-step:10705	 l-p:0.1521080732345581
epoch£º535	 i:6 	 global-step:10706	 l-p:0.13036797940731049
epoch£º535	 i:7 	 global-step:10707	 l-p:0.11417406797409058
epoch£º535	 i:8 	 global-step:10708	 l-p:0.10866665840148926
epoch£º535	 i:9 	 global-step:10709	 l-p:0.1508525013923645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9532, 4.9423, 4.9519],
        [4.9532, 4.9479, 4.9528],
        [4.9532, 4.9433, 4.9521],
        [4.9532, 5.0482, 4.8779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.16689655184745789 
model_pd.l_d.mean(): -19.574819564819336 
model_pd.lagr.mean(): -19.407922744750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5429], device='cuda:0')), ('power', tensor([-20.3433], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.16689655184745789
epoch£º536	 i:1 	 global-step:10721	 l-p:0.08974005281925201
epoch£º536	 i:2 	 global-step:10722	 l-p:0.09456944465637207
epoch£º536	 i:3 	 global-step:10723	 l-p:0.11308105289936066
epoch£º536	 i:4 	 global-step:10724	 l-p:0.12788169085979462
epoch£º536	 i:5 	 global-step:10725	 l-p:0.13630607724189758
epoch£º536	 i:6 	 global-step:10726	 l-p:0.15374313294887543
epoch£º536	 i:7 	 global-step:10727	 l-p:0.16574440896511078
epoch£º536	 i:8 	 global-step:10728	 l-p:0.13369378447532654
epoch£º536	 i:9 	 global-step:10729	 l-p:0.11759067326784134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0950, 5.0949, 5.0950],
        [5.0950, 4.9750, 4.9214],
        [5.0950, 4.9759, 4.9326],
        [5.0950, 5.0950, 5.0950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.11049576848745346 
model_pd.l_d.mean(): -20.838407516479492 
model_pd.lagr.mean(): -20.7279109954834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3928], device='cuda:0')), ('power', tensor([-21.4673], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.11049576848745346
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12504877150058746
epoch£º537	 i:2 	 global-step:10742	 l-p:0.1964309960603714
epoch£º537	 i:3 	 global-step:10743	 l-p:0.06660331785678864
epoch£º537	 i:4 	 global-step:10744	 l-p:0.13366852700710297
epoch£º537	 i:5 	 global-step:10745	 l-p:-0.04800911620259285
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12297088652849197
epoch£º537	 i:7 	 global-step:10747	 l-p:0.09866290539503098
epoch£º537	 i:8 	 global-step:10748	 l-p:0.12719576060771942
epoch£º537	 i:9 	 global-step:10749	 l-p:0.14644213020801544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2458, 5.2150, 5.2370],
        [5.2458, 5.2453, 5.2458],
        [5.2458, 5.2457, 5.2458],
        [5.2458, 5.1969, 5.2246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.126510888338089 
model_pd.l_d.mean(): -20.605575561523438 
model_pd.lagr.mean(): -20.47906494140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3875], device='cuda:0')), ('power', tensor([-21.2265], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.126510888338089
epoch£º538	 i:1 	 global-step:10761	 l-p:1.3613479137420654
epoch£º538	 i:2 	 global-step:10762	 l-p:0.1314915120601654
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11026513576507568
epoch£º538	 i:4 	 global-step:10764	 l-p:0.8295779228210449
epoch£º538	 i:5 	 global-step:10765	 l-p:0.15361790359020233
epoch£º538	 i:6 	 global-step:10766	 l-p:0.12963314354419708
epoch£º538	 i:7 	 global-step:10767	 l-p:0.20868521928787231
epoch£º538	 i:8 	 global-step:10768	 l-p:0.13305465877056122
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.004722337704151869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2075, 5.1280, 5.1508],
        [5.2075, 5.2075, 5.2076],
        [5.2075, 5.1951, 5.2058],
        [5.2075, 5.2075, 5.2075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11607068032026291 
model_pd.l_d.mean(): -20.48670768737793 
model_pd.lagr.mean(): -20.370637893676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4210], device='cuda:0')), ('power', tensor([-21.1406], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11607068032026291
epoch£º539	 i:1 	 global-step:10781	 l-p:-0.2363542914390564
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12913505733013153
epoch£º539	 i:3 	 global-step:10783	 l-p:0.16167102754116058
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12586109340190887
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12418931722640991
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11545990407466888
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11682917922735214
epoch£º539	 i:8 	 global-step:10788	 l-p:0.17374877631664276
epoch£º539	 i:9 	 global-step:10789	 l-p:-0.7843949794769287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2109, 5.1278, 4.9874],
        [5.2109, 5.2109, 5.2109],
        [5.2109, 5.1120, 5.1144],
        [5.2109, 5.3621, 5.2122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.14802588522434235 
model_pd.l_d.mean(): -19.282506942749023 
model_pd.lagr.mean(): -19.13448143005371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5326], device='cuda:0')), ('power', tensor([-20.0372], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.14802588522434235
epoch£º540	 i:1 	 global-step:10801	 l-p:0.6202234625816345
epoch£º540	 i:2 	 global-step:10802	 l-p:0.1302979588508606
epoch£º540	 i:3 	 global-step:10803	 l-p:0.12106471508741379
epoch£º540	 i:4 	 global-step:10804	 l-p:0.8180218935012817
epoch£º540	 i:5 	 global-step:10805	 l-p:0.11852490901947021
epoch£º540	 i:6 	 global-step:10806	 l-p:0.12148863822221756
epoch£º540	 i:7 	 global-step:10807	 l-p:0.12500424683094025
epoch£º540	 i:8 	 global-step:10808	 l-p:0.12858986854553223
epoch£º540	 i:9 	 global-step:10809	 l-p:0.033698920160532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1879, 5.1531, 5.1772],
        [5.1879, 5.1601, 5.1808],
        [5.1879, 5.7124, 5.7896],
        [5.1879, 5.1879, 5.1879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.11149586737155914 
model_pd.l_d.mean(): -19.208837509155273 
model_pd.lagr.mean(): -19.097341537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-19.8958], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.11149586737155914
epoch£º541	 i:1 	 global-step:10821	 l-p:0.12939834594726562
epoch£º541	 i:2 	 global-step:10822	 l-p:0.18762536346912384
epoch£º541	 i:3 	 global-step:10823	 l-p:0.2304806411266327
epoch£º541	 i:4 	 global-step:10824	 l-p:0.1394900232553482
epoch£º541	 i:5 	 global-step:10825	 l-p:0.10844171047210693
epoch£º541	 i:6 	 global-step:10826	 l-p:0.09382923692464828
epoch£º541	 i:7 	 global-step:10827	 l-p:-0.0031574342865496874
epoch£º541	 i:8 	 global-step:10828	 l-p:0.10990728437900543
epoch£º541	 i:9 	 global-step:10829	 l-p:0.10432833433151245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1421, 5.1421, 5.1421],
        [5.1421, 5.0358, 4.9160],
        [5.1421, 5.8381, 6.0536],
        [5.1421, 5.0673, 4.9077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.12166065722703934 
model_pd.l_d.mean(): -18.576398849487305 
model_pd.lagr.mean(): -18.45473861694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5342], device='cuda:0')), ('power', tensor([-19.3251], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.12166065722703934
epoch£º542	 i:1 	 global-step:10841	 l-p:0.13781629502773285
epoch£º542	 i:2 	 global-step:10842	 l-p:0.12256750464439392
epoch£º542	 i:3 	 global-step:10843	 l-p:0.1320054829120636
epoch£º542	 i:4 	 global-step:10844	 l-p:0.1503354012966156
epoch£º542	 i:5 	 global-step:10845	 l-p:0.1250106543302536
epoch£º542	 i:6 	 global-step:10846	 l-p:0.09094733744859695
epoch£º542	 i:7 	 global-step:10847	 l-p:0.13355578482151031
epoch£º542	 i:8 	 global-step:10848	 l-p:0.6367622017860413
epoch£º542	 i:9 	 global-step:10849	 l-p:0.11937238276004791
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 5.0949, 4.9400],
        [5.1723, 5.8149, 5.9846],
        [5.1723, 5.1010, 5.1293],
        [5.1723, 5.1710, 5.1723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.12134263664484024 
model_pd.l_d.mean(): -20.0352840423584 
model_pd.lagr.mean(): -19.913942337036133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4551], device='cuda:0')), ('power', tensor([-20.7191], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.12134263664484024
epoch£º543	 i:1 	 global-step:10861	 l-p:0.11200885474681854
epoch£º543	 i:2 	 global-step:10862	 l-p:0.24866659939289093
epoch£º543	 i:3 	 global-step:10863	 l-p:-0.08179332315921783
epoch£º543	 i:4 	 global-step:10864	 l-p:0.1148226335644722
epoch£º543	 i:5 	 global-step:10865	 l-p:0.10905463993549347
epoch£º543	 i:6 	 global-step:10866	 l-p:0.147230327129364
epoch£º543	 i:7 	 global-step:10867	 l-p:0.15115760266780853
epoch£º543	 i:8 	 global-step:10868	 l-p:0.12236783653497696
epoch£º543	 i:9 	 global-step:10869	 l-p:0.13059282302856445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1575, 5.1564, 5.1575],
        [5.1575, 5.1027, 4.9307],
        [5.1575, 5.1575, 5.1575],
        [5.1575, 5.5638, 5.5573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.06787259131669998 
model_pd.l_d.mean(): -20.5339412689209 
model_pd.lagr.mean(): -20.466068267822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116], device='cuda:0')), ('power', tensor([-21.1788], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.06787259131669998
epoch£º544	 i:1 	 global-step:10881	 l-p:0.12724991142749786
epoch£º544	 i:2 	 global-step:10882	 l-p:-0.0595819428563118
epoch£º544	 i:3 	 global-step:10883	 l-p:0.11330684274435043
epoch£º544	 i:4 	 global-step:10884	 l-p:0.1309424489736557
epoch£º544	 i:5 	 global-step:10885	 l-p:0.12082444876432419
epoch£º544	 i:6 	 global-step:10886	 l-p:0.059196796268224716
epoch£º544	 i:7 	 global-step:10887	 l-p:0.037298932671546936
epoch£º544	 i:8 	 global-step:10888	 l-p:0.14545123279094696
epoch£º544	 i:9 	 global-step:10889	 l-p:0.1326671838760376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0224, 5.0108, 5.0210],
        [5.0224, 4.9772, 5.0061],
        [5.0224, 4.9039, 4.8946],
        [5.0224, 4.9060, 4.9012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.11264053732156754 
model_pd.l_d.mean(): -20.768648147583008 
model_pd.lagr.mean(): -20.656007766723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4455], device='cuda:0')), ('power', tensor([-21.4506], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.11264053732156754
epoch£º545	 i:1 	 global-step:10901	 l-p:0.15104295313358307
epoch£º545	 i:2 	 global-step:10902	 l-p:0.13850735127925873
epoch£º545	 i:3 	 global-step:10903	 l-p:0.12562784552574158
epoch£º545	 i:4 	 global-step:10904	 l-p:0.10409820079803467
epoch£º545	 i:5 	 global-step:10905	 l-p:0.17021983861923218
epoch£º545	 i:6 	 global-step:10906	 l-p:0.2142544835805893
epoch£º545	 i:7 	 global-step:10907	 l-p:0.14844396710395813
epoch£º545	 i:8 	 global-step:10908	 l-p:0.11472345888614655
epoch£º545	 i:9 	 global-step:10909	 l-p:0.12486077100038528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9852, 4.9583, 4.9790],
        [4.9852, 4.9595, 4.9795],
        [4.9852, 4.9851, 4.9852],
        [4.9852, 4.8565, 4.7449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.1376158446073532 
model_pd.l_d.mean(): -19.842145919799805 
model_pd.lagr.mean(): -19.704530715942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-20.5229], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.1376158446073532
epoch£º546	 i:1 	 global-step:10921	 l-p:0.17366403341293335
epoch£º546	 i:2 	 global-step:10922	 l-p:0.15089309215545654
epoch£º546	 i:3 	 global-step:10923	 l-p:0.13895206153392792
epoch£º546	 i:4 	 global-step:10924	 l-p:0.08964879810810089
epoch£º546	 i:5 	 global-step:10925	 l-p:0.058137767016887665
epoch£º546	 i:6 	 global-step:10926	 l-p:0.15449926257133484
epoch£º546	 i:7 	 global-step:10927	 l-p:0.16123786568641663
epoch£º546	 i:8 	 global-step:10928	 l-p:0.13324886560440063
epoch£º546	 i:9 	 global-step:10929	 l-p:-0.14456696808338165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0516, 4.9555, 4.9758],
        [5.0516, 5.5452, 5.6061],
        [5.0516, 5.0506, 5.0516],
        [5.0516, 5.0516, 5.0516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.14285853505134583 
model_pd.l_d.mean(): -20.53180503845215 
model_pd.lagr.mean(): -20.388946533203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4584], device='cuda:0')), ('power', tensor([-21.2244], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.14285853505134583
epoch£º547	 i:1 	 global-step:10941	 l-p:0.05783591791987419
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11924130469560623
epoch£º547	 i:3 	 global-step:10943	 l-p:0.11867084354162216
epoch£º547	 i:4 	 global-step:10944	 l-p:-0.2125261425971985
epoch£º547	 i:5 	 global-step:10945	 l-p:0.16422748565673828
epoch£º547	 i:6 	 global-step:10946	 l-p:0.1456947922706604
epoch£º547	 i:7 	 global-step:10947	 l-p:0.08569206297397614
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13811630010604858
epoch£º547	 i:9 	 global-step:10949	 l-p:0.1223251149058342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0527, 5.0525, 5.0527],
        [5.0527, 4.9651, 4.9915],
        [5.0527, 5.0216, 5.0445],
        [5.0527, 5.0074, 5.0363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.1321805864572525 
model_pd.l_d.mean(): -19.43503189086914 
model_pd.lagr.mean(): -19.3028507232666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5244], device='cuda:0')), ('power', tensor([-20.1831], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.1321805864572525
epoch£º548	 i:1 	 global-step:10961	 l-p:0.1285247951745987
epoch£º548	 i:2 	 global-step:10962	 l-p:0.15573064982891083
epoch£º548	 i:3 	 global-step:10963	 l-p:0.1257043033838272
epoch£º548	 i:4 	 global-step:10964	 l-p:0.18256781995296478
epoch£º548	 i:5 	 global-step:10965	 l-p:0.12877188622951508
epoch£º548	 i:6 	 global-step:10966	 l-p:0.13491089642047882
epoch£º548	 i:7 	 global-step:10967	 l-p:0.09631609916687012
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12073064595460892
epoch£º548	 i:9 	 global-step:10969	 l-p:0.09375638514757156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9983, 5.3321, 5.2834],
        [4.9983, 4.9933, 4.9980],
        [4.9983, 4.9904, 4.9976],
        [4.9983, 4.8659, 4.8279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.11013907194137573 
model_pd.l_d.mean(): -20.581893920898438 
model_pd.lagr.mean(): -20.47175407409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4554], device='cuda:0')), ('power', tensor([-21.2719], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.11013907194137573
epoch£º549	 i:1 	 global-step:10981	 l-p:0.14323300123214722
epoch£º549	 i:2 	 global-step:10982	 l-p:0.15371908247470856
epoch£º549	 i:3 	 global-step:10983	 l-p:0.13089799880981445
epoch£º549	 i:4 	 global-step:10984	 l-p:0.09797979146242142
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11417615413665771
epoch£º549	 i:6 	 global-step:10986	 l-p:0.13994988799095154
epoch£º549	 i:7 	 global-step:10987	 l-p:0.14229848980903625
epoch£º549	 i:8 	 global-step:10988	 l-p:0.13370056450366974
epoch£º549	 i:9 	 global-step:10989	 l-p:0.09961602836847305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9903, 4.9568, 4.9812],
        [4.9903, 4.8609, 4.7383],
        [4.9903, 4.8528, 4.7828],
        [4.9903, 4.9357, 4.9677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.1397235095500946 
model_pd.l_d.mean(): -18.536617279052734 
model_pd.lagr.mean(): -18.396894454956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5868], device='cuda:0')), ('power', tensor([-19.3386], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.1397235095500946
epoch£º550	 i:1 	 global-step:11001	 l-p:0.07856900244951248
epoch£º550	 i:2 	 global-step:11002	 l-p:0.19126807153224945
epoch£º550	 i:3 	 global-step:11003	 l-p:0.0702243372797966
epoch£º550	 i:4 	 global-step:11004	 l-p:0.11960168927907944
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13501258194446564
epoch£º550	 i:6 	 global-step:11006	 l-p:0.14935408532619476
epoch£º550	 i:7 	 global-step:11007	 l-p:0.13189314305782318
epoch£º550	 i:8 	 global-step:11008	 l-p:0.1339520961046219
epoch£º550	 i:9 	 global-step:11009	 l-p:0.11668664962053299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0433, 4.9356, 4.9479],
        [5.0433, 5.1429, 4.9687],
        [5.0433, 5.6209, 5.7448],
        [5.0433, 5.0433, 5.0433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.13038624823093414 
model_pd.l_d.mean(): -18.182409286499023 
model_pd.lagr.mean(): -18.05202293395996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5857], device='cuda:0')), ('power', tensor([-18.9794], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.13038624823093414
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14081670343875885
epoch£º551	 i:2 	 global-step:11022	 l-p:0.09545666724443436
epoch£º551	 i:3 	 global-step:11023	 l-p:0.15722598135471344
epoch£º551	 i:4 	 global-step:11024	 l-p:0.10128748416900635
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15122511982917786
epoch£º551	 i:6 	 global-step:11026	 l-p:0.13839973509311676
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12758022546768188
epoch£º551	 i:8 	 global-step:11028	 l-p:0.12720641493797302
epoch£º551	 i:9 	 global-step:11029	 l-p:0.13943640887737274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9405, 4.8051, 4.6788],
        [4.9405, 4.9405, 4.9405],
        [4.9405, 4.9405, 4.9405],
        [4.9405, 4.9405, 4.9405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11863437294960022 
model_pd.l_d.mean(): -19.957914352416992 
model_pd.lagr.mean(): -19.839279174804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5346], device='cuda:0')), ('power', tensor([-20.7220], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11863437294960022
epoch£º552	 i:1 	 global-step:11041	 l-p:0.2149602472782135
epoch£º552	 i:2 	 global-step:11042	 l-p:0.46429404616355896
epoch£º552	 i:3 	 global-step:11043	 l-p:0.1667679399251938
epoch£º552	 i:4 	 global-step:11044	 l-p:0.14025470614433289
epoch£º552	 i:5 	 global-step:11045	 l-p:0.12383362650871277
epoch£º552	 i:6 	 global-step:11046	 l-p:0.13155315816402435
epoch£º552	 i:7 	 global-step:11047	 l-p:0.18526591360569
epoch£º552	 i:8 	 global-step:11048	 l-p:0.18460491299629211
epoch£º552	 i:9 	 global-step:11049	 l-p:0.20708656311035156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9325, 4.9325, 4.9325],
        [4.9325, 4.7966, 4.6665],
        [4.9325, 5.4311, 5.5006],
        [4.9325, 4.8873, 4.9171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.313019335269928 
model_pd.l_d.mean(): -20.273731231689453 
model_pd.lagr.mean(): -19.960712432861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5432], device='cuda:0')), ('power', tensor([-21.0502], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.313019335269928
epoch£º553	 i:1 	 global-step:11061	 l-p:0.11889215558767319
epoch£º553	 i:2 	 global-step:11062	 l-p:0.11333823949098587
epoch£º553	 i:3 	 global-step:11063	 l-p:0.17725707590579987
epoch£º553	 i:4 	 global-step:11064	 l-p:0.14125342667102814
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08245614916086197
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13427284359931946
epoch£º553	 i:7 	 global-step:11067	 l-p:0.133559912443161
epoch£º553	 i:8 	 global-step:11068	 l-p:0.10517849028110504
epoch£º553	 i:9 	 global-step:11069	 l-p:0.18009601533412933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0400, 5.0188, 5.0360],
        [5.0400, 5.0391, 5.0400],
        [5.0400, 5.0137, 5.0341],
        [5.0400, 4.9842, 4.7928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.07845134288072586 
model_pd.l_d.mean(): -20.023681640625 
model_pd.lagr.mean(): -19.94523048400879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-20.7136], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.07845134288072586
epoch£º554	 i:1 	 global-step:11081	 l-p:0.12999789416790009
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09760135412216187
epoch£º554	 i:3 	 global-step:11083	 l-p:0.11207305639982224
epoch£º554	 i:4 	 global-step:11084	 l-p:0.14769501984119415
epoch£º554	 i:5 	 global-step:11085	 l-p:-1.2874349355697632
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12414230406284332
epoch£º554	 i:7 	 global-step:11087	 l-p:0.14235669374465942
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12482129037380219
epoch£º554	 i:9 	 global-step:11089	 l-p:0.14099502563476562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1075, 5.1075, 5.1075],
        [5.1075, 5.0329, 5.0643],
        [5.1075, 5.0947, 5.1058],
        [5.1075, 5.0184, 5.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.14389358460903168 
model_pd.l_d.mean(): -20.43284797668457 
model_pd.lagr.mean(): -20.28895378112793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4419], device='cuda:0')), ('power', tensor([-21.1075], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.14389358460903168
epoch£º555	 i:1 	 global-step:11101	 l-p:0.10694917291402817
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10853217542171478
epoch£º555	 i:3 	 global-step:11103	 l-p:0.28200986981391907
epoch£º555	 i:4 	 global-step:11104	 l-p:-3.2723569869995117
epoch£º555	 i:5 	 global-step:11105	 l-p:0.06545298546552658
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14363694190979004
epoch£º555	 i:7 	 global-step:11107	 l-p:0.12021083384752274
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11273980140686035
epoch£º555	 i:9 	 global-step:11109	 l-p:0.13048683106899261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2617, 5.1456, 5.0505],
        [5.2617, 5.2617, 5.2618],
        [5.2617, 5.3386, 5.1587],
        [5.2617, 5.2600, 5.2617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.1643955558538437 
model_pd.l_d.mean(): -20.56510353088379 
model_pd.lagr.mean(): -20.400707244873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4008], device='cuda:0')), ('power', tensor([-21.1992], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.1643955558538437
epoch£º556	 i:1 	 global-step:11121	 l-p:0.46240389347076416
epoch£º556	 i:2 	 global-step:11122	 l-p:0.2590906023979187
epoch£º556	 i:3 	 global-step:11123	 l-p:0.11945278197526932
epoch£º556	 i:4 	 global-step:11124	 l-p:0.13505569100379944
epoch£º556	 i:5 	 global-step:11125	 l-p:0.1471976488828659
epoch£º556	 i:6 	 global-step:11126	 l-p:-0.08478646725416183
epoch£º556	 i:7 	 global-step:11127	 l-p:0.12558622658252716
epoch£º556	 i:8 	 global-step:11128	 l-p:0.11381864547729492
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13441398739814758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 5.1622, 5.1711],
        [5.1723, 5.0862, 4.9213],
        [5.1723, 5.1149, 5.1463],
        [5.1723, 5.2400, 5.0557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.11823928356170654 
model_pd.l_d.mean(): -19.957029342651367 
model_pd.lagr.mean(): -19.838790893554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.6799], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.11823928356170654
epoch£º557	 i:1 	 global-step:11141	 l-p:0.12902745604515076
epoch£º557	 i:2 	 global-step:11142	 l-p:-0.013594693504273891
epoch£º557	 i:3 	 global-step:11143	 l-p:0.129522904753685
epoch£º557	 i:4 	 global-step:11144	 l-p:0.12954619526863098
epoch£º557	 i:5 	 global-step:11145	 l-p:0.138152077794075
epoch£º557	 i:6 	 global-step:11146	 l-p:8.76388931274414
epoch£º557	 i:7 	 global-step:11147	 l-p:0.14016138017177582
epoch£º557	 i:8 	 global-step:11148	 l-p:0.1171218529343605
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12117937952280045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0580, 4.9203, 4.8393],
        [5.0580, 5.0667, 4.8683],
        [5.0580, 5.0290, 5.0510],
        [5.0580, 5.0512, 5.0574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.0845932736992836 
model_pd.l_d.mean(): -19.91588592529297 
model_pd.lagr.mean(): -19.8312931060791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4824], device='cuda:0')), ('power', tensor([-20.6263], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.0845932736992836
epoch£º558	 i:1 	 global-step:11161	 l-p:0.047106485813856125
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12660886347293854
epoch£º558	 i:3 	 global-step:11163	 l-p:0.13366779685020447
epoch£º558	 i:4 	 global-step:11164	 l-p:0.11453686654567719
epoch£º558	 i:5 	 global-step:11165	 l-p:0.13761542737483978
epoch£º558	 i:6 	 global-step:11166	 l-p:0.12489482760429382
epoch£º558	 i:7 	 global-step:11167	 l-p:0.15966938436031342
epoch£º558	 i:8 	 global-step:11168	 l-p:0.13328686356544495
epoch£º558	 i:9 	 global-step:11169	 l-p:0.15207374095916748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0038, 4.9337, 4.7385],
        [5.0038, 4.9946, 5.0028],
        [5.0038, 5.0037, 5.0038],
        [5.0038, 5.0031, 5.0038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.1433221399784088 
model_pd.l_d.mean(): -19.889659881591797 
model_pd.lagr.mean(): -19.746337890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5632], device='cuda:0')), ('power', tensor([-20.6823], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.1433221399784088
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15062840282917023
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10487278550863266
epoch£º559	 i:3 	 global-step:11183	 l-p:0.16294819116592407
epoch£º559	 i:4 	 global-step:11184	 l-p:0.1747678965330124
epoch£º559	 i:5 	 global-step:11185	 l-p:0.14243388175964355
epoch£º559	 i:6 	 global-step:11186	 l-p:0.16859470307826996
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15585921704769135
epoch£º559	 i:8 	 global-step:11188	 l-p:0.05608614906668663
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11953997611999512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0694, 5.7018, 5.8641],
        [5.0694, 4.9443, 4.8060],
        [5.0694, 5.7085, 5.8761],
        [5.0694, 5.1003, 4.9035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.11960975080728531 
model_pd.l_d.mean(): -18.703615188598633 
model_pd.lagr.mean(): -18.58400535583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5482], device='cuda:0')), ('power', tensor([-19.4680], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.11960975080728531
epoch£º560	 i:1 	 global-step:11201	 l-p:0.3315136134624481
epoch£º560	 i:2 	 global-step:11202	 l-p:0.08356497436761856
epoch£º560	 i:3 	 global-step:11203	 l-p:-0.1620764285326004
epoch£º560	 i:4 	 global-step:11204	 l-p:0.17913056910037994
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11283687502145767
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13375161588191986
epoch£º560	 i:7 	 global-step:11207	 l-p:0.30457285046577454
epoch£º560	 i:8 	 global-step:11208	 l-p:0.1461489051580429
epoch£º560	 i:9 	 global-step:11209	 l-p:0.12050126492977142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3068, 5.3068, 5.3068],
        [5.3068, 5.1996, 5.0769],
        [5.3068, 5.2980, 5.3059],
        [5.3068, 5.3997, 5.2220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.17948710918426514 
model_pd.l_d.mean(): -19.678449630737305 
model_pd.lagr.mean(): -19.49896240234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-20.3173], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.17948710918426514
epoch£º561	 i:1 	 global-step:11221	 l-p:0.1590515822172165
epoch£º561	 i:2 	 global-step:11222	 l-p:0.1388997882604599
epoch£º561	 i:3 	 global-step:11223	 l-p:0.11402574181556702
epoch£º561	 i:4 	 global-step:11224	 l-p:0.1253809928894043
epoch£º561	 i:5 	 global-step:11225	 l-p:0.11938045918941498
epoch£º561	 i:6 	 global-step:11226	 l-p:0.0865408331155777
epoch£º561	 i:7 	 global-step:11227	 l-p:0.13175363838672638
epoch£º561	 i:8 	 global-step:11228	 l-p:0.25702086091041565
epoch£º561	 i:9 	 global-step:11229	 l-p:0.1624682992696762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2520, 5.2149, 5.2406],
        [5.2520, 5.2514, 5.2520],
        [5.2520, 5.2519, 5.2520],
        [5.2520, 5.1988, 5.0165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.13280153274536133 
model_pd.l_d.mean(): -19.917030334472656 
model_pd.lagr.mean(): -19.784229278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4142], device='cuda:0')), ('power', tensor([-20.5577], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.13280153274536133
epoch£º562	 i:1 	 global-step:11241	 l-p:0.12563619017601013
epoch£º562	 i:2 	 global-step:11242	 l-p:0.1468329131603241
epoch£º562	 i:3 	 global-step:11243	 l-p:0.03823046013712883
epoch£º562	 i:4 	 global-step:11244	 l-p:0.11279716342687607
epoch£º562	 i:5 	 global-step:11245	 l-p:0.13479715585708618
epoch£º562	 i:6 	 global-step:11246	 l-p:0.14212584495544434
epoch£º562	 i:7 	 global-step:11247	 l-p:0.13900575041770935
epoch£º562	 i:8 	 global-step:11248	 l-p:0.053748369216918945
epoch£º562	 i:9 	 global-step:11249	 l-p:0.0929480567574501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0198, 5.0126, 5.0192],
        [5.0198, 4.9192, 4.9453],
        [5.0198, 4.9485, 4.9833],
        [5.0198, 4.9865, 5.0111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.07011837512254715 
model_pd.l_d.mean(): -20.596439361572266 
model_pd.lagr.mean(): -20.526321411132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.3014], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.07011837512254715
epoch£º563	 i:1 	 global-step:11261	 l-p:0.149881049990654
epoch£º563	 i:2 	 global-step:11262	 l-p:0.12864069640636444
epoch£º563	 i:3 	 global-step:11263	 l-p:0.18116067349910736
epoch£º563	 i:4 	 global-step:11264	 l-p:0.09964413195848465
epoch£º563	 i:5 	 global-step:11265	 l-p:0.157869353890419
epoch£º563	 i:6 	 global-step:11266	 l-p:0.14090169966220856
epoch£º563	 i:7 	 global-step:11267	 l-p:0.149653360247612
epoch£º563	 i:8 	 global-step:11268	 l-p:0.113667331635952
epoch£º563	 i:9 	 global-step:11269	 l-p:0.12794460356235504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0211, 4.9124, 4.7373],
        [5.0211, 5.0382, 4.8357],
        [5.0211, 4.8752, 4.7942],
        [5.0211, 4.9087, 4.9257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.06821256130933762 
model_pd.l_d.mean(): -19.694828033447266 
model_pd.lagr.mean(): -19.626615524291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-20.4088], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.06821256130933762
epoch£º564	 i:1 	 global-step:11281	 l-p:0.21594588458538055
epoch£º564	 i:2 	 global-step:11282	 l-p:0.13424304127693176
epoch£º564	 i:3 	 global-step:11283	 l-p:0.13978701829910278
epoch£º564	 i:4 	 global-step:11284	 l-p:0.18547198176383972
epoch£º564	 i:5 	 global-step:11285	 l-p:0.10340903699398041
epoch£º564	 i:6 	 global-step:11286	 l-p:0.12896697223186493
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12688101828098297
epoch£º564	 i:8 	 global-step:11288	 l-p:0.12571460008621216
epoch£º564	 i:9 	 global-step:11289	 l-p:0.11285147070884705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 4.9344, 4.9392],
        [4.9395, 4.8359, 4.8637],
        [4.9395, 5.4518, 5.5271],
        [4.9395, 5.4570, 5.5362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.33859628438949585 
model_pd.l_d.mean(): -20.158246994018555 
model_pd.lagr.mean(): -19.819650650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5349], device='cuda:0')), ('power', tensor([-20.9249], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.33859628438949585
epoch£º565	 i:1 	 global-step:11301	 l-p:0.149542897939682
epoch£º565	 i:2 	 global-step:11302	 l-p:0.2084556519985199
epoch£º565	 i:3 	 global-step:11303	 l-p:0.14556360244750977
epoch£º565	 i:4 	 global-step:11304	 l-p:0.16008135676383972
epoch£º565	 i:5 	 global-step:11305	 l-p:0.16144847869873047
epoch£º565	 i:6 	 global-step:11306	 l-p:0.1506401151418686
epoch£º565	 i:7 	 global-step:11307	 l-p:0.15622422099113464
epoch£º565	 i:8 	 global-step:11308	 l-p:0.06705527752637863
epoch£º565	 i:9 	 global-step:11309	 l-p:0.0919821634888649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0599, 5.0467, 5.0581],
        [5.0599, 5.0582, 5.0598],
        [5.0599, 4.9635, 4.9914],
        [5.0599, 5.0108, 5.0419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.04417197033762932 
model_pd.l_d.mean(): -20.343568801879883 
model_pd.lagr.mean(): -20.299396514892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-21.0515], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.04417197033762932
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11761779338121414
epoch£º566	 i:2 	 global-step:11322	 l-p:0.09667924791574478
epoch£º566	 i:3 	 global-step:11323	 l-p:0.15354463458061218
epoch£º566	 i:4 	 global-step:11324	 l-p:0.1566983014345169
epoch£º566	 i:5 	 global-step:11325	 l-p:0.1161125898361206
epoch£º566	 i:6 	 global-step:11326	 l-p:0.21198955178260803
epoch£º566	 i:7 	 global-step:11327	 l-p:0.06961531937122345
epoch£º566	 i:8 	 global-step:11328	 l-p:0.09825465828180313
epoch£º566	 i:9 	 global-step:11329	 l-p:0.132217139005661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.1301, 5.7265, 5.8547],
        [5.1301, 5.6602, 5.7375],
        [5.1301, 5.0140, 5.0209],
        [5.1301, 5.4733, 5.4193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.1874447911977768 
model_pd.l_d.mean(): -18.529277801513672 
model_pd.lagr.mean(): -18.341833114624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5645], device='cuda:0')), ('power', tensor([-19.3083], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.1874447911977768
epoch£º567	 i:1 	 global-step:11341	 l-p:0.1432926058769226
epoch£º567	 i:2 	 global-step:11342	 l-p:1.9579583406448364
epoch£º567	 i:3 	 global-step:11343	 l-p:0.13857980072498322
epoch£º567	 i:4 	 global-step:11344	 l-p:0.08565818518400192
epoch£º567	 i:5 	 global-step:11345	 l-p:0.13344906270503998
epoch£º567	 i:6 	 global-step:11346	 l-p:0.02518436685204506
epoch£º567	 i:7 	 global-step:11347	 l-p:0.15283918380737305
epoch£º567	 i:8 	 global-step:11348	 l-p:0.1393124908208847
epoch£º567	 i:9 	 global-step:11349	 l-p:0.08889352530241013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0939, 4.9627, 4.8283],
        [5.0939, 5.0847, 5.0929],
        [5.0939, 5.2783, 5.1333],
        [5.0939, 4.9709, 4.8205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.1308511346578598 
model_pd.l_d.mean(): -20.08195686340332 
model_pd.lagr.mean(): -19.95110511779785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.8107], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.1308511346578598
epoch£º568	 i:1 	 global-step:11361	 l-p:0.16633111238479614
epoch£º568	 i:2 	 global-step:11362	 l-p:0.12043958157300949
epoch£º568	 i:3 	 global-step:11363	 l-p:0.12088392674922943
epoch£º568	 i:4 	 global-step:11364	 l-p:0.17293284833431244
epoch£º568	 i:5 	 global-step:11365	 l-p:0.13012699782848358
epoch£º568	 i:6 	 global-step:11366	 l-p:0.16974422335624695
epoch£º568	 i:7 	 global-step:11367	 l-p:0.11567226052284241
epoch£º568	 i:8 	 global-step:11368	 l-p:0.15686221420764923
epoch£º568	 i:9 	 global-step:11369	 l-p:0.11585710197687149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9339, 4.9339, 4.9339],
        [4.9339, 4.9009, 4.9257],
        [4.9339, 5.4392, 5.5082],
        [4.9339, 4.9339, 4.9339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.13379238545894623 
model_pd.l_d.mean(): -19.601964950561523 
model_pd.lagr.mean(): -19.468172073364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-20.3689], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.13379238545894623
epoch£º569	 i:1 	 global-step:11381	 l-p:-0.4239644408226013
epoch£º569	 i:2 	 global-step:11382	 l-p:0.16019010543823242
epoch£º569	 i:3 	 global-step:11383	 l-p:0.1497023105621338
epoch£º569	 i:4 	 global-step:11384	 l-p:0.09256099164485931
epoch£º569	 i:5 	 global-step:11385	 l-p:0.15967613458633423
epoch£º569	 i:6 	 global-step:11386	 l-p:0.2149079144001007
epoch£º569	 i:7 	 global-step:11387	 l-p:0.07585746794939041
epoch£º569	 i:8 	 global-step:11388	 l-p:0.34850433468818665
epoch£º569	 i:9 	 global-step:11389	 l-p:0.177415668964386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9399, 4.9293, 4.9388],
        [4.9399, 4.9399, 4.9399],
        [4.9399, 4.7853, 4.6658],
        [4.9399, 5.1862, 5.0787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.1706165224313736 
model_pd.l_d.mean(): -20.1307430267334 
model_pd.lagr.mean(): -19.960126876831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-20.8727], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.1706165224313736
epoch£º570	 i:1 	 global-step:11401	 l-p:0.16037867963314056
epoch£º570	 i:2 	 global-step:11402	 l-p:0.11621898412704468
epoch£º570	 i:3 	 global-step:11403	 l-p:0.07576984167098999
epoch£º570	 i:4 	 global-step:11404	 l-p:0.13603715598583221
epoch£º570	 i:5 	 global-step:11405	 l-p:0.15931734442710876
epoch£º570	 i:6 	 global-step:11406	 l-p:0.13781623542308807
epoch£º570	 i:7 	 global-step:11407	 l-p:0.1332169473171234
epoch£º570	 i:8 	 global-step:11408	 l-p:0.09462711215019226
epoch£º570	 i:9 	 global-step:11409	 l-p:0.15365202724933624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0660, 4.9212, 4.8721],
        [5.0660, 5.0660, 5.0660],
        [5.0660, 5.0358, 5.0588],
        [5.0660, 5.0607, 5.0657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.17020642757415771 
model_pd.l_d.mean(): -19.365684509277344 
model_pd.lagr.mean(): -19.195478439331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-20.0891], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.17020642757415771
epoch£º571	 i:1 	 global-step:11421	 l-p:0.04172193259000778
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11387482285499573
epoch£º571	 i:3 	 global-step:11423	 l-p:-0.5091077089309692
epoch£º571	 i:4 	 global-step:11424	 l-p:0.09793298691511154
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12461547553539276
epoch£º571	 i:6 	 global-step:11426	 l-p:0.15538178384304047
epoch£º571	 i:7 	 global-step:11427	 l-p:0.09655194729566574
epoch£º571	 i:8 	 global-step:11428	 l-p:0.152082622051239
epoch£º571	 i:9 	 global-step:11429	 l-p:0.12241726368665695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0296, 5.0286, 5.0296],
        [5.0296, 5.6320, 5.7688],
        [5.0296, 5.5366, 5.6003],
        [5.0296, 5.0169, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.12878482043743134 
model_pd.l_d.mean(): -19.58013343811035 
model_pd.lagr.mean(): -19.45134925842285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.3393], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.12878482043743134
epoch£º572	 i:1 	 global-step:11441	 l-p:0.11062009632587433
epoch£º572	 i:2 	 global-step:11442	 l-p:0.12287584692239761
epoch£º572	 i:3 	 global-step:11443	 l-p:0.18762673437595367
epoch£º572	 i:4 	 global-step:11444	 l-p:0.2200440913438797
epoch£º572	 i:5 	 global-step:11445	 l-p:0.1411198079586029
epoch£º572	 i:6 	 global-step:11446	 l-p:0.09054205566644669
epoch£º572	 i:7 	 global-step:11447	 l-p:0.07525520771741867
epoch£º572	 i:8 	 global-step:11448	 l-p:0.5471904277801514
epoch£º572	 i:9 	 global-step:11449	 l-p:0.61322021484375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8790, 4.8200, 4.8557],
        [4.8790, 4.7312, 4.7192],
        [4.8790, 4.7974, 4.8355],
        [4.8790, 4.8787, 4.8790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 1.0134681463241577 
model_pd.l_d.mean(): -19.627521514892578 
model_pd.lagr.mean(): -18.61405372619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5250], device='cuda:0')), ('power', tensor([-20.3782], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:1.0134681463241577
epoch£º573	 i:1 	 global-step:11461	 l-p:0.2216213494539261
epoch£º573	 i:2 	 global-step:11462	 l-p:0.11974819004535675
epoch£º573	 i:3 	 global-step:11463	 l-p:0.1282862275838852
epoch£º573	 i:4 	 global-step:11464	 l-p:0.1238025426864624
epoch£º573	 i:5 	 global-step:11465	 l-p:0.10915526747703552
epoch£º573	 i:6 	 global-step:11466	 l-p:0.15635588765144348
epoch£º573	 i:7 	 global-step:11467	 l-p:0.13425126671791077
epoch£º573	 i:8 	 global-step:11468	 l-p:0.23199518024921417
epoch£º573	 i:9 	 global-step:11469	 l-p:0.11305615305900574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0348, 4.8833, 4.7763],
        [5.0348, 5.0817, 4.8799],
        [5.0348, 5.0348, 5.0348],
        [5.0348, 5.0308, 5.0346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.14139679074287415 
model_pd.l_d.mean(): -20.35060691833496 
model_pd.lagr.mean(): -20.209209442138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-21.0525], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.14139679074287415
epoch£º574	 i:1 	 global-step:11481	 l-p:0.1419551968574524
epoch£º574	 i:2 	 global-step:11482	 l-p:0.08895432949066162
epoch£º574	 i:3 	 global-step:11483	 l-p:0.1145671084523201
epoch£º574	 i:4 	 global-step:11484	 l-p:0.11197247356176376
epoch£º574	 i:5 	 global-step:11485	 l-p:0.15868695080280304
epoch£º574	 i:6 	 global-step:11486	 l-p:0.14379796385765076
epoch£º574	 i:7 	 global-step:11487	 l-p:-0.028282860293984413
epoch£º574	 i:8 	 global-step:11488	 l-p:0.15467172861099243
epoch£º574	 i:9 	 global-step:11489	 l-p:0.11637340486049652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1005, 5.0886, 5.0991],
        [5.1005, 4.9688, 4.9616],
        [5.1005, 5.1005, 5.1005],
        [5.1005, 5.0870, 5.0987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.12246208637952805 
model_pd.l_d.mean(): -20.356975555419922 
model_pd.lagr.mean(): -20.234514236450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4870], device='cuda:0')), ('power', tensor([-21.0769], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.12246208637952805
epoch£º575	 i:1 	 global-step:11501	 l-p:0.11504077911376953
epoch£º575	 i:2 	 global-step:11502	 l-p:0.11161243170499802
epoch£º575	 i:3 	 global-step:11503	 l-p:0.12115472555160522
epoch£º575	 i:4 	 global-step:11504	 l-p:1.4351720809936523
epoch£º575	 i:5 	 global-step:11505	 l-p:0.129113107919693
epoch£º575	 i:6 	 global-step:11506	 l-p:0.12940385937690735
epoch£º575	 i:7 	 global-step:11507	 l-p:0.07906433939933777
epoch£º575	 i:8 	 global-step:11508	 l-p:0.11331365257501602
epoch£º575	 i:9 	 global-step:11509	 l-p:0.15753433108329773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0606, 5.0603, 5.0606],
        [5.0606, 5.0571, 5.0604],
        [5.0606, 5.5378, 5.5765],
        [5.0606, 4.9872, 4.7820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.11698329448699951 
model_pd.l_d.mean(): -20.326221466064453 
model_pd.lagr.mean(): -20.209238052368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-21.0435], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.11698329448699951
epoch£º576	 i:1 	 global-step:11521	 l-p:0.14576171338558197
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13851229846477509
epoch£º576	 i:3 	 global-step:11523	 l-p:0.14728066325187683
epoch£º576	 i:4 	 global-step:11524	 l-p:0.13213671743869781
epoch£º576	 i:5 	 global-step:11525	 l-p:0.1484474539756775
epoch£º576	 i:6 	 global-step:11526	 l-p:0.16499392688274384
epoch£º576	 i:7 	 global-step:11527	 l-p:0.10457495599985123
epoch£º576	 i:8 	 global-step:11528	 l-p:0.11269271373748779
epoch£º576	 i:9 	 global-step:11529	 l-p:0.20138324797153473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9994, 4.8978, 4.9293],
        [4.9994, 4.9994, 4.9994],
        [4.9994, 5.0553, 4.8534],
        [4.9994, 4.9994, 4.9994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.10838645696640015 
model_pd.l_d.mean(): -20.718862533569336 
model_pd.lagr.mean(): -20.610475540161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-21.4201], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.10838645696640015
epoch£º577	 i:1 	 global-step:11541	 l-p:0.20527568459510803
epoch£º577	 i:2 	 global-step:11542	 l-p:0.1711069792509079
epoch£º577	 i:3 	 global-step:11543	 l-p:0.12270580977201462
epoch£º577	 i:4 	 global-step:11544	 l-p:0.09879328310489655
epoch£º577	 i:5 	 global-step:11545	 l-p:0.14456823468208313
epoch£º577	 i:6 	 global-step:11546	 l-p:0.13769377768039703
epoch£º577	 i:7 	 global-step:11547	 l-p:0.13102765381336212
epoch£º577	 i:8 	 global-step:11548	 l-p:0.08983751386404037
epoch£º577	 i:9 	 global-step:11549	 l-p:0.3194481432437897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9572, 4.9572, 4.9572],
        [4.9572, 4.9819, 4.7699],
        [4.9572, 4.8036, 4.7739],
        [4.9572, 4.8035, 4.6547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.13153624534606934 
model_pd.l_d.mean(): -20.574508666992188 
model_pd.lagr.mean(): -20.44297218322754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-21.2913], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.13153624534606934
epoch£º578	 i:1 	 global-step:11561	 l-p:0.16165706515312195
epoch£º578	 i:2 	 global-step:11562	 l-p:0.09468407183885574
epoch£º578	 i:3 	 global-step:11563	 l-p:0.11924758553504944
epoch£º578	 i:4 	 global-step:11564	 l-p:0.0003634119057096541
epoch£º578	 i:5 	 global-step:11565	 l-p:0.23709098994731903
epoch£º578	 i:6 	 global-step:11566	 l-p:0.26583194732666016
epoch£º578	 i:7 	 global-step:11567	 l-p:0.15443164110183716
epoch£º578	 i:8 	 global-step:11568	 l-p:0.3577860891819
epoch£º578	 i:9 	 global-step:11569	 l-p:0.1717798411846161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 5.5197, 5.5993],
        [4.9905, 4.9143, 4.9518],
        [4.9905, 4.8358, 4.6984],
        [4.9905, 4.9349, 4.9694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.14991319179534912 
model_pd.l_d.mean(): -20.48359489440918 
model_pd.lagr.mean(): -20.333681106567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-21.1988], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.14991319179534912
epoch£º579	 i:1 	 global-step:11581	 l-p:0.07620971649885178
epoch£º579	 i:2 	 global-step:11582	 l-p:0.1502295732498169
epoch£º579	 i:3 	 global-step:11583	 l-p:0.10386696457862854
epoch£º579	 i:4 	 global-step:11584	 l-p:0.12160548567771912
epoch£º579	 i:5 	 global-step:11585	 l-p:0.15684601664543152
epoch£º579	 i:6 	 global-step:11586	 l-p:0.09215846657752991
epoch£º579	 i:7 	 global-step:11587	 l-p:0.1146409660577774
epoch£º579	 i:8 	 global-step:11588	 l-p:0.15366987884044647
epoch£º579	 i:9 	 global-step:11589	 l-p:0.057385530322790146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1028, 5.0824, 5.0992],
        [5.1028, 5.0970, 5.1024],
        [5.1028, 5.0406, 4.8316],
        [5.1028, 5.0768, 5.0974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.11075202375650406 
model_pd.l_d.mean(): -20.944442749023438 
model_pd.lagr.mean(): -20.833690643310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3920], device='cuda:0')), ('power', tensor([-21.5737], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.11075202375650406
epoch£º580	 i:1 	 global-step:11601	 l-p:0.14249786734580994
epoch£º580	 i:2 	 global-step:11602	 l-p:0.13062459230422974
epoch£º580	 i:3 	 global-step:11603	 l-p:0.11470002681016922
epoch£º580	 i:4 	 global-step:11604	 l-p:0.0769103392958641
epoch£º580	 i:5 	 global-step:11605	 l-p:0.1969333440065384
epoch£º580	 i:6 	 global-step:11606	 l-p:0.19138985872268677
epoch£º580	 i:7 	 global-step:11607	 l-p:0.07369152456521988
epoch£º580	 i:8 	 global-step:11608	 l-p:0.14255084097385406
epoch£º580	 i:9 	 global-step:11609	 l-p:0.12335141748189926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0563, 5.0563, 5.0563],
        [5.0563, 5.2499, 5.1050],
        [5.0563, 4.9043, 4.7765],
        [5.0563, 4.9606, 4.9940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.1701347380876541 
model_pd.l_d.mean(): -20.287792205810547 
model_pd.lagr.mean(): -20.117656707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4956], device='cuda:0')), ('power', tensor([-21.0157], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.1701347380876541
epoch£º581	 i:1 	 global-step:11621	 l-p:0.10322967171669006
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12475814670324326
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13234460353851318
epoch£º581	 i:4 	 global-step:11624	 l-p:0.11921562999486923
epoch£º581	 i:5 	 global-step:11625	 l-p:0.06919525563716888
epoch£º581	 i:6 	 global-step:11626	 l-p:0.08786620944738388
epoch£º581	 i:7 	 global-step:11627	 l-p:0.15851880609989166
epoch£º581	 i:8 	 global-step:11628	 l-p:0.13520459830760956
epoch£º581	 i:9 	 global-step:11629	 l-p:0.13927125930786133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0570, 5.0065, 5.0392],
        [5.0570, 5.5633, 5.6214],
        [5.0570, 5.0566, 5.0570],
        [5.0570, 5.0462, 5.0558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.14657050371170044 
model_pd.l_d.mean(): -18.50277328491211 
model_pd.lagr.mean(): -18.356203079223633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5652], device='cuda:0')), ('power', tensor([-19.2822], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.14657050371170044
epoch£º582	 i:1 	 global-step:11641	 l-p:0.1412487030029297
epoch£º582	 i:2 	 global-step:11642	 l-p:0.12194662541151047
epoch£º582	 i:3 	 global-step:11643	 l-p:0.13609695434570312
epoch£º582	 i:4 	 global-step:11644	 l-p:0.14146792888641357
epoch£º582	 i:5 	 global-step:11645	 l-p:0.08898518979549408
epoch£º582	 i:6 	 global-step:11646	 l-p:0.09820108115673065
epoch£º582	 i:7 	 global-step:11647	 l-p:0.11971219629049301
epoch£º582	 i:8 	 global-step:11648	 l-p:0.11714822798967361
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13938002288341522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0216, 4.9871, 5.0128],
        [5.0216, 5.1154, 4.9239],
        [5.0216, 5.1370, 4.9543],
        [5.0216, 4.8764, 4.7150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.1375085562467575 
model_pd.l_d.mean(): -20.463882446289062 
model_pd.lagr.mean(): -20.326374053955078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-21.1942], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.1375085562467575
epoch£º583	 i:1 	 global-step:11661	 l-p:0.14952215552330017
epoch£º583	 i:2 	 global-step:11662	 l-p:0.1396913379430771
epoch£º583	 i:3 	 global-step:11663	 l-p:0.11091379821300507
epoch£º583	 i:4 	 global-step:11664	 l-p:0.2245509922504425
epoch£º583	 i:5 	 global-step:11665	 l-p:0.18157969415187836
epoch£º583	 i:6 	 global-step:11666	 l-p:0.09706081449985504
epoch£º583	 i:7 	 global-step:11667	 l-p:0.13043123483657837
epoch£º583	 i:8 	 global-step:11668	 l-p:0.12846817076206207
epoch£º583	 i:9 	 global-step:11669	 l-p:0.1959383487701416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0012, 4.9800, 4.9975],
        [5.0012, 5.0012, 5.0012],
        [5.0012, 4.9455, 4.9803],
        [5.0012, 4.9347, 4.9719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.23016387224197388 
model_pd.l_d.mean(): -20.699363708496094 
model_pd.lagr.mean(): -20.469200134277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4652], device='cuda:0')), ('power', tensor([-21.4007], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.23016387224197388
epoch£º584	 i:1 	 global-step:11681	 l-p:0.12196317315101624
epoch£º584	 i:2 	 global-step:11682	 l-p:0.1491617113351822
epoch£º584	 i:3 	 global-step:11683	 l-p:0.14099286496639252
epoch£º584	 i:4 	 global-step:11684	 l-p:0.11516320705413818
epoch£º584	 i:5 	 global-step:11685	 l-p:0.05055263265967369
epoch£º584	 i:6 	 global-step:11686	 l-p:0.14173084497451782
epoch£º584	 i:7 	 global-step:11687	 l-p:0.18740592896938324
epoch£º584	 i:8 	 global-step:11688	 l-p:0.08380275219678879
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10712737590074539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2096, 5.1376, 5.1730],
        [5.2096, 5.2024, 5.2090],
        [5.2096, 5.2081, 5.2096],
        [5.2096, 5.2002, 5.2086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.279646098613739 
model_pd.l_d.mean(): -20.426223754882812 
model_pd.lagr.mean(): -20.146577835083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4086], device='cuda:0')), ('power', tensor([-21.0667], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.279646098613739
epoch£º585	 i:1 	 global-step:11701	 l-p:0.12114325165748596
epoch£º585	 i:2 	 global-step:11702	 l-p:0.1871839165687561
epoch£º585	 i:3 	 global-step:11703	 l-p:0.505948543548584
epoch£º585	 i:4 	 global-step:11704	 l-p:0.1070239245891571
epoch£º585	 i:5 	 global-step:11705	 l-p:0.11806970089673996
epoch£º585	 i:6 	 global-step:11706	 l-p:0.1319654881954193
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11221948266029358
epoch£º585	 i:8 	 global-step:11708	 l-p:0.12761060893535614
epoch£º585	 i:9 	 global-step:11709	 l-p:0.6399056315422058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2720, 5.2714, 5.2720],
        [5.2720, 5.8460, 5.9421],
        [5.2720, 5.1489, 5.1501],
        [5.2720, 5.2415, 5.2646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.12824872136116028 
model_pd.l_d.mean(): -20.187633514404297 
model_pd.lagr.mean(): -20.059385299682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4243], device='cuda:0')), ('power', tensor([-20.8416], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.12824872136116028
epoch£º586	 i:1 	 global-step:11721	 l-p:0.13754616677761078
epoch£º586	 i:2 	 global-step:11722	 l-p:0.023574260994791985
epoch£º586	 i:3 	 global-step:11723	 l-p:0.1522032767534256
epoch£º586	 i:4 	 global-step:11724	 l-p:0.06239898130297661
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11088865995407104
epoch£º586	 i:6 	 global-step:11726	 l-p:-0.8302182555198669
epoch£º586	 i:7 	 global-step:11727	 l-p:0.13306818902492523
epoch£º586	 i:8 	 global-step:11728	 l-p:0.11473480612039566
epoch£º586	 i:9 	 global-step:11729	 l-p:0.1399209052324295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1239, 4.9747, 4.8439],
        [5.1239, 5.2048, 5.0081],
        [5.1239, 5.1435, 4.9312],
        [5.1239, 5.0705, 5.1040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12355335056781769 
model_pd.l_d.mean(): -20.172840118408203 
model_pd.lagr.mean(): -20.049285888671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4863], device='cuda:0')), ('power', tensor([-20.8900], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12355335056781769
epoch£º587	 i:1 	 global-step:11741	 l-p:0.057360999286174774
epoch£º587	 i:2 	 global-step:11742	 l-p:-0.06485693901777267
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12946543097496033
epoch£º587	 i:4 	 global-step:11744	 l-p:0.13316044211387634
epoch£º587	 i:5 	 global-step:11745	 l-p:0.1963343769311905
epoch£º587	 i:6 	 global-step:11746	 l-p:0.1712769716978073
epoch£º587	 i:7 	 global-step:11747	 l-p:0.17569148540496826
epoch£º587	 i:8 	 global-step:11748	 l-p:0.12701764702796936
epoch£º587	 i:9 	 global-step:11749	 l-p:0.3681311011314392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9596, 4.9596, 4.9596],
        [4.9596, 4.8733, 4.9121],
        [4.9596, 4.7921, 4.7328],
        [4.9596, 4.8855, 4.9244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.13594816625118256 
model_pd.l_d.mean(): -20.495372772216797 
model_pd.lagr.mean(): -20.359424591064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5013], device='cuda:0')), ('power', tensor([-21.2314], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.13594816625118256
epoch£º588	 i:1 	 global-step:11761	 l-p:0.12034503370523453
epoch£º588	 i:2 	 global-step:11762	 l-p:0.11873579770326614
epoch£º588	 i:3 	 global-step:11763	 l-p:0.12433730065822601
epoch£º588	 i:4 	 global-step:11764	 l-p:0.0708976462483406
epoch£º588	 i:5 	 global-step:11765	 l-p:0.18290597200393677
epoch£º588	 i:6 	 global-step:11766	 l-p:0.03547799959778786
epoch£º588	 i:7 	 global-step:11767	 l-p:0.1066620796918869
epoch£º588	 i:8 	 global-step:11768	 l-p:0.15485988557338715
epoch£º588	 i:9 	 global-step:11769	 l-p:0.08498907089233398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8469, 4.8319, 4.8450],
        [4.8469, 4.8461, 4.8469],
        [4.8469, 4.8022, 4.8337],
        [4.8469, 4.8469, 4.8469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.10424444824457169 
model_pd.l_d.mean(): -20.271129608154297 
model_pd.lagr.mean(): -20.166885375976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5635], device='cuda:0')), ('power', tensor([-21.0682], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.10424444824457169
epoch£º589	 i:1 	 global-step:11781	 l-p:0.1345866322517395
epoch£º589	 i:2 	 global-step:11782	 l-p:0.18761701881885529
epoch£º589	 i:3 	 global-step:11783	 l-p:0.12156955152750015
epoch£º589	 i:4 	 global-step:11784	 l-p:0.07157016545534134
epoch£º589	 i:5 	 global-step:11785	 l-p:0.2509021759033203
epoch£º589	 i:6 	 global-step:11786	 l-p:0.17294353246688843
epoch£º589	 i:7 	 global-step:11787	 l-p:0.14332610368728638
epoch£º589	 i:8 	 global-step:11788	 l-p:0.15673282742500305
epoch£º589	 i:9 	 global-step:11789	 l-p:0.1338391751050949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0534, 5.0195, 5.0450],
        [5.0534, 5.0168, 4.7950],
        [5.0534, 4.9973, 5.0322],
        [5.0534, 5.0436, 5.0524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.16179132461547852 
model_pd.l_d.mean(): -20.36068344116211 
model_pd.lagr.mean(): -20.19889259338379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4855], device='cuda:0')), ('power', tensor([-21.0791], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.16179132461547852
epoch£º590	 i:1 	 global-step:11801	 l-p:0.13264450430870056
epoch£º590	 i:2 	 global-step:11802	 l-p:0.12601543962955475
epoch£º590	 i:3 	 global-step:11803	 l-p:0.13344591856002808
epoch£º590	 i:4 	 global-step:11804	 l-p:0.13838747143745422
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13243627548217773
epoch£º590	 i:6 	 global-step:11806	 l-p:0.1271723210811615
epoch£º590	 i:7 	 global-step:11807	 l-p:0.07330579310655594
epoch£º590	 i:8 	 global-step:11808	 l-p:-0.023408740758895874
epoch£º590	 i:9 	 global-step:11809	 l-p:0.13754868507385254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0675, 5.1472, 4.9480],
        [5.0675, 4.9089, 4.7811],
        [5.0675, 5.0674, 5.0675],
        [5.0675, 5.0671, 5.0675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12408547848463058 
model_pd.l_d.mean(): -18.837968826293945 
model_pd.lagr.mean(): -18.713882446289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-19.5839], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12408547848463058
epoch£º591	 i:1 	 global-step:11821	 l-p:0.169842928647995
epoch£º591	 i:2 	 global-step:11822	 l-p:0.12013960629701614
epoch£º591	 i:3 	 global-step:11823	 l-p:0.11772992461919785
epoch£º591	 i:4 	 global-step:11824	 l-p:0.05940429866313934
epoch£º591	 i:5 	 global-step:11825	 l-p:0.11145373433828354
epoch£º591	 i:6 	 global-step:11826	 l-p:0.1018352285027504
epoch£º591	 i:7 	 global-step:11827	 l-p:0.17710353434085846
epoch£º591	 i:8 	 global-step:11828	 l-p:0.09401016682386398
epoch£º591	 i:9 	 global-step:11829	 l-p:0.1329207718372345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.1116, 4.9594, 4.9250],
        [5.1116, 4.9593, 4.9247],
        [5.1116, 5.7144, 5.8399],
        [5.1116, 4.9535, 4.8922]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12696006894111633 
model_pd.l_d.mean(): -20.159963607788086 
model_pd.lagr.mean(): -20.033002853393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-20.8365], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12696006894111633
epoch£º592	 i:1 	 global-step:11841	 l-p:0.08049562573432922
epoch£º592	 i:2 	 global-step:11842	 l-p:-0.37230154871940613
epoch£º592	 i:3 	 global-step:11843	 l-p:0.1421593576669693
epoch£º592	 i:4 	 global-step:11844	 l-p:0.029835838824510574
epoch£º592	 i:5 	 global-step:11845	 l-p:0.12612201273441315
epoch£º592	 i:6 	 global-step:11846	 l-p:0.1398087590932846
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13162072002887726
epoch£º592	 i:8 	 global-step:11848	 l-p:0.175863578915596
epoch£º592	 i:9 	 global-step:11849	 l-p:0.12331860512495041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0649, 4.9946, 5.0326],
        [5.0649, 4.9574, 4.9888],
        [5.0649, 5.0649, 5.0649],
        [5.0649, 5.0649, 5.0649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.051093656569719315 
model_pd.l_d.mean(): -20.777507781982422 
model_pd.lagr.mean(): -20.72641372680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.4476], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.051093656569719315
epoch£º593	 i:1 	 global-step:11861	 l-p:0.10947713255882263
epoch£º593	 i:2 	 global-step:11862	 l-p:0.11276170611381531
epoch£º593	 i:3 	 global-step:11863	 l-p:0.19537225365638733
epoch£º593	 i:4 	 global-step:11864	 l-p:0.14320839941501617
epoch£º593	 i:5 	 global-step:11865	 l-p:0.1437363475561142
epoch£º593	 i:6 	 global-step:11866	 l-p:0.1351625770330429
epoch£º593	 i:7 	 global-step:11867	 l-p:0.20756776630878448
epoch£º593	 i:8 	 global-step:11868	 l-p:0.16067658364772797
epoch£º593	 i:9 	 global-step:11869	 l-p:0.10782385617494583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0352, 4.9341, 4.9694],
        [5.0352, 5.0352, 5.0352],
        [5.0352, 4.9744, 5.0110],
        [5.0352, 5.2765, 5.1545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.1178973913192749 
model_pd.l_d.mean(): -20.341527938842773 
model_pd.lagr.mean(): -20.223630905151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5122], device='cuda:0')), ('power', tensor([-21.0870], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.1178973913192749
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13221320509910583
epoch£º594	 i:2 	 global-step:11882	 l-p:0.11899455636739731
epoch£º594	 i:3 	 global-step:11883	 l-p:0.1344514787197113
epoch£º594	 i:4 	 global-step:11884	 l-p:0.1377784162759781
epoch£º594	 i:5 	 global-step:11885	 l-p:0.06367664039134979
epoch£º594	 i:6 	 global-step:11886	 l-p:0.14008601009845734
epoch£º594	 i:7 	 global-step:11887	 l-p:0.19378630816936493
epoch£º594	 i:8 	 global-step:11888	 l-p:0.12670813500881195
epoch£º594	 i:9 	 global-step:11889	 l-p:0.14282017946243286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0390, 5.2846, 5.1648],
        [5.0390, 4.8800, 4.8467],
        [5.0390, 5.0390, 5.0390],
        [5.0390, 5.0374, 5.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.15006232261657715 
model_pd.l_d.mean(): -20.627811431884766 
model_pd.lagr.mean(): -20.47774887084961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4547], device='cuda:0')), ('power', tensor([-21.3177], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.15006232261657715
epoch£º595	 i:1 	 global-step:11901	 l-p:0.13613246381282806
epoch£º595	 i:2 	 global-step:11902	 l-p:0.16888155043125153
epoch£º595	 i:3 	 global-step:11903	 l-p:0.24701453745365143
epoch£º595	 i:4 	 global-step:11904	 l-p:0.12812232971191406
epoch£º595	 i:5 	 global-step:11905	 l-p:0.13125470280647278
epoch£º595	 i:6 	 global-step:11906	 l-p:0.13003034889698029
epoch£º595	 i:7 	 global-step:11907	 l-p:0.12655717134475708
epoch£º595	 i:8 	 global-step:11908	 l-p:0.12212041765451431
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11940398812294006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9520, 4.9146, 4.9424],
        [4.9520, 4.9512, 4.9520],
        [4.9520, 4.8390, 4.6190],
        [4.9520, 4.9520, 4.9520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11766546964645386 
model_pd.l_d.mean(): -20.585063934326172 
model_pd.lagr.mean(): -20.467397689819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-21.2999], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11766546964645386
epoch£º596	 i:1 	 global-step:11921	 l-p:0.1507941037416458
epoch£º596	 i:2 	 global-step:11922	 l-p:0.16247302293777466
epoch£º596	 i:3 	 global-step:11923	 l-p:-0.060418929904699326
epoch£º596	 i:4 	 global-step:11924	 l-p:0.1250799000263214
epoch£º596	 i:5 	 global-step:11925	 l-p:1.546648383140564
epoch£º596	 i:6 	 global-step:11926	 l-p:0.3066595494747162
epoch£º596	 i:7 	 global-step:11927	 l-p:0.2366415113210678
epoch£º596	 i:8 	 global-step:11928	 l-p:-0.4775086045265198
epoch£º596	 i:9 	 global-step:11929	 l-p:0.12935689091682434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9909, 4.8410, 4.8376],
        [4.9909, 4.9663, 4.9863],
        [4.9909, 4.9908, 4.9909],
        [4.9909, 4.9892, 4.9908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.13735298812389374 
model_pd.l_d.mean(): -20.46721839904785 
model_pd.lagr.mean(): -20.329864501953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4790], device='cuda:0')), ('power', tensor([-21.1801], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.13735298812389374
epoch£º597	 i:1 	 global-step:11941	 l-p:0.13894177973270416
epoch£º597	 i:2 	 global-step:11942	 l-p:0.1260383576154709
epoch£º597	 i:3 	 global-step:11943	 l-p:0.2609591484069824
epoch£º597	 i:4 	 global-step:11944	 l-p:0.1487640142440796
epoch£º597	 i:5 	 global-step:11945	 l-p:0.23870177567005157
epoch£º597	 i:6 	 global-step:11946	 l-p:0.07513831555843353
epoch£º597	 i:7 	 global-step:11947	 l-p:0.15977083146572113
epoch£º597	 i:8 	 global-step:11948	 l-p:0.12866923213005066
epoch£º597	 i:9 	 global-step:11949	 l-p:0.13856151700019836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0719, 5.0711, 5.0719],
        [5.0719, 5.2066, 5.0265],
        [5.0719, 4.9447, 4.9638],
        [5.0719, 5.5097, 5.5125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11331900209188461 
model_pd.l_d.mean(): -19.289770126342773 
model_pd.lagr.mean(): -19.176450729370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5266], device='cuda:0')), ('power', tensor([-20.0385], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11331900209188461
epoch£º598	 i:1 	 global-step:11961	 l-p:0.05165603384375572
epoch£º598	 i:2 	 global-step:11962	 l-p:0.10379249602556229
epoch£º598	 i:3 	 global-step:11963	 l-p:0.12920713424682617
epoch£º598	 i:4 	 global-step:11964	 l-p:0.10376119613647461
epoch£º598	 i:5 	 global-step:11965	 l-p:0.08318612724542618
epoch£º598	 i:6 	 global-step:11966	 l-p:0.1376935988664627
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13576100766658783
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15050379931926727
epoch£º598	 i:9 	 global-step:11969	 l-p:0.12990929186344147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1064, 5.1064, 5.1064],
        [5.1064, 4.9746, 4.9873],
        [5.1064, 5.3663, 5.2513],
        [5.1064, 5.1027, 5.1062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.1319497674703598 
model_pd.l_d.mean(): -19.833799362182617 
model_pd.lagr.mean(): -19.70184898376465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5422], device='cuda:0')), ('power', tensor([-20.6043], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.1319497674703598
epoch£º599	 i:1 	 global-step:11981	 l-p:0.15420731902122498
epoch£º599	 i:2 	 global-step:11982	 l-p:0.1212904080748558
epoch£º599	 i:3 	 global-step:11983	 l-p:0.16424979269504547
epoch£º599	 i:4 	 global-step:11984	 l-p:0.09091366827487946
epoch£º599	 i:5 	 global-step:11985	 l-p:0.0767650231719017
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13557423651218414
epoch£º599	 i:7 	 global-step:11987	 l-p:0.13373199105262756
epoch£º599	 i:8 	 global-step:11988	 l-p:0.09202460944652557
epoch£º599	 i:9 	 global-step:11989	 l-p:0.1226816177368164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0510, 5.1084, 4.8969],
        [5.0510, 5.0510, 5.0510],
        [5.0510, 5.0510, 5.0510],
        [5.0510, 5.0510, 5.0510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.131548210978508 
model_pd.l_d.mean(): -19.636629104614258 
model_pd.lagr.mean(): -19.505081176757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.3391], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.131548210978508
epoch£º600	 i:1 	 global-step:12001	 l-p:0.14139661192893982
epoch£º600	 i:2 	 global-step:12002	 l-p:0.16866181790828705
epoch£º600	 i:3 	 global-step:12003	 l-p:0.08540838956832886
epoch£º600	 i:4 	 global-step:12004	 l-p:0.1014210507273674
epoch£º600	 i:5 	 global-step:12005	 l-p:0.13065551221370697
epoch£º600	 i:6 	 global-step:12006	 l-p:0.21879354119300842
epoch£º600	 i:7 	 global-step:12007	 l-p:0.11772323399782181
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12763811647891998
epoch£º600	 i:9 	 global-step:12009	 l-p:0.1295364797115326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0468, 5.0172, 4.7877],
        [5.0468, 4.8799, 4.7377],
        [5.0468, 5.0267, 5.0435],
        [5.0468, 5.0462, 5.0468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.09814052283763885 
model_pd.l_d.mean(): -20.652462005615234 
model_pd.lagr.mean(): -20.5543212890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.3233], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.09814052283763885
epoch£º601	 i:1 	 global-step:12021	 l-p:0.1258489340543747
epoch£º601	 i:2 	 global-step:12022	 l-p:0.1785433292388916
epoch£º601	 i:3 	 global-step:12023	 l-p:0.1505308896303177
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13025419414043427
epoch£º601	 i:5 	 global-step:12025	 l-p:0.10668384283781052
epoch£º601	 i:6 	 global-step:12026	 l-p:0.1935357004404068
epoch£º601	 i:7 	 global-step:12027	 l-p:0.10698982328176498
epoch£º601	 i:8 	 global-step:12028	 l-p:0.10212893038988113
epoch£º601	 i:9 	 global-step:12029	 l-p:0.12850971519947052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0458, 5.0458, 5.0458],
        [5.0458, 5.0812, 4.8626],
        [5.0458, 5.0220, 5.0414],
        [5.0458, 5.0374, 4.8097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.13299250602722168 
model_pd.l_d.mean(): -20.409439086914062 
model_pd.lagr.mean(): -20.276447296142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-21.1239], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.13299250602722168
epoch£º602	 i:1 	 global-step:12041	 l-p:0.07133777439594269
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11842507123947144
epoch£º602	 i:3 	 global-step:12043	 l-p:0.1123683899641037
epoch£º602	 i:4 	 global-step:12044	 l-p:0.11006930470466614
epoch£º602	 i:5 	 global-step:12045	 l-p:0.1983475387096405
epoch£º602	 i:6 	 global-step:12046	 l-p:0.1268070936203003
epoch£º602	 i:7 	 global-step:12047	 l-p:0.09486325830221176
epoch£º602	 i:8 	 global-step:12048	 l-p:0.15020187199115753
epoch£º602	 i:9 	 global-step:12049	 l-p:0.16574645042419434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0782, 4.9605, 4.9887],
        [5.0782, 5.0674, 5.0771],
        [5.0782, 5.0782, 5.0782],
        [5.0782, 4.9680, 5.0005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.09954404830932617 
model_pd.l_d.mean(): -20.76789665222168 
model_pd.lagr.mean(): -20.668352127075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-21.4239], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.09954404830932617
epoch£º603	 i:1 	 global-step:12061	 l-p:0.17487914860248566
epoch£º603	 i:2 	 global-step:12062	 l-p:0.1346760392189026
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12858770787715912
epoch£º603	 i:4 	 global-step:12064	 l-p:0.09759853035211563
epoch£º603	 i:5 	 global-step:12065	 l-p:0.13119544088840485
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14099706709384918
epoch£º603	 i:7 	 global-step:12067	 l-p:-0.12566542625427246
epoch£º603	 i:8 	 global-step:12068	 l-p:0.11494143307209015
epoch£º603	 i:9 	 global-step:12069	 l-p:0.121950663626194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.4132, 5.3173],
        [5.1191, 5.1191, 5.1191],
        [5.1191, 5.0555, 5.0928],
        [5.1191, 4.9563, 4.9059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 1.4368809461593628 
model_pd.l_d.mean(): -20.531932830810547 
model_pd.lagr.mean(): -19.09505271911621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.2066], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:1.4368809461593628
epoch£º604	 i:1 	 global-step:12081	 l-p:0.15417322516441345
epoch£º604	 i:2 	 global-step:12082	 l-p:0.10977878421545029
epoch£º604	 i:3 	 global-step:12083	 l-p:0.08587344735860825
epoch£º604	 i:4 	 global-step:12084	 l-p:0.12649933993816376
epoch£º604	 i:5 	 global-step:12085	 l-p:0.11217200756072998
epoch£º604	 i:6 	 global-step:12086	 l-p:3.4331448078155518
epoch£º604	 i:7 	 global-step:12087	 l-p:0.10874752700328827
epoch£º604	 i:8 	 global-step:12088	 l-p:0.1399800032377243
epoch£º604	 i:9 	 global-step:12089	 l-p:0.11945150047540665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1749, 5.1531, 4.9292],
        [5.1749, 5.0720, 5.1050],
        [5.1749, 5.1221, 5.1561],
        [5.1749, 5.1318, 5.1620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11014679074287415 
model_pd.l_d.mean(): -20.425426483154297 
model_pd.lagr.mean(): -20.315279006958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4293], device='cuda:0')), ('power', tensor([-21.0871], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11014679074287415
epoch£º605	 i:1 	 global-step:12101	 l-p:0.11657746881246567
epoch£º605	 i:2 	 global-step:12102	 l-p:0.13630753755569458
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12680239975452423
epoch£º605	 i:4 	 global-step:12104	 l-p:0.05564706027507782
epoch£º605	 i:5 	 global-step:12105	 l-p:0.1415117084980011
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11199808120727539
epoch£º605	 i:7 	 global-step:12107	 l-p:0.31362131237983704
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12511730194091797
epoch£º605	 i:9 	 global-step:12109	 l-p:-0.0073791551403701305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1224, 5.1224, 5.1224],
        [5.1224, 5.3872, 5.2732],
        [5.1224, 4.9595, 4.9099],
        [5.1224, 5.1220, 5.1224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.10597560554742813 
model_pd.l_d.mean(): -20.689800262451172 
model_pd.lagr.mean(): -20.583824157714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-21.3471], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.10597560554742813
epoch£º606	 i:1 	 global-step:12121	 l-p:0.1372290700674057
epoch£º606	 i:2 	 global-step:12122	 l-p:0.1281573325395584
epoch£º606	 i:3 	 global-step:12123	 l-p:0.08973564207553864
epoch£º606	 i:4 	 global-step:12124	 l-p:0.13937363028526306
epoch£º606	 i:5 	 global-step:12125	 l-p:0.06698638945817947
epoch£º606	 i:6 	 global-step:12126	 l-p:0.14293012022972107
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14455872774124146
epoch£º606	 i:8 	 global-step:12128	 l-p:0.1346331685781479
epoch£º606	 i:9 	 global-step:12129	 l-p:0.08042296022176743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0559, 5.0466, 5.0550],
        [5.0559, 4.8833, 4.8100],
        [5.0559, 5.0545, 5.0558],
        [5.0559, 4.8926, 4.8598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14753566682338715 
model_pd.l_d.mean(): -20.483753204345703 
model_pd.lagr.mean(): -20.336217880249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-21.1882], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14753566682338715
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13061942160129547
epoch£º607	 i:2 	 global-step:12142	 l-p:0.11156634241342545
epoch£º607	 i:3 	 global-step:12143	 l-p:0.15049900114536285
epoch£º607	 i:4 	 global-step:12144	 l-p:0.16946037113666534
epoch£º607	 i:5 	 global-step:12145	 l-p:0.11504024267196655
epoch£º607	 i:6 	 global-step:12146	 l-p:0.09673197567462921
epoch£º607	 i:7 	 global-step:12147	 l-p:0.12537172436714172
epoch£º607	 i:8 	 global-step:12148	 l-p:0.13905438780784607
epoch£º607	 i:9 	 global-step:12149	 l-p:0.18269982933998108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0215, 5.0201, 5.0214],
        [5.0215, 5.0159, 5.0211],
        [5.0215, 4.9590, 4.9969],
        [5.0215, 5.2320, 5.0895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.1845005601644516 
model_pd.l_d.mean(): -19.589357376098633 
model_pd.lagr.mean(): -19.404857635498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5560], device='cuda:0')), ('power', tensor([-20.3714], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.1845005601644516
epoch£º608	 i:1 	 global-step:12161	 l-p:0.14588841795921326
epoch£º608	 i:2 	 global-step:12162	 l-p:0.1429850310087204
epoch£º608	 i:3 	 global-step:12163	 l-p:0.18249674141407013
epoch£º608	 i:4 	 global-step:12164	 l-p:0.07352757453918457
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11875699460506439
epoch£º608	 i:6 	 global-step:12166	 l-p:0.1350865364074707
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11697383970022202
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15512417256832123
epoch£º608	 i:9 	 global-step:12169	 l-p:0.13323082029819489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0414, 5.0414, 5.0414],
        [5.0414, 5.0241, 5.0389],
        [5.0414, 5.2569, 5.1164],
        [5.0414, 4.9318, 4.9665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.14310993254184723 
model_pd.l_d.mean(): -19.252120971679688 
model_pd.lagr.mean(): -19.109010696411133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5644], device='cuda:0')), ('power', tensor([-20.0390], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.14310993254184723
epoch£º609	 i:1 	 global-step:12181	 l-p:0.13679832220077515
epoch£º609	 i:2 	 global-step:12182	 l-p:0.1566510796546936
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13247214257717133
epoch£º609	 i:4 	 global-step:12184	 l-p:0.11525575071573257
epoch£º609	 i:5 	 global-step:12185	 l-p:0.14283719658851624
epoch£º609	 i:6 	 global-step:12186	 l-p:0.1693250983953476
epoch£º609	 i:7 	 global-step:12187	 l-p:0.1372714638710022
epoch£º609	 i:8 	 global-step:12188	 l-p:0.10121671110391617
epoch£º609	 i:9 	 global-step:12189	 l-p:0.10680554807186127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0411, 4.9313, 4.9661],
        [5.0411, 5.0407, 5.0411],
        [5.0411, 5.0410, 5.0411],
        [5.0411, 5.0073, 5.0331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08126933872699738 
model_pd.l_d.mean(): -20.413555145263672 
model_pd.lagr.mean(): -20.332284927368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-21.1210], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.08126933872699738
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14332512021064758
epoch£º610	 i:2 	 global-step:12202	 l-p:0.1286257654428482
epoch£º610	 i:3 	 global-step:12203	 l-p:0.12485010176897049
epoch£º610	 i:4 	 global-step:12204	 l-p:0.12891876697540283
epoch£º610	 i:5 	 global-step:12205	 l-p:0.12963956594467163
epoch£º610	 i:6 	 global-step:12206	 l-p:0.04654502868652344
epoch£º610	 i:7 	 global-step:12207	 l-p:0.18777352571487427
epoch£º610	 i:8 	 global-step:12208	 l-p:0.15442244708538055
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14469407498836517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0911, 4.9227, 4.8669],
        [5.0911, 5.3853, 5.2892],
        [5.0911, 5.6162, 5.6786],
        [5.0911, 5.0901, 5.0911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.09743878990411758 
model_pd.l_d.mean(): -20.59203338623047 
model_pd.lagr.mean(): -20.49459457397461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4199], device='cuda:0')), ('power', tensor([-21.2459], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.09743878990411758
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11958806961774826
epoch£º611	 i:2 	 global-step:12222	 l-p:0.09038643538951874
epoch£º611	 i:3 	 global-step:12223	 l-p:0.16110669076442719
epoch£º611	 i:4 	 global-step:12224	 l-p:0.1405235379934311
epoch£º611	 i:5 	 global-step:12225	 l-p:0.11348982900381088
epoch£º611	 i:6 	 global-step:12226	 l-p:0.12795184552669525
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12967923283576965
epoch£º611	 i:8 	 global-step:12228	 l-p:-0.07486031949520111
epoch£º611	 i:9 	 global-step:12229	 l-p:0.14946305751800537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1013, 5.0607, 4.8300],
        [5.1013, 5.1013, 5.1013],
        [5.1013, 4.9410, 4.9124],
        [5.1013, 5.1004, 5.1013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14523281157016754 
model_pd.l_d.mean(): -19.504446029663086 
model_pd.lagr.mean(): -19.35921287536621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.2167], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14523281157016754
epoch£º612	 i:1 	 global-step:12241	 l-p:0.13006709516048431
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09841381013393402
epoch£º612	 i:3 	 global-step:12243	 l-p:0.1303526908159256
epoch£º612	 i:4 	 global-step:12244	 l-p:0.11350074410438538
epoch£º612	 i:5 	 global-step:12245	 l-p:0.1352631002664566
epoch£º612	 i:6 	 global-step:12246	 l-p:0.14889562129974365
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14914344251155853
epoch£º612	 i:8 	 global-step:12248	 l-p:0.19512087106704712
epoch£º612	 i:9 	 global-step:12249	 l-p:0.06652425974607468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0455, 5.0454, 5.0456],
        [5.0455, 5.0213, 5.0411],
        [5.0455, 5.0424, 5.0454],
        [5.0455, 4.9894, 5.0254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.20692351460456848 
model_pd.l_d.mean(): -20.58988380432129 
model_pd.lagr.mean(): -20.382959365844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4615], device='cuda:0')), ('power', tensor([-21.2862], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.20692351460456848
epoch£º613	 i:1 	 global-step:12261	 l-p:0.16724644601345062
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12041239440441132
epoch£º613	 i:3 	 global-step:12263	 l-p:0.051461562514305115
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11439303308725357
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12238887697458267
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14089256525039673
epoch£º613	 i:7 	 global-step:12267	 l-p:0.09224735200405121
epoch£º613	 i:8 	 global-step:12268	 l-p:0.10687209665775299
epoch£º613	 i:9 	 global-step:12269	 l-p:0.1309940665960312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0898, 4.9940, 4.7700],
        [5.0898, 4.9158, 4.8241],
        [5.0898, 5.0865, 5.0897],
        [5.0898, 5.0882, 5.0898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13651469349861145 
model_pd.l_d.mean(): -20.441328048706055 
model_pd.lagr.mean(): -20.304813385009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.1436], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13651469349861145
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14801208674907684
epoch£º614	 i:2 	 global-step:12282	 l-p:0.021065562963485718
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12765634059906006
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12845240533351898
epoch£º614	 i:5 	 global-step:12285	 l-p:0.13963326811790466
epoch£º614	 i:6 	 global-step:12286	 l-p:0.17815455794334412
epoch£º614	 i:7 	 global-step:12287	 l-p:0.10873305052518845
epoch£º614	 i:8 	 global-step:12288	 l-p:0.15074239671230316
epoch£º614	 i:9 	 global-step:12289	 l-p:0.11701469123363495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0581, 4.8808, 4.7698],
        [5.0581, 4.8811, 4.7964],
        [5.0581, 5.1050, 4.8865],
        [5.0581, 5.0580, 5.0581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13324637711048126 
model_pd.l_d.mean(): -19.139310836791992 
model_pd.lagr.mean(): -19.006065368652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5190], device='cuda:0')), ('power', tensor([-19.8786], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13324637711048126
epoch£º615	 i:1 	 global-step:12301	 l-p:0.09040891379117966
epoch£º615	 i:2 	 global-step:12302	 l-p:0.13641206920146942
epoch£º615	 i:3 	 global-step:12303	 l-p:0.10396014899015427
epoch£º615	 i:4 	 global-step:12304	 l-p:0.131318598985672
epoch£º615	 i:5 	 global-step:12305	 l-p:0.1301945298910141
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14736413955688477
epoch£º615	 i:7 	 global-step:12307	 l-p:0.1731584072113037
epoch£º615	 i:8 	 global-step:12308	 l-p:0.11954879760742188
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11673364043235779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0612, 5.0591, 5.0611],
        [5.0612, 5.0288, 5.0537],
        [5.0612, 5.2228, 5.0516],
        [5.0612, 4.9206, 4.7189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.13237649202346802 
model_pd.l_d.mean(): -18.73988914489746 
model_pd.lagr.mean(): -18.607513427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6000], device='cuda:0')), ('power', tensor([-19.5575], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.13237649202346802
epoch£º616	 i:1 	 global-step:12321	 l-p:0.1542847603559494
epoch£º616	 i:2 	 global-step:12322	 l-p:0.1530923843383789
epoch£º616	 i:3 	 global-step:12323	 l-p:0.1416609287261963
epoch£º616	 i:4 	 global-step:12324	 l-p:0.06403623521327972
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10521098971366882
epoch£º616	 i:6 	 global-step:12326	 l-p:0.13079744577407837
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12300382554531097
epoch£º616	 i:8 	 global-step:12328	 l-p:0.1360936313867569
epoch£º616	 i:9 	 global-step:12329	 l-p:0.12925630807876587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 4.9998, 5.0367],
        [5.0588, 4.9188, 4.9324],
        [5.0588, 5.5995, 5.6740],
        [5.0588, 5.0008, 5.0375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.14427828788757324 
model_pd.l_d.mean(): -20.2552490234375 
model_pd.lagr.mean(): -20.110971450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.9651], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.14427828788757324
epoch£º617	 i:1 	 global-step:12341	 l-p:0.10998298227787018
epoch£º617	 i:2 	 global-step:12342	 l-p:0.16729049384593964
epoch£º617	 i:3 	 global-step:12343	 l-p:0.11610204726457596
epoch£º617	 i:4 	 global-step:12344	 l-p:0.12987129390239716
epoch£º617	 i:5 	 global-step:12345	 l-p:0.14858439564704895
epoch£º617	 i:6 	 global-step:12346	 l-p:0.1273507922887802
epoch£º617	 i:7 	 global-step:12347	 l-p:0.20729930698871613
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11983299255371094
epoch£º617	 i:9 	 global-step:12349	 l-p:0.14425887167453766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0212, 4.8387, 4.7354],
        [5.0212, 4.9602, 4.9980],
        [5.0212, 5.5246, 5.5729],
        [5.0212, 4.8529, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.12927360832691193 
model_pd.l_d.mean(): -20.301450729370117 
model_pd.lagr.mean(): -20.172176361083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4897], device='cuda:0')), ('power', tensor([-21.0235], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.12927360832691193
epoch£º618	 i:1 	 global-step:12361	 l-p:0.1714957058429718
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10437951982021332
epoch£º618	 i:3 	 global-step:12363	 l-p:0.2738970220088959
epoch£º618	 i:4 	 global-step:12364	 l-p:0.12166916579008102
epoch£º618	 i:5 	 global-step:12365	 l-p:0.16150939464569092
epoch£º618	 i:6 	 global-step:12366	 l-p:0.13739275932312012
epoch£º618	 i:7 	 global-step:12367	 l-p:0.1601962298154831
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08913344144821167
epoch£º618	 i:9 	 global-step:12369	 l-p:0.15820260345935822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0098, 4.9510, 4.9883],
        [5.0098, 4.8956, 4.9311],
        [5.0098, 4.8917, 4.9253],
        [5.0098, 5.0936, 4.8868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.17512226104736328 
model_pd.l_d.mean(): -20.36284637451172 
model_pd.lagr.mean(): -20.187725067138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5117], device='cuda:0')), ('power', tensor([-21.1080], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.17512226104736328
epoch£º619	 i:1 	 global-step:12381	 l-p:0.15353794395923615
epoch£º619	 i:2 	 global-step:12382	 l-p:0.1207563579082489
epoch£º619	 i:3 	 global-step:12383	 l-p:0.0915197804570198
epoch£º619	 i:4 	 global-step:12384	 l-p:0.17963199317455292
epoch£º619	 i:5 	 global-step:12385	 l-p:0.1090485230088234
epoch£º619	 i:6 	 global-step:12386	 l-p:0.14517496526241302
epoch£º619	 i:7 	 global-step:12387	 l-p:0.117039255797863
epoch£º619	 i:8 	 global-step:12388	 l-p:0.12715096771717072
epoch£º619	 i:9 	 global-step:12389	 l-p:0.1742001324892044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0453, 5.0228, 5.0415],
        [5.0453, 4.9065, 4.9231],
        [5.0453, 5.0453, 5.0453],
        [5.0453, 5.0114, 4.7748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.1326332688331604 
model_pd.l_d.mean(): -20.87537384033203 
model_pd.lagr.mean(): -20.742740631103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.5494], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.1326332688331604
epoch£º620	 i:1 	 global-step:12401	 l-p:0.19090519845485687
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12689214944839478
epoch£º620	 i:3 	 global-step:12403	 l-p:0.11000016331672668
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12417612969875336
epoch£º620	 i:5 	 global-step:12405	 l-p:0.10538581758737564
epoch£º620	 i:6 	 global-step:12406	 l-p:0.15597109496593475
epoch£º620	 i:7 	 global-step:12407	 l-p:0.14134842157363892
epoch£º620	 i:8 	 global-step:12408	 l-p:0.14303666353225708
epoch£º620	 i:9 	 global-step:12409	 l-p:0.13199764490127563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0439, 5.0930, 4.8734],
        [5.0439, 5.0430, 5.0439],
        [5.0439, 5.0439, 5.0439],
        [5.0439, 5.0439, 5.0439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.15247753262519836 
model_pd.l_d.mean(): -20.88119888305664 
model_pd.lagr.mean(): -20.728721618652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4161], device='cuda:0')), ('power', tensor([-21.5344], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.15247753262519836
epoch£º621	 i:1 	 global-step:12421	 l-p:0.1327344924211502
epoch£º621	 i:2 	 global-step:12422	 l-p:0.13118286430835724
epoch£º621	 i:3 	 global-step:12423	 l-p:0.19631542265415192
epoch£º621	 i:4 	 global-step:12424	 l-p:0.1952241063117981
epoch£º621	 i:5 	 global-step:12425	 l-p:0.0813700258731842
epoch£º621	 i:6 	 global-step:12426	 l-p:0.11324501037597656
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12790659070014954
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14926065504550934
epoch£º621	 i:9 	 global-step:12429	 l-p:0.06374699622392654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0590, 5.0590, 5.0590],
        [5.0590, 4.8857, 4.8363],
        [5.0590, 4.9537, 4.7262],
        [5.0590, 4.9085, 4.9102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.1661728322505951 
model_pd.l_d.mean(): -20.77635383605957 
model_pd.lagr.mean(): -20.61018180847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-21.4469], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.1661728322505951
epoch£º622	 i:1 	 global-step:12441	 l-p:0.13431745767593384
epoch£º622	 i:2 	 global-step:12442	 l-p:0.13221706449985504
epoch£º622	 i:3 	 global-step:12443	 l-p:0.10827060788869858
epoch£º622	 i:4 	 global-step:12444	 l-p:0.12240197509527206
epoch£º622	 i:5 	 global-step:12445	 l-p:0.11386418342590332
epoch£º622	 i:6 	 global-step:12446	 l-p:0.173816978931427
epoch£º622	 i:7 	 global-step:12447	 l-p:0.1500743329524994
epoch£º622	 i:8 	 global-step:12448	 l-p:0.12900516390800476
epoch£º622	 i:9 	 global-step:12449	 l-p:0.12517623603343964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0255, 4.9918, 5.0177],
        [5.0255, 4.9129, 4.9490],
        [5.0255, 5.0191, 5.0250],
        [5.0255, 5.0253, 5.0255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.15730828046798706 
model_pd.l_d.mean(): -20.308006286621094 
model_pd.lagr.mean(): -20.150697708129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-20.9894], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.15730828046798706
epoch£º623	 i:1 	 global-step:12461	 l-p:0.13037006556987762
epoch£º623	 i:2 	 global-step:12462	 l-p:0.17857062816619873
epoch£º623	 i:3 	 global-step:12463	 l-p:0.2574061453342438
epoch£º623	 i:4 	 global-step:12464	 l-p:0.12134131789207458
epoch£º623	 i:5 	 global-step:12465	 l-p:0.14931127429008484
epoch£º623	 i:6 	 global-step:12466	 l-p:0.13099060952663422
epoch£º623	 i:7 	 global-step:12467	 l-p:0.12978480756282806
epoch£º623	 i:8 	 global-step:12468	 l-p:0.14564090967178345
epoch£º623	 i:9 	 global-step:12469	 l-p:0.12239885330200195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0054, 4.9971, 5.0047],
        [5.0054, 5.3586, 5.3016],
        [5.0054, 5.0037, 5.0053],
        [5.0054, 5.0054, 5.0054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.19581270217895508 
model_pd.l_d.mean(): -20.51166534423828 
model_pd.lagr.mean(): -20.315853118896484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-21.2270], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.19581270217895508
epoch£º624	 i:1 	 global-step:12481	 l-p:0.13101910054683685
epoch£º624	 i:2 	 global-step:12482	 l-p:0.15456758439540863
epoch£º624	 i:3 	 global-step:12483	 l-p:0.1166170984506607
epoch£º624	 i:4 	 global-step:12484	 l-p:0.20905856788158417
epoch£º624	 i:5 	 global-step:12485	 l-p:0.14985299110412598
epoch£º624	 i:6 	 global-step:12486	 l-p:0.1489856243133545
epoch£º624	 i:7 	 global-step:12487	 l-p:0.09742394834756851
epoch£º624	 i:8 	 global-step:12488	 l-p:0.11091986298561096
epoch£º624	 i:9 	 global-step:12489	 l-p:0.15669041872024536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0556, 5.0556, 5.0556],
        [5.0556, 5.0434, 5.0542],
        [5.0556, 5.0544, 5.0556],
        [5.0556, 4.8807, 4.7291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.14895211160182953 
model_pd.l_d.mean(): -20.328781127929688 
model_pd.lagr.mean(): -20.179828643798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4869], device='cuda:0')), ('power', tensor([-21.0483], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.14895211160182953
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09753347933292389
epoch£º625	 i:2 	 global-step:12502	 l-p:0.08190101385116577
epoch£º625	 i:3 	 global-step:12503	 l-p:0.1355116218328476
epoch£º625	 i:4 	 global-step:12504	 l-p:0.14333179593086243
epoch£º625	 i:5 	 global-step:12505	 l-p:0.15233218669891357
epoch£º625	 i:6 	 global-step:12506	 l-p:0.10001645982265472
epoch£º625	 i:7 	 global-step:12507	 l-p:0.1273956149816513
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12877172231674194
epoch£º625	 i:9 	 global-step:12509	 l-p:0.13036654889583588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0870, 5.0459, 5.0757],
        [5.0870, 4.9134, 4.8600],
        [5.0870, 5.0870, 5.0870],
        [5.0870, 5.4852, 5.4546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.04464549571275711 
model_pd.l_d.mean(): -20.83514976501465 
model_pd.lagr.mean(): -20.790504455566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4127], device='cuda:0')), ('power', tensor([-21.4844], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.04464549571275711
epoch£º626	 i:1 	 global-step:12521	 l-p:0.15403255820274353
epoch£º626	 i:2 	 global-step:12522	 l-p:0.12329596281051636
epoch£º626	 i:3 	 global-step:12523	 l-p:0.13646036386489868
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10615845769643784
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11287015676498413
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10181383043527603
epoch£º626	 i:7 	 global-step:12527	 l-p:0.1584959626197815
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11939455568790436
epoch£º626	 i:9 	 global-step:12529	 l-p:0.13127614557743073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0998, 5.0061, 5.0461],
        [5.0998, 4.9299, 4.7749],
        [5.0998, 5.0132, 5.0540],
        [5.0998, 5.0686, 5.0929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.11054733395576477 
model_pd.l_d.mean(): -20.752498626708984 
model_pd.lagr.mean(): -20.641950607299805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-21.3998], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.11054733395576477
epoch£º627	 i:1 	 global-step:12541	 l-p:0.11013130098581314
epoch£º627	 i:2 	 global-step:12542	 l-p:0.15505535900592804
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11640112847089767
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12952445447444916
epoch£º627	 i:5 	 global-step:12545	 l-p:0.13016508519649506
epoch£º627	 i:6 	 global-step:12546	 l-p:0.15687623620033264
epoch£º627	 i:7 	 global-step:12547	 l-p:0.050713472068309784
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15896962583065033
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15139663219451904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0642, 4.8822, 4.7880],
        [5.0642, 5.0639, 5.0642],
        [5.0642, 5.0634, 5.0642],
        [5.0642, 5.0000, 5.0387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07448771595954895 
model_pd.l_d.mean(): -20.78499412536621 
model_pd.lagr.mean(): -20.710506439208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4262], device='cuda:0')), ('power', tensor([-21.4474], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.07448771595954895
epoch£º628	 i:1 	 global-step:12561	 l-p:0.14590266346931458
epoch£º628	 i:2 	 global-step:12562	 l-p:0.1748039573431015
epoch£º628	 i:3 	 global-step:12563	 l-p:0.1681593358516693
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10136888176202774
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14049333333969116
epoch£º628	 i:6 	 global-step:12566	 l-p:0.1294894516468048
epoch£º628	 i:7 	 global-step:12567	 l-p:0.12182479351758957
epoch£º628	 i:8 	 global-step:12568	 l-p:0.1521892547607422
epoch£º628	 i:9 	 global-step:12569	 l-p:0.10777654498815536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0463, 5.0454, 5.0463],
        [5.0463, 4.8694, 4.8208],
        [5.0463, 5.0419, 5.0461],
        [5.0463, 5.2496, 5.0982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.145039901137352 
model_pd.l_d.mean(): -21.05024528503418 
model_pd.lagr.mean(): -20.90520477294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3937], device='cuda:0')), ('power', tensor([-21.6823], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.145039901137352
epoch£º629	 i:1 	 global-step:12581	 l-p:0.12953053414821625
epoch£º629	 i:2 	 global-step:12582	 l-p:0.1308973878622055
epoch£º629	 i:3 	 global-step:12583	 l-p:0.1277501881122589
epoch£º629	 i:4 	 global-step:12584	 l-p:0.18523547053337097
epoch£º629	 i:5 	 global-step:12585	 l-p:0.09458458423614502
epoch£º629	 i:6 	 global-step:12586	 l-p:0.1055162250995636
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12031139433383942
epoch£º629	 i:8 	 global-step:12588	 l-p:0.21075180172920227
epoch£º629	 i:9 	 global-step:12589	 l-p:0.14793847501277924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0357, 5.0357],
        [5.0357, 4.8576, 4.8094],
        [5.0357, 4.8495, 4.7376],
        [5.0357, 5.0357, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.16023549437522888 
model_pd.l_d.mean(): -20.063419342041016 
model_pd.lagr.mean(): -19.903182983398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4954], device='cuda:0')), ('power', tensor([-20.7887], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.16023549437522888
epoch£º630	 i:1 	 global-step:12601	 l-p:0.14955103397369385
epoch£º630	 i:2 	 global-step:12602	 l-p:0.20693343877792358
epoch£º630	 i:3 	 global-step:12603	 l-p:0.10694710165262222
epoch£º630	 i:4 	 global-step:12604	 l-p:0.11032078415155411
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13930773735046387
epoch£º630	 i:6 	 global-step:12606	 l-p:0.10171631723642349
epoch£º630	 i:7 	 global-step:12607	 l-p:0.1215011477470398
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13866907358169556
epoch£º630	 i:9 	 global-step:12609	 l-p:0.12256979942321777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0485, 5.0322, 5.0463],
        [5.0485, 4.9856, 4.7445],
        [5.0485, 5.0485, 5.0485],
        [5.0485, 4.9239, 4.9547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.1185087189078331 
model_pd.l_d.mean(): -20.992324829101562 
model_pd.lagr.mean(): -20.873815536499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3981], device='cuda:0')), ('power', tensor([-21.6283], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.1185087189078331
epoch£º631	 i:1 	 global-step:12621	 l-p:0.11619307100772858
epoch£º631	 i:2 	 global-step:12622	 l-p:0.15118159353733063
epoch£º631	 i:3 	 global-step:12623	 l-p:0.1567392647266388
epoch£º631	 i:4 	 global-step:12624	 l-p:0.21265403926372528
epoch£º631	 i:5 	 global-step:12625	 l-p:0.18684431910514832
epoch£º631	 i:6 	 global-step:12626	 l-p:0.09465166181325912
epoch£º631	 i:7 	 global-step:12627	 l-p:0.12605340778827667
epoch£º631	 i:8 	 global-step:12628	 l-p:0.13030143082141876
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12726496160030365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0298, 4.9759, 5.0117],
        [5.0298, 5.0131, 5.0275],
        [5.0298, 5.0246, 5.0295],
        [5.0298, 5.0257, 5.0296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.13139720261096954 
model_pd.l_d.mean(): -19.5068359375 
model_pd.lagr.mean(): -19.375438690185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5070], device='cuda:0')), ('power', tensor([-20.2378], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.13139720261096954
epoch£º632	 i:1 	 global-step:12641	 l-p:0.15930403769016266
epoch£º632	 i:2 	 global-step:12642	 l-p:0.1318368911743164
epoch£º632	 i:3 	 global-step:12643	 l-p:0.1507602334022522
epoch£º632	 i:4 	 global-step:12644	 l-p:0.1430751383304596
epoch£º632	 i:5 	 global-step:12645	 l-p:0.13642646372318268
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12640024721622467
epoch£º632	 i:7 	 global-step:12647	 l-p:0.22122207283973694
epoch£º632	 i:8 	 global-step:12648	 l-p:0.1194750964641571
epoch£º632	 i:9 	 global-step:12649	 l-p:0.22551387548446655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9919, 4.9777, 4.9902],
        [4.9919, 4.8021, 4.6664],
        [4.9919, 4.8818, 4.9212],
        [4.9919, 5.5243, 5.5927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.13805752992630005 
model_pd.l_d.mean(): -19.875396728515625 
model_pd.lagr.mean(): -19.73733901977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-20.6221], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.13805752992630005
epoch£º633	 i:1 	 global-step:12661	 l-p:0.17882846295833588
epoch£º633	 i:2 	 global-step:12662	 l-p:0.13444453477859497
epoch£º633	 i:3 	 global-step:12663	 l-p:0.208746999502182
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12735114991664886
epoch£º633	 i:5 	 global-step:12665	 l-p:0.3157775402069092
epoch£º633	 i:6 	 global-step:12666	 l-p:0.1566542237997055
epoch£º633	 i:7 	 global-step:12667	 l-p:0.2074957638978958
epoch£º633	 i:8 	 global-step:12668	 l-p:0.15434560179710388
epoch£º633	 i:9 	 global-step:12669	 l-p:0.10176164656877518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0181, 5.0181, 5.0181],
        [5.0181, 5.0143, 5.0179],
        [5.0181, 4.9170, 4.9582],
        [5.0181, 4.9266, 4.9689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.1265624463558197 
model_pd.l_d.mean(): -20.083995819091797 
model_pd.lagr.mean(): -19.957433700561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5097], device='cuda:0')), ('power', tensor([-20.8241], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.1265624463558197
epoch£º634	 i:1 	 global-step:12681	 l-p:0.20463378727436066
epoch£º634	 i:2 	 global-step:12682	 l-p:0.17917302250862122
epoch£º634	 i:3 	 global-step:12683	 l-p:0.1688881665468216
epoch£º634	 i:4 	 global-step:12684	 l-p:0.06360813230276108
epoch£º634	 i:5 	 global-step:12685	 l-p:0.1504489779472351
epoch£º634	 i:6 	 global-step:12686	 l-p:0.1361878514289856
epoch£º634	 i:7 	 global-step:12687	 l-p:0.07409626990556717
epoch£º634	 i:8 	 global-step:12688	 l-p:0.11319859325885773
epoch£º634	 i:9 	 global-step:12689	 l-p:0.14121530950069427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1012, 4.9590, 4.9730],
        [5.1012, 4.9684, 4.9914],
        [5.1012, 5.1003, 5.1012],
        [5.1012, 4.9758, 4.7559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.14348983764648438 
model_pd.l_d.mean(): -20.620071411132812 
model_pd.lagr.mean(): -20.476581573486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.3058], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.14348983764648438
epoch£º635	 i:1 	 global-step:12701	 l-p:0.1337728351354599
epoch£º635	 i:2 	 global-step:12702	 l-p:0.11371549963951111
epoch£º635	 i:3 	 global-step:12703	 l-p:0.14135035872459412
epoch£º635	 i:4 	 global-step:12704	 l-p:0.06269073486328125
epoch£º635	 i:5 	 global-step:12705	 l-p:0.1036476120352745
epoch£º635	 i:6 	 global-step:12706	 l-p:0.14535056054592133
epoch£º635	 i:7 	 global-step:12707	 l-p:0.123907670378685
epoch£º635	 i:8 	 global-step:12708	 l-p:0.1106441393494606
epoch£º635	 i:9 	 global-step:12709	 l-p:0.8390328884124756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1405, 5.0821, 5.1190],
        [5.1405, 5.1233, 5.1380],
        [5.1405, 5.0940, 4.8572],
        [5.1405, 5.1404, 5.1405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11570414155721664 
model_pd.l_d.mean(): -20.49806022644043 
model_pd.lagr.mean(): -20.382356643676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-21.1528], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11570414155721664
epoch£º636	 i:1 	 global-step:12721	 l-p:0.12167369574308395
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10349971801042557
epoch£º636	 i:3 	 global-step:12723	 l-p:0.12173404544591904
epoch£º636	 i:4 	 global-step:12724	 l-p:0.058122627437114716
epoch£º636	 i:5 	 global-step:12725	 l-p:0.07073355466127396
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13336648046970367
epoch£º636	 i:7 	 global-step:12727	 l-p:-0.03566867858171463
epoch£º636	 i:8 	 global-step:12728	 l-p:0.1806890070438385
epoch£º636	 i:9 	 global-step:12729	 l-p:0.1595296412706375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1150, 4.9473, 4.9173],
        [5.1150, 5.6862, 5.7776],
        [5.1150, 4.9908, 5.0199],
        [5.1150, 5.0985, 5.1127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15612220764160156 
model_pd.l_d.mean(): -19.46739959716797 
model_pd.lagr.mean(): -19.311277389526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4852], device='cuda:0')), ('power', tensor([-20.1757], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15612220764160156
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12627744674682617
epoch£º637	 i:2 	 global-step:12742	 l-p:0.09833136945962906
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14405590295791626
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11780912429094315
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12473724037408829
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.005263457074761391
epoch£º637	 i:7 	 global-step:12747	 l-p:0.12166649848222733
epoch£º637	 i:8 	 global-step:12748	 l-p:0.1692342907190323
epoch£º637	 i:9 	 global-step:12749	 l-p:0.0953151136636734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0913, 5.0910, 5.0913],
        [5.0913, 5.0913, 5.0913],
        [5.0913, 5.3025, 5.1531],
        [5.0913, 5.5061, 5.4847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.14035697281360626 
model_pd.l_d.mean(): -20.422269821166992 
model_pd.lagr.mean(): -20.28191375732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5023], device='cuda:0')), ('power', tensor([-21.1585], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.14035697281360626
epoch£º638	 i:1 	 global-step:12761	 l-p:0.08806923031806946
epoch£º638	 i:2 	 global-step:12762	 l-p:0.04845312982797623
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12268234044313431
epoch£º638	 i:4 	 global-step:12764	 l-p:0.14204303920269012
epoch£º638	 i:5 	 global-step:12765	 l-p:0.10612060129642487
epoch£º638	 i:6 	 global-step:12766	 l-p:0.15711374580860138
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14701198041439056
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14204376935958862
epoch£º638	 i:9 	 global-step:12769	 l-p:0.1303340047597885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0902, 5.5396, 5.5423],
        [5.0902, 5.0865, 5.0900],
        [5.0902, 4.9107, 4.8490],
        [5.0902, 4.9175, 4.8803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.10376773774623871 
model_pd.l_d.mean(): -18.346399307250977 
model_pd.lagr.mean(): -18.242631912231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6318], device='cuda:0')), ('power', tensor([-19.1923], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.10376773774623871
epoch£º639	 i:1 	 global-step:12781	 l-p:0.1823747605085373
epoch£º639	 i:2 	 global-step:12782	 l-p:0.12608562409877777
epoch£º639	 i:3 	 global-step:12783	 l-p:0.08521025627851486
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13929882645606995
epoch£º639	 i:5 	 global-step:12785	 l-p:0.068704754114151
epoch£º639	 i:6 	 global-step:12786	 l-p:0.13993118703365326
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14029818773269653
epoch£º639	 i:8 	 global-step:12788	 l-p:-2.9650089740753174
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13622210919857025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 4.9624, 4.9119],
        [5.1361, 4.9592, 4.8237],
        [5.1361, 5.1361, 5.1361],
        [5.1361, 5.1188, 5.1336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.0977870374917984 
model_pd.l_d.mean(): -20.042505264282227 
model_pd.lagr.mean(): -19.944717407226562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-20.7932], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.0977870374917984
epoch£º640	 i:1 	 global-step:12801	 l-p:0.14086037874221802
epoch£º640	 i:2 	 global-step:12802	 l-p:-0.3964157700538635
epoch£º640	 i:3 	 global-step:12803	 l-p:0.09024820476770401
epoch£º640	 i:4 	 global-step:12804	 l-p:0.1671920120716095
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11808229237794876
epoch£º640	 i:6 	 global-step:12806	 l-p:0.12618865072727203
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12091593444347382
epoch£º640	 i:8 	 global-step:12808	 l-p:0.12743501365184784
epoch£º640	 i:9 	 global-step:12809	 l-p:0.13843365013599396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 5.1268, 5.1281],
        [5.1281, 5.0963, 4.8576],
        [5.1281, 5.1281, 5.1281],
        [5.1281, 5.0692, 5.1065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.009804563596844673 
model_pd.l_d.mean(): -20.636308670043945 
model_pd.lagr.mean(): -20.62650489807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-21.3178], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.009804563596844673
epoch£º641	 i:1 	 global-step:12821	 l-p:0.09084537625312805
epoch£º641	 i:2 	 global-step:12822	 l-p:0.1062740907073021
epoch£º641	 i:3 	 global-step:12823	 l-p:0.10844705998897552
epoch£º641	 i:4 	 global-step:12824	 l-p:0.1498824805021286
epoch£º641	 i:5 	 global-step:12825	 l-p:0.1758660078048706
epoch£º641	 i:6 	 global-step:12826	 l-p:0.13378101587295532
epoch£º641	 i:7 	 global-step:12827	 l-p:-0.044230856001377106
epoch£º641	 i:8 	 global-step:12828	 l-p:0.13322506844997406
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13573673367500305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1010, 5.1008, 5.1010],
        [5.1010, 4.9278, 4.8905],
        [5.1010, 5.1010, 5.1010],
        [5.1010, 5.5751, 5.5943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.0028606057167053223 
model_pd.l_d.mean(): -19.195024490356445 
model_pd.lagr.mean(): -19.192163467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5668], device='cuda:0')), ('power', tensor([-19.9837], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.0028606057167053223
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15790072083473206
epoch£º642	 i:2 	 global-step:12842	 l-p:0.18665015697479248
epoch£º642	 i:3 	 global-step:12843	 l-p:0.13068819046020508
epoch£º642	 i:4 	 global-step:12844	 l-p:0.09638357162475586
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14656095206737518
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12922382354736328
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11429760605096817
epoch£º642	 i:8 	 global-step:12848	 l-p:0.09749005734920502
epoch£º642	 i:9 	 global-step:12849	 l-p:0.18432967364788055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0391, 4.9731, 5.0130],
        [5.0391, 5.0391, 5.0391],
        [5.0391, 4.9123, 4.6808],
        [5.0391, 5.0391, 5.0391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.10311603546142578 
model_pd.l_d.mean(): -19.040864944458008 
model_pd.lagr.mean(): -18.937747955322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5624], device='cuda:0')), ('power', tensor([-19.8234], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.10311603546142578
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12235262244939804
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13252438604831696
epoch£º643	 i:3 	 global-step:12863	 l-p:0.10844799131155014
epoch£º643	 i:4 	 global-step:12864	 l-p:0.23285403847694397
epoch£º643	 i:5 	 global-step:12865	 l-p:0.1029043048620224
epoch£º643	 i:6 	 global-step:12866	 l-p:0.18306906521320343
epoch£º643	 i:7 	 global-step:12867	 l-p:0.12312464416027069
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13687986135482788
epoch£º643	 i:9 	 global-step:12869	 l-p:0.15579961240291595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0601, 4.8715, 4.7788],
        [5.0601, 4.9362, 4.7050],
        [5.0601, 4.8820, 4.8416],
        [5.0601, 4.9627, 5.0046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.14960376918315887 
model_pd.l_d.mean(): -20.281015396118164 
model_pd.lagr.mean(): -20.131412506103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4999], device='cuda:0')), ('power', tensor([-21.0133], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.14960376918315887
epoch£º644	 i:1 	 global-step:12881	 l-p:0.18106147646903992
epoch£º644	 i:2 	 global-step:12882	 l-p:0.14615269005298615
epoch£º644	 i:3 	 global-step:12883	 l-p:0.1280146688222885
epoch£º644	 i:4 	 global-step:12884	 l-p:0.1601373255252838
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14797843992710114
epoch£º644	 i:6 	 global-step:12886	 l-p:0.1333889365196228
epoch£º644	 i:7 	 global-step:12887	 l-p:0.04312289506196976
epoch£º644	 i:8 	 global-step:12888	 l-p:0.06970936805009842
epoch£º644	 i:9 	 global-step:12889	 l-p:0.067318856716156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1118, 5.0549, 5.0917],
        [5.1118, 4.9480, 4.9338],
        [5.1118, 5.0910, 4.8510],
        [5.1118, 5.1086, 5.1116]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14549630880355835 
model_pd.l_d.mean(): -20.06273651123047 
model_pd.lagr.mean(): -19.917240142822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5000], device='cuda:0')), ('power', tensor([-20.7927], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14549630880355835
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.038954686373472214
epoch£º645	 i:2 	 global-step:12902	 l-p:0.11805229634046555
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11267827451229095
epoch£º645	 i:4 	 global-step:12904	 l-p:0.12254434078931808
epoch£º645	 i:5 	 global-step:12905	 l-p:0.031862176954746246
epoch£º645	 i:6 	 global-step:12906	 l-p:0.14280138909816742
epoch£º645	 i:7 	 global-step:12907	 l-p:0.08511548489332199
epoch£º645	 i:8 	 global-step:12908	 l-p:0.10180072486400604
epoch£º645	 i:9 	 global-step:12909	 l-p:0.12599913775920868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1804, 5.1120, 4.8751],
        [5.1804, 5.6751, 5.7048],
        [5.1804, 5.7153, 5.7744],
        [5.1804, 5.0138, 4.9831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10295139253139496 
model_pd.l_d.mean(): -19.395423889160156 
model_pd.lagr.mean(): -19.29247283935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.1448], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10295139253139496
epoch£º646	 i:1 	 global-step:12921	 l-p:0.1593327820301056
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12483059614896774
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13586938381195068
epoch£º646	 i:4 	 global-step:12924	 l-p:-0.7708505988121033
epoch£º646	 i:5 	 global-step:12925	 l-p:0.049477510154247284
epoch£º646	 i:6 	 global-step:12926	 l-p:0.1207810491323471
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12411347776651382
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13445782661437988
epoch£º646	 i:9 	 global-step:12929	 l-p:0.22324904799461365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1576, 5.1576, 5.1576],
        [5.1576, 5.1573, 5.1576],
        [5.1576, 5.0429, 4.8169],
        [5.1576, 5.1573, 5.1576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.1369027942419052 
model_pd.l_d.mean(): -20.069082260131836 
model_pd.lagr.mean(): -19.932180404663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5038], device='cuda:0')), ('power', tensor([-20.8029], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.1369027942419052
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1313396394252777
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12418877333402634
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12084724009037018
epoch£º647	 i:4 	 global-step:12944	 l-p:0.11078459769487381
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12897677719593048
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.004185905214399099
epoch£º647	 i:7 	 global-step:12947	 l-p:0.09971945732831955
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14779365062713623
epoch£º647	 i:9 	 global-step:12949	 l-p:0.09288343042135239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0789, 5.0548, 4.8118],
        [5.0789, 5.0789, 5.0789],
        [5.0789, 5.0417, 5.0697],
        [5.0789, 5.0744, 5.0787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.11906281858682632 
model_pd.l_d.mean(): -20.786821365356445 
model_pd.lagr.mean(): -20.66775894165039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4383], device='cuda:0')), ('power', tensor([-21.4617], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.11906281858682632
epoch£º648	 i:1 	 global-step:12961	 l-p:0.12776373326778412
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17464229464530945
epoch£º648	 i:3 	 global-step:12963	 l-p:0.1439783275127411
epoch£º648	 i:4 	 global-step:12964	 l-p:0.18199200928211212
epoch£º648	 i:5 	 global-step:12965	 l-p:0.14718705415725708
epoch£º648	 i:6 	 global-step:12966	 l-p:0.13219256699085236
epoch£º648	 i:7 	 global-step:12967	 l-p:0.10136926174163818
epoch£º648	 i:8 	 global-step:12968	 l-p:0.12077751755714417
epoch£º648	 i:9 	 global-step:12969	 l-p:0.09791450947523117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0658, 5.0291, 5.0569],
        [5.0658, 5.0629, 5.0657],
        [5.0658, 5.0635, 5.0657],
        [5.0658, 4.8865, 4.8461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12290022522211075 
model_pd.l_d.mean(): -20.087743759155273 
model_pd.lagr.mean(): -19.96484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-20.7941], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12290022522211075
epoch£º649	 i:1 	 global-step:12981	 l-p:0.12195231765508652
epoch£º649	 i:2 	 global-step:12982	 l-p:0.13039779663085938
epoch£º649	 i:3 	 global-step:12983	 l-p:0.1516832709312439
epoch£º649	 i:4 	 global-step:12984	 l-p:0.1856652796268463
epoch£º649	 i:5 	 global-step:12985	 l-p:0.1341111809015274
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13315542042255402
epoch£º649	 i:7 	 global-step:12987	 l-p:0.1544322520494461
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06374756991863251
epoch£º649	 i:9 	 global-step:12989	 l-p:0.2215346246957779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0338, 4.9406, 4.9839],
        [5.0338, 4.9862, 5.0197],
        [5.0338, 4.8719, 4.6622],
        [5.0338, 4.8854, 4.9022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.10788436979055405 
model_pd.l_d.mean(): -20.731016159057617 
model_pd.lagr.mean(): -20.623132705688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4539], device='cuda:0')), ('power', tensor([-21.4211], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.10788436979055405
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11293476074934006
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16721175611019135
epoch£º650	 i:3 	 global-step:13003	 l-p:0.06731981039047241
epoch£º650	 i:4 	 global-step:13004	 l-p:0.14004075527191162
epoch£º650	 i:5 	 global-step:13005	 l-p:0.1546790450811386
epoch£º650	 i:6 	 global-step:13006	 l-p:0.15452425181865692
epoch£º650	 i:7 	 global-step:13007	 l-p:0.1547688990831375
epoch£º650	 i:8 	 global-step:13008	 l-p:0.1494765430688858
epoch£º650	 i:9 	 global-step:13009	 l-p:0.15575875341892242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0659, 5.0659, 5.0659],
        [5.0659, 5.5027, 5.4953],
        [5.0659, 5.0659, 5.0659],
        [5.0659, 4.8977, 4.8846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.16008520126342773 
model_pd.l_d.mean(): -20.303998947143555 
model_pd.lagr.mean(): -20.14391326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-21.0345], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.16008520126342773
epoch£º651	 i:1 	 global-step:13021	 l-p:0.11940207332372665
epoch£º651	 i:2 	 global-step:13022	 l-p:0.0863645151257515
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11871601641178131
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12886138260364532
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13704706728458405
epoch£º651	 i:6 	 global-step:13026	 l-p:0.16189071536064148
epoch£º651	 i:7 	 global-step:13027	 l-p:0.18470175564289093
epoch£º651	 i:8 	 global-step:13028	 l-p:0.24312026798725128
epoch£º651	 i:9 	 global-step:13029	 l-p:0.10012096911668777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0319, 4.9207, 4.9612],
        [5.0319, 5.0202, 5.0307],
        [5.0319, 5.0319, 5.0319],
        [5.0319, 5.0306, 5.0318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.1066448986530304 
model_pd.l_d.mean(): -20.564794540405273 
model_pd.lagr.mean(): -20.458148956298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4527], device='cuda:0')), ('power', tensor([-21.2520], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.1066448986530304
epoch£º652	 i:1 	 global-step:13041	 l-p:0.160600945353508
epoch£º652	 i:2 	 global-step:13042	 l-p:0.1282660961151123
epoch£º652	 i:3 	 global-step:13043	 l-p:0.1577727347612381
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08275018632411957
epoch£º652	 i:5 	 global-step:13045	 l-p:0.18318802118301392
epoch£º652	 i:6 	 global-step:13046	 l-p:0.15361648797988892
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1591750681400299
epoch£º652	 i:8 	 global-step:13048	 l-p:0.12656036019325256
epoch£º652	 i:9 	 global-step:13049	 l-p:0.10350129753351212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1107, 5.2397, 5.0449],
        [5.1107, 5.0151, 4.7745],
        [5.1107, 5.2475, 5.0563],
        [5.1107, 5.0448, 5.0846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.16841194033622742 
model_pd.l_d.mean(): -20.18349266052246 
model_pd.lagr.mean(): -20.01508140563965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4789], device='cuda:0')), ('power', tensor([-20.8932], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.16841194033622742
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11979873478412628
epoch£º653	 i:2 	 global-step:13062	 l-p:0.14829201996326447
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12552523612976074
epoch£º653	 i:4 	 global-step:13064	 l-p:0.14136546850204468
epoch£º653	 i:5 	 global-step:13065	 l-p:0.02139720320701599
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08543676882982254
epoch£º653	 i:7 	 global-step:13067	 l-p:0.1308840960264206
epoch£º653	 i:8 	 global-step:13068	 l-p:0.11828763782978058
epoch£º653	 i:9 	 global-step:13069	 l-p:0.06881088763475418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1322, 5.2452, 5.0429],
        [5.1322, 4.9778, 4.9813],
        [5.1322, 5.1322, 5.1322],
        [5.1322, 4.9607, 4.9328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12408071011304855 
model_pd.l_d.mean(): -19.66248321533203 
model_pd.lagr.mean(): -19.538402557373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-20.3784], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12408071011304855
epoch£º654	 i:1 	 global-step:13081	 l-p:0.05553809180855751
epoch£º654	 i:2 	 global-step:13082	 l-p:0.1422061175107956
epoch£º654	 i:3 	 global-step:13083	 l-p:0.08691288530826569
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13805294036865234
epoch£º654	 i:5 	 global-step:13085	 l-p:0.07367517799139023
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12878428399562836
epoch£º654	 i:7 	 global-step:13087	 l-p:0.11293968558311462
epoch£º654	 i:8 	 global-step:13088	 l-p:0.30446577072143555
epoch£º654	 i:9 	 global-step:13089	 l-p:0.13702702522277832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1569, 4.9755, 4.8364],
        [5.1569, 5.0038, 4.8022],
        [5.1569, 4.9821, 4.9410],
        [5.1569, 5.0316, 5.0623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11586051434278488 
model_pd.l_d.mean(): -20.545133590698242 
model_pd.lagr.mean(): -20.42927360534668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.2113], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11586051434278488
epoch£º655	 i:1 	 global-step:13101	 l-p:0.06432534009218216
epoch£º655	 i:2 	 global-step:13102	 l-p:0.14322209358215332
epoch£º655	 i:3 	 global-step:13103	 l-p:0.1767914891242981
epoch£º655	 i:4 	 global-step:13104	 l-p:0.04874265193939209
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12992778420448303
epoch£º655	 i:6 	 global-step:13106	 l-p:0.1308833509683609
epoch£º655	 i:7 	 global-step:13107	 l-p:0.10650944709777832
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.12025431543588638
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12350104004144669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 5.0972, 5.1143],
        [5.1175, 5.0853, 5.1104],
        [5.1175, 5.1029, 5.1157],
        [5.1175, 5.0114, 4.7730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11822917312383652 
model_pd.l_d.mean(): -20.598472595214844 
model_pd.lagr.mean(): -20.480243682861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-21.2663], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11822917312383652
epoch£º656	 i:1 	 global-step:13121	 l-p:0.13069234788417816
epoch£º656	 i:2 	 global-step:13122	 l-p:0.05127614736557007
epoch£º656	 i:3 	 global-step:13123	 l-p:0.1359572410583496
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13965074717998505
epoch£º656	 i:5 	 global-step:13125	 l-p:0.22154712677001953
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14671839773654938
epoch£º656	 i:7 	 global-step:13127	 l-p:0.060369785875082016
epoch£º656	 i:8 	 global-step:13128	 l-p:0.14217975735664368
epoch£º656	 i:9 	 global-step:13129	 l-p:0.12118930369615555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0792, 4.9027, 4.8740],
        [5.0792, 4.8870, 4.7643],
        [5.0792, 5.0656, 4.8215],
        [5.0792, 4.9826, 4.7378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.14163120090961456 
model_pd.l_d.mean(): -20.439870834350586 
model_pd.lagr.mean(): -20.29823875427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4808], device='cuda:0')), ('power', tensor([-21.1543], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.14163120090961456
epoch£º657	 i:1 	 global-step:13141	 l-p:0.09071774035692215
epoch£º657	 i:2 	 global-step:13142	 l-p:0.17320796847343445
epoch£º657	 i:3 	 global-step:13143	 l-p:0.14243003726005554
epoch£º657	 i:4 	 global-step:13144	 l-p:0.118824802339077
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10162759572267532
epoch£º657	 i:6 	 global-step:13146	 l-p:0.13810338079929352
epoch£º657	 i:7 	 global-step:13147	 l-p:0.1249466985464096
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1433931142091751
epoch£º657	 i:9 	 global-step:13149	 l-p:0.010159301571547985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1161, 5.1161, 5.1161],
        [5.1161, 4.9308, 4.7829],
        [5.1161, 4.9329, 4.8766],
        [5.1161, 4.9291, 4.8539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07673857361078262 
model_pd.l_d.mean(): -20.224611282348633 
model_pd.lagr.mean(): -20.147872924804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4828], device='cuda:0')), ('power', tensor([-20.9388], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.07673857361078262
epoch£º658	 i:1 	 global-step:13161	 l-p:0.14967618882656097
epoch£º658	 i:2 	 global-step:13162	 l-p:0.1623249053955078
epoch£º658	 i:3 	 global-step:13163	 l-p:0.10343848168849945
epoch£º658	 i:4 	 global-step:13164	 l-p:0.13808351755142212
epoch£º658	 i:5 	 global-step:13165	 l-p:0.1082698330283165
epoch£º658	 i:6 	 global-step:13166	 l-p:0.0007627320010215044
epoch£º658	 i:7 	 global-step:13167	 l-p:0.1278252750635147
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14917561411857605
epoch£º658	 i:9 	 global-step:13169	 l-p:0.12447851151227951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1174, 5.0947, 4.8502],
        [5.1174, 5.1170, 5.1174],
        [5.1174, 4.9277, 4.8123],
        [5.1174, 5.1164, 5.1173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12218032777309418 
model_pd.l_d.mean(): -20.48151969909668 
model_pd.lagr.mean(): -20.359338760375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.1374], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12218032777309418
epoch£º659	 i:1 	 global-step:13181	 l-p:0.11025229096412659
epoch£º659	 i:2 	 global-step:13182	 l-p:0.16244609653949738
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13325470685958862
epoch£º659	 i:4 	 global-step:13184	 l-p:0.1181056946516037
epoch£º659	 i:5 	 global-step:13185	 l-p:0.1319914609193802
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10920143127441406
epoch£º659	 i:7 	 global-step:13187	 l-p:0.13130122423171997
epoch£º659	 i:8 	 global-step:13188	 l-p:0.19921617209911346
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13273555040359497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0368, 5.2170, 5.0478],
        [5.0368, 4.9185, 4.9575],
        [5.0368, 5.0160, 4.7679],
        [5.0368, 5.0367, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14549650251865387 
model_pd.l_d.mean(): -19.683696746826172 
model_pd.lagr.mean(): -19.53820037841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5672], device='cuda:0')), ('power', tensor([-20.4781], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14549650251865387
epoch£º660	 i:1 	 global-step:13201	 l-p:0.16171962022781372
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13798290491104126
epoch£º660	 i:3 	 global-step:13203	 l-p:0.1272232085466385
epoch£º660	 i:4 	 global-step:13204	 l-p:0.1712953746318817
epoch£º660	 i:5 	 global-step:13205	 l-p:0.13234102725982666
epoch£º660	 i:6 	 global-step:13206	 l-p:0.11611207574605942
epoch£º660	 i:7 	 global-step:13207	 l-p:0.12451203167438507
epoch£º660	 i:8 	 global-step:13208	 l-p:0.1250472068786621
epoch£º660	 i:9 	 global-step:13209	 l-p:0.21433125436306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0435, 5.0426, 5.0435],
        [5.0435, 4.9430, 4.9864],
        [5.0435, 5.0169, 5.0386],
        [5.0435, 4.8980, 4.9202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.1381750851869583 
model_pd.l_d.mean(): -19.972829818725586 
model_pd.lagr.mean(): -19.834653854370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.6928], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.1381750851869583
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13298045098781586
epoch£º661	 i:2 	 global-step:13222	 l-p:0.1556394398212433
epoch£º661	 i:3 	 global-step:13223	 l-p:0.1948622614145279
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11462295055389404
epoch£º661	 i:5 	 global-step:13225	 l-p:0.1105925664305687
epoch£º661	 i:6 	 global-step:13226	 l-p:0.13525822758674622
epoch£º661	 i:7 	 global-step:13227	 l-p:0.1737004816532135
epoch£º661	 i:8 	 global-step:13228	 l-p:0.10919627547264099
epoch£º661	 i:9 	 global-step:13229	 l-p:0.1877395212650299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0473, 5.0473, 5.0473],
        [5.0473, 5.0473, 5.0473],
        [5.0473, 5.0255, 5.0438],
        [5.0473, 4.8746, 4.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13616415858268738 
model_pd.l_d.mean(): -20.906845092773438 
model_pd.lagr.mean(): -20.770681381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4074], device='cuda:0')), ('power', tensor([-21.5514], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13616415858268738
epoch£º662	 i:1 	 global-step:13241	 l-p:0.10366403311491013
epoch£º662	 i:2 	 global-step:13242	 l-p:0.14994359016418457
epoch£º662	 i:3 	 global-step:13243	 l-p:0.0996231660246849
epoch£º662	 i:4 	 global-step:13244	 l-p:0.1073458269238472
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12836194038391113
epoch£º662	 i:6 	 global-step:13246	 l-p:0.15607000887393951
epoch£º662	 i:7 	 global-step:13247	 l-p:0.15273070335388184
epoch£º662	 i:8 	 global-step:13248	 l-p:0.1442277878522873
epoch£º662	 i:9 	 global-step:13249	 l-p:0.1471259891986847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0890, 5.2136, 5.0148],
        [5.0890, 5.0292, 5.0677],
        [5.0890, 4.8950, 4.7711],
        [5.0890, 5.0762, 5.0876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.1281653791666031 
model_pd.l_d.mean(): -20.588665008544922 
model_pd.lagr.mean(): -20.460498809814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4294], device='cuda:0')), ('power', tensor([-21.2522], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.1281653791666031
epoch£º663	 i:1 	 global-step:13261	 l-p:0.1157245859503746
epoch£º663	 i:2 	 global-step:13262	 l-p:0.08203652501106262
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10777899622917175
epoch£º663	 i:4 	 global-step:13264	 l-p:0.13540808856487274
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19755053520202637
epoch£º663	 i:6 	 global-step:13266	 l-p:0.17013640701770782
epoch£º663	 i:7 	 global-step:13267	 l-p:0.10153989493846893
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10464918613433838
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12362832576036453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0985, 5.0984, 5.0985],
        [5.0985, 4.9083, 4.7596],
        [5.0985, 5.0967, 5.0984],
        [5.0985, 4.9353, 4.9339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.1306086629629135 
model_pd.l_d.mean(): -20.381860733032227 
model_pd.lagr.mean(): -20.251251220703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-21.0861], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.1306086629629135
epoch£º664	 i:1 	 global-step:13281	 l-p:0.15900424122810364
epoch£º664	 i:2 	 global-step:13282	 l-p:0.1254778653383255
epoch£º664	 i:3 	 global-step:13283	 l-p:0.11174121499061584
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12908324599266052
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12744534015655518
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16445422172546387
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12586259841918945
epoch£º664	 i:8 	 global-step:13288	 l-p:0.16984103620052338
epoch£º664	 i:9 	 global-step:13289	 l-p:0.055182915180921555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0592, 5.0569, 5.0591],
        [5.0592, 5.0568, 5.0591],
        [5.0592, 4.8625, 4.7225],
        [5.0592, 4.9857, 5.0281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.10022079199552536 
model_pd.l_d.mean(): -20.40863037109375 
model_pd.lagr.mean(): -20.308408737182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-21.1390], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.10022079199552536
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13810136914253235
epoch£º665	 i:2 	 global-step:13302	 l-p:0.18003787100315094
epoch£º665	 i:3 	 global-step:13303	 l-p:0.12992917001247406
epoch£º665	 i:4 	 global-step:13304	 l-p:0.12526100873947144
epoch£º665	 i:5 	 global-step:13305	 l-p:0.13669854402542114
epoch£º665	 i:6 	 global-step:13306	 l-p:0.1371285766363144
epoch£º665	 i:7 	 global-step:13307	 l-p:0.17854346334934235
epoch£º665	 i:8 	 global-step:13308	 l-p:0.10145660489797592
epoch£º665	 i:9 	 global-step:13309	 l-p:0.14435015618801117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0539, 5.0539, 5.0539],
        [5.0539, 4.9699, 4.7168],
        [5.0539, 4.9862, 4.7324],
        [5.0539, 4.8938, 4.9017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12464961409568787 
model_pd.l_d.mean(): -18.877668380737305 
model_pd.lagr.mean(): -18.753019332885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5549], device='cuda:0')), ('power', tensor([-19.6508], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12464961409568787
epoch£º666	 i:1 	 global-step:13321	 l-p:0.093437060713768
epoch£º666	 i:2 	 global-step:13322	 l-p:0.13256672024726868
epoch£º666	 i:3 	 global-step:13323	 l-p:0.15519237518310547
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10689214617013931
epoch£º666	 i:5 	 global-step:13325	 l-p:0.16047565639019012
epoch£º666	 i:6 	 global-step:13326	 l-p:0.15717025101184845
epoch£º666	 i:7 	 global-step:13327	 l-p:0.137544184923172
epoch£º666	 i:8 	 global-step:13328	 l-p:0.14255458116531372
epoch£º666	 i:9 	 global-step:13329	 l-p:0.2026010900735855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0519, 5.5351, 5.5582],
        [5.0519, 4.8573, 4.7916],
        [5.0519, 5.2972, 5.1627],
        [5.0519, 4.8560, 4.6988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.12838761508464813 
model_pd.l_d.mean(): -20.272701263427734 
model_pd.lagr.mean(): -20.14431381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-21.0173], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.12838761508464813
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11504008620977402
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14056482911109924
epoch£º667	 i:3 	 global-step:13343	 l-p:0.17405568063259125
epoch£º667	 i:4 	 global-step:13344	 l-p:0.16336216032505035
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17680154740810394
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11814069747924805
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11095017939805984
epoch£º667	 i:8 	 global-step:13348	 l-p:0.13684022426605225
epoch£º667	 i:9 	 global-step:13349	 l-p:0.1102171316742897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0650, 5.0356, 5.0592],
        [5.0650, 5.0650, 5.0650],
        [5.0650, 4.8818, 4.8502],
        [5.0650, 4.9616, 4.7109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11656152456998825 
model_pd.l_d.mean(): -19.968748092651367 
model_pd.lagr.mean(): -19.85218620300293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5179], device='cuda:0')), ('power', tensor([-20.7160], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11656152456998825
epoch£º668	 i:1 	 global-step:13361	 l-p:0.14067552983760834
epoch£º668	 i:2 	 global-step:13362	 l-p:0.1268327236175537
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15685375034809113
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12023534625768661
epoch£º668	 i:5 	 global-step:13365	 l-p:0.15909864008426666
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11891581863164902
epoch£º668	 i:7 	 global-step:13367	 l-p:0.16371001303195953
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11247982829809189
epoch£º668	 i:9 	 global-step:13369	 l-p:0.24231499433517456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0349, 4.8802, 4.8972],
        [5.0349, 5.0348, 5.0349],
        [5.0349, 5.0349, 5.0349],
        [5.0349, 4.8392, 4.6683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13681957125663757 
model_pd.l_d.mean(): -20.103567123413086 
model_pd.lagr.mean(): -19.966747283935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4365], device='cuda:0')), ('power', tensor([-20.7691], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13681957125663757
epoch£º669	 i:1 	 global-step:13381	 l-p:0.16480837762355804
epoch£º669	 i:2 	 global-step:13382	 l-p:0.13302776217460632
epoch£º669	 i:3 	 global-step:13383	 l-p:0.15883973240852356
epoch£º669	 i:4 	 global-step:13384	 l-p:0.16984295845031738
epoch£º669	 i:5 	 global-step:13385	 l-p:0.11139635741710663
epoch£º669	 i:6 	 global-step:13386	 l-p:0.15358604490756989
epoch£º669	 i:7 	 global-step:13387	 l-p:0.19842204451560974
epoch£º669	 i:8 	 global-step:13388	 l-p:0.18380588293075562
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11161260306835175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0397, 5.0397, 5.0397],
        [5.0397, 4.9695, 5.0116],
        [5.0397, 5.0382, 5.0397],
        [5.0397, 5.0397, 5.0397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.2535800039768219 
model_pd.l_d.mean(): -20.62137794494629 
model_pd.lagr.mean(): -20.3677978515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-21.3409], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.2535800039768219
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11767324060201645
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13910642266273499
epoch£º670	 i:3 	 global-step:13403	 l-p:0.13329213857650757
epoch£º670	 i:4 	 global-step:13404	 l-p:0.15530987083911896
epoch£º670	 i:5 	 global-step:13405	 l-p:0.10079919546842575
epoch£º670	 i:6 	 global-step:13406	 l-p:0.15504471957683563
epoch£º670	 i:7 	 global-step:13407	 l-p:0.12605442106723785
epoch£º670	 i:8 	 global-step:13408	 l-p:0.149900883436203
epoch£º670	 i:9 	 global-step:13409	 l-p:0.1577121615409851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0233, 5.0126, 5.0223],
        [5.0233, 5.0197, 5.0232],
        [5.0233, 4.8295, 4.7809],
        [5.0233, 5.0196, 5.0232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.14362584054470062 
model_pd.l_d.mean(): -20.746139526367188 
model_pd.lagr.mean(): -20.602514266967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4386], device='cuda:0')), ('power', tensor([-21.4208], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.14362584054470062
epoch£º671	 i:1 	 global-step:13421	 l-p:0.18013252317905426
epoch£º671	 i:2 	 global-step:13422	 l-p:0.117258220911026
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16954723000526428
epoch£º671	 i:4 	 global-step:13424	 l-p:0.32766222953796387
epoch£º671	 i:5 	 global-step:13425	 l-p:0.21060273051261902
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09699664264917374
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12405559420585632
epoch£º671	 i:8 	 global-step:13428	 l-p:0.1850520223379135
epoch£º671	 i:9 	 global-step:13429	 l-p:0.15055890381336212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0188, 4.9976, 5.0155],
        [5.0188, 4.8772, 4.6328],
        [5.0188, 5.0188, 5.0188],
        [5.0188, 4.8328, 4.8056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.27404114603996277 
model_pd.l_d.mean(): -20.131153106689453 
model_pd.lagr.mean(): -19.857112884521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5174], device='cuda:0')), ('power', tensor([-20.8796], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.27404114603996277
epoch£º672	 i:1 	 global-step:13441	 l-p:0.1362406462430954
epoch£º672	 i:2 	 global-step:13442	 l-p:0.15961381793022156
epoch£º672	 i:3 	 global-step:13443	 l-p:0.15927007794380188
epoch£º672	 i:4 	 global-step:13444	 l-p:0.1071782112121582
epoch£º672	 i:5 	 global-step:13445	 l-p:0.1180466040968895
epoch£º672	 i:6 	 global-step:13446	 l-p:0.14896021783351898
epoch£º672	 i:7 	 global-step:13447	 l-p:0.13837213814258575
epoch£º672	 i:8 	 global-step:13448	 l-p:0.12010882049798965
epoch£º672	 i:9 	 global-step:13449	 l-p:0.24271129071712494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0190, 5.0187, 5.0190],
        [5.0190, 5.0190, 5.0190],
        [5.0190, 5.0058, 4.7540],
        [5.0190, 5.0085, 5.0180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.13450871407985687 
model_pd.l_d.mean(): -20.24911117553711 
model_pd.lagr.mean(): -20.11460304260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4694], device='cuda:0')), ('power', tensor([-20.9498], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.13450871407985687
epoch£º673	 i:1 	 global-step:13461	 l-p:0.16479654610157013
epoch£º673	 i:2 	 global-step:13462	 l-p:0.19679893553256989
epoch£º673	 i:3 	 global-step:13463	 l-p:0.3113328814506531
epoch£º673	 i:4 	 global-step:13464	 l-p:0.1319456398487091
epoch£º673	 i:5 	 global-step:13465	 l-p:0.15829451382160187
epoch£º673	 i:6 	 global-step:13466	 l-p:0.1760668158531189
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09522528201341629
epoch£º673	 i:8 	 global-step:13468	 l-p:0.15664975345134735
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09776971489191055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0321, 4.9210, 4.9643],
        [5.0321, 4.9353, 4.9805],
        [5.0321, 4.8850, 4.9104],
        [5.0321, 4.8268, 4.7090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.1700696498155594 
model_pd.l_d.mean(): -19.559228897094727 
model_pd.lagr.mean(): -19.38916015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5593], device='cuda:0')), ('power', tensor([-20.3443], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.1700696498155594
epoch£º674	 i:1 	 global-step:13481	 l-p:0.1324520856142044
epoch£º674	 i:2 	 global-step:13482	 l-p:0.10706867277622223
epoch£º674	 i:3 	 global-step:13483	 l-p:0.1341639906167984
epoch£º674	 i:4 	 global-step:13484	 l-p:0.0758054330945015
epoch£º674	 i:5 	 global-step:13485	 l-p:0.13573525846004486
epoch£º674	 i:6 	 global-step:13486	 l-p:0.21745726466178894
epoch£º674	 i:7 	 global-step:13487	 l-p:0.16548395156860352
epoch£º674	 i:8 	 global-step:13488	 l-p:0.12973694503307343
epoch£º674	 i:9 	 global-step:13489	 l-p:0.1392173320055008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0955, 4.9312, 4.9341],
        [5.0955, 4.8978, 4.8117],
        [5.0955, 5.0021, 5.0465],
        [5.0955, 5.0240, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10016711801290512 
model_pd.l_d.mean(): -20.162748336791992 
model_pd.lagr.mean(): -20.06258201599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4667], device='cuda:0')), ('power', tensor([-20.8598], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10016711801290512
epoch£º675	 i:1 	 global-step:13501	 l-p:0.131992906332016
epoch£º675	 i:2 	 global-step:13502	 l-p:0.13300104439258575
epoch£º675	 i:3 	 global-step:13503	 l-p:0.12855061888694763
epoch£º675	 i:4 	 global-step:13504	 l-p:0.15922881662845612
epoch£º675	 i:5 	 global-step:13505	 l-p:0.1500757932662964
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12137530744075775
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12214698642492294
epoch£º675	 i:8 	 global-step:13508	 l-p:0.158345028758049
epoch£º675	 i:9 	 global-step:13509	 l-p:0.04632551968097687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0966, 5.0966, 5.0966],
        [5.0966, 4.9076, 4.7285],
        [5.0966, 5.0847, 5.0954],
        [5.0966, 5.0927, 5.0964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.10182687640190125 
model_pd.l_d.mean(): -20.462526321411133 
model_pd.lagr.mean(): -20.360698699951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-21.1444], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.10182687640190125
epoch£º676	 i:1 	 global-step:13521	 l-p:0.14946632087230682
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18721546232700348
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12613670527935028
epoch£º676	 i:4 	 global-step:13524	 l-p:0.14724180102348328
epoch£º676	 i:5 	 global-step:13525	 l-p:0.0801771804690361
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19918563961982727
epoch£º676	 i:7 	 global-step:13527	 l-p:0.14278432726860046
epoch£º676	 i:8 	 global-step:13528	 l-p:0.1218089684844017
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12680411338806152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 5.0493, 5.0493],
        [5.0493, 4.9507, 4.9959],
        [5.0493, 4.8590, 4.8208],
        [5.0493, 5.0330, 5.0472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.13020217418670654 
model_pd.l_d.mean(): -20.59540557861328 
model_pd.lagr.mean(): -20.4652042388916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-21.2905], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.13020217418670654
epoch£º677	 i:1 	 global-step:13541	 l-p:0.14922977983951569
epoch£º677	 i:2 	 global-step:13542	 l-p:0.1152944266796112
epoch£º677	 i:3 	 global-step:13543	 l-p:0.14432379603385925
epoch£º677	 i:4 	 global-step:13544	 l-p:0.1877209097146988
epoch£º677	 i:5 	 global-step:13545	 l-p:0.1313249170780182
epoch£º677	 i:6 	 global-step:13546	 l-p:0.4883999526500702
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.14212433993816376
epoch£º677	 i:8 	 global-step:13548	 l-p:0.16518568992614746
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11265630275011063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9839, 4.9524, 4.9774],
        [4.9839, 4.9823, 4.9839],
        [4.9839, 4.9562, 4.9787],
        [4.9839, 4.9442, 4.9742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.018307017162442207 
model_pd.l_d.mean(): -19.92511749267578 
model_pd.lagr.mean(): -19.943424224853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-20.7254], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.018307017162442207
epoch£º678	 i:1 	 global-step:13561	 l-p:0.18157503008842468
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13056302070617676
epoch£º678	 i:3 	 global-step:13563	 l-p:0.24635295569896698
epoch£º678	 i:4 	 global-step:13564	 l-p:0.1297512650489807
epoch£º678	 i:5 	 global-step:13565	 l-p:0.13505469262599945
epoch£º678	 i:6 	 global-step:13566	 l-p:0.1722787767648697
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14695461094379425
epoch£º678	 i:8 	 global-step:13568	 l-p:0.1446414589881897
epoch£º678	 i:9 	 global-step:13569	 l-p:0.2160971760749817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9947, 4.9947, 4.9947],
        [4.9947, 4.9515, 4.6910],
        [4.9947, 4.9680, 4.9899],
        [4.9947, 4.9448, 4.9800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.18330296874046326 
model_pd.l_d.mean(): -20.444076538085938 
model_pd.lagr.mean(): -20.260772705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-21.1696], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.18330296874046326
epoch£º679	 i:1 	 global-step:13581	 l-p:0.164926677942276
epoch£º679	 i:2 	 global-step:13582	 l-p:0.1713447868824005
epoch£º679	 i:3 	 global-step:13583	 l-p:1.0248667001724243
epoch£º679	 i:4 	 global-step:13584	 l-p:0.2232893705368042
epoch£º679	 i:5 	 global-step:13585	 l-p:0.11942598223686218
epoch£º679	 i:6 	 global-step:13586	 l-p:0.16356422007083893
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11931832879781723
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12281552702188492
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11459948122501373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0319, 5.0251, 5.0314],
        [5.0319, 5.0294, 5.0318],
        [5.0319, 4.9945, 5.0231],
        [5.0319, 5.2217, 5.0537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.20836356282234192 
model_pd.l_d.mean(): -20.949535369873047 
model_pd.lagr.mean(): -20.74117088317871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.6136], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.20836356282234192
epoch£º680	 i:1 	 global-step:13601	 l-p:0.14134496450424194
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15312382578849792
epoch£º680	 i:3 	 global-step:13603	 l-p:0.16656924784183502
epoch£º680	 i:4 	 global-step:13604	 l-p:0.2000097632408142
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15227468311786652
epoch£º680	 i:6 	 global-step:13606	 l-p:0.1359570324420929
epoch£º680	 i:7 	 global-step:13607	 l-p:0.11465255171060562
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12351022660732269
epoch£º680	 i:9 	 global-step:13609	 l-p:0.11755967885255814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0907, 4.9142, 4.9012],
        [5.0907, 5.4796, 5.4326],
        [5.0907, 5.0898, 5.0907],
        [5.0907, 5.0898, 5.0907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.10435131192207336 
model_pd.l_d.mean(): -20.374958038330078 
model_pd.lagr.mean(): -20.270606994628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.0708], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.10435131192207336
epoch£º681	 i:1 	 global-step:13621	 l-p:0.1327061951160431
epoch£º681	 i:2 	 global-step:13622	 l-p:0.17093168199062347
epoch£º681	 i:3 	 global-step:13623	 l-p:0.14549829065799713
epoch£º681	 i:4 	 global-step:13624	 l-p:0.146527498960495
epoch£º681	 i:5 	 global-step:13625	 l-p:0.08413400501012802
epoch£º681	 i:6 	 global-step:13626	 l-p:0.0471811518073082
epoch£º681	 i:7 	 global-step:13627	 l-p:0.1456441432237625
epoch£º681	 i:8 	 global-step:13628	 l-p:0.15783555805683136
epoch£º681	 i:9 	 global-step:13629	 l-p:0.1099570244550705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0879, 4.9935, 5.0385],
        [5.0879, 4.9662, 5.0058],
        [5.0879, 5.0855, 5.0878],
        [5.0879, 5.0730, 5.0861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14042659103870392 
model_pd.l_d.mean(): -20.53933334350586 
model_pd.lagr.mean(): -20.398906707763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.2337], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14042659103870392
epoch£º682	 i:1 	 global-step:13641	 l-p:0.1261981576681137
epoch£º682	 i:2 	 global-step:13642	 l-p:0.12343011796474457
epoch£º682	 i:3 	 global-step:13643	 l-p:0.18955588340759277
epoch£º682	 i:4 	 global-step:13644	 l-p:0.1705988347530365
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12872883677482605
epoch£º682	 i:6 	 global-step:13646	 l-p:0.1363150030374527
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14047694206237793
epoch£º682	 i:8 	 global-step:13648	 l-p:0.110632985830307
epoch£º682	 i:9 	 global-step:13649	 l-p:0.5839335322380066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9798, 4.8608, 4.9043],
        [4.9798, 4.9437, 4.9716],
        [4.9798, 4.9798, 4.9798],
        [4.9798, 4.8276, 4.8536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.14622674882411957 
model_pd.l_d.mean(): -20.864988327026367 
model_pd.lagr.mean(): -20.718761444091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4610], device='cuda:0')), ('power', tensor([-21.5639], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.14622674882411957
epoch£º683	 i:1 	 global-step:13661	 l-p:-1.4079231023788452
epoch£º683	 i:2 	 global-step:13662	 l-p:0.1127428412437439
epoch£º683	 i:3 	 global-step:13663	 l-p:0.13539765775203705
epoch£º683	 i:4 	 global-step:13664	 l-p:0.8473242521286011
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12502989172935486
epoch£º683	 i:6 	 global-step:13666	 l-p:0.07298792898654938
epoch£º683	 i:7 	 global-step:13667	 l-p:0.1621905118227005
epoch£º683	 i:8 	 global-step:13668	 l-p:0.20684954524040222
epoch£º683	 i:9 	 global-step:13669	 l-p:0.11475568264722824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9767, 4.8164, 4.5708],
        [4.9767, 4.9767, 4.9767],
        [4.9767, 5.2004, 5.0528],
        [4.9767, 4.9744, 4.9766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.12181378901004791 
model_pd.l_d.mean(): -20.362648010253906 
model_pd.lagr.mean(): -20.240833282470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5042], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.12181378901004791
epoch£º684	 i:1 	 global-step:13681	 l-p:0.12429527938365936
epoch£º684	 i:2 	 global-step:13682	 l-p:0.1308886855840683
epoch£º684	 i:3 	 global-step:13683	 l-p:0.21798160672187805
epoch£º684	 i:4 	 global-step:13684	 l-p:0.1228136196732521
epoch£º684	 i:5 	 global-step:13685	 l-p:0.29906243085861206
epoch£º684	 i:6 	 global-step:13686	 l-p:0.36588114500045776
epoch£º684	 i:7 	 global-step:13687	 l-p:1.445565104484558
epoch£º684	 i:8 	 global-step:13688	 l-p:0.13105963170528412
epoch£º684	 i:9 	 global-step:13689	 l-p:-0.322550892829895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0020, 4.9233, 4.9681],
        [5.0020, 5.3552, 5.2873],
        [5.0020, 4.7898, 4.6646],
        [5.0020, 5.0019, 5.0020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.19699382781982422 
model_pd.l_d.mean(): -20.631317138671875 
model_pd.lagr.mean(): -20.434322357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-21.3509], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.19699382781982422
epoch£º685	 i:1 	 global-step:13701	 l-p:0.17470456659793854
epoch£º685	 i:2 	 global-step:13702	 l-p:0.11771740019321442
epoch£º685	 i:3 	 global-step:13703	 l-p:0.2738088369369507
epoch£º685	 i:4 	 global-step:13704	 l-p:0.12956462800502777
epoch£º685	 i:5 	 global-step:13705	 l-p:0.11556749045848846
epoch£º685	 i:6 	 global-step:13706	 l-p:0.07557109743356705
epoch£º685	 i:7 	 global-step:13707	 l-p:0.16424094140529633
epoch£º685	 i:8 	 global-step:13708	 l-p:0.10039627552032471
epoch£º685	 i:9 	 global-step:13709	 l-p:0.1262514591217041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1174, 5.1171, 5.1174],
        [5.1174, 5.1168, 5.1174],
        [5.1174, 5.0870, 5.1112],
        [5.1174, 5.3586, 5.2169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.057172518223524094 
model_pd.l_d.mean(): -19.03984832763672 
model_pd.lagr.mean(): -18.982675552368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5816], device='cuda:0')), ('power', tensor([-19.8420], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.057172518223524094
epoch£º686	 i:1 	 global-step:13721	 l-p:0.13578195869922638
epoch£º686	 i:2 	 global-step:13722	 l-p:0.08810104429721832
epoch£º686	 i:3 	 global-step:13723	 l-p:0.14586065709590912
epoch£º686	 i:4 	 global-step:13724	 l-p:0.10867582261562347
epoch£º686	 i:5 	 global-step:13725	 l-p:0.10485387593507767
epoch£º686	 i:6 	 global-step:13726	 l-p:0.09308735281229019
epoch£º686	 i:7 	 global-step:13727	 l-p:0.13275223970413208
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13392160832881927
epoch£º686	 i:9 	 global-step:13729	 l-p:0.13901734352111816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1498, 5.3617, 5.2022],
        [5.1498, 4.9799, 4.7705],
        [5.1498, 4.9638, 4.9233],
        [5.1498, 5.2463, 5.0298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.05537984147667885 
model_pd.l_d.mean(): -19.749204635620117 
model_pd.lagr.mean(): -19.693824768066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5439], device='cuda:0')), ('power', tensor([-20.5206], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.05537984147667885
epoch£º687	 i:1 	 global-step:13741	 l-p:0.17046184837818146
epoch£º687	 i:2 	 global-step:13742	 l-p:0.13640934228897095
epoch£º687	 i:3 	 global-step:13743	 l-p:0.1305169016122818
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14870737493038177
epoch£º687	 i:5 	 global-step:13745	 l-p:0.105158232152462
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08648187667131424
epoch£º687	 i:7 	 global-step:13747	 l-p:0.13815295696258545
epoch£º687	 i:8 	 global-step:13748	 l-p:0.14080674946308136
epoch£º687	 i:9 	 global-step:13749	 l-p:0.018252218142151833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 5.1151, 5.1151],
        [5.1151, 4.9209, 4.7479],
        [5.1151, 4.9604, 4.9775],
        [5.1151, 5.1150, 5.1151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.046357639133930206 
model_pd.l_d.mean(): -19.472637176513672 
model_pd.lagr.mean(): -19.426279067993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5266], device='cuda:0')), ('power', tensor([-20.2233], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.046357639133930206
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13484178483486176
epoch£º688	 i:2 	 global-step:13762	 l-p:0.15084019303321838
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17830988764762878
epoch£º688	 i:4 	 global-step:13764	 l-p:0.151493638753891
epoch£º688	 i:5 	 global-step:13765	 l-p:0.121985524892807
epoch£º688	 i:6 	 global-step:13766	 l-p:0.04710598662495613
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13453711569309235
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12255725264549255
epoch£º688	 i:9 	 global-step:13769	 l-p:0.1323619931936264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0940, 5.0293, 5.0701],
        [5.0940, 5.0869, 5.0935],
        [5.0940, 5.0940, 5.0940],
        [5.0940, 5.0940, 5.0940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.17907606065273285 
model_pd.l_d.mean(): -20.67348861694336 
model_pd.lagr.mean(): -20.494413375854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4451], device='cuda:0')), ('power', tensor([-21.3540], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.17907606065273285
epoch£º689	 i:1 	 global-step:13781	 l-p:0.137030690908432
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12149753421545029
epoch£º689	 i:3 	 global-step:13783	 l-p:0.1279713213443756
epoch£º689	 i:4 	 global-step:13784	 l-p:0.12258325517177582
epoch£º689	 i:5 	 global-step:13785	 l-p:0.1218361184000969
epoch£º689	 i:6 	 global-step:13786	 l-p:0.19937542080879211
epoch£º689	 i:7 	 global-step:13787	 l-p:0.09592156857252121
epoch£º689	 i:8 	 global-step:13788	 l-p:0.2877466380596161
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12710325419902802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0212, 5.0211, 5.0212],
        [5.0212, 5.4324, 5.4020],
        [5.0212, 5.0206, 5.0212],
        [5.0212, 5.0212, 5.0212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13980534672737122 
model_pd.l_d.mean(): -20.61582374572754 
model_pd.lagr.mean(): -20.47601890563965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-21.3286], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13980534672737122
epoch£º690	 i:1 	 global-step:13801	 l-p:0.08113305270671844
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12522689998149872
epoch£º690	 i:3 	 global-step:13803	 l-p:0.5718253254890442
epoch£º690	 i:4 	 global-step:13804	 l-p:0.13288530707359314
epoch£º690	 i:5 	 global-step:13805	 l-p:0.20642921328544617
epoch£º690	 i:6 	 global-step:13806	 l-p:0.23561137914657593
epoch£º690	 i:7 	 global-step:13807	 l-p:0.17754079401493073
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13701072335243225
epoch£º690	 i:9 	 global-step:13809	 l-p:0.13400483131408691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0332, 5.0331, 5.0332],
        [5.0332, 4.9283, 4.9741],
        [5.0332, 4.9307, 4.6676],
        [5.0332, 5.0309, 5.0331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.1128641664981842 
model_pd.l_d.mean(): -19.58162498474121 
model_pd.lagr.mean(): -19.468761444091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5226], device='cuda:0')), ('power', tensor([-20.3294], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.1128641664981842
epoch£º691	 i:1 	 global-step:13821	 l-p:0.09909980744123459
epoch£º691	 i:2 	 global-step:13822	 l-p:0.17280706763267517
epoch£º691	 i:3 	 global-step:13823	 l-p:0.1723204404115677
epoch£º691	 i:4 	 global-step:13824	 l-p:0.19469444453716278
epoch£º691	 i:5 	 global-step:13825	 l-p:0.10194877535104752
epoch£º691	 i:6 	 global-step:13826	 l-p:0.2141386717557907
epoch£º691	 i:7 	 global-step:13827	 l-p:0.11278107762336731
epoch£º691	 i:8 	 global-step:13828	 l-p:0.1180591955780983
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11876019835472107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0918, 5.0915, 5.0918],
        [5.0918, 5.0912, 5.0918],
        [5.0918, 5.0292, 5.0694],
        [5.0918, 5.0918, 5.0918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.08517014980316162 
model_pd.l_d.mean(): -20.846460342407227 
model_pd.lagr.mean(): -20.761289596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4080], device='cuda:0')), ('power', tensor([-21.4910], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.08517014980316162
epoch£º692	 i:1 	 global-step:13841	 l-p:0.12660393118858337
epoch£º692	 i:2 	 global-step:13842	 l-p:0.16172009706497192
epoch£º692	 i:3 	 global-step:13843	 l-p:0.1435004621744156
epoch£º692	 i:4 	 global-step:13844	 l-p:0.10685301572084427
epoch£º692	 i:5 	 global-step:13845	 l-p:0.12034823000431061
epoch£º692	 i:6 	 global-step:13846	 l-p:0.14538820087909698
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12363673001527786
epoch£º692	 i:8 	 global-step:13848	 l-p:0.200978621840477
epoch£º692	 i:9 	 global-step:13849	 l-p:0.1132291927933693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0586, 5.0549, 5.0584],
        [5.0586, 5.1702, 4.9594],
        [5.0586, 5.4250, 5.3622],
        [5.0586, 5.4654, 5.4299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.09640023857355118 
model_pd.l_d.mean(): -19.558744430541992 
model_pd.lagr.mean(): -19.462345123291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5818], device='cuda:0')), ('power', tensor([-20.3668], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.09640023857355118
epoch£º693	 i:1 	 global-step:13861	 l-p:0.075357586145401
epoch£º693	 i:2 	 global-step:13862	 l-p:0.15740302205085754
epoch£º693	 i:3 	 global-step:13863	 l-p:0.15929725766181946
epoch£º693	 i:4 	 global-step:13864	 l-p:0.17603042721748352
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13178181648254395
epoch£º693	 i:6 	 global-step:13866	 l-p:0.12221074849367142
epoch£º693	 i:7 	 global-step:13867	 l-p:0.20504163205623627
epoch£º693	 i:8 	 global-step:13868	 l-p:0.20514899492263794
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11529678851366043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0551, 4.9915, 5.0323],
        [5.0551, 4.8483, 4.6812],
        [5.0551, 5.0550, 5.0551],
        [5.0551, 5.0551, 5.0551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.1007680669426918 
model_pd.l_d.mean(): -19.196645736694336 
model_pd.lagr.mean(): -19.09587860107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5564], device='cuda:0')), ('power', tensor([-19.9747], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.1007680669426918
epoch£º694	 i:1 	 global-step:13881	 l-p:0.17821642756462097
epoch£º694	 i:2 	 global-step:13882	 l-p:0.13274329900741577
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12823814153671265
epoch£º694	 i:4 	 global-step:13884	 l-p:0.10996554791927338
epoch£º694	 i:5 	 global-step:13885	 l-p:0.18423576653003693
epoch£º694	 i:6 	 global-step:13886	 l-p:0.14494459331035614
epoch£º694	 i:7 	 global-step:13887	 l-p:0.14837734401226044
epoch£º694	 i:8 	 global-step:13888	 l-p:0.1313495934009552
epoch£º694	 i:9 	 global-step:13889	 l-p:0.13431334495544434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0819, 4.8755, 4.7888],
        [5.0819, 4.9318, 4.6868],
        [5.0819, 4.9405, 4.6908],
        [5.0819, 5.0816, 5.0819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12577424943447113 
model_pd.l_d.mean(): -19.54804801940918 
model_pd.lagr.mean(): -19.422273635864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4961], device='cuda:0')), ('power', tensor([-20.2684], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12577424943447113
epoch£º695	 i:1 	 global-step:13901	 l-p:0.1771409958600998
epoch£º695	 i:2 	 global-step:13902	 l-p:0.09384731203317642
epoch£º695	 i:3 	 global-step:13903	 l-p:0.13354095816612244
epoch£º695	 i:4 	 global-step:13904	 l-p:0.16861213743686676
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12129664421081543
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13758035004138947
epoch£º695	 i:7 	 global-step:13907	 l-p:0.09965191036462784
epoch£º695	 i:8 	 global-step:13908	 l-p:0.1319284439086914
epoch£º695	 i:9 	 global-step:13909	 l-p:0.13411323726177216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0869, 5.0781, 5.0861],
        [5.0869, 4.9371, 4.6920],
        [5.0869, 5.0429, 5.0752],
        [5.0869, 5.4394, 5.3658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.19377677142620087 
model_pd.l_d.mean(): -20.806676864624023 
model_pd.lagr.mean(): -20.612899780273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.4720], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.19377677142620087
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11913907527923584
epoch£º696	 i:2 	 global-step:13922	 l-p:0.16564474999904633
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11963740736246109
epoch£º696	 i:4 	 global-step:13924	 l-p:0.14237242937088013
epoch£º696	 i:5 	 global-step:13925	 l-p:0.19251741468906403
epoch£º696	 i:6 	 global-step:13926	 l-p:0.13089467585086823
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11795712262392044
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13733670115470886
epoch£º696	 i:9 	 global-step:13929	 l-p:0.11439691483974457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0151, 5.0151, 5.0151],
        [5.0151, 5.0151, 5.0151],
        [5.0151, 5.0979, 4.8735],
        [5.0151, 5.0142, 5.0151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.13277240097522736 
model_pd.l_d.mean(): -20.118497848510742 
model_pd.lagr.mean(): -19.98572540283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4687], device='cuda:0')), ('power', tensor([-20.8171], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.13277240097522736
epoch£º697	 i:1 	 global-step:13941	 l-p:0.16634704172611237
epoch£º697	 i:2 	 global-step:13942	 l-p:-0.8349173069000244
epoch£º697	 i:3 	 global-step:13943	 l-p:0.12937869131565094
epoch£º697	 i:4 	 global-step:13944	 l-p:0.26461854577064514
epoch£º697	 i:5 	 global-step:13945	 l-p:0.1481231302022934
epoch£º697	 i:6 	 global-step:13946	 l-p:0.1838834285736084
epoch£º697	 i:7 	 global-step:13947	 l-p:0.15941192209720612
epoch£º697	 i:8 	 global-step:13948	 l-p:0.14191140234470367
epoch£º697	 i:9 	 global-step:13949	 l-p:0.18163959681987762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0263, 4.9899, 5.0181],
        [5.0263, 5.0238, 5.0262],
        [5.0263, 4.8414, 4.8309],
        [5.0263, 4.8104, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.1456252783536911 
model_pd.l_d.mean(): -20.585729598999023 
model_pd.lagr.mean(): -20.44010353088379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4642], device='cuda:0')), ('power', tensor([-21.2848], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.1456252783536911
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11895471066236496
epoch£º698	 i:2 	 global-step:13962	 l-p:0.21630960702896118
epoch£º698	 i:3 	 global-step:13963	 l-p:0.15009458363056183
epoch£º698	 i:4 	 global-step:13964	 l-p:0.13235384225845337
epoch£º698	 i:5 	 global-step:13965	 l-p:0.17362339794635773
epoch£º698	 i:6 	 global-step:13966	 l-p:0.2965019941329956
epoch£º698	 i:7 	 global-step:13967	 l-p:0.16724416613578796
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09532352536916733
epoch£º698	 i:9 	 global-step:13969	 l-p:0.12010044604539871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0538, 5.0468, 5.0533],
        [5.0538, 4.9905, 5.0314],
        [5.0538, 4.8409, 4.6993],
        [5.0538, 5.0538, 5.0538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.14075715839862823 
model_pd.l_d.mean(): -20.441980361938477 
model_pd.lagr.mean(): -20.301223754882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4835], device='cuda:0')), ('power', tensor([-21.1592], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.14075715839862823
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12368598580360413
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14946827292442322
epoch£º699	 i:3 	 global-step:13983	 l-p:0.13305190205574036
epoch£º699	 i:4 	 global-step:13984	 l-p:0.17823998630046844
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06491589546203613
epoch£º699	 i:6 	 global-step:13986	 l-p:0.10197950154542923
epoch£º699	 i:7 	 global-step:13987	 l-p:0.1794351488351822
epoch£º699	 i:8 	 global-step:13988	 l-p:0.1471235752105713
epoch£º699	 i:9 	 global-step:13989	 l-p:0.1345113217830658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1199, 5.1149, 5.1196],
        [5.1199, 4.9165, 4.8401],
        [5.1199, 4.9315, 4.9030],
        [5.1199, 5.0555, 5.0964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.1103825569152832 
model_pd.l_d.mean(): -20.43654441833496 
model_pd.lagr.mean(): -20.326162338256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.1008], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.1103825569152832
epoch£º700	 i:1 	 global-step:14001	 l-p:0.12130691111087799
epoch£º700	 i:2 	 global-step:14002	 l-p:0.08162502199411392
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12442607432603836
epoch£º700	 i:4 	 global-step:14004	 l-p:0.12245526909828186
epoch£º700	 i:5 	 global-step:14005	 l-p:0.15874026715755463
epoch£º700	 i:6 	 global-step:14006	 l-p:0.08305114507675171
epoch£º700	 i:7 	 global-step:14007	 l-p:0.14312651753425598
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11710456758737564
epoch£º700	 i:9 	 global-step:14009	 l-p:0.15609608590602875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1259, 5.1187, 5.1254],
        [5.1259, 5.0474, 5.0919],
        [5.1259, 5.1259, 5.1259],
        [5.1259, 4.9625, 4.9741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.12365760654211044 
model_pd.l_d.mean(): -20.12421417236328 
model_pd.lagr.mean(): -20.00055694580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4850], device='cuda:0')), ('power', tensor([-20.8395], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.12365760654211044
epoch£º701	 i:1 	 global-step:14021	 l-p:0.1456906646490097
epoch£º701	 i:2 	 global-step:14022	 l-p:0.13456398248672485
epoch£º701	 i:3 	 global-step:14023	 l-p:0.161127969622612
epoch£º701	 i:4 	 global-step:14024	 l-p:0.0892251580953598
epoch£º701	 i:5 	 global-step:14025	 l-p:0.08848997950553894
epoch£º701	 i:6 	 global-step:14026	 l-p:0.14219136536121368
epoch£º701	 i:7 	 global-step:14027	 l-p:0.07933685928583145
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12921485304832458
epoch£º701	 i:9 	 global-step:14029	 l-p:0.16014093160629272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1042, 5.0412, 5.0818],
        [5.1042, 5.2761, 5.0927],
        [5.1042, 5.0431, 5.0830],
        [5.1042, 5.1042, 5.1042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.12791742384433746 
model_pd.l_d.mean(): -20.2979793548584 
model_pd.lagr.mean(): -20.170061111450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4806], device='cuda:0')), ('power', tensor([-21.0106], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.12791742384433746
epoch£º702	 i:1 	 global-step:14041	 l-p:0.19203411042690277
epoch£º702	 i:2 	 global-step:14042	 l-p:0.04453733190894127
epoch£º702	 i:3 	 global-step:14043	 l-p:0.09900811314582825
epoch£º702	 i:4 	 global-step:14044	 l-p:0.13246266543865204
epoch£º702	 i:5 	 global-step:14045	 l-p:0.1407059282064438
epoch£º702	 i:6 	 global-step:14046	 l-p:0.10627741366624832
epoch£º702	 i:7 	 global-step:14047	 l-p:0.16591636836528778
epoch£º702	 i:8 	 global-step:14048	 l-p:0.15765142440795898
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12697160243988037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0716, 5.0687, 5.0715],
        [5.0716, 5.3107, 5.1656],
        [5.0716, 5.0592, 5.0703],
        [5.0716, 5.3354, 5.2049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.14773517847061157 
model_pd.l_d.mean(): -20.464397430419922 
model_pd.lagr.mean(): -20.316661834716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4917], device='cuda:0')), ('power', tensor([-21.1903], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.14773517847061157
epoch£º703	 i:1 	 global-step:14061	 l-p:0.13258886337280273
epoch£º703	 i:2 	 global-step:14062	 l-p:0.08704738318920135
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13739794492721558
epoch£º703	 i:4 	 global-step:14064	 l-p:0.20653188228607178
epoch£º703	 i:5 	 global-step:14065	 l-p:0.1884249895811081
epoch£º703	 i:6 	 global-step:14066	 l-p:0.1192498728632927
epoch£º703	 i:7 	 global-step:14067	 l-p:0.1257229894399643
epoch£º703	 i:8 	 global-step:14068	 l-p:0.07052171975374222
epoch£º703	 i:9 	 global-step:14069	 l-p:0.15296152234077454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0968, 5.0968, 5.0968],
        [5.0968, 5.0524, 5.0850],
        [5.0968, 5.0166, 5.0618],
        [5.0968, 4.9250, 4.6919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13242945075035095 
model_pd.l_d.mean(): -21.065553665161133 
model_pd.lagr.mean(): -20.933124542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3743], device='cuda:0')), ('power', tensor([-21.6781], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13242945075035095
epoch£º704	 i:1 	 global-step:14081	 l-p:0.09226907789707184
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16689735651016235
epoch£º704	 i:3 	 global-step:14083	 l-p:0.09225352853536606
epoch£º704	 i:4 	 global-step:14084	 l-p:0.15292182564735413
epoch£º704	 i:5 	 global-step:14085	 l-p:0.13841871917247772
epoch£º704	 i:6 	 global-step:14086	 l-p:0.1078120768070221
epoch£º704	 i:7 	 global-step:14087	 l-p:0.14280728995800018
epoch£º704	 i:8 	 global-step:14088	 l-p:0.1119629517197609
epoch£º704	 i:9 	 global-step:14089	 l-p:0.13760414719581604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.1420, 5.1420],
        [5.1419, 5.1376, 5.1417],
        [5.1419, 4.9860, 4.7460],
        [5.1419, 5.2442, 5.0257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): -0.038279540836811066 
model_pd.l_d.mean(): -20.145517349243164 
model_pd.lagr.mean(): -20.18379783630371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5060], device='cuda:0')), ('power', tensor([-20.8825], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:-0.038279540836811066
epoch£º705	 i:1 	 global-step:14101	 l-p:0.15328721702098846
epoch£º705	 i:2 	 global-step:14102	 l-p:0.14043526351451874
epoch£º705	 i:3 	 global-step:14103	 l-p:0.150677889585495
epoch£º705	 i:4 	 global-step:14104	 l-p:0.1148475781083107
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11382465064525604
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12963756918907166
epoch£º705	 i:7 	 global-step:14107	 l-p:0.14073783159255981
epoch£º705	 i:8 	 global-step:14108	 l-p:0.0841890498995781
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12329522520303726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1154, 5.0410, 5.0850],
        [5.1154, 5.3018, 5.1250],
        [5.1154, 5.0793, 5.1073],
        [5.1154, 4.9042, 4.7737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.155883327126503 
model_pd.l_d.mean(): -20.512758255004883 
model_pd.lagr.mean(): -20.356874465942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.2129], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.155883327126503
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12525708973407745
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14589561522006989
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10717123746871948
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10283143818378448
epoch£º706	 i:5 	 global-step:14125	 l-p:0.20557768642902374
epoch£º706	 i:6 	 global-step:14126	 l-p:0.12315373122692108
epoch£º706	 i:7 	 global-step:14127	 l-p:0.2649206817150116
epoch£º706	 i:8 	 global-step:14128	 l-p:0.2065497189760208
epoch£º706	 i:9 	 global-step:14129	 l-p:0.1365518569946289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0226, 5.0223, 5.0226],
        [5.0226, 5.0226, 5.0226],
        [5.0226, 5.0226, 5.0226],
        [5.0226, 4.9144, 4.9619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.5064736604690552 
model_pd.l_d.mean(): -20.916662216186523 
model_pd.lagr.mean(): -20.410188674926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4365], device='cuda:0')), ('power', tensor([-21.5911], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.5064736604690552
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11220277845859528
epoch£º707	 i:2 	 global-step:14142	 l-p:0.2128758579492569
epoch£º707	 i:3 	 global-step:14143	 l-p:0.12994299829006195
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12767712771892548
epoch£º707	 i:5 	 global-step:14145	 l-p:0.23822082579135895
epoch£º707	 i:6 	 global-step:14146	 l-p:0.17834390699863434
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14420416951179504
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12399398535490036
epoch£º707	 i:9 	 global-step:14149	 l-p:0.11358356475830078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0459, 4.9723, 5.0167],
        [5.0459, 4.8763, 4.8900],
        [5.0459, 4.9968, 4.7281],
        [5.0459, 4.9715, 5.0161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13488590717315674 
model_pd.l_d.mean(): -20.375669479370117 
model_pd.lagr.mean(): -20.24078369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4951], device='cuda:0')), ('power', tensor([-21.1041], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13488590717315674
epoch£º708	 i:1 	 global-step:14161	 l-p:0.11214984953403473
epoch£º708	 i:2 	 global-step:14162	 l-p:0.16299760341644287
epoch£º708	 i:3 	 global-step:14163	 l-p:0.16913051903247833
epoch£º708	 i:4 	 global-step:14164	 l-p:0.13812223076820374
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06546987593173981
epoch£º708	 i:6 	 global-step:14166	 l-p:0.3126654624938965
epoch£º708	 i:7 	 global-step:14167	 l-p:0.19298812747001648
epoch£º708	 i:8 	 global-step:14168	 l-p:0.14642494916915894
epoch£º708	 i:9 	 global-step:14169	 l-p:0.17398233711719513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0763, 4.8696, 4.8113],
        [5.0763, 5.0387, 5.0676],
        [5.0763, 5.5394, 5.5389],
        [5.0763, 4.9023, 4.6626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10683420300483704 
model_pd.l_d.mean(): -20.188196182250977 
model_pd.lagr.mean(): -20.081361770629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-20.8986], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10683420300483704
epoch£º709	 i:1 	 global-step:14181	 l-p:0.15140333771705627
epoch£º709	 i:2 	 global-step:14182	 l-p:0.07637979090213776
epoch£º709	 i:3 	 global-step:14183	 l-p:0.09791766107082367
epoch£º709	 i:4 	 global-step:14184	 l-p:0.16133975982666016
epoch£º709	 i:5 	 global-step:14185	 l-p:0.12015165388584137
epoch£º709	 i:6 	 global-step:14186	 l-p:0.14117568731307983
epoch£º709	 i:7 	 global-step:14187	 l-p:0.15056629478931427
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12542058527469635
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15837828814983368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1289, 5.1118, 5.1266],
        [5.1289, 4.9272, 4.8732],
        [5.1289, 4.9582, 4.9651],
        [5.1289, 5.1017, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12187402695417404 
model_pd.l_d.mean(): -20.503116607666016 
model_pd.lagr.mean(): -20.381242752075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4305], device='cuda:0')), ('power', tensor([-21.1668], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12187402695417404
epoch£º710	 i:1 	 global-step:14201	 l-p:0.1260535717010498
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12574325501918793
epoch£º710	 i:3 	 global-step:14203	 l-p:0.06354518979787827
epoch£º710	 i:4 	 global-step:14204	 l-p:0.15383918583393097
epoch£º710	 i:5 	 global-step:14205	 l-p:0.09397031366825104
epoch£º710	 i:6 	 global-step:14206	 l-p:0.2061411589384079
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1437860131263733
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1520977020263672
epoch£º710	 i:9 	 global-step:14209	 l-p:0.13213439285755157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0894, 5.0785, 5.0883],
        [5.0894, 5.1577, 4.9233],
        [5.0894, 5.0837, 4.8241],
        [5.0894, 5.0894, 5.0894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.1472356617450714 
model_pd.l_d.mean(): -19.749073028564453 
model_pd.lagr.mean(): -19.601837158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5402], device='cuda:0')), ('power', tensor([-20.5167], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.1472356617450714
epoch£º711	 i:1 	 global-step:14221	 l-p:0.18259485065937042
epoch£º711	 i:2 	 global-step:14222	 l-p:0.09396007657051086
epoch£º711	 i:3 	 global-step:14223	 l-p:0.15575075149536133
epoch£º711	 i:4 	 global-step:14224	 l-p:0.14570802450180054
epoch£º711	 i:5 	 global-step:14225	 l-p:0.1521083265542984
epoch£º711	 i:6 	 global-step:14226	 l-p:0.018034061416983604
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12161823362112045
epoch£º711	 i:8 	 global-step:14228	 l-p:0.08103414624929428
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11815933138132095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1280, 4.8643],
        [5.1740, 5.2790, 5.0597],
        [5.1740, 5.1020, 5.1453],
        [5.1740, 5.4124, 5.2620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): 1.4023046493530273 
model_pd.l_d.mean(): -20.208097457885742 
model_pd.lagr.mean(): -18.80579376220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5069], device='cuda:0')), ('power', tensor([-20.9466], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:1.4023046493530273
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11639904975891113
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13195671141147614
epoch£º712	 i:3 	 global-step:14243	 l-p:0.13579344749450684
epoch£º712	 i:4 	 global-step:14244	 l-p:0.11559876799583435
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12322219461202621
epoch£º712	 i:6 	 global-step:14246	 l-p:0.1375075876712799
epoch£º712	 i:7 	 global-step:14247	 l-p:0.07928725332021713
epoch£º712	 i:8 	 global-step:14248	 l-p:0.060308873653411865
epoch£º712	 i:9 	 global-step:14249	 l-p:0.12351080030202866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1627, 5.1557, 5.1622],
        [5.1627, 4.9538, 4.8081],
        [5.1627, 5.1627, 5.1627],
        [5.1627, 4.9891, 4.7606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08234132826328278 
model_pd.l_d.mean(): -20.882030487060547 
model_pd.lagr.mean(): -20.7996883392334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3828], device='cuda:0')), ('power', tensor([-21.5012], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.08234132826328278
epoch£º713	 i:1 	 global-step:14261	 l-p:0.029257288202643394
epoch£º713	 i:2 	 global-step:14262	 l-p:0.13501717150211334
epoch£º713	 i:3 	 global-step:14263	 l-p:0.1311257928609848
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15515829622745514
epoch£º713	 i:5 	 global-step:14265	 l-p:0.14056673645973206
epoch£º713	 i:6 	 global-step:14266	 l-p:0.11987534910440445
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12201181054115295
epoch£º713	 i:8 	 global-step:14268	 l-p:0.05854959413409233
epoch£º713	 i:9 	 global-step:14269	 l-p:0.179024800658226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1074, 5.1074, 5.1074],
        [5.1074, 5.0462, 4.7770],
        [5.1074, 4.9401, 4.6961],
        [5.1074, 5.1032, 5.1072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.14955008029937744 
model_pd.l_d.mean(): -20.600658416748047 
model_pd.lagr.mean(): -20.451108932495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-21.2834], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.14955008029937744
epoch£º714	 i:1 	 global-step:14281	 l-p:0.16759376227855682
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13712947070598602
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14233806729316711
epoch£º714	 i:4 	 global-step:14284	 l-p:0.19933369755744934
epoch£º714	 i:5 	 global-step:14285	 l-p:0.20348107814788818
epoch£º714	 i:6 	 global-step:14286	 l-p:0.11088366061449051
epoch£º714	 i:7 	 global-step:14287	 l-p:0.1034107357263565
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12665492296218872
epoch£º714	 i:9 	 global-step:14289	 l-p:0.1002553179860115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0191, 5.0070, 5.0178],
        [5.0191, 5.3762, 5.3041],
        [5.0191, 4.7936, 4.6523],
        [5.0191, 5.0191, 5.0191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.15025904774665833 
model_pd.l_d.mean(): -20.399911880493164 
model_pd.lagr.mean(): -20.249652862548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4933], device='cuda:0')), ('power', tensor([-21.1267], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.15025904774665833
epoch£º715	 i:1 	 global-step:14301	 l-p:0.30319130420684814
epoch£º715	 i:2 	 global-step:14302	 l-p:0.12601445615291595
epoch£º715	 i:3 	 global-step:14303	 l-p:0.17038705945014954
epoch£º715	 i:4 	 global-step:14304	 l-p:0.22125178575515747
epoch£º715	 i:5 	 global-step:14305	 l-p:0.29260939359664917
epoch£º715	 i:6 	 global-step:14306	 l-p:0.13378220796585083
epoch£º715	 i:7 	 global-step:14307	 l-p:-0.024835314601659775
epoch£º715	 i:8 	 global-step:14308	 l-p:0.1412191390991211
epoch£º715	 i:9 	 global-step:14309	 l-p:0.17005138099193573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0081, 4.8834, 4.6087],
        [5.0081, 5.0055, 5.0080],
        [5.0081, 5.0081, 5.0081],
        [5.0081, 5.3421, 5.2550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.354867160320282 
model_pd.l_d.mean(): -20.36867904663086 
model_pd.lagr.mean(): -20.013811111450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5140], device='cuda:0')), ('power', tensor([-21.1163], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.354867160320282
epoch£º716	 i:1 	 global-step:14321	 l-p:0.14370858669281006
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14885102212429047
epoch£º716	 i:3 	 global-step:14323	 l-p:0.1334090232849121
epoch£º716	 i:4 	 global-step:14324	 l-p:0.46851858496665955
epoch£º716	 i:5 	 global-step:14325	 l-p:0.1443536877632141
epoch£º716	 i:6 	 global-step:14326	 l-p:0.13876308500766754
epoch£º716	 i:7 	 global-step:14327	 l-p:0.1839597225189209
epoch£º716	 i:8 	 global-step:14328	 l-p:0.09471334517002106
epoch£º716	 i:9 	 global-step:14329	 l-p:0.13708148896694183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0507, 4.9496, 4.9979],
        [5.0507, 4.9220, 4.9654],
        [5.0507, 5.0454, 5.0504],
        [5.0507, 5.1622, 4.9460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.1530030518770218 
model_pd.l_d.mean(): -20.874189376831055 
model_pd.lagr.mean(): -20.7211856842041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4261], device='cuda:0')), ('power', tensor([-21.5375], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.1530030518770218
epoch£º717	 i:1 	 global-step:14341	 l-p:0.11034524440765381
epoch£º717	 i:2 	 global-step:14342	 l-p:0.24108301103115082
epoch£º717	 i:3 	 global-step:14343	 l-p:0.1088085025548935
epoch£º717	 i:4 	 global-step:14344	 l-p:0.1352287083864212
epoch£º717	 i:5 	 global-step:14345	 l-p:0.08287792652845383
epoch£º717	 i:6 	 global-step:14346	 l-p:0.17343631386756897
epoch£º717	 i:7 	 global-step:14347	 l-p:0.24389229714870453
epoch£º717	 i:8 	 global-step:14348	 l-p:6.7244791984558105
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12352292984724045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0247, 4.8218, 4.7920],
        [5.0247, 4.8576, 4.8785],
        [5.0247, 5.4175, 5.3685],
        [5.0247, 4.8855, 4.9257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.1328846663236618 
model_pd.l_d.mean(): -20.88722038269043 
model_pd.lagr.mean(): -20.754335403442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4261], device='cuda:0')), ('power', tensor([-21.5507], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.1328846663236618
epoch£º718	 i:1 	 global-step:14361	 l-p:0.11007695645093918
epoch£º718	 i:2 	 global-step:14362	 l-p:-1.209506630897522
epoch£º718	 i:3 	 global-step:14363	 l-p:0.1825239211320877
epoch£º718	 i:4 	 global-step:14364	 l-p:0.20245540142059326
epoch£º718	 i:5 	 global-step:14365	 l-p:0.11243404448032379
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12572577595710754
epoch£º718	 i:7 	 global-step:14367	 l-p:0.12122854590415955
epoch£º718	 i:8 	 global-step:14368	 l-p:0.35758522152900696
epoch£º718	 i:9 	 global-step:14369	 l-p:0.12472820281982422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0274, 4.8777, 4.9122],
        [5.0274, 4.9279, 4.6510],
        [5.0274, 5.0977, 4.8625],
        [5.0274, 5.0049, 5.0239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.16299743950366974 
model_pd.l_d.mean(): -18.66575050354004 
model_pd.lagr.mean(): -18.50275230407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6129], device='cuda:0')), ('power', tensor([-19.4957], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.16299743950366974
epoch£º719	 i:1 	 global-step:14381	 l-p:0.3757484257221222
epoch£º719	 i:2 	 global-step:14382	 l-p:0.14631924033164978
epoch£º719	 i:3 	 global-step:14383	 l-p:0.10883083939552307
epoch£º719	 i:4 	 global-step:14384	 l-p:0.14362533390522003
epoch£º719	 i:5 	 global-step:14385	 l-p:0.19336795806884766
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13861696422100067
epoch£º719	 i:7 	 global-step:14387	 l-p:0.11199015378952026
epoch£º719	 i:8 	 global-step:14388	 l-p:0.16937564313411713
epoch£º719	 i:9 	 global-step:14389	 l-p:0.15291088819503784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0614, 4.9917, 5.0353],
        [5.0614, 4.8604, 4.8296],
        [5.0614, 4.8794, 4.6365],
        [5.0614, 4.8408, 4.7420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.12242257595062256 
model_pd.l_d.mean(): -20.37040901184082 
model_pd.lagr.mean(): -20.24798583984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-21.0788], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.12242257595062256
epoch£º720	 i:1 	 global-step:14401	 l-p:0.13980160653591156
epoch£º720	 i:2 	 global-step:14402	 l-p:0.09129518270492554
epoch£º720	 i:3 	 global-step:14403	 l-p:0.15807341039180756
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12526172399520874
epoch£º720	 i:5 	 global-step:14405	 l-p:0.1528586745262146
epoch£º720	 i:6 	 global-step:14406	 l-p:0.1468135267496109
epoch£º720	 i:7 	 global-step:14407	 l-p:0.22822989523410797
epoch£º720	 i:8 	 global-step:14408	 l-p:0.1503678262233734
epoch£º720	 i:9 	 global-step:14409	 l-p:0.13390319049358368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.0817, 4.9415, 4.6760],
        [5.0817, 5.4924, 5.4530],
        [5.0817, 4.8607, 4.7418],
        [5.0817, 4.8644, 4.6934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11935983598232269 
model_pd.l_d.mean(): -19.614103317260742 
model_pd.lagr.mean(): -19.49474334716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5168], device='cuda:0')), ('power', tensor([-20.3563], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11935983598232269
epoch£º721	 i:1 	 global-step:14421	 l-p:0.10552874952554703
epoch£º721	 i:2 	 global-step:14422	 l-p:0.1509110927581787
epoch£º721	 i:3 	 global-step:14423	 l-p:0.1524442434310913
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10754966735839844
epoch£º721	 i:5 	 global-step:14425	 l-p:0.12574972212314606
epoch£º721	 i:6 	 global-step:14426	 l-p:0.15527662634849548
epoch£º721	 i:7 	 global-step:14427	 l-p:0.11383994668722153
epoch£º721	 i:8 	 global-step:14428	 l-p:0.1333155632019043
epoch£º721	 i:9 	 global-step:14429	 l-p:0.15716005861759186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1164, 5.1164, 5.1164],
        [5.1164, 5.1139, 5.1163],
        [5.1164, 5.1164, 5.1164],
        [5.1164, 5.0700, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.09269125014543533 
model_pd.l_d.mean(): -20.21509552001953 
model_pd.lagr.mean(): -20.122404098510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.9369], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.09269125014543533
epoch£º722	 i:1 	 global-step:14441	 l-p:0.12614735960960388
epoch£º722	 i:2 	 global-step:14442	 l-p:0.1481560915708542
epoch£º722	 i:3 	 global-step:14443	 l-p:0.12915252149105072
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13221637904644012
epoch£º722	 i:5 	 global-step:14445	 l-p:0.21216678619384766
epoch£º722	 i:6 	 global-step:14446	 l-p:0.11630799621343613
epoch£º722	 i:7 	 global-step:14447	 l-p:0.1311654895544052
epoch£º722	 i:8 	 global-step:14448	 l-p:0.12409457564353943
epoch£º722	 i:9 	 global-step:14449	 l-p:0.1695990115404129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 4.9900, 5.0319],
        [5.0543, 4.9652, 4.6888],
        [5.0543, 5.0542, 5.0543],
        [5.0543, 5.0543, 5.0543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.11351897567510605 
model_pd.l_d.mean(): -20.52289581298828 
model_pd.lagr.mean(): -20.40937614440918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-21.2362], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.11351897567510605
epoch£º723	 i:1 	 global-step:14461	 l-p:0.1722678691148758
epoch£º723	 i:2 	 global-step:14462	 l-p:0.18106167018413544
epoch£º723	 i:3 	 global-step:14463	 l-p:0.11675867438316345
epoch£º723	 i:4 	 global-step:14464	 l-p:0.21462653577327728
epoch£º723	 i:5 	 global-step:14465	 l-p:0.17961183190345764
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10027425736188889
epoch£º723	 i:7 	 global-step:14467	 l-p:0.1257503479719162
epoch£º723	 i:8 	 global-step:14468	 l-p:0.23342135548591614
epoch£º723	 i:9 	 global-step:14469	 l-p:0.10592886805534363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0722, 5.0695, 5.0721],
        [5.0722, 5.0197, 4.7467],
        [5.0722, 5.0604, 5.0711],
        [5.0722, 4.8604, 4.8021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.11575549840927124 
model_pd.l_d.mean(): -19.81676483154297 
model_pd.lagr.mean(): -19.70100975036621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5048], device='cuda:0')), ('power', tensor([-20.5489], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.11575549840927124
epoch£º724	 i:1 	 global-step:14481	 l-p:0.09566007554531097
epoch£º724	 i:2 	 global-step:14482	 l-p:0.1823432892560959
epoch£º724	 i:3 	 global-step:14483	 l-p:0.14035792648792267
epoch£º724	 i:4 	 global-step:14484	 l-p:0.118825763463974
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14414405822753906
epoch£º724	 i:6 	 global-step:14486	 l-p:0.1299440562725067
epoch£º724	 i:7 	 global-step:14487	 l-p:0.13856665790081024
epoch£º724	 i:8 	 global-step:14488	 l-p:0.21168138086795807
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10419400036334991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0854, 5.0812, 5.0852],
        [5.0854, 4.9441, 4.6778],
        [5.0854, 4.9066, 4.6621],
        [5.0854, 4.9563, 4.9996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13130371272563934 
model_pd.l_d.mean(): -20.57080078125 
model_pd.lagr.mean(): -20.439496994018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4313], device='cuda:0')), ('power', tensor([-21.2361], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13130371272563934
epoch£º725	 i:1 	 global-step:14501	 l-p:0.1259329617023468
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13087131083011627
epoch£º725	 i:3 	 global-step:14503	 l-p:0.16648483276367188
epoch£º725	 i:4 	 global-step:14504	 l-p:0.13175709545612335
epoch£º725	 i:5 	 global-step:14505	 l-p:0.3390061855316162
epoch£º725	 i:6 	 global-step:14506	 l-p:0.2151643931865692
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10584244132041931
epoch£º725	 i:8 	 global-step:14508	 l-p:0.11476591974496841
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12707309424877167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0199, 4.8955, 4.9420],
        [5.0199, 4.7905, 4.6588],
        [5.0199, 4.9729, 4.6979],
        [5.0199, 5.2172, 5.0450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.24182893335819244 
model_pd.l_d.mean(): -19.22648811340332 
model_pd.lagr.mean(): -18.98465919494629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5774], device='cuda:0')), ('power', tensor([-20.0263], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.24182893335819244
epoch£º726	 i:1 	 global-step:14521	 l-p:0.2105945497751236
epoch£º726	 i:2 	 global-step:14522	 l-p:0.23151496052742004
epoch£º726	 i:3 	 global-step:14523	 l-p:0.20571662485599518
epoch£º726	 i:4 	 global-step:14524	 l-p:0.15688970685005188
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13222861289978027
epoch£º726	 i:6 	 global-step:14526	 l-p:0.3744657039642334
epoch£º726	 i:7 	 global-step:14527	 l-p:0.07456888258457184
epoch£º726	 i:8 	 global-step:14528	 l-p:0.15543830394744873
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12373469769954681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0589, 5.0589, 5.0589],
        [5.0589, 5.2018, 4.9993],
        [5.0589, 5.4628, 5.4186],
        [5.0589, 5.0589, 5.0589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.1647636890411377 
model_pd.l_d.mean(): -20.53857421875 
model_pd.lagr.mean(): -20.373809814453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4804], device='cuda:0')), ('power', tensor([-21.2537], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.1647636890411377
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12512637674808502
epoch£º727	 i:2 	 global-step:14542	 l-p:0.15633855760097504
epoch£º727	 i:3 	 global-step:14543	 l-p:0.11847613751888275
epoch£º727	 i:4 	 global-step:14544	 l-p:0.1619253158569336
epoch£º727	 i:5 	 global-step:14545	 l-p:0.31070536375045776
epoch£º727	 i:6 	 global-step:14546	 l-p:0.13608179986476898
epoch£º727	 i:7 	 global-step:14547	 l-p:0.19083049893379211
epoch£º727	 i:8 	 global-step:14548	 l-p:0.15908408164978027
epoch£º727	 i:9 	 global-step:14549	 l-p:0.08612733334302902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0865, 5.0731, 5.0850],
        [5.0865, 5.0857, 5.0865],
        [5.0865, 4.9048, 4.6609],
        [5.0865, 5.5299, 5.5118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.1408025473356247 
model_pd.l_d.mean(): -20.477313995361328 
model_pd.lagr.mean(): -20.336511611938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-21.1794], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.1408025473356247
epoch£º728	 i:1 	 global-step:14561	 l-p:0.11383488029241562
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12808941304683685
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1511276364326477
epoch£º728	 i:4 	 global-step:14564	 l-p:0.14402678608894348
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13323718309402466
epoch£º728	 i:6 	 global-step:14566	 l-p:-0.21026721596717834
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13737162947654724
epoch£º728	 i:8 	 global-step:14568	 l-p:0.11300931870937347
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10157384723424911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1871, 5.0534, 5.0921],
        [5.1871, 5.1150, 5.1588],
        [5.1871, 5.0260, 5.0451],
        [5.1871, 5.1871, 5.1871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.5289216637611389 
model_pd.l_d.mean(): -19.3056697845459 
model_pd.lagr.mean(): -18.776748657226562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5648], device='cuda:0')), ('power', tensor([-20.0936], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.5289216637611389
epoch£º729	 i:1 	 global-step:14581	 l-p:0.1112368106842041
epoch£º729	 i:2 	 global-step:14582	 l-p:0.15752264857292175
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12689650058746338
epoch£º729	 i:4 	 global-step:14584	 l-p:0.137867271900177
epoch£º729	 i:5 	 global-step:14585	 l-p:0.1112324669957161
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12411872297525406
epoch£º729	 i:7 	 global-step:14587	 l-p:0.009283012710511684
epoch£º729	 i:8 	 global-step:14588	 l-p:0.1337939202785492
epoch£º729	 i:9 	 global-step:14589	 l-p:0.11812704056501389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1375, 5.0682, 4.7957],
        [5.1375, 5.3557, 5.1920],
        [5.1375, 4.9679, 4.7215],
        [5.1375, 4.9772, 4.7243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.01753184199333191 
model_pd.l_d.mean(): -19.29149627685547 
model_pd.lagr.mean(): -19.273963928222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5480], device='cuda:0')), ('power', tensor([-20.0621], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.01753184199333191
epoch£º730	 i:1 	 global-step:14601	 l-p:0.1358785182237625
epoch£º730	 i:2 	 global-step:14602	 l-p:0.16376280784606934
epoch£º730	 i:3 	 global-step:14603	 l-p:0.12025430053472519
epoch£º730	 i:4 	 global-step:14604	 l-p:0.1396903544664383
epoch£º730	 i:5 	 global-step:14605	 l-p:0.13219311833381653
epoch£º730	 i:6 	 global-step:14606	 l-p:0.13426804542541504
epoch£º730	 i:7 	 global-step:14607	 l-p:0.13763286173343658
epoch£º730	 i:8 	 global-step:14608	 l-p:0.24166539311408997
epoch£º730	 i:9 	 global-step:14609	 l-p:0.08697424083948135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0554, 5.3720, 5.2699],
        [5.0554, 4.8751, 4.8829],
        [5.0554, 4.8283, 4.6856],
        [5.0554, 5.0446, 5.0544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.1181303933262825 
model_pd.l_d.mean(): -20.267995834350586 
model_pd.lagr.mean(): -20.149866104125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-21.0125], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.1181303933262825
epoch£º731	 i:1 	 global-step:14621	 l-p:0.1277385652065277
epoch£º731	 i:2 	 global-step:14622	 l-p:0.1840551644563675
epoch£º731	 i:3 	 global-step:14623	 l-p:0.20170307159423828
epoch£º731	 i:4 	 global-step:14624	 l-p:0.13608920574188232
epoch£º731	 i:5 	 global-step:14625	 l-p:0.20133857429027557
epoch£º731	 i:6 	 global-step:14626	 l-p:3.4160728454589844
epoch£º731	 i:7 	 global-step:14627	 l-p:0.2652148902416229
epoch£º731	 i:8 	 global-step:14628	 l-p:-0.04688740521669388
epoch£º731	 i:9 	 global-step:14629	 l-p:0.11875323951244354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0115, 4.9411, 4.6609],
        [5.0115, 5.0091, 5.0114],
        [5.0115, 5.0115, 5.0115],
        [5.0115, 5.0078, 5.0113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.23868031799793243 
model_pd.l_d.mean(): -19.529003143310547 
model_pd.lagr.mean(): -19.29032325744629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5502], device='cuda:0')), ('power', tensor([-20.3044], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.23868031799793243
epoch£º732	 i:1 	 global-step:14641	 l-p:0.23939208686351776
epoch£º732	 i:2 	 global-step:14642	 l-p:0.12429118901491165
epoch£º732	 i:3 	 global-step:14643	 l-p:0.12775996327400208
epoch£º732	 i:4 	 global-step:14644	 l-p:0.21088790893554688
epoch£º732	 i:5 	 global-step:14645	 l-p:0.11266141384840012
epoch£º732	 i:6 	 global-step:14646	 l-p:0.1799941211938858
epoch£º732	 i:7 	 global-step:14647	 l-p:0.09276033192873001
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13710367679595947
epoch£º732	 i:9 	 global-step:14649	 l-p:0.21233294904232025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0695, 5.0695, 5.0695],
        [5.0695, 5.0576, 5.0683],
        [5.0695, 4.9919, 5.0380],
        [5.0695, 5.0369, 5.0629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13220761716365814 
model_pd.l_d.mean(): -19.849775314331055 
model_pd.lagr.mean(): -19.717567443847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5129], device='cuda:0')), ('power', tensor([-20.5905], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13220761716365814
epoch£º733	 i:1 	 global-step:14661	 l-p:0.1033826395869255
epoch£º733	 i:2 	 global-step:14662	 l-p:0.21372072398662567
epoch£º733	 i:3 	 global-step:14663	 l-p:0.16555394232273102
epoch£º733	 i:4 	 global-step:14664	 l-p:0.06799663603305817
epoch£º733	 i:5 	 global-step:14665	 l-p:0.1486683487892151
epoch£º733	 i:6 	 global-step:14666	 l-p:0.15747956931591034
epoch£º733	 i:7 	 global-step:14667	 l-p:0.10585854202508926
epoch£º733	 i:8 	 global-step:14668	 l-p:0.1617685854434967
epoch£º733	 i:9 	 global-step:14669	 l-p:0.10467355698347092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1383, 5.1371, 5.1383],
        [5.1383, 5.2657, 5.0535],
        [5.1383, 5.4848, 5.3989],
        [5.1383, 4.9424, 4.7230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.1537543684244156 
model_pd.l_d.mean(): -20.43229103088379 
model_pd.lagr.mean(): -20.278535842895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4530], device='cuda:0')), ('power', tensor([-21.1183], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.1537543684244156
epoch£º734	 i:1 	 global-step:14681	 l-p:0.10924848169088364
epoch£º734	 i:2 	 global-step:14682	 l-p:0.09708341211080551
epoch£º734	 i:3 	 global-step:14683	 l-p:0.15895147621631622
epoch£º734	 i:4 	 global-step:14684	 l-p:0.1065315529704094
epoch£º734	 i:5 	 global-step:14685	 l-p:0.13788454234600067
epoch£º734	 i:6 	 global-step:14686	 l-p:0.060887228697538376
epoch£º734	 i:7 	 global-step:14687	 l-p:0.1049901619553566
epoch£º734	 i:8 	 global-step:14688	 l-p:0.130795419216156
epoch£º734	 i:9 	 global-step:14689	 l-p:0.21536773443222046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1015, 4.8798, 4.7801],
        [5.1015, 5.0192, 5.0661],
        [5.1015, 5.0166, 4.7398],
        [5.1015, 4.9860, 4.7112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.14422789216041565 
model_pd.l_d.mean(): -20.018259048461914 
model_pd.lagr.mean(): -19.87403106689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.7218], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.14422789216041565
epoch£º735	 i:1 	 global-step:14701	 l-p:0.0946304127573967
epoch£º735	 i:2 	 global-step:14702	 l-p:0.10425855219364166
epoch£º735	 i:3 	 global-step:14703	 l-p:0.13050958514213562
epoch£º735	 i:4 	 global-step:14704	 l-p:0.21875625848770142
epoch£º735	 i:5 	 global-step:14705	 l-p:0.1320147067308426
epoch£º735	 i:6 	 global-step:14706	 l-p:0.1155654564499855
epoch£º735	 i:7 	 global-step:14707	 l-p:0.15725499391555786
epoch£º735	 i:8 	 global-step:14708	 l-p:0.14766669273376465
epoch£º735	 i:9 	 global-step:14709	 l-p:0.2406015694141388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0672, 5.3986, 5.3047],
        [5.0672, 5.0010, 5.0438],
        [5.0672, 4.8398, 4.6877],
        [5.0672, 4.8393, 4.6961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16598829627037048 
model_pd.l_d.mean(): -20.447891235351562 
model_pd.lagr.mean(): -20.281902313232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4917], device='cuda:0')), ('power', tensor([-21.1736], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16598829627037048
epoch£º736	 i:1 	 global-step:14721	 l-p:0.1123763918876648
epoch£º736	 i:2 	 global-step:14722	 l-p:0.19423000514507294
epoch£º736	 i:3 	 global-step:14723	 l-p:0.14005637168884277
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09761123359203339
epoch£º736	 i:5 	 global-step:14725	 l-p:0.13075895607471466
epoch£º736	 i:6 	 global-step:14726	 l-p:0.13877329230308533
epoch£º736	 i:7 	 global-step:14727	 l-p:0.13297973573207855
epoch£º736	 i:8 	 global-step:14728	 l-p:0.09486988186836243
epoch£º736	 i:9 	 global-step:14729	 l-p:0.17956379055976868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0976, 5.0645, 5.0908],
        [5.0976, 5.5346, 5.5102],
        [5.0976, 4.9945, 4.7169],
        [5.0976, 5.4670, 5.3967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12353074550628662 
model_pd.l_d.mean(): -20.689186096191406 
model_pd.lagr.mean(): -20.565654754638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4215], device='cuda:0')), ('power', tensor([-21.3457], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12353074550628662
epoch£º737	 i:1 	 global-step:14741	 l-p:0.19937162101268768
epoch£º737	 i:2 	 global-step:14742	 l-p:0.08402357995510101
epoch£º737	 i:3 	 global-step:14743	 l-p:0.1100422739982605
epoch£º737	 i:4 	 global-step:14744	 l-p:0.17882497608661652
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12780462205410004
epoch£º737	 i:6 	 global-step:14746	 l-p:0.1416548788547516
epoch£º737	 i:7 	 global-step:14747	 l-p:0.132986918091774
epoch£º737	 i:8 	 global-step:14748	 l-p:0.14300578832626343
epoch£º737	 i:9 	 global-step:14749	 l-p:0.12098708748817444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1101, 5.0979, 5.1089],
        [5.1101, 5.1076, 5.1100],
        [5.1101, 5.1094, 5.1101],
        [5.1101, 5.1101, 5.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.10324370861053467 
model_pd.l_d.mean(): -20.271146774291992 
model_pd.lagr.mean(): -20.167903900146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4866], device='cuda:0')), ('power', tensor([-20.9896], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.10324370861053467
epoch£º738	 i:1 	 global-step:14761	 l-p:0.13582389056682587
epoch£º738	 i:2 	 global-step:14762	 l-p:0.08905377984046936
epoch£º738	 i:3 	 global-step:14763	 l-p:0.1435135453939438
epoch£º738	 i:4 	 global-step:14764	 l-p:0.11694511026144028
epoch£º738	 i:5 	 global-step:14765	 l-p:0.14484483003616333
epoch£º738	 i:6 	 global-step:14766	 l-p:0.12768380343914032
epoch£º738	 i:7 	 global-step:14767	 l-p:0.1513078808784485
epoch£º738	 i:8 	 global-step:14768	 l-p:0.14531415700912476
epoch£º738	 i:9 	 global-step:14769	 l-p:0.16287797689437866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0998, 5.0998, 5.0998],
        [5.0998, 5.0998, 5.0998],
        [5.0998, 5.0394, 4.7621],
        [5.0998, 4.9680, 5.0118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.1501508355140686 
model_pd.l_d.mean(): -19.718812942504883 
model_pd.lagr.mean(): -19.568662643432617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5280], device='cuda:0')), ('power', tensor([-20.4736], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.1501508355140686
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13028079271316528
epoch£º739	 i:2 	 global-step:14782	 l-p:0.17879875004291534
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15832297503948212
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12642259895801544
epoch£º739	 i:5 	 global-step:14785	 l-p:0.18230445683002472
epoch£º739	 i:6 	 global-step:14786	 l-p:0.09392058849334717
epoch£º739	 i:7 	 global-step:14787	 l-p:0.13253207504749298
epoch£º739	 i:8 	 global-step:14788	 l-p:0.08590630441904068
epoch£º739	 i:9 	 global-step:14789	 l-p:0.0960414856672287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1211, 5.1211, 5.1211],
        [5.1211, 5.1099, 5.1200],
        [5.1211, 4.9107, 4.8608],
        [5.1211, 4.9179, 4.6975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.1318875253200531 
model_pd.l_d.mean(): -19.220335006713867 
model_pd.lagr.mean(): -19.08844757080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5320], device='cuda:0')), ('power', tensor([-19.9738], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.1318875253200531
epoch£º740	 i:1 	 global-step:14801	 l-p:0.08674537390470505
epoch£º740	 i:2 	 global-step:14802	 l-p:0.10215051472187042
epoch£º740	 i:3 	 global-step:14803	 l-p:0.0777025893330574
epoch£º740	 i:4 	 global-step:14804	 l-p:0.14023293554782867
epoch£º740	 i:5 	 global-step:14805	 l-p:0.1372004896402359
epoch£º740	 i:6 	 global-step:14806	 l-p:0.14899124205112457
epoch£º740	 i:7 	 global-step:14807	 l-p:0.16089411079883575
epoch£º740	 i:8 	 global-step:14808	 l-p:0.13736775517463684
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15344201028347015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1242, 4.9614, 4.7008],
        [5.1242, 5.5389, 5.4973],
        [5.1242, 4.9695, 4.7046],
        [5.1242, 5.1242, 5.1242]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11708945780992508 
model_pd.l_d.mean(): -20.338062286376953 
model_pd.lagr.mean(): -20.220972061157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-21.0549], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11708945780992508
epoch£º741	 i:1 	 global-step:14821	 l-p:0.11131563037633896
epoch£º741	 i:2 	 global-step:14822	 l-p:0.1285250186920166
epoch£º741	 i:3 	 global-step:14823	 l-p:0.15786267817020416
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07253343611955643
epoch£º741	 i:5 	 global-step:14825	 l-p:0.11490762233734131
epoch£º741	 i:6 	 global-step:14826	 l-p:0.14667272567749023
epoch£º741	 i:7 	 global-step:14827	 l-p:0.14677678048610687
epoch£º741	 i:8 	 global-step:14828	 l-p:0.1594531387090683
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14792554080486298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0916, 5.4978, 5.4514],
        [5.0916, 4.9656, 5.0119],
        [5.0916, 5.0875, 5.0914],
        [5.0916, 4.8671, 4.7774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.13394388556480408 
model_pd.l_d.mean(): -19.84402084350586 
model_pd.lagr.mean(): -19.7100772857666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5496], device='cuda:0')), ('power', tensor([-20.6223], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.13394388556480408
epoch£º742	 i:1 	 global-step:14841	 l-p:0.11329736560583115
epoch£º742	 i:2 	 global-step:14842	 l-p:0.11410856992006302
epoch£º742	 i:3 	 global-step:14843	 l-p:0.13136403262615204
epoch£º742	 i:4 	 global-step:14844	 l-p:0.22724349796772003
epoch£º742	 i:5 	 global-step:14845	 l-p:0.14423531293869019
epoch£º742	 i:6 	 global-step:14846	 l-p:0.1944206953048706
epoch£º742	 i:7 	 global-step:14847	 l-p:0.12122948467731476
epoch£º742	 i:8 	 global-step:14848	 l-p:0.06652254611253738
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12494174391031265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1165, 5.1165, 5.1165],
        [5.1165, 5.0522, 5.0944],
        [5.1165, 5.0808, 4.8066],
        [5.1165, 4.9447, 4.6870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.050970353186130524 
model_pd.l_d.mean(): -19.88131332397461 
model_pd.lagr.mean(): -19.83034324645996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5188], device='cuda:0')), ('power', tensor([-20.6285], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.050970353186130524
epoch£º743	 i:1 	 global-step:14861	 l-p:0.12875768542289734
epoch£º743	 i:2 	 global-step:14862	 l-p:0.13251017034053802
epoch£º743	 i:3 	 global-step:14863	 l-p:0.13243886828422546
epoch£º743	 i:4 	 global-step:14864	 l-p:0.12345365434885025
epoch£º743	 i:5 	 global-step:14865	 l-p:0.126119464635849
epoch£º743	 i:6 	 global-step:14866	 l-p:0.16243712604045868
epoch£º743	 i:7 	 global-step:14867	 l-p:0.14148055016994476
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12386302649974823
epoch£º743	 i:9 	 global-step:14869	 l-p:0.16046595573425293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1029, 5.0185, 5.0663],
        [5.1029, 4.9685, 5.0121],
        [5.1029, 5.0199, 5.0674],
        [5.1029, 5.0648, 5.0943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.13293619453907013 
model_pd.l_d.mean(): -20.91764259338379 
model_pd.lagr.mean(): -20.784706115722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3929], device='cuda:0')), ('power', tensor([-21.5475], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.13293619453907013
epoch£º744	 i:1 	 global-step:14881	 l-p:0.1307094395160675
epoch£º744	 i:2 	 global-step:14882	 l-p:0.11009012907743454
epoch£º744	 i:3 	 global-step:14883	 l-p:0.11165779083967209
epoch£º744	 i:4 	 global-step:14884	 l-p:0.2109193205833435
epoch£º744	 i:5 	 global-step:14885	 l-p:0.0945114716887474
epoch£º744	 i:6 	 global-step:14886	 l-p:0.15134787559509277
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13353469967842102
epoch£º744	 i:8 	 global-step:14888	 l-p:-0.3719017803668976
epoch£º744	 i:9 	 global-step:14889	 l-p:0.283640593290329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0212, 4.9878, 5.0145],
        [5.0212, 5.0211, 5.0212],
        [5.0212, 5.4250, 5.3785],
        [5.0212, 4.8643, 4.5856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.3149909973144531 
model_pd.l_d.mean(): -19.69441032409668 
model_pd.lagr.mean(): -19.379419326782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5540], device='cuda:0')), ('power', tensor([-20.4755], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.3149909973144531
epoch£º745	 i:1 	 global-step:14901	 l-p:0.13959716260433197
epoch£º745	 i:2 	 global-step:14902	 l-p:0.2696543335914612
epoch£º745	 i:3 	 global-step:14903	 l-p:0.11013536155223846
epoch£º745	 i:4 	 global-step:14904	 l-p:0.1338600516319275
epoch£º745	 i:5 	 global-step:14905	 l-p:0.12117466330528259
epoch£º745	 i:6 	 global-step:14906	 l-p:0.3207615911960602
epoch£º745	 i:7 	 global-step:14907	 l-p:0.15610741078853607
epoch£º745	 i:8 	 global-step:14908	 l-p:0.1812712848186493
epoch£º745	 i:9 	 global-step:14909	 l-p:0.1184135377407074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0788, 5.0722, 5.0784],
        [5.0788, 5.0692, 5.0780],
        [5.0788, 5.1237, 4.8721],
        [5.0788, 5.0788, 5.0789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.1459004133939743 
model_pd.l_d.mean(): -19.259904861450195 
model_pd.lagr.mean(): -19.114004135131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5534], device='cuda:0')), ('power', tensor([-20.0357], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.1459004133939743
epoch£º746	 i:1 	 global-step:14921	 l-p:0.19902434945106506
epoch£º746	 i:2 	 global-step:14922	 l-p:0.1272386908531189
epoch£º746	 i:3 	 global-step:14923	 l-p:0.1324382722377777
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12258631736040115
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12823830544948578
epoch£º746	 i:6 	 global-step:14926	 l-p:0.13319753110408783
epoch£º746	 i:7 	 global-step:14927	 l-p:0.10744789242744446
epoch£º746	 i:8 	 global-step:14928	 l-p:0.11012983322143555
epoch£º746	 i:9 	 global-step:14929	 l-p:0.1212402880191803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 5.0843, 5.1217],
        [5.1373, 5.0993, 5.1287],
        [5.1373, 5.1464, 4.8828],
        [5.1373, 5.1671, 4.9103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18068350851535797 
model_pd.l_d.mean(): -19.935136795043945 
model_pd.lagr.mean(): -19.754453659057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5003], device='cuda:0')), ('power', tensor([-20.6640], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18068350851535797
epoch£º747	 i:1 	 global-step:14941	 l-p:0.16315433382987976
epoch£º747	 i:2 	 global-step:14942	 l-p:0.13543595373630524
epoch£º747	 i:3 	 global-step:14943	 l-p:0.10478484630584717
epoch£º747	 i:4 	 global-step:14944	 l-p:0.13263949751853943
epoch£º747	 i:5 	 global-step:14945	 l-p:0.10010389238595963
epoch£º747	 i:6 	 global-step:14946	 l-p:0.1533774882555008
epoch£º747	 i:7 	 global-step:14947	 l-p:0.0816483199596405
epoch£º747	 i:8 	 global-step:14948	 l-p:0.14681008458137512
epoch£º747	 i:9 	 global-step:14949	 l-p:0.14295190572738647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0780, 4.8854, 4.8828],
        [5.0780, 5.4754, 5.4222],
        [5.0780, 4.9681, 5.0179],
        [5.0780, 4.8710, 4.8438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.1609858274459839 
model_pd.l_d.mean(): -20.736099243164062 
model_pd.lagr.mean(): -20.57511329650879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.4117], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.1609858274459839
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15106548368930817
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1788640320301056
epoch£º748	 i:3 	 global-step:14963	 l-p:0.13044729828834534
epoch£º748	 i:4 	 global-step:14964	 l-p:0.09176336228847504
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2505691349506378
epoch£º748	 i:6 	 global-step:14966	 l-p:0.10228460282087326
epoch£º748	 i:7 	 global-step:14967	 l-p:0.16719207167625427
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12537464499473572
epoch£º748	 i:9 	 global-step:14969	 l-p:0.12499593198299408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0762, 5.4328, 5.3522],
        [5.0762, 5.0531, 5.0726],
        [5.0762, 5.0490, 5.0715],
        [5.0762, 4.8527, 4.6495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.13795050978660583 
model_pd.l_d.mean(): -20.092391967773438 
model_pd.lagr.mean(): -19.95444107055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5266], device='cuda:0')), ('power', tensor([-20.8499], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.13795050978660583
epoch£º749	 i:1 	 global-step:14981	 l-p:0.1299610435962677
epoch£º749	 i:2 	 global-step:14982	 l-p:0.13082721829414368
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13450263440608978
epoch£º749	 i:4 	 global-step:14984	 l-p:0.10998422652482986
epoch£º749	 i:5 	 global-step:14985	 l-p:0.1531328409910202
epoch£º749	 i:6 	 global-step:14986	 l-p:0.28210029006004333
epoch£º749	 i:7 	 global-step:14987	 l-p:0.20067469775676727
epoch£º749	 i:8 	 global-step:14988	 l-p:0.1434038281440735
epoch£º749	 i:9 	 global-step:14989	 l-p:0.12489602714776993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0770, 4.9824, 4.6975],
        [5.0770, 5.3915, 5.2837],
        [5.0770, 5.0726, 5.0768],
        [5.0770, 5.0716, 5.0767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.1200372502207756 
model_pd.l_d.mean(): -20.41416358947754 
model_pd.lagr.mean(): -20.294126510620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-21.1255], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.1200372502207756
epoch£º750	 i:1 	 global-step:15001	 l-p:0.1973983496427536
epoch£º750	 i:2 	 global-step:15002	 l-p:0.19925282895565033
epoch£º750	 i:3 	 global-step:15003	 l-p:0.20103339850902557
epoch£º750	 i:4 	 global-step:15004	 l-p:0.03962674364447594
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13681916892528534
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12451914697885513
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12249715626239777
epoch£º750	 i:8 	 global-step:15008	 l-p:0.12434025853872299
epoch£º750	 i:9 	 global-step:15009	 l-p:0.1335049271583557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1138, 5.0013, 5.0506],
        [5.1138, 5.0488, 4.7675],
        [5.1138, 5.1138, 5.1138],
        [5.1138, 4.9432, 4.9645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.15325269103050232 
model_pd.l_d.mean(): -20.67666244506836 
model_pd.lagr.mean(): -20.523408889770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566], device='cuda:0')), ('power', tensor([-21.3690], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.15325269103050232
epoch£º751	 i:1 	 global-step:15021	 l-p:0.12697386741638184
epoch£º751	 i:2 	 global-step:15022	 l-p:0.16177725791931152
epoch£º751	 i:3 	 global-step:15023	 l-p:0.17267148196697235
epoch£º751	 i:4 	 global-step:15024	 l-p:0.08208919316530228
epoch£º751	 i:5 	 global-step:15025	 l-p:0.11799097806215286
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14494769275188446
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13116158545017242
epoch£º751	 i:8 	 global-step:15028	 l-p:0.1490868479013443
epoch£º751	 i:9 	 global-step:15029	 l-p:0.09805186092853546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1067, 5.1067, 5.1067],
        [5.1067, 5.0660, 5.0971],
        [5.1067, 4.9795, 5.0263],
        [5.1067, 5.1024, 5.1065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.10566011071205139 
model_pd.l_d.mean(): -19.461578369140625 
model_pd.lagr.mean(): -19.355918884277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5425], device='cuda:0')), ('power', tensor([-20.2284], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.10566011071205139
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12246652692556381
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09309197217226028
epoch£º752	 i:3 	 global-step:15043	 l-p:0.12936557829380035
epoch£º752	 i:4 	 global-step:15044	 l-p:0.1700742095708847
epoch£º752	 i:5 	 global-step:15045	 l-p:0.17692594230175018
epoch£º752	 i:6 	 global-step:15046	 l-p:0.13021157681941986
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13629643619060516
epoch£º752	 i:8 	 global-step:15048	 l-p:0.12922172248363495
epoch£º752	 i:9 	 global-step:15049	 l-p:0.1211051344871521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 5.1305, 5.1323],
        [5.1324, 5.1324, 5.1324],
        [5.1324, 5.0000, 5.0448],
        [5.1324, 4.9166, 4.8627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13511952757835388 
model_pd.l_d.mean(): -20.586856842041016 
model_pd.lagr.mean(): -20.451736450195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4586], device='cuda:0')), ('power', tensor([-21.2802], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13511952757835388
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14213289320468903
epoch£º753	 i:2 	 global-step:15062	 l-p:0.13004513084888458
epoch£º753	 i:3 	 global-step:15063	 l-p:0.15680085122585297
epoch£º753	 i:4 	 global-step:15064	 l-p:0.15498089790344238
epoch£º753	 i:5 	 global-step:15065	 l-p:0.06886465102434158
epoch£º753	 i:6 	 global-step:15066	 l-p:0.2009688764810562
epoch£º753	 i:7 	 global-step:15067	 l-p:0.15449929237365723
epoch£º753	 i:8 	 global-step:15068	 l-p:0.13974931836128235
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12749961018562317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0832, 5.0777, 5.0829],
        [5.0832, 5.0791, 5.0830],
        [5.0832, 5.1766, 4.9441],
        [5.0832, 5.0830, 5.0832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.1188768669962883 
model_pd.l_d.mean(): -20.517351150512695 
model_pd.lagr.mean(): -20.398473739624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.2095], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.1188768669962883
epoch£º754	 i:1 	 global-step:15081	 l-p:0.11024566739797592
epoch£º754	 i:2 	 global-step:15082	 l-p:0.15906310081481934
epoch£º754	 i:3 	 global-step:15083	 l-p:0.22211456298828125
epoch£º754	 i:4 	 global-step:15084	 l-p:0.19709354639053345
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12352265417575836
epoch£º754	 i:6 	 global-step:15086	 l-p:0.13015618920326233
epoch£º754	 i:7 	 global-step:15087	 l-p:0.15002937614917755
epoch£º754	 i:8 	 global-step:15088	 l-p:0.1451931595802307
epoch£º754	 i:9 	 global-step:15089	 l-p:0.11261430382728577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0717, 4.9076, 4.9379],
        [5.0717, 5.0296, 5.0616],
        [5.0717, 4.9670, 4.6796],
        [5.0717, 4.8432, 4.7662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10898599028587341 
model_pd.l_d.mean(): -20.11964988708496 
model_pd.lagr.mean(): -20.010663986206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5030], device='cuda:0')), ('power', tensor([-20.8533], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10898599028587341
epoch£º755	 i:1 	 global-step:15101	 l-p:0.1449843794107437
epoch£º755	 i:2 	 global-step:15102	 l-p:0.15540586411952972
epoch£º755	 i:3 	 global-step:15103	 l-p:0.169550359249115
epoch£º755	 i:4 	 global-step:15104	 l-p:0.1500903218984604
epoch£º755	 i:5 	 global-step:15105	 l-p:-0.23966647684574127
epoch£º755	 i:6 	 global-step:15106	 l-p:0.12252254039049149
epoch£º755	 i:7 	 global-step:15107	 l-p:0.14350055158138275
epoch£º755	 i:8 	 global-step:15108	 l-p:-0.29935434460639954
epoch£º755	 i:9 	 global-step:15109	 l-p:0.630529522895813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9967, 4.8508, 4.5611],
        [4.9967, 4.9967, 4.9967],
        [4.9967, 4.7543, 4.5703],
        [4.9967, 4.7586, 4.6707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.12459038197994232 
model_pd.l_d.mean(): -20.656864166259766 
model_pd.lagr.mean(): -20.53227424621582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-21.3596], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.12459038197994232
epoch£º756	 i:1 	 global-step:15121	 l-p:-0.03683536499738693
epoch£º756	 i:2 	 global-step:15122	 l-p:-4.6338348388671875
epoch£º756	 i:3 	 global-step:15123	 l-p:0.22068727016448975
epoch£º756	 i:4 	 global-step:15124	 l-p:0.3237868845462799
epoch£º756	 i:5 	 global-step:15125	 l-p:0.17541591823101044
epoch£º756	 i:6 	 global-step:15126	 l-p:0.11255702376365662
epoch£º756	 i:7 	 global-step:15127	 l-p:0.14207811653614044
epoch£º756	 i:8 	 global-step:15128	 l-p:0.07946518808603287
epoch£º756	 i:9 	 global-step:15129	 l-p:0.1606626808643341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 4.8679, 4.6161],
        [5.0683, 4.9621, 4.6738],
        [5.0683, 4.9333, 4.9794],
        [5.0683, 5.0683, 5.0683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.1469622701406479 
model_pd.l_d.mean(): -18.73040008544922 
model_pd.lagr.mean(): -18.583436965942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5649], device='cuda:0')), ('power', tensor([-19.5121], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.1469622701406479
epoch£º757	 i:1 	 global-step:15141	 l-p:0.1577386111021042
epoch£º757	 i:2 	 global-step:15142	 l-p:0.1492336094379425
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12077571451663971
epoch£º757	 i:4 	 global-step:15144	 l-p:0.162886381149292
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13178381323814392
epoch£º757	 i:6 	 global-step:15146	 l-p:0.11545789241790771
epoch£º757	 i:7 	 global-step:15147	 l-p:0.07607179135084152
epoch£º757	 i:8 	 global-step:15148	 l-p:0.11515417695045471
epoch£º757	 i:9 	 global-step:15149	 l-p:0.16219516098499298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 4.9651, 4.9669],
        [5.1524, 4.9911, 5.0201],
        [5.1524, 5.0816, 5.1262],
        [5.1524, 4.9252, 4.8237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.04449230432510376 
model_pd.l_d.mean(): -20.760807037353516 
model_pd.lagr.mean(): -20.7163143157959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4097], device='cuda:0')), ('power', tensor([-21.4062], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.04449230432510376
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13391157984733582
epoch£º758	 i:2 	 global-step:15162	 l-p:0.1526380330324173
epoch£º758	 i:3 	 global-step:15163	 l-p:0.09134966880083084
epoch£º758	 i:4 	 global-step:15164	 l-p:0.16943439841270447
epoch£º758	 i:5 	 global-step:15165	 l-p:0.10874523222446442
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11145904660224915
epoch£º758	 i:7 	 global-step:15167	 l-p:0.08453825861215591
epoch£º758	 i:8 	 global-step:15168	 l-p:0.03711531683802605
epoch£º758	 i:9 	 global-step:15169	 l-p:0.12924276292324066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1933, 5.1932, 5.1933],
        [5.1933, 5.1933, 5.1933],
        [5.1933, 5.0953, 4.8150],
        [5.1933, 5.1933, 5.1933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.03458217531442642 
model_pd.l_d.mean(): -19.841829299926758 
model_pd.lagr.mean(): -19.807247161865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-20.5182], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.03458217531442642
epoch£º759	 i:1 	 global-step:15181	 l-p:0.1441592276096344
epoch£º759	 i:2 	 global-step:15182	 l-p:0.12402985244989395
epoch£º759	 i:3 	 global-step:15183	 l-p:0.144551619887352
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11945177614688873
epoch£º759	 i:5 	 global-step:15185	 l-p:0.15465091168880463
epoch£º759	 i:6 	 global-step:15186	 l-p:0.0824533998966217
epoch£º759	 i:7 	 global-step:15187	 l-p:0.12874183058738708
epoch£º759	 i:8 	 global-step:15188	 l-p:0.013766316697001457
epoch£º759	 i:9 	 global-step:15189	 l-p:0.11791505664587021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 5.1164, 5.1424],
        [5.1489, 4.9611, 4.9637],
        [5.1489, 4.9218, 4.7437],
        [5.1489, 5.0797, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.07809320092201233 
model_pd.l_d.mean(): -19.27602767944336 
model_pd.lagr.mean(): -19.197935104370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5509], device='cuda:0')), ('power', tensor([-20.0494], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.07809320092201233
epoch£º760	 i:1 	 global-step:15201	 l-p:0.13181038200855255
epoch£º760	 i:2 	 global-step:15202	 l-p:0.07913010567426682
epoch£º760	 i:3 	 global-step:15203	 l-p:0.15304073691368103
epoch£º760	 i:4 	 global-step:15204	 l-p:0.14013513922691345
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12334317713975906
epoch£º760	 i:6 	 global-step:15206	 l-p:0.09656386822462082
epoch£º760	 i:7 	 global-step:15207	 l-p:0.1424623429775238
epoch£º760	 i:8 	 global-step:15208	 l-p:0.14709220826625824
epoch£º760	 i:9 	 global-step:15209	 l-p:0.11975332349538803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1062, 5.0895, 5.1042],
        [5.1062, 4.9108, 4.6574],
        [5.1062, 4.8952, 4.8647],
        [5.1062, 5.0577, 4.7749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.13947288691997528 
model_pd.l_d.mean(): -18.4334659576416 
model_pd.lagr.mean(): -18.29399299621582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6124], device='cuda:0')), ('power', tensor([-19.2605], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.13947288691997528
epoch£º761	 i:1 	 global-step:15221	 l-p:0.09468795359134674
epoch£º761	 i:2 	 global-step:15222	 l-p:0.12626388669013977
epoch£º761	 i:3 	 global-step:15223	 l-p:0.13902214169502258
epoch£º761	 i:4 	 global-step:15224	 l-p:0.1350703090429306
epoch£º761	 i:5 	 global-step:15225	 l-p:-0.39151260256767273
epoch£º761	 i:6 	 global-step:15226	 l-p:0.19266901910305023
epoch£º761	 i:7 	 global-step:15227	 l-p:0.1675317883491516
epoch£º761	 i:8 	 global-step:15228	 l-p:0.11276420950889587
epoch£º761	 i:9 	 global-step:15229	 l-p:0.8001782894134521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9962, 4.7916, 4.7864],
        [4.9962, 4.7793, 4.7538],
        [4.9962, 4.7619, 4.5384],
        [4.9962, 4.9962, 4.9962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13728033006191254 
model_pd.l_d.mean(): -20.730077743530273 
model_pd.lagr.mean(): -20.592798233032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4728], device='cuda:0')), ('power', tensor([-21.4395], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13728033006191254
epoch£º762	 i:1 	 global-step:15241	 l-p:0.591705858707428
epoch£º762	 i:2 	 global-step:15242	 l-p:0.13818146288394928
epoch£º762	 i:3 	 global-step:15243	 l-p:0.13285256922245026
epoch£º762	 i:4 	 global-step:15244	 l-p:0.13507579267024994
epoch£º762	 i:5 	 global-step:15245	 l-p:0.13113805651664734
epoch£º762	 i:6 	 global-step:15246	 l-p:0.305602490901947
epoch£º762	 i:7 	 global-step:15247	 l-p:-0.04051106423139572
epoch£º762	 i:8 	 global-step:15248	 l-p:0.22742557525634766
epoch£º762	 i:9 	 global-step:15249	 l-p:0.05876212567090988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9988, 4.7513, 4.6166],
        [4.9988, 4.9934, 4.9985],
        [4.9988, 4.9568, 4.9890],
        [4.9988, 5.1760, 4.9860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.15248659253120422 
model_pd.l_d.mean(): -19.466218948364258 
model_pd.lagr.mean(): -19.313732147216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5475], device='cuda:0')), ('power', tensor([-20.2382], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.15248659253120422
epoch£º763	 i:1 	 global-step:15261	 l-p:-2.0624992847442627
epoch£º763	 i:2 	 global-step:15262	 l-p:0.10819961130619049
epoch£º763	 i:3 	 global-step:15263	 l-p:0.12011168897151947
epoch£º763	 i:4 	 global-step:15264	 l-p:0.14661487936973572
epoch£º763	 i:5 	 global-step:15265	 l-p:0.12619586288928986
epoch£º763	 i:6 	 global-step:15266	 l-p:0.3624959886074066
epoch£º763	 i:7 	 global-step:15267	 l-p:0.15276850759983063
epoch£º763	 i:8 	 global-step:15268	 l-p:0.14539538323879242
epoch£º763	 i:9 	 global-step:15269	 l-p:0.14305832982063293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0844, 5.0080, 5.0549],
        [5.0844, 5.0807, 5.0843],
        [5.0844, 4.8635, 4.6374],
        [5.0844, 4.8713, 4.8415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.17590777575969696 
model_pd.l_d.mean(): -20.8029727935791 
model_pd.lagr.mean(): -20.627065658569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4405], device='cuda:0')), ('power', tensor([-21.4802], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.17590777575969696
epoch£º764	 i:1 	 global-step:15281	 l-p:0.10145760327577591
epoch£º764	 i:2 	 global-step:15282	 l-p:0.14758428931236267
epoch£º764	 i:3 	 global-step:15283	 l-p:0.17793008685112
epoch£º764	 i:4 	 global-step:15284	 l-p:0.12941840291023254
epoch£º764	 i:5 	 global-step:15285	 l-p:0.13021770119667053
epoch£º764	 i:6 	 global-step:15286	 l-p:0.13340365886688232
epoch£º764	 i:7 	 global-step:15287	 l-p:0.17864029109477997
epoch£º764	 i:8 	 global-step:15288	 l-p:0.07246760278940201
epoch£º764	 i:9 	 global-step:15289	 l-p:0.08136641979217529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.0412, 5.0915],
        [5.1456, 5.1397, 5.1452],
        [5.1456, 5.1455, 5.1456],
        [5.1456, 5.0273, 5.0766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.11842715740203857 
model_pd.l_d.mean(): -18.578462600708008 
model_pd.lagr.mean(): -18.46003532409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5735], device='cuda:0')), ('power', tensor([-19.3673], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.11842715740203857
epoch£º765	 i:1 	 global-step:15301	 l-p:0.17751482129096985
epoch£º765	 i:2 	 global-step:15302	 l-p:0.09122215956449509
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11931350082159042
epoch£º765	 i:4 	 global-step:15304	 l-p:0.1590636819601059
epoch£º765	 i:5 	 global-step:15305	 l-p:0.13069714605808258
epoch£º765	 i:6 	 global-step:15306	 l-p:0.3333451747894287
epoch£º765	 i:7 	 global-step:15307	 l-p:0.12830600142478943
epoch£º765	 i:8 	 global-step:15308	 l-p:0.07012636214494705
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08567643165588379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2305, 5.6074, 5.5320],
        [5.2305, 5.0053, 4.8792],
        [5.2305, 5.1837, 5.2181],
        [5.2305, 5.3609, 5.1420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12235242873430252 
model_pd.l_d.mean(): -20.512561798095703 
model_pd.lagr.mean(): -20.390209197998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4268], device='cuda:0')), ('power', tensor([-21.1727], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12235242873430252
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12309261411428452
epoch£º766	 i:2 	 global-step:15322	 l-p:0.14003120362758636
epoch£º766	 i:3 	 global-step:15323	 l-p:-0.02456965483725071
epoch£º766	 i:4 	 global-step:15324	 l-p:0.13130468130111694
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12296666949987411
epoch£º766	 i:6 	 global-step:15326	 l-p:0.08867137879133224
epoch£º766	 i:7 	 global-step:15327	 l-p:0.11745700240135193
epoch£º766	 i:8 	 global-step:15328	 l-p:0.060107361525297165
epoch£º766	 i:9 	 global-step:15329	 l-p:0.12719540297985077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2320, 5.1701, 5.2114],
        [5.2320, 5.2319, 5.2320],
        [5.2320, 5.2320, 5.2320],
        [5.2320, 5.5046, 5.3631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.11512956023216248 
model_pd.l_d.mean(): -20.779645919799805 
model_pd.lagr.mean(): -20.66451644897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3844], device='cuda:0')), ('power', tensor([-21.3993], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.11512956023216248
epoch£º767	 i:1 	 global-step:15341	 l-p:0.05873379483819008
epoch£º767	 i:2 	 global-step:15342	 l-p:0.11269601434469223
epoch£º767	 i:3 	 global-step:15343	 l-p:-0.043302323669195175
epoch£º767	 i:4 	 global-step:15344	 l-p:0.1299917846918106
epoch£º767	 i:5 	 global-step:15345	 l-p:0.13397566974163055
epoch£º767	 i:6 	 global-step:15346	 l-p:0.1671464443206787
epoch£º767	 i:7 	 global-step:15347	 l-p:0.14412474632263184
epoch£º767	 i:8 	 global-step:15348	 l-p:0.09855546057224274
epoch£º767	 i:9 	 global-step:15349	 l-p:-1.6714682579040527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1984, 4.9690, 4.8357],
        [5.1984, 5.7168, 5.7403],
        [5.1984, 5.1512, 5.1859],
        [5.1984, 5.1412, 4.8594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.14603516459465027 
model_pd.l_d.mean(): -19.482810974121094 
model_pd.lagr.mean(): -19.336776733398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4828], device='cuda:0')), ('power', tensor([-20.1889], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.14603516459465027
epoch£º768	 i:1 	 global-step:15361	 l-p:0.12280502170324326
epoch£º768	 i:2 	 global-step:15362	 l-p:0.12004416435956955
epoch£º768	 i:3 	 global-step:15363	 l-p:0.08931495249271393
epoch£º768	 i:4 	 global-step:15364	 l-p:0.1079099029302597
epoch£º768	 i:5 	 global-step:15365	 l-p:-0.14336229860782623
epoch£º768	 i:6 	 global-step:15366	 l-p:0.1324142962694168
epoch£º768	 i:7 	 global-step:15367	 l-p:0.17231127619743347
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13563375174999237
epoch£º768	 i:9 	 global-step:15369	 l-p:0.11102643609046936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 4.9060, 4.7134],
        [5.1359, 5.1126, 4.8339],
        [5.1359, 5.1041, 4.8235],
        [5.1359, 5.1220, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.1263289749622345 
model_pd.l_d.mean(): -20.260278701782227 
model_pd.lagr.mean(): -20.133949279785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-20.9673], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.1263289749622345
epoch£º769	 i:1 	 global-step:15381	 l-p:0.15782441198825836
epoch£º769	 i:2 	 global-step:15382	 l-p:0.10063084214925766
epoch£º769	 i:3 	 global-step:15383	 l-p:0.2728424072265625
epoch£º769	 i:4 	 global-step:15384	 l-p:0.12306395918130875
epoch£º769	 i:5 	 global-step:15385	 l-p:0.13150472939014435
epoch£º769	 i:6 	 global-step:15386	 l-p:0.1925136148929596
epoch£º769	 i:7 	 global-step:15387	 l-p:0.13378308713436127
epoch£º769	 i:8 	 global-step:15388	 l-p:0.23443900048732758
epoch£º769	 i:9 	 global-step:15389	 l-p:0.13901658356189728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0102, 5.0102, 5.0102],
        [5.0102, 4.9342, 4.6399],
        [5.0102, 4.7705, 4.5543],
        [5.0102, 4.8928, 4.9451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.07500381022691727 
model_pd.l_d.mean(): -18.34160041809082 
model_pd.lagr.mean(): -18.2665958404541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6391], device='cuda:0')), ('power', tensor([-19.1948], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.07500381022691727
epoch£º770	 i:1 	 global-step:15401	 l-p:0.16345332562923431
epoch£º770	 i:2 	 global-step:15402	 l-p:0.12770703434944153
epoch£º770	 i:3 	 global-step:15403	 l-p:1.0716885328292847
epoch£º770	 i:4 	 global-step:15404	 l-p:0.38020846247673035
epoch£º770	 i:5 	 global-step:15405	 l-p:0.13497017323970795
epoch£º770	 i:6 	 global-step:15406	 l-p:0.15119634568691254
epoch£º770	 i:7 	 global-step:15407	 l-p:0.16340872645378113
epoch£º770	 i:8 	 global-step:15408	 l-p:0.11140129715204239
epoch£º770	 i:9 	 global-step:15409	 l-p:0.060196489095687866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9691, 4.9690, 4.9691],
        [4.9691, 4.9254, 4.9586],
        [4.9691, 4.9643, 4.9688],
        [4.9691, 4.7154, 4.5374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.13085836172103882 
model_pd.l_d.mean(): -20.82638931274414 
model_pd.lagr.mean(): -20.695531845092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-21.5107], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.13085836172103882
epoch£º771	 i:1 	 global-step:15421	 l-p:0.1194276288151741
epoch£º771	 i:2 	 global-step:15422	 l-p:0.04820476844906807
epoch£º771	 i:3 	 global-step:15423	 l-p:0.14240792393684387
epoch£º771	 i:4 	 global-step:15424	 l-p:-0.0693344995379448
epoch£º771	 i:5 	 global-step:15425	 l-p:0.17848901450634003
epoch£º771	 i:6 	 global-step:15426	 l-p:0.1518508344888687
epoch£º771	 i:7 	 global-step:15427	 l-p:0.20945735275745392
epoch£º771	 i:8 	 global-step:15428	 l-p:252.63929748535156
epoch£º771	 i:9 	 global-step:15429	 l-p:0.29385632276535034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9954, 4.9533, 4.9856],
        [4.9954, 4.9213, 4.9682],
        [4.9954, 5.2659, 5.1288],
        [4.9954, 4.9519, 4.9850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.11615794897079468 
model_pd.l_d.mean(): -20.83293914794922 
model_pd.lagr.mean(): -20.716781616210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4561], device='cuda:0')), ('power', tensor([-21.5264], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.11615794897079468
epoch£º772	 i:1 	 global-step:15441	 l-p:0.15739205479621887
epoch£º772	 i:2 	 global-step:15442	 l-p:0.10428693890571594
epoch£º772	 i:3 	 global-step:15443	 l-p:0.14697086811065674
epoch£º772	 i:4 	 global-step:15444	 l-p:0.214339941740036
epoch£º772	 i:5 	 global-step:15445	 l-p:0.11645402014255524
epoch£º772	 i:6 	 global-step:15446	 l-p:0.11591657251119614
epoch£º772	 i:7 	 global-step:15447	 l-p:0.18003630638122559
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07751400023698807
epoch£º772	 i:9 	 global-step:15449	 l-p:0.1638650894165039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 5.1333, 5.1333],
        [5.1333, 5.0960, 5.1252],
        [5.1333, 4.9034, 4.8250],
        [5.1333, 4.9062, 4.8378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13484491407871246 
model_pd.l_d.mean(): -21.015647888183594 
model_pd.lagr.mean(): -20.880802154541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3822], device='cuda:0')), ('power', tensor([-21.6357], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13484491407871246
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13170772790908813
epoch£º773	 i:2 	 global-step:15462	 l-p:0.08668122440576553
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07304456830024719
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14067189395427704
epoch£º773	 i:5 	 global-step:15465	 l-p:0.15634460747241974
epoch£º773	 i:6 	 global-step:15466	 l-p:0.5314880013465881
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13179334998130798
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11659057438373566
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10865405201911926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2258, 5.1286, 5.1783],
        [5.2258, 5.1863, 5.2167],
        [5.2258, 4.9969, 4.8704],
        [5.2258, 5.2068, 5.2233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.08361060917377472 
model_pd.l_d.mean(): -20.720054626464844 
model_pd.lagr.mean(): -20.636444091796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050], device='cuda:0')), ('power', tensor([-21.3602], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.08361060917377472
epoch£º774	 i:1 	 global-step:15481	 l-p:0.1184382438659668
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12812694907188416
epoch£º774	 i:3 	 global-step:15483	 l-p:0.21985027194023132
epoch£º774	 i:4 	 global-step:15484	 l-p:0.13627153635025024
epoch£º774	 i:5 	 global-step:15485	 l-p:0.06876906007528305
epoch£º774	 i:6 	 global-step:15486	 l-p:0.1374431699514389
epoch£º774	 i:7 	 global-step:15487	 l-p:5.3821539878845215
epoch£º774	 i:8 	 global-step:15488	 l-p:0.127679243683815
epoch£º774	 i:9 	 global-step:15489	 l-p:0.1174684688448906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2587, 5.0353, 4.9393],
        [5.2587, 5.0500, 5.0090],
        [5.2587, 5.2587, 5.2587],
        [5.2587, 5.3913, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.1267167627811432 
model_pd.l_d.mean(): -18.812044143676758 
model_pd.lagr.mean(): -18.685327529907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5994], device='cuda:0')), ('power', tensor([-19.6299], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.1267167627811432
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14286261796951294
epoch£º775	 i:2 	 global-step:15502	 l-p:0.09757307916879654
epoch£º775	 i:3 	 global-step:15503	 l-p:0.5700124502182007
epoch£º775	 i:4 	 global-step:15504	 l-p:0.09374838322401047
epoch£º775	 i:5 	 global-step:15505	 l-p:0.11385134607553482
epoch£º775	 i:6 	 global-step:15506	 l-p:0.19676277041435242
epoch£º775	 i:7 	 global-step:15507	 l-p:0.1401536762714386
epoch£º775	 i:8 	 global-step:15508	 l-p:0.14007887244224548
epoch£º775	 i:9 	 global-step:15509	 l-p:0.06626038998365402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2103, 5.0387, 5.0604],
        [5.2103, 4.9923, 4.9372],
        [5.2103, 4.9979, 4.9577],
        [5.2103, 5.5756, 5.4913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.47410011291503906 
model_pd.l_d.mean(): -20.617815017700195 
model_pd.lagr.mean(): -20.143714904785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.2833], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.47410011291503906
epoch£º776	 i:1 	 global-step:15521	 l-p:0.12077566981315613
epoch£º776	 i:2 	 global-step:15522	 l-p:0.14124014973640442
epoch£º776	 i:3 	 global-step:15523	 l-p:0.1285332441329956
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12713533639907837
epoch£º776	 i:5 	 global-step:15525	 l-p:0.11736148595809937
epoch£º776	 i:6 	 global-step:15526	 l-p:0.1326582431793213
epoch£º776	 i:7 	 global-step:15527	 l-p:0.11604785919189453
epoch£º776	 i:8 	 global-step:15528	 l-p:0.1620364934206009
epoch£º776	 i:9 	 global-step:15529	 l-p:0.1409134417772293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1225, 4.9106, 4.8862],
        [5.1225, 5.0383, 5.0873],
        [5.1225, 4.8994, 4.8507],
        [5.1225, 4.8814, 4.7305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.11990224570035934 
model_pd.l_d.mean(): -20.619314193725586 
model_pd.lagr.mean(): -20.499412536621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4438], device='cuda:0')), ('power', tensor([-21.2979], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.11990224570035934
epoch£º777	 i:1 	 global-step:15541	 l-p:0.14494319260120392
epoch£º777	 i:2 	 global-step:15542	 l-p:0.1580209881067276
epoch£º777	 i:3 	 global-step:15543	 l-p:0.16649390757083893
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16430683434009552
epoch£º777	 i:5 	 global-step:15545	 l-p:0.09298671782016754
epoch£º777	 i:6 	 global-step:15546	 l-p:0.1428431272506714
epoch£º777	 i:7 	 global-step:15547	 l-p:0.24780665338039398
epoch£º777	 i:8 	 global-step:15548	 l-p:0.053558558225631714
epoch£º777	 i:9 	 global-step:15549	 l-p:0.169798344373703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0801, 5.0791, 5.0801],
        [5.0801, 5.0013, 5.0494],
        [5.0801, 5.0630, 5.0780],
        [5.0801, 5.0732, 5.0796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.1876547634601593 
model_pd.l_d.mean(): -19.050209045410156 
model_pd.lagr.mean(): -18.8625545501709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6148], device='cuda:0')), ('power', tensor([-19.8863], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.1876547634601593
epoch£º778	 i:1 	 global-step:15561	 l-p:0.15089380741119385
epoch£º778	 i:2 	 global-step:15562	 l-p:0.09006457775831223
epoch£º778	 i:3 	 global-step:15563	 l-p:0.200123593211174
epoch£º778	 i:4 	 global-step:15564	 l-p:0.09732484817504883
epoch£º778	 i:5 	 global-step:15565	 l-p:0.07916014641523361
epoch£º778	 i:6 	 global-step:15566	 l-p:0.1745602786540985
epoch£º778	 i:7 	 global-step:15567	 l-p:0.1449972242116928
epoch£º778	 i:8 	 global-step:15568	 l-p:0.13929517567157745
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14932167530059814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1321, 4.9741, 5.0109],
        [5.1321, 4.8950, 4.7107],
        [5.1321, 5.1320, 5.1321],
        [5.1321, 5.0541, 5.1016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.1136847659945488 
model_pd.l_d.mean(): -20.49747085571289 
model_pd.lagr.mean(): -20.383785247802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4514], device='cuda:0')), ('power', tensor([-21.1825], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.1136847659945488
epoch£º779	 i:1 	 global-step:15581	 l-p:0.16983310878276825
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13263511657714844
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12267711013555527
epoch£º779	 i:4 	 global-step:15584	 l-p:0.15415920317173004
epoch£º779	 i:5 	 global-step:15585	 l-p:0.059431154280900955
epoch£º779	 i:6 	 global-step:15586	 l-p:0.1321859359741211
epoch£º779	 i:7 	 global-step:15587	 l-p:0.1387484073638916
epoch£º779	 i:8 	 global-step:15588	 l-p:0.1534140706062317
epoch£º779	 i:9 	 global-step:15589	 l-p:0.1654081493616104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.1091, 5.1141],
        [5.1144, 5.0958, 5.1120],
        [5.1144, 5.2610, 5.0490],
        [5.1144, 5.2543, 5.0389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.17593590915203094 
model_pd.l_d.mean(): -18.650129318237305 
model_pd.lagr.mean(): -18.474193572998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5722], device='cuda:0')), ('power', tensor([-19.4384], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.17593590915203094
epoch£º780	 i:1 	 global-step:15601	 l-p:0.06593140214681625
epoch£º780	 i:2 	 global-step:15602	 l-p:0.1408078521490097
epoch£º780	 i:3 	 global-step:15603	 l-p:0.1117778941988945
epoch£º780	 i:4 	 global-step:15604	 l-p:0.15786048769950867
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11340345442295074
epoch£º780	 i:6 	 global-step:15606	 l-p:0.1747226119041443
epoch£º780	 i:7 	 global-step:15607	 l-p:0.11562427878379822
epoch£º780	 i:8 	 global-step:15608	 l-p:0.07440915703773499
epoch£º780	 i:9 	 global-step:15609	 l-p:0.12998072803020477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1852, 5.3494, 5.1452],
        [5.1852, 5.1852, 5.1852],
        [5.1852, 5.1792, 5.1848],
        [5.1852, 5.1825, 5.1851]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.16519145667552948 
model_pd.l_d.mean(): -20.503170013427734 
model_pd.lagr.mean(): -20.33797836303711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4503], device='cuda:0')), ('power', tensor([-21.1872], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.16519145667552948
epoch£º781	 i:1 	 global-step:15621	 l-p:0.12219112366437912
epoch£º781	 i:2 	 global-step:15622	 l-p:0.11768355965614319
epoch£º781	 i:3 	 global-step:15623	 l-p:0.09643981605768204
epoch£º781	 i:4 	 global-step:15624	 l-p:0.12758836150169373
epoch£º781	 i:5 	 global-step:15625	 l-p:0.05870487168431282
epoch£º781	 i:6 	 global-step:15626	 l-p:0.11050311475992203
epoch£º781	 i:7 	 global-step:15627	 l-p:-0.04768877476453781
epoch£º781	 i:8 	 global-step:15628	 l-p:0.13870470225811005
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11990319192409515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1814, 5.1633, 5.1790],
        [5.1814, 5.2416, 4.9885],
        [5.1814, 5.1177, 5.1603],
        [5.1814, 5.1139, 5.1579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.0422983318567276 
model_pd.l_d.mean(): -20.127466201782227 
model_pd.lagr.mean(): -20.085166931152344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-20.7812], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.0422983318567276
epoch£º782	 i:1 	 global-step:15641	 l-p:0.14644883573055267
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12409853935241699
epoch£º782	 i:3 	 global-step:15643	 l-p:0.1326003074645996
epoch£º782	 i:4 	 global-step:15644	 l-p:0.13542507588863373
epoch£º782	 i:5 	 global-step:15645	 l-p:0.12021947652101517
epoch£º782	 i:6 	 global-step:15646	 l-p:0.09470515698194504
epoch£º782	 i:7 	 global-step:15647	 l-p:0.12900088727474213
epoch£º782	 i:8 	 global-step:15648	 l-p:0.1695810854434967
epoch£º782	 i:9 	 global-step:15649	 l-p:0.18688277900218964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1037, 5.0478, 4.7570],
        [5.1037, 4.8746, 4.8181],
        [5.1037, 5.0143, 4.7192],
        [5.1037, 5.0592, 5.0928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.16758756339550018 
model_pd.l_d.mean(): -20.681140899658203 
model_pd.lagr.mean(): -20.513553619384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4263], device='cuda:0')), ('power', tensor([-21.3426], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.16758756339550018
epoch£º783	 i:1 	 global-step:15661	 l-p:0.17142333090305328
epoch£º783	 i:2 	 global-step:15662	 l-p:0.12839017808437347
epoch£º783	 i:3 	 global-step:15663	 l-p:0.12090949714183807
epoch£º783	 i:4 	 global-step:15664	 l-p:0.1297299861907959
epoch£º783	 i:5 	 global-step:15665	 l-p:0.10218945890665054
epoch£º783	 i:6 	 global-step:15666	 l-p:0.1551971435546875
epoch£º783	 i:7 	 global-step:15667	 l-p:0.14800401031970978
epoch£º783	 i:8 	 global-step:15668	 l-p:0.0886058583855629
epoch£º783	 i:9 	 global-step:15669	 l-p:0.17889411747455597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 5.0513, 4.7603],
        [5.1089, 5.1089, 5.1089],
        [5.1089, 5.0068, 5.0587],
        [5.1089, 4.9988, 5.0509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.1173224076628685 
model_pd.l_d.mean(): -20.226932525634766 
model_pd.lagr.mean(): -20.109609603881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5132], device='cuda:0')), ('power', tensor([-20.9722], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.1173224076628685
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13682328164577484
epoch£º784	 i:2 	 global-step:15682	 l-p:0.10304302722215652
epoch£º784	 i:3 	 global-step:15683	 l-p:0.1775125116109848
epoch£º784	 i:4 	 global-step:15684	 l-p:0.09431201964616776
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13496479392051697
epoch£º784	 i:6 	 global-step:15686	 l-p:0.14987342059612274
epoch£º784	 i:7 	 global-step:15687	 l-p:0.14311407506465912
epoch£º784	 i:8 	 global-step:15688	 l-p:0.1016683578491211
epoch£º784	 i:9 	 global-step:15689	 l-p:0.12436988204717636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1707, 5.1707, 5.1707],
        [5.1707, 5.1663, 5.1705],
        [5.1707, 4.9647, 4.7170],
        [5.1707, 5.0032, 5.0327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.08689986914396286 
model_pd.l_d.mean(): -19.93121337890625 
model_pd.lagr.mean(): -19.84431266784668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4433], device='cuda:0')), ('power', tensor([-20.6018], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.08689986914396286
epoch£º785	 i:1 	 global-step:15701	 l-p:0.13867563009262085
epoch£º785	 i:2 	 global-step:15702	 l-p:0.02811303548514843
epoch£º785	 i:3 	 global-step:15703	 l-p:0.10775530338287354
epoch£º785	 i:4 	 global-step:15704	 l-p:0.11728489398956299
epoch£º785	 i:5 	 global-step:15705	 l-p:0.16449041664600372
epoch£º785	 i:6 	 global-step:15706	 l-p:0.11805842071771622
epoch£º785	 i:7 	 global-step:15707	 l-p:0.16382434964179993
epoch£º785	 i:8 	 global-step:15708	 l-p:0.11521352827548981
epoch£º785	 i:9 	 global-step:15709	 l-p:0.16120895743370056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1190, 5.1190, 5.1190],
        [5.1190, 5.5049, 5.4356],
        [5.1190, 5.0996, 4.8172],
        [5.1190, 5.0544, 5.0977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.12748870253562927 
model_pd.l_d.mean(): -20.725322723388672 
model_pd.lagr.mean(): -20.59783363342285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4244], device='cuda:0')), ('power', tensor([-21.3852], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.12748870253562927
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13892899453639984
epoch£º786	 i:2 	 global-step:15722	 l-p:0.1555681675672531
epoch£º786	 i:3 	 global-step:15723	 l-p:0.21916842460632324
epoch£º786	 i:4 	 global-step:15724	 l-p:0.1246047392487526
epoch£º786	 i:5 	 global-step:15725	 l-p:0.1317448765039444
epoch£º786	 i:6 	 global-step:15726	 l-p:0.1301492303609848
epoch£º786	 i:7 	 global-step:15727	 l-p:0.17632229626178741
epoch£º786	 i:8 	 global-step:15728	 l-p:0.11775051057338715
epoch£º786	 i:9 	 global-step:15729	 l-p:0.1669187843799591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0940, 5.0256, 4.7313],
        [5.0940, 5.0492, 5.0830],
        [5.0940, 5.0756, 5.0916],
        [5.0940, 5.0138, 5.0624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.15980543196201324 
model_pd.l_d.mean(): -19.42827606201172 
model_pd.lagr.mean(): -19.268470764160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5744], device='cuda:0')), ('power', tensor([-20.2273], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.15980543196201324
epoch£º787	 i:1 	 global-step:15741	 l-p:0.1208728775382042
epoch£º787	 i:2 	 global-step:15742	 l-p:0.14785169064998627
epoch£º787	 i:3 	 global-step:15743	 l-p:0.1069142296910286
epoch£º787	 i:4 	 global-step:15744	 l-p:0.14108262956142426
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14692732691764832
epoch£º787	 i:6 	 global-step:15746	 l-p:0.12057677656412125
epoch£º787	 i:7 	 global-step:15747	 l-p:0.21622401475906372
epoch£º787	 i:8 	 global-step:15748	 l-p:0.17710837721824646
epoch£º787	 i:9 	 global-step:15749	 l-p:0.14790111780166626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 4.8429, 4.7528],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 4.8373, 4.7138],
        [5.0849, 5.1573, 4.9084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.14242132008075714 
model_pd.l_d.mean(): -20.722118377685547 
model_pd.lagr.mean(): -20.579696655273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4301], device='cuda:0')), ('power', tensor([-21.3879], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.14242132008075714
epoch£º788	 i:1 	 global-step:15761	 l-p:0.16124789416790009
epoch£º788	 i:2 	 global-step:15762	 l-p:0.1246214359998703
epoch£º788	 i:3 	 global-step:15763	 l-p:0.11534259468317032
epoch£º788	 i:4 	 global-step:15764	 l-p:0.19992686808109283
epoch£º788	 i:5 	 global-step:15765	 l-p:0.1484544575214386
epoch£º788	 i:6 	 global-step:15766	 l-p:0.12973856925964355
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12454038113355637
epoch£º788	 i:8 	 global-step:15768	 l-p:0.14788596332073212
epoch£º788	 i:9 	 global-step:15769	 l-p:0.13555537164211273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1207, 5.0748, 5.1092],
        [5.1207, 4.8889, 4.8253],
        [5.1207, 5.1207, 5.1207],
        [5.1207, 5.1207, 5.1207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.11545132100582123 
model_pd.l_d.mean(): -20.41742515563965 
model_pd.lagr.mean(): -20.301973342895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-21.1110], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.11545132100582123
epoch£º789	 i:1 	 global-step:15781	 l-p:0.14099574089050293
epoch£º789	 i:2 	 global-step:15782	 l-p:0.16767051815986633
epoch£º789	 i:3 	 global-step:15783	 l-p:0.1403413861989975
epoch£º789	 i:4 	 global-step:15784	 l-p:0.1662459522485733
epoch£º789	 i:5 	 global-step:15785	 l-p:0.12842392921447754
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11477302014827728
epoch£º789	 i:7 	 global-step:15787	 l-p:0.11676616221666336
epoch£º789	 i:8 	 global-step:15788	 l-p:0.14582332968711853
epoch£º789	 i:9 	 global-step:15789	 l-p:0.10406318306922913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1439, 5.0710, 5.1172],
        [5.1439, 5.1439, 5.1439],
        [5.1439, 5.1028, 4.8150],
        [5.1439, 4.9136, 4.8497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.11133110523223877 
model_pd.l_d.mean(): -20.919919967651367 
model_pd.lagr.mean(): -20.8085880279541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3977], device='cuda:0')), ('power', tensor([-21.5547], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.11133110523223877
epoch£º790	 i:1 	 global-step:15801	 l-p:0.18245929479599
epoch£º790	 i:2 	 global-step:15802	 l-p:0.1405848115682602
epoch£º790	 i:3 	 global-step:15803	 l-p:0.12445570528507233
epoch£º790	 i:4 	 global-step:15804	 l-p:0.12188419699668884
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12782058119773865
epoch£º790	 i:6 	 global-step:15806	 l-p:0.16320368647575378
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12574154138565063
epoch£º790	 i:8 	 global-step:15808	 l-p:0.1443997472524643
epoch£º790	 i:9 	 global-step:15809	 l-p:0.09161368012428284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1222, 4.8997, 4.8611],
        [5.1222, 5.5552, 5.5170],
        [5.1222, 5.0142, 5.0665],
        [5.1222, 4.9288, 4.9375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.16345085203647614 
model_pd.l_d.mean(): -20.6560001373291 
model_pd.lagr.mean(): -20.492549896240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-21.3057], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.16345085203647614
epoch£º791	 i:1 	 global-step:15821	 l-p:0.15553827583789825
epoch£º791	 i:2 	 global-step:15822	 l-p:0.07880028337240219
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12327422946691513
epoch£º791	 i:4 	 global-step:15824	 l-p:0.1053888276219368
epoch£º791	 i:5 	 global-step:15825	 l-p:0.13735783100128174
epoch£º791	 i:6 	 global-step:15826	 l-p:0.10041574388742447
epoch£º791	 i:7 	 global-step:15827	 l-p:0.1569535881280899
epoch£º791	 i:8 	 global-step:15828	 l-p:0.1285119354724884
epoch£º791	 i:9 	 global-step:15829	 l-p:0.1557852327823639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1424, 4.9039, 4.7083],
        [5.1424, 5.0168, 4.7224],
        [5.1424, 5.0739, 5.1187],
        [5.1424, 5.0041, 5.0513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.1331118792295456 
model_pd.l_d.mean(): -20.79412269592285 
model_pd.lagr.mean(): -20.6610107421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4095], device='cuda:0')), ('power', tensor([-21.4396], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.1331118792295456
epoch£º792	 i:1 	 global-step:15841	 l-p:0.06733951717615128
epoch£º792	 i:2 	 global-step:15842	 l-p:0.14196671545505524
epoch£º792	 i:3 	 global-step:15843	 l-p:0.142178475856781
epoch£º792	 i:4 	 global-step:15844	 l-p:0.11857230216264725
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12558326125144958
epoch£º792	 i:6 	 global-step:15846	 l-p:0.08995748311281204
epoch£º792	 i:7 	 global-step:15847	 l-p:0.1902550607919693
epoch£º792	 i:8 	 global-step:15848	 l-p:0.15204675495624542
epoch£º792	 i:9 	 global-step:15849	 l-p:0.3504747748374939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0807, 5.0806, 5.0807],
        [5.0807, 5.0779, 5.0806],
        [5.0807, 5.0806, 5.0807],
        [5.0807, 5.0336, 5.0688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.1314450204372406 
model_pd.l_d.mean(): -20.785202026367188 
model_pd.lagr.mean(): -20.653757095336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4107], device='cuda:0')), ('power', tensor([-21.4318], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.1314450204372406
epoch£º793	 i:1 	 global-step:15861	 l-p:0.1452552229166031
epoch£º793	 i:2 	 global-step:15862	 l-p:0.10916326195001602
epoch£º793	 i:3 	 global-step:15863	 l-p:0.11539026349782944
epoch£º793	 i:4 	 global-step:15864	 l-p:0.11657317727804184
epoch£º793	 i:5 	 global-step:15865	 l-p:0.3501017689704895
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13372394442558289
epoch£º793	 i:7 	 global-step:15867	 l-p:0.1815434843301773
epoch£º793	 i:8 	 global-step:15868	 l-p:0.1517544388771057
epoch£º793	 i:9 	 global-step:15869	 l-p:0.2715946137905121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0656, 4.8125, 4.6638],
        [5.0656, 5.0439, 5.0625],
        [5.0656, 4.8141, 4.6893],
        [5.0656, 5.0656, 5.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.1625627726316452 
model_pd.l_d.mean(): -20.26108169555664 
model_pd.lagr.mean(): -20.09851837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5075], device='cuda:0')), ('power', tensor([-21.0008], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.1625627726316452
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11300785839557648
epoch£º794	 i:2 	 global-step:15882	 l-p:0.1879740059375763
epoch£º794	 i:3 	 global-step:15883	 l-p:0.1297016143798828
epoch£º794	 i:4 	 global-step:15884	 l-p:0.1534425914287567
epoch£º794	 i:5 	 global-step:15885	 l-p:0.16434356570243835
epoch£º794	 i:6 	 global-step:15886	 l-p:0.1079440489411354
epoch£º794	 i:7 	 global-step:15887	 l-p:0.18205036222934723
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11668478697538376
epoch£º794	 i:9 	 global-step:15889	 l-p:0.1589202582836151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1280, 5.1280, 5.1280],
        [5.1280, 4.9273, 4.6600],
        [5.1280, 5.0860, 5.1182],
        [5.1280, 5.0879, 5.1190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.10478923469781876 
model_pd.l_d.mean(): -20.37944221496582 
model_pd.lagr.mean(): -20.2746524810791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587], device='cuda:0')), ('power', tensor([-21.0706], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.10478923469781876
epoch£º795	 i:1 	 global-step:15901	 l-p:0.14596007764339447
epoch£º795	 i:2 	 global-step:15902	 l-p:0.15800495445728302
epoch£º795	 i:3 	 global-step:15903	 l-p:0.1338234841823578
epoch£º795	 i:4 	 global-step:15904	 l-p:0.15105369687080383
epoch£º795	 i:5 	 global-step:15905	 l-p:0.09942881017923355
epoch£º795	 i:6 	 global-step:15906	 l-p:0.19448336958885193
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12269933521747589
epoch£º795	 i:8 	 global-step:15908	 l-p:0.1161031499505043
epoch£º795	 i:9 	 global-step:15909	 l-p:0.12571248412132263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1109, 5.0601, 5.0972],
        [5.1109, 5.0591, 4.7664],
        [5.1109, 4.9797, 5.0301],
        [5.1109, 5.1109, 5.1109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.1487034112215042 
model_pd.l_d.mean(): -19.97569465637207 
model_pd.lagr.mean(): -19.82699203491211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4947], device='cuda:0')), ('power', tensor([-20.6992], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.1487034112215042
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12480504810810089
epoch£º796	 i:2 	 global-step:15922	 l-p:0.10258272290229797
epoch£º796	 i:3 	 global-step:15923	 l-p:0.2109077423810959
epoch£º796	 i:4 	 global-step:15924	 l-p:0.17104488611221313
epoch£º796	 i:5 	 global-step:15925	 l-p:0.12722301483154297
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11585001647472382
epoch£º796	 i:7 	 global-step:15927	 l-p:0.16426776349544525
epoch£º796	 i:8 	 global-step:15928	 l-p:0.13902531564235687
epoch£º796	 i:9 	 global-step:15929	 l-p:0.17696186900138855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0952, 5.0057, 5.0567],
        [5.0952, 5.0788, 5.0933],
        [5.0952, 5.0946, 5.0952],
        [5.0952, 5.0952, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.12295273691415787 
model_pd.l_d.mean(): -20.535463333129883 
model_pd.lagr.mean(): -20.41250991821289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.2345], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.12295273691415787
epoch£º797	 i:1 	 global-step:15941	 l-p:0.1200626939535141
epoch£º797	 i:2 	 global-step:15942	 l-p:0.1489911824464798
epoch£º797	 i:3 	 global-step:15943	 l-p:0.18706859648227692
epoch£º797	 i:4 	 global-step:15944	 l-p:0.2507922053337097
epoch£º797	 i:5 	 global-step:15945	 l-p:0.12110434472560883
epoch£º797	 i:6 	 global-step:15946	 l-p:0.16643653810024261
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11513873934745789
epoch£º797	 i:8 	 global-step:15948	 l-p:0.1754557192325592
epoch£º797	 i:9 	 global-step:15949	 l-p:0.19858497381210327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0672, 5.0948, 4.8254],
        [5.0672, 5.0626, 5.0670],
        [5.0672, 4.9330, 4.9838],
        [5.0672, 5.3316, 5.1845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.15201318264007568 
model_pd.l_d.mean(): -19.908742904663086 
model_pd.lagr.mean(): -19.756729125976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-20.6188], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.15201318264007568
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11452753841876984
epoch£º798	 i:2 	 global-step:15962	 l-p:0.24642257392406464
epoch£º798	 i:3 	 global-step:15963	 l-p:0.14359639585018158
epoch£º798	 i:4 	 global-step:15964	 l-p:-0.2864646911621094
epoch£º798	 i:5 	 global-step:15965	 l-p:0.18619054555892944
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11496874690055847
epoch£º798	 i:7 	 global-step:15967	 l-p:0.21343356370925903
epoch£º798	 i:8 	 global-step:15968	 l-p:0.21010735630989075
epoch£º798	 i:9 	 global-step:15969	 l-p:0.19036191701889038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0588, 5.0161, 5.0489],
        [5.0588, 5.0575, 5.0588],
        [5.0588, 5.0589, 5.0588],
        [5.0588, 5.0561, 5.0588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.11300596594810486 
model_pd.l_d.mean(): -21.015625 
model_pd.lagr.mean(): -20.902618408203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4003], device='cuda:0')), ('power', tensor([-21.6541], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.11300596594810486
epoch£º799	 i:1 	 global-step:15981	 l-p:0.585023045539856
epoch£º799	 i:2 	 global-step:15982	 l-p:0.16247078776359558
epoch£º799	 i:3 	 global-step:15983	 l-p:0.11842641234397888
epoch£º799	 i:4 	 global-step:15984	 l-p:0.12794706225395203
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10666646808385849
epoch£º799	 i:6 	 global-step:15986	 l-p:0.11933383345603943
epoch£º799	 i:7 	 global-step:15987	 l-p:0.11973275989294052
epoch£º799	 i:8 	 global-step:15988	 l-p:0.2890133559703827
epoch£º799	 i:9 	 global-step:15989	 l-p:0.1301746964454651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0748, 4.8542, 4.8311],
        [5.0748, 5.2391, 5.0340],
        [5.0748, 5.0670, 5.0742],
        [5.0748, 4.8206, 4.6680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.10813632607460022 
model_pd.l_d.mean(): -18.539474487304688 
model_pd.lagr.mean(): -18.431337356567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5970], device='cuda:0')), ('power', tensor([-19.3519], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.10813632607460022
epoch£º800	 i:1 	 global-step:16001	 l-p:0.11029908806085587
epoch£º800	 i:2 	 global-step:16002	 l-p:0.14803995192050934
epoch£º800	 i:3 	 global-step:16003	 l-p:0.23139514029026031
epoch£º800	 i:4 	 global-step:16004	 l-p:0.15162041783332825
epoch£º800	 i:5 	 global-step:16005	 l-p:0.12230836600065231
epoch£º800	 i:6 	 global-step:16006	 l-p:0.24929198622703552
epoch£º800	 i:7 	 global-step:16007	 l-p:0.1393410712480545
epoch£º800	 i:8 	 global-step:16008	 l-p:0.18863719701766968
epoch£º800	 i:9 	 global-step:16009	 l-p:0.1319614052772522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[5.0869, 5.2544, 5.0509],
        [5.0869, 5.2091, 4.9819],
        [5.0869, 4.8754, 4.6070],
        [5.0869, 4.8637, 4.8344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.12797443568706512 
model_pd.l_d.mean(): -19.73238754272461 
model_pd.lagr.mean(): -19.604413986206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-20.4597], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.12797443568706512
epoch£º801	 i:1 	 global-step:16021	 l-p:0.1303703635931015
epoch£º801	 i:2 	 global-step:16022	 l-p:0.10128890722990036
epoch£º801	 i:3 	 global-step:16023	 l-p:0.1558288186788559
epoch£º801	 i:4 	 global-step:16024	 l-p:0.15219616889953613
epoch£º801	 i:5 	 global-step:16025	 l-p:0.16766469180583954
epoch£º801	 i:6 	 global-step:16026	 l-p:0.11906952410936356
epoch£º801	 i:7 	 global-step:16027	 l-p:0.131118044257164
epoch£º801	 i:8 	 global-step:16028	 l-p:0.210590660572052
epoch£º801	 i:9 	 global-step:16029	 l-p:0.1831604540348053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1009, 5.0975, 5.1008],
        [5.1009, 4.9815, 5.0344],
        [5.1009, 5.1009, 5.1010],
        [5.1009, 4.8519, 4.7371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.18897253274917603 
model_pd.l_d.mean(): -19.10103988647461 
model_pd.lagr.mean(): -18.912067413330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5262], device='cuda:0')), ('power', tensor([-19.8473], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.18897253274917603
epoch£º802	 i:1 	 global-step:16041	 l-p:0.15459206700325012
epoch£º802	 i:2 	 global-step:16042	 l-p:0.18182127177715302
epoch£º802	 i:3 	 global-step:16043	 l-p:0.1252773106098175
epoch£º802	 i:4 	 global-step:16044	 l-p:0.08503825962543488
epoch£º802	 i:5 	 global-step:16045	 l-p:0.1319345086812973
epoch£º802	 i:6 	 global-step:16046	 l-p:0.14413604140281677
epoch£º802	 i:7 	 global-step:16047	 l-p:0.11731744557619095
epoch£º802	 i:8 	 global-step:16048	 l-p:0.12855665385723114
epoch£º802	 i:9 	 global-step:16049	 l-p:0.1347913295030594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1287, 4.9070, 4.6586],
        [5.1287, 4.8799, 4.7281],
        [5.1287, 4.8892, 4.8089],
        [5.1287, 4.8799, 4.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.1393207162618637 
model_pd.l_d.mean(): -20.723228454589844 
model_pd.lagr.mean(): -20.583908081054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-21.3941], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.1393207162618637
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12177307903766632
epoch£º803	 i:2 	 global-step:16062	 l-p:0.09755914658308029
epoch£º803	 i:3 	 global-step:16063	 l-p:0.14308679103851318
epoch£º803	 i:4 	 global-step:16064	 l-p:0.16130021214485168
epoch£º803	 i:5 	 global-step:16065	 l-p:0.16050457954406738
epoch£º803	 i:6 	 global-step:16066	 l-p:0.10222724825143814
epoch£º803	 i:7 	 global-step:16067	 l-p:0.15182699263095856
epoch£º803	 i:8 	 global-step:16068	 l-p:0.16152037680149078
epoch£º803	 i:9 	 global-step:16069	 l-p:0.1117880791425705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1368, 5.1341, 5.1367],
        [5.1368, 5.1021, 5.1298],
        [5.1368, 4.8943, 4.7987],
        [5.1368, 5.0116, 4.7134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.12881846725940704 
model_pd.l_d.mean(): -19.831714630126953 
model_pd.lagr.mean(): -19.702896118164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-20.5977], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.12881846725940704
epoch£º804	 i:1 	 global-step:16081	 l-p:0.1566382497549057
epoch£º804	 i:2 	 global-step:16082	 l-p:0.10097476840019226
epoch£º804	 i:3 	 global-step:16083	 l-p:0.1355401575565338
epoch£º804	 i:4 	 global-step:16084	 l-p:0.11969911307096481
epoch£º804	 i:5 	 global-step:16085	 l-p:0.11382142454385757
epoch£º804	 i:6 	 global-step:16086	 l-p:0.15653343498706818
epoch£º804	 i:7 	 global-step:16087	 l-p:0.12836332619190216
epoch£º804	 i:8 	 global-step:16088	 l-p:0.0885823592543602
epoch£º804	 i:9 	 global-step:16089	 l-p:0.19260287284851074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1226, 5.1226, 5.1226],
        [5.1226, 5.1226, 5.1226],
        [5.1226, 5.4230, 5.2960],
        [5.1226, 5.0070, 5.0598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.12529531121253967 
model_pd.l_d.mean(): -19.465805053710938 
model_pd.lagr.mean(): -19.34050941467285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5302], device='cuda:0')), ('power', tensor([-20.2201], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.12529531121253967
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12798970937728882
epoch£º805	 i:2 	 global-step:16102	 l-p:0.17298243939876556
epoch£º805	 i:3 	 global-step:16103	 l-p:0.10471376031637192
epoch£º805	 i:4 	 global-step:16104	 l-p:0.15298724174499512
epoch£º805	 i:5 	 global-step:16105	 l-p:0.14141835272312164
epoch£º805	 i:6 	 global-step:16106	 l-p:0.17192399501800537
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13239483535289764
epoch£º805	 i:8 	 global-step:16108	 l-p:0.12428157776594162
epoch£º805	 i:9 	 global-step:16109	 l-p:0.13380847871303558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1180, 5.1181],
        [5.1181, 5.1174, 5.1181],
        [5.1181, 5.1181, 5.1181],
        [5.1181, 5.1156, 5.1180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.07656615227460861 
model_pd.l_d.mean(): -20.367496490478516 
model_pd.lagr.mean(): -20.290929794311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4726], device='cuda:0')), ('power', tensor([-21.0728], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.07656615227460861
epoch£º806	 i:1 	 global-step:16121	 l-p:0.1782706379890442
epoch£º806	 i:2 	 global-step:16122	 l-p:0.10956279188394547
epoch£º806	 i:3 	 global-step:16123	 l-p:0.16474805772304535
epoch£º806	 i:4 	 global-step:16124	 l-p:0.14455199241638184
epoch£º806	 i:5 	 global-step:16125	 l-p:0.13159145414829254
epoch£º806	 i:6 	 global-step:16126	 l-p:0.12887661159038544
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12565013766288757
epoch£º806	 i:8 	 global-step:16128	 l-p:0.18617461621761322
epoch£º806	 i:9 	 global-step:16129	 l-p:0.1299223154783249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1280, 5.1274, 5.1280],
        [5.1280, 5.1275, 5.1280],
        [5.1280, 5.1267, 5.1280],
        [5.1280, 5.1280, 5.1280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.09542615711688995 
model_pd.l_d.mean(): -20.91773223876953 
model_pd.lagr.mean(): -20.82230567932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3983], device='cuda:0')), ('power', tensor([-21.5532], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.09542615711688995
epoch£º807	 i:1 	 global-step:16141	 l-p:0.15524530410766602
epoch£º807	 i:2 	 global-step:16142	 l-p:0.14550937712192535
epoch£º807	 i:3 	 global-step:16143	 l-p:0.15039603412151337
epoch£º807	 i:4 	 global-step:16144	 l-p:0.1277659833431244
epoch£º807	 i:5 	 global-step:16145	 l-p:0.07867944240570068
epoch£º807	 i:6 	 global-step:16146	 l-p:0.12377849966287613
epoch£º807	 i:7 	 global-step:16147	 l-p:0.14924393594264984
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12138029932975769
epoch£º807	 i:9 	 global-step:16149	 l-p:0.18606090545654297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1346, 5.1292, 5.1343],
        [5.1346, 5.1216, 5.1333],
        [5.1346, 5.1346, 5.1346],
        [5.1346, 5.1304, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.15433070063591003 
model_pd.l_d.mean(): -20.04938507080078 
model_pd.lagr.mean(): -19.89505386352539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4533], device='cuda:0')), ('power', tensor([-20.7315], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.15433070063591003
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12471267580986023
epoch£º808	 i:2 	 global-step:16162	 l-p:0.15322963893413544
epoch£º808	 i:3 	 global-step:16163	 l-p:0.14388833940029144
epoch£º808	 i:4 	 global-step:16164	 l-p:0.058675553649663925
epoch£º808	 i:5 	 global-step:16165	 l-p:0.1642795354127884
epoch£º808	 i:6 	 global-step:16166	 l-p:0.1442507654428482
epoch£º808	 i:7 	 global-step:16167	 l-p:0.12315278500318527
epoch£º808	 i:8 	 global-step:16168	 l-p:0.13782061636447906
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10856176167726517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 5.1449, 5.1449],
        [5.1449, 5.1444, 5.1449],
        [5.1449, 5.0852, 5.1266],
        [5.1449, 5.1448, 5.1449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.11607643216848373 
model_pd.l_d.mean(): -19.290813446044922 
model_pd.lagr.mean(): -19.17473793029785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5431], device='cuda:0')), ('power', tensor([-20.0564], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.11607643216848373
epoch£º809	 i:1 	 global-step:16181	 l-p:0.10623881965875626
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12720057368278503
epoch£º809	 i:3 	 global-step:16183	 l-p:0.1036883145570755
epoch£º809	 i:4 	 global-step:16184	 l-p:0.12168457359075546
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12818993628025055
epoch£º809	 i:6 	 global-step:16186	 l-p:0.1393865942955017
epoch£º809	 i:7 	 global-step:16187	 l-p:0.1598794162273407
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12417109310626984
epoch£º809	 i:9 	 global-step:16189	 l-p:0.1328904926776886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 4.9785, 4.9953],
        [5.1653, 4.9259, 4.7236],
        [5.1653, 5.6657, 5.6711],
        [5.1653, 5.1633, 5.1653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.17337432503700256 
model_pd.l_d.mean(): -19.871334075927734 
model_pd.lagr.mean(): -19.697959899902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-20.5540], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.17337432503700256
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15791723132133484
epoch£º810	 i:2 	 global-step:16202	 l-p:0.08909015357494354
epoch£º810	 i:3 	 global-step:16203	 l-p:0.10405793786048889
epoch£º810	 i:4 	 global-step:16204	 l-p:0.12203098833560944
epoch£º810	 i:5 	 global-step:16205	 l-p:0.07214110344648361
epoch£º810	 i:6 	 global-step:16206	 l-p:0.1350797563791275
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11462558805942535
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12310723960399628
epoch£º810	 i:9 	 global-step:16209	 l-p:0.1356537938117981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1598, 4.9127, 4.7681],
        [5.1598, 5.0161, 4.7211],
        [5.1598, 5.0713, 5.1219],
        [5.1598, 5.0217, 5.0701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.053645893931388855 
model_pd.l_d.mean(): -19.553232192993164 
model_pd.lagr.mean(): -19.49958610534668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5387], device='cuda:0')), ('power', tensor([-20.3172], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.053645893931388855
epoch£º811	 i:1 	 global-step:16221	 l-p:0.16021369397640228
epoch£º811	 i:2 	 global-step:16222	 l-p:0.10581725835800171
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12351003289222717
epoch£º811	 i:4 	 global-step:16224	 l-p:0.1609165221452713
epoch£º811	 i:5 	 global-step:16225	 l-p:0.15816184878349304
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12694303691387177
epoch£º811	 i:7 	 global-step:16227	 l-p:0.10905689746141434
epoch£º811	 i:8 	 global-step:16228	 l-p:0.09163910895586014
epoch£º811	 i:9 	 global-step:16229	 l-p:0.1337539404630661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 5.1620, 5.1678],
        [5.1682, 5.1466, 5.1651],
        [5.1682, 5.3634, 5.1724],
        [5.1682, 5.1428, 5.1641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.028745850548148155 
model_pd.l_d.mean(): -20.702640533447266 
model_pd.lagr.mean(): -20.67389488220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.3700], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.028745850548148155
epoch£º812	 i:1 	 global-step:16241	 l-p:0.11739947646856308
epoch£º812	 i:2 	 global-step:16242	 l-p:0.10487951338291168
epoch£º812	 i:3 	 global-step:16243	 l-p:0.13737526535987854
epoch£º812	 i:4 	 global-step:16244	 l-p:0.15465064346790314
epoch£º812	 i:5 	 global-step:16245	 l-p:0.14711692929267883
epoch£º812	 i:6 	 global-step:16246	 l-p:0.11715232580900192
epoch£º812	 i:7 	 global-step:16247	 l-p:0.12903761863708496
epoch£º812	 i:8 	 global-step:16248	 l-p:0.13390101492404938
epoch£º812	 i:9 	 global-step:16249	 l-p:0.12383648008108139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1588, 5.0532, 5.1058],
        [5.1588, 4.9282, 4.8737],
        [5.1588, 4.9385, 4.9076],
        [5.1588, 5.1588, 5.1588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.14101114869117737 
model_pd.l_d.mean(): -19.810787200927734 
model_pd.lagr.mean(): -19.669776916503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-20.5068], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.14101114869117737
epoch£º813	 i:1 	 global-step:16261	 l-p:0.09898767620325089
epoch£º813	 i:2 	 global-step:16262	 l-p:0.1259535700082779
epoch£º813	 i:3 	 global-step:16263	 l-p:0.12371200323104858
epoch£º813	 i:4 	 global-step:16264	 l-p:0.1878606379032135
epoch£º813	 i:5 	 global-step:16265	 l-p:0.101129911839962
epoch£º813	 i:6 	 global-step:16266	 l-p:0.10695997625589371
epoch£º813	 i:7 	 global-step:16267	 l-p:0.14075393974781036
epoch£º813	 i:8 	 global-step:16268	 l-p:0.1727009415626526
epoch£º813	 i:9 	 global-step:16269	 l-p:0.11475308984518051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1154, 5.1154, 5.1154],
        [5.1154, 5.1154, 5.1154],
        [5.1154, 5.0713, 5.1049],
        [5.1154, 5.1154, 5.1154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.1352156102657318 
model_pd.l_d.mean(): -20.71906852722168 
model_pd.lagr.mean(): -20.583852767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4094], device='cuda:0')), ('power', tensor([-21.3636], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.1352156102657318
epoch£º814	 i:1 	 global-step:16281	 l-p:0.2030280977487564
epoch£º814	 i:2 	 global-step:16282	 l-p:0.12267319113016129
epoch£º814	 i:3 	 global-step:16283	 l-p:0.10911352187395096
epoch£º814	 i:4 	 global-step:16284	 l-p:0.14874577522277832
epoch£º814	 i:5 	 global-step:16285	 l-p:0.18663032352924347
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13832323253154755
epoch£º814	 i:7 	 global-step:16287	 l-p:0.09664005041122437
epoch£º814	 i:8 	 global-step:16288	 l-p:0.13143226504325867
epoch£º814	 i:9 	 global-step:16289	 l-p:0.17224517464637756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0924, 5.0924, 5.0924],
        [5.0924, 5.2640, 5.0613],
        [5.0924, 5.0081, 5.0584],
        [5.0924, 4.9890, 5.0424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.11137790232896805 
model_pd.l_d.mean(): -19.04700469970703 
model_pd.lagr.mean(): -18.935626983642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5749], device='cuda:0')), ('power', tensor([-19.8424], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.11137790232896805
epoch£º815	 i:1 	 global-step:16301	 l-p:0.094992995262146
epoch£º815	 i:2 	 global-step:16302	 l-p:0.14042530953884125
epoch£º815	 i:3 	 global-step:16303	 l-p:0.21925871074199677
epoch£º815	 i:4 	 global-step:16304	 l-p:0.12184523046016693
epoch£º815	 i:5 	 global-step:16305	 l-p:0.12929314374923706
epoch£º815	 i:6 	 global-step:16306	 l-p:0.18904556334018707
epoch£º815	 i:7 	 global-step:16307	 l-p:0.13729190826416016
epoch£º815	 i:8 	 global-step:16308	 l-p:0.17123614251613617
epoch£º815	 i:9 	 global-step:16309	 l-p:0.24780552089214325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0839, 5.0019, 5.0517],
        [5.0839, 4.9401, 4.9890],
        [5.0839, 5.0757, 5.0833],
        [5.0839, 5.0395, 5.0732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.15536697208881378 
model_pd.l_d.mean(): -20.876493453979492 
model_pd.lagr.mean(): -20.721126556396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4020], device='cuda:0')), ('power', tensor([-21.5152], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.15536697208881378
epoch£º816	 i:1 	 global-step:16321	 l-p:0.1329171508550644
epoch£º816	 i:2 	 global-step:16322	 l-p:0.13148903846740723
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07612822949886322
epoch£º816	 i:4 	 global-step:16324	 l-p:0.33693498373031616
epoch£º816	 i:5 	 global-step:16325	 l-p:0.12346738576889038
epoch£º816	 i:6 	 global-step:16326	 l-p:0.22151751816272736
epoch£º816	 i:7 	 global-step:16327	 l-p:0.29255083203315735
epoch£º816	 i:8 	 global-step:16328	 l-p:0.1322038173675537
epoch£º816	 i:9 	 global-step:16329	 l-p:0.13167373836040497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0664, 5.2425, 5.0426],
        [5.0664, 5.0663, 5.0664],
        [5.0664, 5.0653, 5.0664],
        [5.0664, 4.8121, 4.6127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.09712062031030655 
model_pd.l_d.mean(): -18.63195037841797 
model_pd.lagr.mean(): -18.53483009338379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6074], device='cuda:0')), ('power', tensor([-19.4560], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.09712062031030655
epoch£º817	 i:1 	 global-step:16341	 l-p:0.16694220900535583
epoch£º817	 i:2 	 global-step:16342	 l-p:0.11794397234916687
epoch£º817	 i:3 	 global-step:16343	 l-p:0.17855407297611237
epoch£º817	 i:4 	 global-step:16344	 l-p:0.11853732913732529
epoch£º817	 i:5 	 global-step:16345	 l-p:0.2577591836452484
epoch£º817	 i:6 	 global-step:16346	 l-p:0.1330367773771286
epoch£º817	 i:7 	 global-step:16347	 l-p:0.30514252185821533
epoch£º817	 i:8 	 global-step:16348	 l-p:0.1772950440645218
epoch£º817	 i:9 	 global-step:16349	 l-p:0.12066268920898438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0871, 5.3520, 5.2029],
        [5.0871, 5.2438, 5.0329],
        [5.0871, 5.0871, 5.0871],
        [5.0871, 5.0150, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.11354286968708038 
model_pd.l_d.mean(): -19.262319564819336 
model_pd.lagr.mean(): -19.14877700805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5861], device='cuda:0')), ('power', tensor([-20.0715], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.11354286968708038
epoch£º818	 i:1 	 global-step:16361	 l-p:0.15021401643753052
epoch£º818	 i:2 	 global-step:16362	 l-p:0.25774604082107544
epoch£º818	 i:3 	 global-step:16363	 l-p:0.13685493171215057
epoch£º818	 i:4 	 global-step:16364	 l-p:0.2145565301179886
epoch£º818	 i:5 	 global-step:16365	 l-p:0.1023685559630394
epoch£º818	 i:6 	 global-step:16366	 l-p:0.08804334700107574
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12883128225803375
epoch£º818	 i:8 	 global-step:16368	 l-p:0.1262465864419937
epoch£º818	 i:9 	 global-step:16369	 l-p:0.16588646173477173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1147, 4.9980, 5.0514],
        [5.1147, 4.8740, 4.6487],
        [5.1147, 5.1099, 5.1144],
        [5.1147, 5.1709, 4.9110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.1260952204465866 
model_pd.l_d.mean(): -20.874736785888672 
model_pd.lagr.mean(): -20.748641967773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249], device='cuda:0')), ('power', tensor([-21.5368], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.1260952204465866
epoch£º819	 i:1 	 global-step:16381	 l-p:0.2114935666322708
epoch£º819	 i:2 	 global-step:16382	 l-p:0.1093861535191536
epoch£º819	 i:3 	 global-step:16383	 l-p:0.11719311773777008
epoch£º819	 i:4 	 global-step:16384	 l-p:0.20131778717041016
epoch£º819	 i:5 	 global-step:16385	 l-p:0.1260686218738556
epoch£º819	 i:6 	 global-step:16386	 l-p:0.10973136872053146
epoch£º819	 i:7 	 global-step:16387	 l-p:0.17669086158275604
epoch£º819	 i:8 	 global-step:16388	 l-p:0.16850920021533966
epoch£º819	 i:9 	 global-step:16389	 l-p:0.08393308520317078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1008, 4.8735, 4.6193],
        [5.1008, 5.0995, 5.1008],
        [5.1008, 4.8474, 4.6653],
        [5.1008, 5.1008, 5.1008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.19973286986351013 
model_pd.l_d.mean(): -19.554065704345703 
model_pd.lagr.mean(): -19.354331970214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5017], device='cuda:0')), ('power', tensor([-20.2802], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.19973286986351013
epoch£º820	 i:1 	 global-step:16401	 l-p:0.11345235258340836
epoch£º820	 i:2 	 global-step:16402	 l-p:0.16674664616584778
epoch£º820	 i:3 	 global-step:16403	 l-p:0.132765531539917
epoch£º820	 i:4 	 global-step:16404	 l-p:0.13474109768867493
epoch£º820	 i:5 	 global-step:16405	 l-p:0.1042146161198616
epoch£º820	 i:6 	 global-step:16406	 l-p:0.18306979537010193
epoch£º820	 i:7 	 global-step:16407	 l-p:0.13174518942832947
epoch£º820	 i:8 	 global-step:16408	 l-p:0.17226800322532654
epoch£º820	 i:9 	 global-step:16409	 l-p:0.10180560499429703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1204, 4.9683, 5.0131],
        [5.1204, 5.0365, 5.0866],
        [5.1204, 4.9567, 4.9955],
        [5.1204, 4.8686, 4.7405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.11934696137905121 
model_pd.l_d.mean(): -20.971710205078125 
model_pd.lagr.mean(): -20.85236358642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3943], device='cuda:0')), ('power', tensor([-21.6036], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.11934696137905121
epoch£º821	 i:1 	 global-step:16421	 l-p:0.16892801225185394
epoch£º821	 i:2 	 global-step:16422	 l-p:0.11285438388586044
epoch£º821	 i:3 	 global-step:16423	 l-p:0.14432692527770996
epoch£º821	 i:4 	 global-step:16424	 l-p:0.1635444015264511
epoch£º821	 i:5 	 global-step:16425	 l-p:0.09767249971628189
epoch£º821	 i:6 	 global-step:16426	 l-p:0.18009430170059204
epoch£º821	 i:7 	 global-step:16427	 l-p:0.12739931046962738
epoch£º821	 i:8 	 global-step:16428	 l-p:0.12136827409267426
epoch£º821	 i:9 	 global-step:16429	 l-p:0.09661038964986801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1576, 5.1502, 5.1571],
        [5.1576, 5.1006, 4.8050],
        [5.1576, 5.0488, 4.7484],
        [5.1576, 5.1573, 5.1576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.1330108493566513 
model_pd.l_d.mean(): -20.058069229125977 
model_pd.lagr.mean(): -19.925058364868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4667], device='cuda:0')), ('power', tensor([-20.7540], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.1330108493566513
epoch£º822	 i:1 	 global-step:16441	 l-p:0.08137088268995285
epoch£º822	 i:2 	 global-step:16442	 l-p:0.14763198792934418
epoch£º822	 i:3 	 global-step:16443	 l-p:0.08352682739496231
epoch£º822	 i:4 	 global-step:16444	 l-p:0.11972825974225998
epoch£º822	 i:5 	 global-step:16445	 l-p:0.12769187986850739
epoch£º822	 i:6 	 global-step:16446	 l-p:0.11373957991600037
epoch£º822	 i:7 	 global-step:16447	 l-p:0.0978633314371109
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13454604148864746
epoch£º822	 i:9 	 global-step:16449	 l-p:0.1833929717540741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.0849, 5.1348],
        [5.1696, 5.1696, 5.1696],
        [5.1696, 4.9489, 4.9182],
        [5.1696, 5.4911, 5.3746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.20131318271160126 
model_pd.l_d.mean(): -20.14571189880371 
model_pd.lagr.mean(): -19.944398880004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-20.8946], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.20131318271160126
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14417876303195953
epoch£º823	 i:2 	 global-step:16462	 l-p:0.11738070845603943
epoch£º823	 i:3 	 global-step:16463	 l-p:0.16068390011787415
epoch£º823	 i:4 	 global-step:16464	 l-p:0.12743356823921204
epoch£º823	 i:5 	 global-step:16465	 l-p:0.09521586447954178
epoch£º823	 i:6 	 global-step:16466	 l-p:0.0365866981446743
epoch£º823	 i:7 	 global-step:16467	 l-p:0.13794079422950745
epoch£º823	 i:8 	 global-step:16468	 l-p:0.0839105024933815
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11677499860525131
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 4.9168, 4.7663],
        [5.1657, 5.1401, 5.1616],
        [5.1657, 5.0624, 5.1151],
        [5.1657, 5.1656, 5.1657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.10476663708686829 
model_pd.l_d.mean(): -20.318445205688477 
model_pd.lagr.mean(): -20.21367835998535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.0239], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.10476663708686829
epoch£º824	 i:1 	 global-step:16481	 l-p:0.10888072848320007
epoch£º824	 i:2 	 global-step:16482	 l-p:0.15584519505500793
epoch£º824	 i:3 	 global-step:16483	 l-p:0.12210846692323685
epoch£º824	 i:4 	 global-step:16484	 l-p:0.13859209418296814
epoch£º824	 i:5 	 global-step:16485	 l-p:0.037644509226083755
epoch£º824	 i:6 	 global-step:16486	 l-p:0.1329440027475357
epoch£º824	 i:7 	 global-step:16487	 l-p:0.14338025450706482
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13781222701072693
epoch£º824	 i:9 	 global-step:16489	 l-p:0.14320008456707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 5.5437, 5.4712],
        [5.1547, 5.1543, 5.1547],
        [5.1547, 5.1547, 5.1547],
        [5.1547, 5.1143, 5.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.07781200110912323 
model_pd.l_d.mean(): -19.8109188079834 
model_pd.lagr.mean(): -19.73310661315918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5051], device='cuda:0')), ('power', tensor([-20.5433], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.07781200110912323
epoch£º825	 i:1 	 global-step:16501	 l-p:0.1177077516913414
epoch£º825	 i:2 	 global-step:16502	 l-p:0.1329314261674881
epoch£º825	 i:3 	 global-step:16503	 l-p:0.12420070171356201
epoch£º825	 i:4 	 global-step:16504	 l-p:0.1577204167842865
epoch£º825	 i:5 	 global-step:16505	 l-p:0.12275905162096024
epoch£º825	 i:6 	 global-step:16506	 l-p:0.12366548925638199
epoch£º825	 i:7 	 global-step:16507	 l-p:0.14940647780895233
epoch£º825	 i:8 	 global-step:16508	 l-p:0.15239492058753967
epoch£º825	 i:9 	 global-step:16509	 l-p:0.13656872510910034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1377, 5.1341, 5.1375],
        [5.1377, 5.1278, 5.1369],
        [5.1377, 5.1350, 5.1376],
        [5.1377, 4.8926, 4.8013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12600663304328918 
model_pd.l_d.mean(): -20.68240737915039 
model_pd.lagr.mean(): -20.556400299072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4211], device='cuda:0')), ('power', tensor([-21.3386], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12600663304328918
epoch£º826	 i:1 	 global-step:16521	 l-p:0.17016209661960602
epoch£º826	 i:2 	 global-step:16522	 l-p:0.13710950314998627
epoch£º826	 i:3 	 global-step:16523	 l-p:0.07983420044183731
epoch£º826	 i:4 	 global-step:16524	 l-p:0.10048045963048935
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12508773803710938
epoch£º826	 i:6 	 global-step:16526	 l-p:0.1384037435054779
epoch£º826	 i:7 	 global-step:16527	 l-p:0.14644229412078857
epoch£º826	 i:8 	 global-step:16528	 l-p:0.1423639953136444
epoch£º826	 i:9 	 global-step:16529	 l-p:0.14273646473884583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 5.1471, 5.1478],
        [5.1478, 5.1072, 5.1387],
        [5.1478, 5.1478, 5.1478],
        [5.1478, 4.9980, 5.0432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.1250290423631668 
model_pd.l_d.mean(): -20.37204360961914 
model_pd.lagr.mean(): -20.24701499938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-21.0529], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.1250290423631668
epoch£º827	 i:1 	 global-step:16541	 l-p:0.1376364827156067
epoch£º827	 i:2 	 global-step:16542	 l-p:0.11493907868862152
epoch£º827	 i:3 	 global-step:16543	 l-p:0.0930895209312439
epoch£º827	 i:4 	 global-step:16544	 l-p:0.16511332988739014
epoch£º827	 i:5 	 global-step:16545	 l-p:0.18237286806106567
epoch£º827	 i:6 	 global-step:16546	 l-p:0.08391404896974564
epoch£º827	 i:7 	 global-step:16547	 l-p:0.13607297837734222
epoch£º827	 i:8 	 global-step:16548	 l-p:0.1286698877811432
epoch£º827	 i:9 	 global-step:16549	 l-p:0.1157764196395874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1590, 5.0468, 5.1000],
        [5.1590, 4.9671, 4.9810],
        [5.1590, 5.1141, 5.1481],
        [5.1590, 5.1202, 5.1506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11854615062475204 
model_pd.l_d.mean(): -20.46180534362793 
model_pd.lagr.mean(): -20.343259811401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-21.1316], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11854615062475204
epoch£º828	 i:1 	 global-step:16561	 l-p:0.06839551031589508
epoch£º828	 i:2 	 global-step:16562	 l-p:0.06841356307268143
epoch£º828	 i:3 	 global-step:16563	 l-p:0.12908026576042175
epoch£º828	 i:4 	 global-step:16564	 l-p:0.15151570737361908
epoch£º828	 i:5 	 global-step:16565	 l-p:0.171986922621727
epoch£º828	 i:6 	 global-step:16566	 l-p:0.14492367208003998
epoch£º828	 i:7 	 global-step:16567	 l-p:0.1381152868270874
epoch£º828	 i:8 	 global-step:16568	 l-p:0.12498685717582703
epoch£º828	 i:9 	 global-step:16569	 l-p:0.1214984804391861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1665, 5.0917, 5.1390],
        [5.1665, 5.0846, 4.7851],
        [5.1665, 5.1663, 5.1665],
        [5.1665, 5.1659, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.07619358599185944 
model_pd.l_d.mean(): -19.602798461914062 
model_pd.lagr.mean(): -19.5266056060791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-20.3177], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.07619358599185944
epoch£º829	 i:1 	 global-step:16581	 l-p:0.10846933722496033
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13344009220600128
epoch£º829	 i:3 	 global-step:16583	 l-p:0.06471792608499527
epoch£º829	 i:4 	 global-step:16584	 l-p:0.1487840861082077
epoch£º829	 i:5 	 global-step:16585	 l-p:0.1095975935459137
epoch£º829	 i:6 	 global-step:16586	 l-p:0.19709716737270355
epoch£º829	 i:7 	 global-step:16587	 l-p:0.16170187294483185
epoch£º829	 i:8 	 global-step:16588	 l-p:0.13042715191841125
epoch£º829	 i:9 	 global-step:16589	 l-p:0.09078996628522873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.1605, 5.1611],
        [5.1611, 4.9407, 4.9135],
        [5.1611, 4.9113, 4.7756],
        [5.1611, 5.1611, 5.1611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.14434851706027985 
model_pd.l_d.mean(): -20.622709274291992 
model_pd.lagr.mean(): -20.478361129760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.2875], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.14434851706027985
epoch£º830	 i:1 	 global-step:16601	 l-p:0.10766073316335678
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13627108931541443
epoch£º830	 i:3 	 global-step:16603	 l-p:0.08449823409318924
epoch£º830	 i:4 	 global-step:16604	 l-p:0.12950709462165833
epoch£º830	 i:5 	 global-step:16605	 l-p:0.14808928966522217
epoch£º830	 i:6 	 global-step:16606	 l-p:0.1688094288110733
epoch£º830	 i:7 	 global-step:16607	 l-p:0.08012144267559052
epoch£º830	 i:8 	 global-step:16608	 l-p:0.20614564418792725
epoch£º830	 i:9 	 global-step:16609	 l-p:0.11720164865255356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1249, 5.1249],
        [5.1249, 5.1222, 5.1248],
        [5.1249, 5.1103, 5.1233],
        [5.1249, 5.1249, 5.1249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13492931425571442 
model_pd.l_d.mean(): -20.0445613861084 
model_pd.lagr.mean(): -19.909631729125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13492931425571442
epoch£º831	 i:1 	 global-step:16621	 l-p:0.09868919104337692
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13777482509613037
epoch£º831	 i:3 	 global-step:16623	 l-p:0.1685396283864975
epoch£º831	 i:4 	 global-step:16624	 l-p:0.08883889764547348
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15426050126552582
epoch£º831	 i:6 	 global-step:16626	 l-p:0.16282619535923004
epoch£º831	 i:7 	 global-step:16627	 l-p:0.1510833203792572
epoch£º831	 i:8 	 global-step:16628	 l-p:0.1441803127527237
epoch£º831	 i:9 	 global-step:16629	 l-p:0.1553192436695099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1134, 5.1092, 5.1132],
        [5.1134, 4.8880, 4.8585],
        [5.1134, 4.9582, 4.6573],
        [5.1134, 4.8879, 4.8583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.10151912271976471 
model_pd.l_d.mean(): -20.260202407836914 
model_pd.lagr.mean(): -20.15868377685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4731], device='cuda:0')), ('power', tensor([-20.9649], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.10151912271976471
epoch£º832	 i:1 	 global-step:16641	 l-p:0.1475048065185547
epoch£º832	 i:2 	 global-step:16642	 l-p:0.09415055811405182
epoch£º832	 i:3 	 global-step:16643	 l-p:0.08536496758460999
epoch£º832	 i:4 	 global-step:16644	 l-p:0.13116860389709473
epoch£º832	 i:5 	 global-step:16645	 l-p:0.17197158932685852
epoch£º832	 i:6 	 global-step:16646	 l-p:0.2761015295982361
epoch£º832	 i:7 	 global-step:16647	 l-p:0.15992064774036407
epoch£º832	 i:8 	 global-step:16648	 l-p:0.14668697118759155
epoch£º832	 i:9 	 global-step:16649	 l-p:0.18205823004245758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1000, 5.1000, 5.1000],
        [5.1000, 5.0929, 5.0995],
        [5.1000, 4.9887, 5.0428],
        [5.1000, 5.0962, 5.0998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.13427427411079407 
model_pd.l_d.mean(): -19.797115325927734 
model_pd.lagr.mean(): -19.662841796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5335], device='cuda:0')), ('power', tensor([-20.5584], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.13427427411079407
epoch£º833	 i:1 	 global-step:16661	 l-p:0.14851953089237213
epoch£º833	 i:2 	 global-step:16662	 l-p:0.1354341059923172
epoch£º833	 i:3 	 global-step:16663	 l-p:0.1385187953710556
epoch£º833	 i:4 	 global-step:16664	 l-p:0.10185007005929947
epoch£º833	 i:5 	 global-step:16665	 l-p:0.11888900399208069
epoch£º833	 i:6 	 global-step:16666	 l-p:0.11137319356203079
epoch£º833	 i:7 	 global-step:16667	 l-p:0.07871334999799728
epoch£º833	 i:8 	 global-step:16668	 l-p:0.22854365408420563
epoch£º833	 i:9 	 global-step:16669	 l-p:0.3442462980747223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 5.0903, 5.0903],
        [5.0903, 5.4902, 5.4263],
        [5.0903, 5.0902, 5.0903],
        [5.0903, 4.9362, 4.9818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.15816384553909302 
model_pd.l_d.mean(): -19.74297523498535 
model_pd.lagr.mean(): -19.58481216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5816], device='cuda:0')), ('power', tensor([-20.5528], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.15816384553909302
epoch£º834	 i:1 	 global-step:16681	 l-p:0.12492650747299194
epoch£º834	 i:2 	 global-step:16682	 l-p:0.11909148842096329
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12896910309791565
epoch£º834	 i:4 	 global-step:16684	 l-p:0.3074268102645874
epoch£º834	 i:5 	 global-step:16685	 l-p:0.10537645220756531
epoch£º834	 i:6 	 global-step:16686	 l-p:0.25301337242126465
epoch£º834	 i:7 	 global-step:16687	 l-p:0.11096669733524323
epoch£º834	 i:8 	 global-step:16688	 l-p:0.09937307238578796
epoch£º834	 i:9 	 global-step:16689	 l-p:0.14504073560237885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0925, 5.0924, 5.0925],
        [5.0925, 4.9713, 5.0252],
        [5.0925, 4.8367, 4.6455],
        [5.0925, 4.9001, 4.6097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.08058588951826096 
model_pd.l_d.mean(): -19.875293731689453 
model_pd.lagr.mean(): -19.794708251953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5008], device='cuda:0')), ('power', tensor([-20.6041], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.08058588951826096
epoch£º835	 i:1 	 global-step:16701	 l-p:0.149952694773674
epoch£º835	 i:2 	 global-step:16702	 l-p:0.11967349797487259
epoch£º835	 i:3 	 global-step:16703	 l-p:0.22047221660614014
epoch£º835	 i:4 	 global-step:16704	 l-p:0.23529483377933502
epoch£º835	 i:5 	 global-step:16705	 l-p:0.1332942396402359
epoch£º835	 i:6 	 global-step:16706	 l-p:0.10080216825008392
epoch£º835	 i:7 	 global-step:16707	 l-p:0.180180162191391
epoch£º835	 i:8 	 global-step:16708	 l-p:0.13933968544006348
epoch£º835	 i:9 	 global-step:16709	 l-p:0.13296689093112946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1057, 5.0927, 5.1044],
        [5.1057, 5.1181, 4.8394],
        [5.1057, 5.1047, 5.1056],
        [5.1057, 5.0006, 5.0544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.14981038868427277 
model_pd.l_d.mean(): -20.576261520385742 
model_pd.lagr.mean(): -20.426450729370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4663], device='cuda:0')), ('power', tensor([-21.2774], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.14981038868427277
epoch£º836	 i:1 	 global-step:16721	 l-p:0.1374398171901703
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11899901926517487
epoch£º836	 i:3 	 global-step:16723	 l-p:0.10368543118238449
epoch£º836	 i:4 	 global-step:16724	 l-p:0.1710856407880783
epoch£º836	 i:5 	 global-step:16725	 l-p:0.13278810679912567
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12679818272590637
epoch£º836	 i:7 	 global-step:16727	 l-p:0.14392955601215363
epoch£º836	 i:8 	 global-step:16728	 l-p:0.13536833226680756
epoch£º836	 i:9 	 global-step:16729	 l-p:0.18935661017894745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1198, 5.0493, 5.0954],
        [5.1198, 5.1195, 5.1198],
        [5.1198, 5.0241, 4.7196],
        [5.1198, 4.9029, 4.8886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.1884378045797348 
model_pd.l_d.mean(): -20.89837074279785 
model_pd.lagr.mean(): -20.709932327270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-21.5591], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.1884378045797348
epoch£º837	 i:1 	 global-step:16741	 l-p:0.13259775936603546
epoch£º837	 i:2 	 global-step:16742	 l-p:0.1267278641462326
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12711435556411743
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11956541985273361
epoch£º837	 i:5 	 global-step:16745	 l-p:0.13247841596603394
epoch£º837	 i:6 	 global-step:16746	 l-p:0.1876489669084549
epoch£º837	 i:7 	 global-step:16747	 l-p:0.08528183400630951
epoch£º837	 i:8 	 global-step:16748	 l-p:0.15271088480949402
epoch£º837	 i:9 	 global-step:16749	 l-p:0.12595488131046295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1270, 5.1270, 5.1270],
        [5.1270, 5.4990, 5.4152],
        [5.1270, 4.9761, 5.0222],
        [5.1270, 5.0062, 5.0597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.10894694924354553 
model_pd.l_d.mean(): -19.77096939086914 
model_pd.lagr.mean(): -19.66202163696289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5869], device='cuda:0')), ('power', tensor([-20.5865], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.10894694924354553
epoch£º838	 i:1 	 global-step:16761	 l-p:0.1540732979774475
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12324626743793488
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12709873914718628
epoch£º838	 i:4 	 global-step:16764	 l-p:0.11727698147296906
epoch£º838	 i:5 	 global-step:16765	 l-p:0.1526585966348648
epoch£º838	 i:6 	 global-step:16766	 l-p:0.12009723484516144
epoch£º838	 i:7 	 global-step:16767	 l-p:0.18509811162948608
epoch£º838	 i:8 	 global-step:16768	 l-p:0.13070879876613617
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11833499372005463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1463, 5.1461, 5.1463],
        [5.1463, 5.1461, 5.1463],
        [5.1463, 5.1462, 5.1463],
        [5.1463, 5.1436, 5.1462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.12270867079496384 
model_pd.l_d.mean(): -20.328271865844727 
model_pd.lagr.mean(): -20.205562591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-21.0423], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.12270867079496384
epoch£º839	 i:1 	 global-step:16781	 l-p:0.1720844954252243
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13368910551071167
epoch£º839	 i:3 	 global-step:16783	 l-p:0.18415148556232452
epoch£º839	 i:4 	 global-step:16784	 l-p:0.1688954383134842
epoch£º839	 i:5 	 global-step:16785	 l-p:0.10059916228055954
epoch£º839	 i:6 	 global-step:16786	 l-p:0.07801344990730286
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11763335764408112
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11970542371273041
epoch£º839	 i:9 	 global-step:16789	 l-p:0.11650263518095016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1439, 5.1440, 5.1440],
        [5.1439, 5.0210, 4.7172],
        [5.1439, 5.0204, 5.0734],
        [5.1439, 5.1440, 5.1440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.10844095796346664 
model_pd.l_d.mean(): -19.299829483032227 
model_pd.lagr.mean(): -19.191389083862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-20.0716], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.10844095796346664
epoch£º840	 i:1 	 global-step:16801	 l-p:0.1014779731631279
epoch£º840	 i:2 	 global-step:16802	 l-p:0.089735247194767
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11963106691837311
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17704497277736664
epoch£º840	 i:5 	 global-step:16805	 l-p:0.16122733056545258
epoch£º840	 i:6 	 global-step:16806	 l-p:0.1280825138092041
epoch£º840	 i:7 	 global-step:16807	 l-p:0.15763378143310547
epoch£º840	 i:8 	 global-step:16808	 l-p:0.09860772639513016
epoch£º840	 i:9 	 global-step:16809	 l-p:0.13963082432746887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1531, 4.9155, 4.6847],
        [5.1531, 5.1473, 5.1527],
        [5.1531, 5.1510, 5.1530],
        [5.1531, 5.0348, 4.7314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.07849957048892975 
model_pd.l_d.mean(): -20.511735916137695 
model_pd.lagr.mean(): -20.433237075805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.2099], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:0.07849957048892975
epoch£º841	 i:1 	 global-step:16821	 l-p:0.09580251574516296
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12304708361625671
epoch£º841	 i:3 	 global-step:16823	 l-p:0.1853281706571579
epoch£º841	 i:4 	 global-step:16824	 l-p:0.12654605507850647
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11564237624406815
epoch£º841	 i:6 	 global-step:16826	 l-p:0.15599222481250763
epoch£º841	 i:7 	 global-step:16827	 l-p:0.10588397830724716
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13021185994148254
epoch£º841	 i:9 	 global-step:16829	 l-p:0.1623944640159607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1447, 5.0615, 5.1115],
        [5.1447, 4.9458, 4.6653],
        [5.1447, 5.3259, 5.1255],
        [5.1447, 5.1447, 5.1447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.13078351318836212 
model_pd.l_d.mean(): -20.902536392211914 
model_pd.lagr.mean(): -20.771753311157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3999], device='cuda:0')), ('power', tensor([-21.5394], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.13078351318836212
epoch£º842	 i:1 	 global-step:16841	 l-p:0.06761261820793152
epoch£º842	 i:2 	 global-step:16842	 l-p:0.17203855514526367
epoch£º842	 i:3 	 global-step:16843	 l-p:0.12289252877235413
epoch£º842	 i:4 	 global-step:16844	 l-p:0.1362636685371399
epoch£º842	 i:5 	 global-step:16845	 l-p:0.11141080409288406
epoch£º842	 i:6 	 global-step:16846	 l-p:0.12768517434597015
epoch£º842	 i:7 	 global-step:16847	 l-p:0.10681872814893723
epoch£º842	 i:8 	 global-step:16848	 l-p:0.22326251864433289
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11915194988250732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1393, 5.0055, 4.7018],
        [5.1393, 5.1393, 5.1393],
        [5.1393, 5.1241, 5.1376],
        [5.1393, 5.3123, 5.1075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.10667295008897781 
model_pd.l_d.mean(): -19.966062545776367 
model_pd.lagr.mean(): -19.859390258789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4882], device='cuda:0')), ('power', tensor([-20.6829], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.10667295008897781
epoch£º843	 i:1 	 global-step:16861	 l-p:0.1176413893699646
epoch£º843	 i:2 	 global-step:16862	 l-p:0.10120829939842224
epoch£º843	 i:3 	 global-step:16863	 l-p:0.14252719283103943
epoch£º843	 i:4 	 global-step:16864	 l-p:0.19405071437358856
epoch£º843	 i:5 	 global-step:16865	 l-p:0.08342184126377106
epoch£º843	 i:6 	 global-step:16866	 l-p:0.1484350860118866
epoch£º843	 i:7 	 global-step:16867	 l-p:0.10950072854757309
epoch£º843	 i:8 	 global-step:16868	 l-p:0.14286398887634277
epoch£º843	 i:9 	 global-step:16869	 l-p:0.1747313290834427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1442, 4.9920, 4.6913],
        [5.1442, 5.1442, 5.1442],
        [5.1442, 4.9107, 4.8617],
        [5.1442, 5.0090, 5.0601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.12126564979553223 
model_pd.l_d.mean(): -20.670541763305664 
model_pd.lagr.mean(): -20.54927635192871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4312], device='cuda:0')), ('power', tensor([-21.3368], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.12126564979553223
epoch£º844	 i:1 	 global-step:16881	 l-p:0.12919820845127106
epoch£º844	 i:2 	 global-step:16882	 l-p:0.2033541053533554
epoch£º844	 i:3 	 global-step:16883	 l-p:0.05193996801972389
epoch£º844	 i:4 	 global-step:16884	 l-p:0.14276060461997986
epoch£º844	 i:5 	 global-step:16885	 l-p:0.14023800194263458
epoch£º844	 i:6 	 global-step:16886	 l-p:0.14070449769496918
epoch£º844	 i:7 	 global-step:16887	 l-p:0.15184617042541504
epoch£º844	 i:8 	 global-step:16888	 l-p:0.12032662332057953
epoch£º844	 i:9 	 global-step:16889	 l-p:0.13060523569583893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1343, 5.1343, 5.1343],
        [5.1343, 5.1168, 5.1322],
        [5.1343, 4.9570, 4.9880],
        [5.1343, 4.8836, 4.7797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.13649122416973114 
model_pd.l_d.mean(): -20.443477630615234 
model_pd.lagr.mean(): -20.30698585510254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-21.1423], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.13649122416973114
epoch£º845	 i:1 	 global-step:16901	 l-p:0.1279762238264084
epoch£º845	 i:2 	 global-step:16902	 l-p:0.12840920686721802
epoch£º845	 i:3 	 global-step:16903	 l-p:0.17676414549350739
epoch£º845	 i:4 	 global-step:16904	 l-p:0.13473404943943024
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12111810594797134
epoch£º845	 i:6 	 global-step:16906	 l-p:0.11095431447029114
epoch£º845	 i:7 	 global-step:16907	 l-p:0.13491059839725494
epoch£º845	 i:8 	 global-step:16908	 l-p:0.12454535812139511
epoch£º845	 i:9 	 global-step:16909	 l-p:0.15225724875926971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1306, 5.0250, 5.0789],
        [5.1306, 5.0912, 5.1220],
        [5.1306, 5.0566, 5.1040],
        [5.1306, 5.1306, 5.1306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.13603545725345612 
model_pd.l_d.mean(): -19.53241539001465 
model_pd.lagr.mean(): -19.396379470825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5192], device='cuda:0')), ('power', tensor([-20.2762], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.13603545725345612
epoch£º846	 i:1 	 global-step:16921	 l-p:0.17177684605121613
epoch£º846	 i:2 	 global-step:16922	 l-p:0.19978462159633636
epoch£º846	 i:3 	 global-step:16923	 l-p:0.09261470288038254
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12542599439620972
epoch£º846	 i:5 	 global-step:16925	 l-p:0.07107546925544739
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11836740374565125
epoch£º846	 i:7 	 global-step:16927	 l-p:0.14046095311641693
epoch£º846	 i:8 	 global-step:16928	 l-p:0.1443205028772354
epoch£º846	 i:9 	 global-step:16929	 l-p:0.14422845840454102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1455, 5.0973, 5.1332],
        [5.1455, 5.1454, 5.1455],
        [5.1455, 5.0940, 5.1317],
        [5.1455, 5.0379, 5.0917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12307050079107285 
model_pd.l_d.mean(): -20.74376678466797 
model_pd.lagr.mean(): -20.620697021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4121], device='cuda:0')), ('power', tensor([-21.3914], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12307050079107285
epoch£º847	 i:1 	 global-step:16941	 l-p:0.15295061469078064
epoch£º847	 i:2 	 global-step:16942	 l-p:0.1452554315328598
epoch£º847	 i:3 	 global-step:16943	 l-p:0.09058143198490143
epoch£º847	 i:4 	 global-step:16944	 l-p:0.1491818130016327
epoch£º847	 i:5 	 global-step:16945	 l-p:0.17464163899421692
epoch£º847	 i:6 	 global-step:16946	 l-p:0.06409399956464767
epoch£º847	 i:7 	 global-step:16947	 l-p:0.15117622911930084
epoch£º847	 i:8 	 global-step:16948	 l-p:0.16885268688201904
epoch£º847	 i:9 	 global-step:16949	 l-p:0.12001798301935196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1387, 5.0662, 5.1131],
        [5.1387, 4.9361, 4.9424],
        [5.1387, 5.1382, 5.1387],
        [5.1387, 4.9253, 4.6534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.17486490309238434 
model_pd.l_d.mean(): -20.807235717773438 
model_pd.lagr.mean(): -20.632369995117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4084], device='cuda:0')), ('power', tensor([-21.4517], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.17486490309238434
epoch£º848	 i:1 	 global-step:16961	 l-p:0.06594323366880417
epoch£º848	 i:2 	 global-step:16962	 l-p:0.13769826292991638
epoch£º848	 i:3 	 global-step:16963	 l-p:0.152165025472641
epoch£º848	 i:4 	 global-step:16964	 l-p:0.16186298429965973
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13882170617580414
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13156260550022125
epoch£º848	 i:7 	 global-step:16967	 l-p:0.15613053739070892
epoch£º848	 i:8 	 global-step:16968	 l-p:0.0809747576713562
epoch£º848	 i:9 	 global-step:16969	 l-p:0.09552501142024994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 4.9294, 4.8797],
        [5.1626, 5.1359, 4.8441],
        [5.1626, 4.9414, 4.9173],
        [5.1626, 5.1148, 5.1505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12449032813310623 
model_pd.l_d.mean(): -18.955310821533203 
model_pd.lagr.mean(): -18.830820083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5517], device='cuda:0')), ('power', tensor([-19.7260], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12449032813310623
epoch£º849	 i:1 	 global-step:16981	 l-p:0.13684329390525818
epoch£º849	 i:2 	 global-step:16982	 l-p:0.12938928604125977
epoch£º849	 i:3 	 global-step:16983	 l-p:0.11719636619091034
epoch£º849	 i:4 	 global-step:16984	 l-p:0.08247680217027664
epoch£º849	 i:5 	 global-step:16985	 l-p:0.16308528184890747
epoch£º849	 i:6 	 global-step:16986	 l-p:0.1383080631494522
epoch£º849	 i:7 	 global-step:16987	 l-p:0.07392089813947678
epoch£º849	 i:8 	 global-step:16988	 l-p:0.1252838522195816
epoch£º849	 i:9 	 global-step:16989	 l-p:0.15949220955371857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[5.1604, 5.5045, 5.4004],
        [5.1604, 4.9083, 4.7247],
        [5.1604, 5.0074, 5.0524],
        [5.1604, 5.5211, 5.4278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.11181516200304031 
model_pd.l_d.mean(): -20.323524475097656 
model_pd.lagr.mean(): -20.21170997619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4747], device='cuda:0')), ('power', tensor([-21.0305], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.11181516200304031
epoch£º850	 i:1 	 global-step:17001	 l-p:0.1415034979581833
epoch£º850	 i:2 	 global-step:17002	 l-p:0.11536483466625214
epoch£º850	 i:3 	 global-step:17003	 l-p:0.13976387679576874
epoch£º850	 i:4 	 global-step:17004	 l-p:0.10000084340572357
epoch£º850	 i:5 	 global-step:17005	 l-p:0.17597068846225739
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11371736228466034
epoch£º850	 i:7 	 global-step:17007	 l-p:0.20166976749897003
epoch£º850	 i:8 	 global-step:17008	 l-p:0.10345857590436935
epoch£º850	 i:9 	 global-step:17009	 l-p:0.12054387480020523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1382, 5.1382, 5.1382],
        [5.1382, 5.1380, 5.1382],
        [5.1382, 5.0884, 5.1253],
        [5.1382, 4.9523, 4.6602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.16072587668895721 
model_pd.l_d.mean(): -19.676841735839844 
model_pd.lagr.mean(): -19.516115188598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5590], device='cuda:0')), ('power', tensor([-20.4628], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.16072587668895721
epoch£º851	 i:1 	 global-step:17021	 l-p:0.10967355221509933
epoch£º851	 i:2 	 global-step:17022	 l-p:0.18852126598358154
epoch£º851	 i:3 	 global-step:17023	 l-p:0.11573541164398193
epoch£º851	 i:4 	 global-step:17024	 l-p:0.11721181869506836
epoch£º851	 i:5 	 global-step:17025	 l-p:0.09999513626098633
epoch£º851	 i:6 	 global-step:17026	 l-p:0.18555709719657898
epoch£º851	 i:7 	 global-step:17027	 l-p:0.08197334408760071
epoch£º851	 i:8 	 global-step:17028	 l-p:0.13323740661144257
epoch£º851	 i:9 	 global-step:17029	 l-p:0.1188550814986229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 5.6205, 5.6053],
        [5.1451, 4.8896, 4.7526],
        [5.1451, 4.8962, 4.8030],
        [5.1451, 5.1451, 5.1451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.09986726194620132 
model_pd.l_d.mean(): -19.724552154541016 
model_pd.lagr.mean(): -19.624685287475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-20.4626], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.09986726194620132
epoch£º852	 i:1 	 global-step:17041	 l-p:0.09628923237323761
epoch£º852	 i:2 	 global-step:17042	 l-p:0.14951416850090027
epoch£º852	 i:3 	 global-step:17043	 l-p:0.1457769274711609
epoch£º852	 i:4 	 global-step:17044	 l-p:0.1131039410829544
epoch£º852	 i:5 	 global-step:17045	 l-p:0.1231943666934967
epoch£º852	 i:6 	 global-step:17046	 l-p:0.16290168464183807
epoch£º852	 i:7 	 global-step:17047	 l-p:0.1389397382736206
epoch£º852	 i:8 	 global-step:17048	 l-p:0.15402834117412567
epoch£º852	 i:9 	 global-step:17049	 l-p:0.13832855224609375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1273, 5.1513, 4.8751],
        [5.1273, 5.1160, 5.1263],
        [5.1273, 4.9661, 5.0085],
        [5.1273, 4.9897, 5.0411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.17560726404190063 
model_pd.l_d.mean(): -19.553325653076172 
model_pd.lagr.mean(): -19.377717971801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5207], device='cuda:0')), ('power', tensor([-20.2989], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.17560726404190063
epoch£º853	 i:1 	 global-step:17061	 l-p:0.1245725154876709
epoch£º853	 i:2 	 global-step:17062	 l-p:0.12526506185531616
epoch£º853	 i:3 	 global-step:17063	 l-p:0.15122048556804657
epoch£º853	 i:4 	 global-step:17064	 l-p:0.16766875982284546
epoch£º853	 i:5 	 global-step:17065	 l-p:0.1385941505432129
epoch£º853	 i:6 	 global-step:17066	 l-p:0.1758417785167694
epoch£º853	 i:7 	 global-step:17067	 l-p:0.08392718434333801
epoch£º853	 i:8 	 global-step:17068	 l-p:0.15805670619010925
epoch£º853	 i:9 	 global-step:17069	 l-p:0.1124899834394455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 4.9130, 4.6283],
        [5.1181, 5.1180, 5.1181],
        [5.1181, 5.4434, 5.3281],
        [5.1181, 5.1180, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.1374872326850891 
model_pd.l_d.mean(): -20.093521118164062 
model_pd.lagr.mean(): -19.95603370666504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5337], device='cuda:0')), ('power', tensor([-20.8582], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.1374872326850891
epoch£º854	 i:1 	 global-step:17081	 l-p:0.24573028087615967
epoch£º854	 i:2 	 global-step:17082	 l-p:0.11099081486463547
epoch£º854	 i:3 	 global-step:17083	 l-p:0.17162500321865082
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12650805711746216
epoch£º854	 i:5 	 global-step:17085	 l-p:0.09641047567129135
epoch£º854	 i:6 	 global-step:17086	 l-p:0.11942081153392792
epoch£º854	 i:7 	 global-step:17087	 l-p:0.1426108330488205
epoch£º854	 i:8 	 global-step:17088	 l-p:0.12802958488464355
epoch£º854	 i:9 	 global-step:17089	 l-p:0.1350458860397339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1152, 5.0133, 4.7048],
        [5.1152, 5.2357, 5.0025],
        [5.1152, 5.1095, 5.1148],
        [5.1152, 4.8775, 4.8290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.15949244797229767 
model_pd.l_d.mean(): -20.64752769470215 
model_pd.lagr.mean(): -20.488035202026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.3333], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.15949244797229767
epoch£º855	 i:1 	 global-step:17101	 l-p:0.13299717009067535
epoch£º855	 i:2 	 global-step:17102	 l-p:0.12043140828609467
epoch£º855	 i:3 	 global-step:17103	 l-p:0.11759871989488602
epoch£º855	 i:4 	 global-step:17104	 l-p:0.1674337387084961
epoch£º855	 i:5 	 global-step:17105	 l-p:0.16152292490005493
epoch£º855	 i:6 	 global-step:17106	 l-p:0.1426170915365219
epoch£º855	 i:7 	 global-step:17107	 l-p:0.27110642194747925
epoch£º855	 i:8 	 global-step:17108	 l-p:0.15738052129745483
epoch£º855	 i:9 	 global-step:17109	 l-p:0.12603531777858734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 4.8879, 4.8999],
        [5.0903, 5.0840, 5.0899],
        [5.0903, 4.8856, 4.5957],
        [5.0903, 5.0903, 5.0903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.1875884234905243 
model_pd.l_d.mean(): -19.829158782958984 
model_pd.lagr.mean(): -19.641571044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-20.5856], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.1875884234905243
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13432838022708893
epoch£º856	 i:2 	 global-step:17122	 l-p:0.16820983588695526
epoch£º856	 i:3 	 global-step:17123	 l-p:0.11591111123561859
epoch£º856	 i:4 	 global-step:17124	 l-p:0.281360387802124
epoch£º856	 i:5 	 global-step:17125	 l-p:0.20462332665920258
epoch£º856	 i:6 	 global-step:17126	 l-p:0.15215453505516052
epoch£º856	 i:7 	 global-step:17127	 l-p:0.11968866735696793
epoch£º856	 i:8 	 global-step:17128	 l-p:0.113652303814888
epoch£º856	 i:9 	 global-step:17129	 l-p:0.10993417352437973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1048, 5.0001, 4.6905],
        [5.1048, 4.8984, 4.9047],
        [5.1048, 5.0857, 5.1023],
        [5.1048, 5.0374, 4.7328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.11217926442623138 
model_pd.l_d.mean(): -19.289752960205078 
model_pd.lagr.mean(): -19.177574157714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6032], device='cuda:0')), ('power', tensor([-20.1167], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.11217926442623138
epoch£º857	 i:1 	 global-step:17141	 l-p:0.15873244404792786
epoch£º857	 i:2 	 global-step:17142	 l-p:0.17693106830120087
epoch£º857	 i:3 	 global-step:17143	 l-p:0.2554701268672943
epoch£º857	 i:4 	 global-step:17144	 l-p:0.09851899743080139
epoch£º857	 i:5 	 global-step:17145	 l-p:0.11988414824008942
epoch£º857	 i:6 	 global-step:17146	 l-p:0.1267371028661728
epoch£º857	 i:7 	 global-step:17147	 l-p:0.16460426151752472
epoch£º857	 i:8 	 global-step:17148	 l-p:0.13038548827171326
epoch£º857	 i:9 	 global-step:17149	 l-p:0.10611692070960999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1343, 5.1347],
        [5.1347, 4.9683, 5.0079],
        [5.1347, 5.1347, 5.1347],
        [5.1347, 5.1272, 5.1342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.17338094115257263 
model_pd.l_d.mean(): -20.662649154663086 
model_pd.lagr.mean(): -20.489267349243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4241], device='cuda:0')), ('power', tensor([-21.3216], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.17338094115257263
epoch£º858	 i:1 	 global-step:17161	 l-p:0.14945214986801147
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12935712933540344
epoch£º858	 i:3 	 global-step:17163	 l-p:0.13947950303554535
epoch£º858	 i:4 	 global-step:17164	 l-p:0.07090332359075546
epoch£º858	 i:5 	 global-step:17165	 l-p:0.1574036329984665
epoch£º858	 i:6 	 global-step:17166	 l-p:0.12114717066287994
epoch£º858	 i:7 	 global-step:17167	 l-p:0.10693135112524033
epoch£º858	 i:8 	 global-step:17168	 l-p:0.16885219514369965
epoch£º858	 i:9 	 global-step:17169	 l-p:0.1030157059431076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1496, 4.8946, 4.7082],
        [5.1496, 5.1495, 5.1496],
        [5.1496, 5.5589, 5.4975],
        [5.1496, 5.0990, 5.1363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.12042101472616196 
model_pd.l_d.mean(): -20.554800033569336 
model_pd.lagr.mean(): -20.43437957763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-21.2229], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.12042101472616196
epoch£º859	 i:1 	 global-step:17181	 l-p:0.0991278812289238
epoch£º859	 i:2 	 global-step:17182	 l-p:0.14718307554721832
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11408384144306183
epoch£º859	 i:4 	 global-step:17184	 l-p:0.10214342176914215
epoch£º859	 i:5 	 global-step:17185	 l-p:0.15549415349960327
epoch£º859	 i:6 	 global-step:17186	 l-p:0.13844892382621765
epoch£º859	 i:7 	 global-step:17187	 l-p:0.20665481686592102
epoch£º859	 i:8 	 global-step:17188	 l-p:0.14307048916816711
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15905581414699554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1133, 5.0669, 5.1020],
        [5.1133, 5.1051, 5.1127],
        [5.1133, 4.9578, 5.0039],
        [5.1133, 5.0985, 5.1117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.1106685921549797 
model_pd.l_d.mean(): -20.50568962097168 
model_pd.lagr.mean(): -20.395021438598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4588], device='cuda:0')), ('power', tensor([-21.1984], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.1106685921549797
epoch£º860	 i:1 	 global-step:17201	 l-p:0.15553168952465057
epoch£º860	 i:2 	 global-step:17202	 l-p:0.14978834986686707
epoch£º860	 i:3 	 global-step:17203	 l-p:0.21545514464378357
epoch£º860	 i:4 	 global-step:17204	 l-p:0.14660172164440155
epoch£º860	 i:5 	 global-step:17205	 l-p:0.09867601096630096
epoch£º860	 i:6 	 global-step:17206	 l-p:0.130623459815979
epoch£º860	 i:7 	 global-step:17207	 l-p:0.13380056619644165
epoch£º860	 i:8 	 global-step:17208	 l-p:0.17680048942565918
epoch£º860	 i:9 	 global-step:17209	 l-p:0.1634400486946106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1064, 4.8501, 4.7461],
        [5.1064, 5.2187, 4.9811],
        [5.1064, 5.0175, 5.0695],
        [5.1064, 5.0941, 5.1052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.17269563674926758 
model_pd.l_d.mean(): -20.639497756958008 
model_pd.lagr.mean(): -20.4668025970459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.3252], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.17269563674926758
epoch£º861	 i:1 	 global-step:17221	 l-p:0.13280124962329865
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12300032377243042
epoch£º861	 i:3 	 global-step:17223	 l-p:0.17482633888721466
epoch£º861	 i:4 	 global-step:17224	 l-p:0.13563892245292664
epoch£º861	 i:5 	 global-step:17225	 l-p:0.15467987954616547
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12367638200521469
epoch£º861	 i:7 	 global-step:17227	 l-p:0.1130281388759613
epoch£º861	 i:8 	 global-step:17228	 l-p:0.12723001837730408
epoch£º861	 i:9 	 global-step:17229	 l-p:0.18675827980041504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1182, 4.8790, 4.6295],
        [5.1182, 4.9212, 4.9376],
        [5.1182, 5.1103, 5.1177],
        [5.1182, 5.1182, 5.1182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13169905543327332 
model_pd.l_d.mean(): -20.411657333374023 
model_pd.lagr.mean(): -20.279958724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4540], device='cuda:0')), ('power', tensor([-21.0984], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13169905543327332
epoch£º862	 i:1 	 global-step:17241	 l-p:0.13895735144615173
epoch£º862	 i:2 	 global-step:17242	 l-p:0.1294350028038025
epoch£º862	 i:3 	 global-step:17243	 l-p:0.11513516306877136
epoch£º862	 i:4 	 global-step:17244	 l-p:0.186207115650177
epoch£º862	 i:5 	 global-step:17245	 l-p:0.19459350407123566
epoch£º862	 i:6 	 global-step:17246	 l-p:0.13973769545555115
epoch£º862	 i:7 	 global-step:17247	 l-p:0.13929326832294464
epoch£º862	 i:8 	 global-step:17248	 l-p:0.16643457114696503
epoch£º862	 i:9 	 global-step:17249	 l-p:0.1261749565601349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1150, 5.0752, 5.1064],
        [5.1150, 5.0943, 5.1122],
        [5.1150, 5.0331, 5.0833],
        [5.1150, 4.8805, 4.6224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.1624867171049118 
model_pd.l_d.mean(): -19.583425521850586 
model_pd.lagr.mean(): -19.42093849182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5038], device='cuda:0')), ('power', tensor([-20.3120], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.1624867171049118
epoch£º863	 i:1 	 global-step:17261	 l-p:0.17183654010295868
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13110564649105072
epoch£º863	 i:3 	 global-step:17263	 l-p:0.1212322935461998
epoch£º863	 i:4 	 global-step:17264	 l-p:0.09232839196920395
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12276861071586609
epoch£º863	 i:6 	 global-step:17266	 l-p:0.11535945534706116
epoch£º863	 i:7 	 global-step:17267	 l-p:0.2015097588300705
epoch£º863	 i:8 	 global-step:17268	 l-p:0.15536560118198395
epoch£º863	 i:9 	 global-step:17269	 l-p:0.11630941182374954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1330, 5.0348, 4.7266],
        [5.1330, 5.1330, 5.1330],
        [5.1330, 5.1330, 5.1330],
        [5.1330, 5.5299, 5.4601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.16613028943538666 
model_pd.l_d.mean(): -19.501848220825195 
model_pd.lagr.mean(): -19.335718154907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4865], device='cuda:0')), ('power', tensor([-20.2118], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.16613028943538666
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12390453368425369
epoch£º864	 i:2 	 global-step:17282	 l-p:0.10895330458879471
epoch£º864	 i:3 	 global-step:17283	 l-p:0.18285059928894043
epoch£º864	 i:4 	 global-step:17284	 l-p:0.1381770670413971
epoch£º864	 i:5 	 global-step:17285	 l-p:0.06611889600753784
epoch£º864	 i:6 	 global-step:17286	 l-p:0.12587395310401917
epoch£º864	 i:7 	 global-step:17287	 l-p:0.12384229153394699
epoch£º864	 i:8 	 global-step:17288	 l-p:0.14340467751026154
epoch£º864	 i:9 	 global-step:17289	 l-p:0.14434285461902618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1481, 5.1481, 5.1481],
        [5.1481, 5.1471, 5.1481],
        [5.1481, 5.1481, 5.1481],
        [5.1481, 5.1481, 5.1481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.1279233992099762 
model_pd.l_d.mean(): -20.38848114013672 
model_pd.lagr.mean(): -20.260557174682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.0603], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.1279233992099762
epoch£º865	 i:1 	 global-step:17301	 l-p:0.18698245286941528
epoch£º865	 i:2 	 global-step:17302	 l-p:0.15363940596580505
epoch£º865	 i:3 	 global-step:17303	 l-p:0.07697275280952454
epoch£º865	 i:4 	 global-step:17304	 l-p:0.17399844527244568
epoch£º865	 i:5 	 global-step:17305	 l-p:0.0932607650756836
epoch£º865	 i:6 	 global-step:17306	 l-p:0.11289254575967789
epoch£º865	 i:7 	 global-step:17307	 l-p:0.09574964642524719
epoch£º865	 i:8 	 global-step:17308	 l-p:0.1308566778898239
epoch£º865	 i:9 	 global-step:17309	 l-p:0.14177002012729645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1602, 5.3265, 5.1157],
        [5.1602, 5.6379, 5.6227],
        [5.1602, 5.0990, 5.1414],
        [5.1602, 5.1574, 5.1601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.07955805957317352 
model_pd.l_d.mean(): -19.66765594482422 
model_pd.lagr.mean(): -19.588098526000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5017], device='cuda:0')), ('power', tensor([-20.3950], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.07955805957317352
epoch£º866	 i:1 	 global-step:17321	 l-p:0.16496051847934723
epoch£º866	 i:2 	 global-step:17322	 l-p:0.12343793362379074
epoch£º866	 i:3 	 global-step:17323	 l-p:0.14365744590759277
epoch£º866	 i:4 	 global-step:17324	 l-p:0.11063388735055923
epoch£º866	 i:5 	 global-step:17325	 l-p:0.16525346040725708
epoch£º866	 i:6 	 global-step:17326	 l-p:0.11044996231794357
epoch£º866	 i:7 	 global-step:17327	 l-p:0.12409810721874237
epoch£º866	 i:8 	 global-step:17328	 l-p:0.12143896520137787
epoch£º866	 i:9 	 global-step:17329	 l-p:0.11380863189697266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1598, 5.1598, 5.1598],
        [5.1598, 4.9871, 5.0223],
        [5.1598, 5.2712, 5.0322],
        [5.1598, 5.1564, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.1338329166173935 
model_pd.l_d.mean(): -19.48908042907715 
model_pd.lagr.mean(): -19.355247497558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5314], device='cuda:0')), ('power', tensor([-20.2448], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.1338329166173935
epoch£º867	 i:1 	 global-step:17341	 l-p:0.10962847620248795
epoch£º867	 i:2 	 global-step:17342	 l-p:0.21368278563022614
epoch£º867	 i:3 	 global-step:17343	 l-p:0.12744958698749542
epoch£º867	 i:4 	 global-step:17344	 l-p:0.11031052470207214
epoch£º867	 i:5 	 global-step:17345	 l-p:0.1101352646946907
epoch£º867	 i:6 	 global-step:17346	 l-p:0.12270506471395493
epoch£º867	 i:7 	 global-step:17347	 l-p:0.16237613558769226
epoch£º867	 i:8 	 global-step:17348	 l-p:0.10030703991651535
epoch£º867	 i:9 	 global-step:17349	 l-p:0.1425057351589203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[5.1144, 4.8522, 4.6953],
        [5.1144, 5.1718, 4.9079],
        [5.1144, 4.9075, 4.9138],
        [5.1144, 4.8562, 4.6538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.17125281691551208 
model_pd.l_d.mean(): -20.606660842895508 
model_pd.lagr.mean(): -20.435407638549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.3058], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.17125281691551208
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11954127252101898
epoch£º868	 i:2 	 global-step:17362	 l-p:0.20918338000774384
epoch£º868	 i:3 	 global-step:17363	 l-p:0.16540759801864624
epoch£º868	 i:4 	 global-step:17364	 l-p:0.20592635869979858
epoch£º868	 i:5 	 global-step:17365	 l-p:0.10945186018943787
epoch£º868	 i:6 	 global-step:17366	 l-p:0.1080150231719017
epoch£º868	 i:7 	 global-step:17367	 l-p:0.17907698452472687
epoch£º868	 i:8 	 global-step:17368	 l-p:0.1305573284626007
epoch£º868	 i:9 	 global-step:17369	 l-p:0.09894756227731705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0923, 5.4789, 5.4031],
        [5.0923, 5.0885, 5.0921],
        [5.0923, 4.9035, 4.6026],
        [5.0923, 4.9371, 4.9847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.13782159984111786 
model_pd.l_d.mean(): -19.540334701538086 
model_pd.lagr.mean(): -19.40251350402832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5745], device='cuda:0')), ('power', tensor([-20.3407], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.13782159984111786
epoch£º869	 i:1 	 global-step:17381	 l-p:0.09309262782335281
epoch£º869	 i:2 	 global-step:17382	 l-p:0.08831042796373367
epoch£º869	 i:3 	 global-step:17383	 l-p:0.1834227591753006
epoch£º869	 i:4 	 global-step:17384	 l-p:0.12694256007671356
epoch£º869	 i:5 	 global-step:17385	 l-p:0.2987090051174164
epoch£º869	 i:6 	 global-step:17386	 l-p:0.19934532046318054
epoch£º869	 i:7 	 global-step:17387	 l-p:1.8201512098312378
epoch£º869	 i:8 	 global-step:17388	 l-p:0.1371423453092575
epoch£º869	 i:9 	 global-step:17389	 l-p:0.1658448427915573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[5.0728, 4.8629, 4.8695],
        [5.0728, 4.8065, 4.6660],
        [5.0728, 4.8180, 4.7376],
        [5.0728, 4.8672, 4.8787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.12905706465244293 
model_pd.l_d.mean(): -20.995149612426758 
model_pd.lagr.mean(): -20.866092681884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4065], device='cuda:0')), ('power', tensor([-21.6398], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.12905706465244293
epoch£º870	 i:1 	 global-step:17401	 l-p:0.23330406844615936
epoch£º870	 i:2 	 global-step:17402	 l-p:0.10911466181278229
epoch£º870	 i:3 	 global-step:17403	 l-p:0.8223443627357483
epoch£º870	 i:4 	 global-step:17404	 l-p:0.12286274880170822
epoch£º870	 i:5 	 global-step:17405	 l-p:0.18116122484207153
epoch£º870	 i:6 	 global-step:17406	 l-p:0.15009386837482452
epoch£º870	 i:7 	 global-step:17407	 l-p:0.19246366620063782
epoch£º870	 i:8 	 global-step:17408	 l-p:0.12149051576852798
epoch£º870	 i:9 	 global-step:17409	 l-p:0.21130293607711792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0800, 4.8203, 4.7191],
        [5.0800, 5.0624, 5.0779],
        [5.0800, 4.9359, 4.6226],
        [5.0800, 4.8471, 4.8185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.1775263100862503 
model_pd.l_d.mean(): -20.266151428222656 
model_pd.lagr.mean(): -20.088624954223633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5183], device='cuda:0')), ('power', tensor([-21.0171], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.1775263100862503
epoch£º871	 i:1 	 global-step:17421	 l-p:0.17045024037361145
epoch£º871	 i:2 	 global-step:17422	 l-p:0.16561885178089142
epoch£º871	 i:3 	 global-step:17423	 l-p:0.13475282490253448
epoch£º871	 i:4 	 global-step:17424	 l-p:0.22686485946178436
epoch£º871	 i:5 	 global-step:17425	 l-p:0.1272628903388977
epoch£º871	 i:6 	 global-step:17426	 l-p:0.1257832944393158
epoch£º871	 i:7 	 global-step:17427	 l-p:0.17778818309307098
epoch£º871	 i:8 	 global-step:17428	 l-p:0.1063871905207634
epoch£º871	 i:9 	 global-step:17429	 l-p:0.14479540288448334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1159, 5.0742, 5.1066],
        [5.1159, 5.0477, 4.7417],
        [5.1159, 5.1160, 5.1160],
        [5.1159, 5.0838, 5.1100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.21233771741390228 
model_pd.l_d.mean(): -20.443098068237305 
model_pd.lagr.mean(): -20.23076057434082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4824], device='cuda:0')), ('power', tensor([-21.1592], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.21233771741390228
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13180698454380035
epoch£º872	 i:2 	 global-step:17442	 l-p:0.11620208621025085
epoch£º872	 i:3 	 global-step:17443	 l-p:0.10472354292869568
epoch£º872	 i:4 	 global-step:17444	 l-p:0.14482800662517548
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13416065275669098
epoch£º872	 i:6 	 global-step:17446	 l-p:0.1720707267522812
epoch£º872	 i:7 	 global-step:17447	 l-p:0.0892576351761818
epoch£º872	 i:8 	 global-step:17448	 l-p:0.17157036066055298
epoch£º872	 i:9 	 global-step:17449	 l-p:0.12336357682943344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 5.0074, 5.0613],
        [5.1351, 5.1275, 5.1346],
        [5.1351, 4.8820, 4.7886],
        [5.1351, 4.9054, 4.8755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.11009536683559418 
model_pd.l_d.mean(): -19.4252872467041 
model_pd.lagr.mean(): -19.3151912689209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5365], device='cuda:0')), ('power', tensor([-20.1856], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.11009536683559418
epoch£º873	 i:1 	 global-step:17461	 l-p:0.14262057840824127
epoch£º873	 i:2 	 global-step:17462	 l-p:0.16520942747592926
epoch£º873	 i:3 	 global-step:17463	 l-p:0.09762383997440338
epoch£º873	 i:4 	 global-step:17464	 l-p:0.14471933245658875
epoch£º873	 i:5 	 global-step:17465	 l-p:0.17087405920028687
epoch£º873	 i:6 	 global-step:17466	 l-p:0.1567862629890442
epoch£º873	 i:7 	 global-step:17467	 l-p:0.17004553973674774
epoch£º873	 i:8 	 global-step:17468	 l-p:0.15109677612781525
epoch£º873	 i:9 	 global-step:17469	 l-p:0.1200489029288292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1111, 5.1111, 5.1111],
        [5.1111, 5.1108, 5.1111],
        [5.1111, 5.1111, 5.1111],
        [5.1111, 5.2832, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.1189301460981369 
model_pd.l_d.mean(): -20.687408447265625 
model_pd.lagr.mean(): -20.568477630615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-21.3472], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.1189301460981369
epoch£º874	 i:1 	 global-step:17481	 l-p:0.1495288610458374
epoch£º874	 i:2 	 global-step:17482	 l-p:0.13793537020683289
epoch£º874	 i:3 	 global-step:17483	 l-p:0.1666632443666458
epoch£º874	 i:4 	 global-step:17484	 l-p:0.17739033699035645
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12491274625062943
epoch£º874	 i:6 	 global-step:17486	 l-p:0.19603939354419708
epoch£º874	 i:7 	 global-step:17487	 l-p:0.1384097784757614
epoch£º874	 i:8 	 global-step:17488	 l-p:0.11322115361690521
epoch£º874	 i:9 	 global-step:17489	 l-p:0.13625600934028625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1211, 5.0773, 5.1109],
        [5.1211, 4.9597, 5.0038],
        [5.1211, 5.1211, 5.1211],
        [5.1211, 4.9332, 4.6343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.10119651257991791 
model_pd.l_d.mean(): -20.698362350463867 
model_pd.lagr.mean(): -20.597166061401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.3502], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:0.10119651257991791
epoch£º875	 i:1 	 global-step:17501	 l-p:0.15627656877040863
epoch£º875	 i:2 	 global-step:17502	 l-p:0.1997576355934143
epoch£º875	 i:3 	 global-step:17503	 l-p:0.19715704023838043
epoch£º875	 i:4 	 global-step:17504	 l-p:0.11368288099765778
epoch£º875	 i:5 	 global-step:17505	 l-p:0.11168010532855988
epoch£º875	 i:6 	 global-step:17506	 l-p:0.1378462165594101
epoch£º875	 i:7 	 global-step:17507	 l-p:0.11029288917779922
epoch£º875	 i:8 	 global-step:17508	 l-p:0.08999703079462051
epoch£º875	 i:9 	 global-step:17509	 l-p:0.20021139085292816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1293, 4.9790, 4.6701],
        [5.1293, 5.5869, 5.5577],
        [5.1293, 5.5406, 5.4797],
        [5.1293, 5.1293, 5.1293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.1994890570640564 
model_pd.l_d.mean(): -19.496671676635742 
model_pd.lagr.mean(): -19.297182083129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-20.2573], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.1994890570640564
epoch£º876	 i:1 	 global-step:17521	 l-p:0.10263866931200027
epoch£º876	 i:2 	 global-step:17522	 l-p:0.10774151235818863
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13162338733673096
epoch£º876	 i:4 	 global-step:17524	 l-p:0.12475298345088959
epoch£º876	 i:5 	 global-step:17525	 l-p:0.1074240580201149
epoch£º876	 i:6 	 global-step:17526	 l-p:0.13156555593013763
epoch£º876	 i:7 	 global-step:17527	 l-p:0.15969610214233398
epoch£º876	 i:8 	 global-step:17528	 l-p:0.12412425875663757
epoch£º876	 i:9 	 global-step:17529	 l-p:0.1743038147687912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1296, 5.1281, 5.1296],
        [5.1296, 5.1296, 5.1296],
        [5.1296, 4.8872, 4.6396],
        [5.1296, 5.5869, 5.5573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.11056934297084808 
model_pd.l_d.mean(): -19.18328094482422 
model_pd.lagr.mean(): -19.072711944580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5119], device='cuda:0')), ('power', tensor([-19.9158], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.11056934297084808
epoch£º877	 i:1 	 global-step:17541	 l-p:0.14891624450683594
epoch£º877	 i:2 	 global-step:17542	 l-p:0.1545538604259491
epoch£º877	 i:3 	 global-step:17543	 l-p:0.11307475715875626
epoch£º877	 i:4 	 global-step:17544	 l-p:0.11637035012245178
epoch£º877	 i:5 	 global-step:17545	 l-p:0.14066824316978455
epoch£º877	 i:6 	 global-step:17546	 l-p:0.16389982402324677
epoch£º877	 i:7 	 global-step:17547	 l-p:0.13063956797122955
epoch£º877	 i:8 	 global-step:17548	 l-p:0.17847587168216705
epoch£º877	 i:9 	 global-step:17549	 l-p:0.1533222794532776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1280, 5.1279, 5.1280],
        [5.1280, 5.4515, 5.3329],
        [5.1280, 5.4591, 5.3453],
        [5.1280, 5.1279, 5.1280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.1719895452260971 
model_pd.l_d.mean(): -20.66455841064453 
model_pd.lagr.mean(): -20.492568969726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4295], device='cuda:0')), ('power', tensor([-21.3291], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.1719895452260971
epoch£º878	 i:1 	 global-step:17561	 l-p:0.20061126351356506
epoch£º878	 i:2 	 global-step:17562	 l-p:0.0977788120508194
epoch£º878	 i:3 	 global-step:17563	 l-p:0.14408650994300842
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11925967782735825
epoch£º878	 i:5 	 global-step:17565	 l-p:0.13497065007686615
epoch£º878	 i:6 	 global-step:17566	 l-p:0.13117697834968567
epoch£º878	 i:7 	 global-step:17567	 l-p:0.10183779150247574
epoch£º878	 i:8 	 global-step:17568	 l-p:0.13999809324741364
epoch£º878	 i:9 	 global-step:17569	 l-p:0.12344357371330261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1363, 5.1361, 5.1363],
        [5.1363, 5.0514, 5.1025],
        [5.1363, 4.9287, 4.6417],
        [5.1363, 5.1036, 5.1302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.14784471690654755 
model_pd.l_d.mean(): -20.597238540649414 
model_pd.lagr.mean(): -20.44939422607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4392], device='cuda:0')), ('power', tensor([-21.2709], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.14784471690654755
epoch£º879	 i:1 	 global-step:17581	 l-p:0.1003204733133316
epoch£º879	 i:2 	 global-step:17582	 l-p:0.13069871068000793
epoch£º879	 i:3 	 global-step:17583	 l-p:0.19676198065280914
epoch£º879	 i:4 	 global-step:17584	 l-p:0.06405895203351974
epoch£º879	 i:5 	 global-step:17585	 l-p:0.14016292989253998
epoch£º879	 i:6 	 global-step:17586	 l-p:0.11113379895687103
epoch£º879	 i:7 	 global-step:17587	 l-p:0.13306313753128052
epoch£º879	 i:8 	 global-step:17588	 l-p:0.21354232728481293
epoch£º879	 i:9 	 global-step:17589	 l-p:0.13954311609268188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1248, 4.8633, 4.6719],
        [5.1248, 5.0333, 5.0860],
        [5.1248, 4.8785, 4.8147],
        [5.1248, 5.1246, 5.1248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.1667073369026184 
model_pd.l_d.mean(): -20.180192947387695 
model_pd.lagr.mean(): -20.013484954833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5253], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.1667073369026184
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12493260949850082
epoch£º880	 i:2 	 global-step:17602	 l-p:0.1504998803138733
epoch£º880	 i:3 	 global-step:17603	 l-p:0.11703498661518097
epoch£º880	 i:4 	 global-step:17604	 l-p:0.17469333112239838
epoch£º880	 i:5 	 global-step:17605	 l-p:0.21151000261306763
epoch£º880	 i:6 	 global-step:17606	 l-p:0.1330370455980301
epoch£º880	 i:7 	 global-step:17607	 l-p:0.135872021317482
epoch£º880	 i:8 	 global-step:17608	 l-p:0.10108863562345505
epoch£º880	 i:9 	 global-step:17609	 l-p:0.14429602026939392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1129, 5.1129, 5.1129],
        [5.1129, 5.0056, 5.0607],
        [5.1129, 4.8655, 4.8020],
        [5.1129, 5.4119, 5.2780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.16974250972270966 
model_pd.l_d.mean(): -19.337284088134766 
model_pd.lagr.mean(): -19.16754150390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-20.0660], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.16974250972270966
epoch£º881	 i:1 	 global-step:17621	 l-p:0.24146445095539093
epoch£º881	 i:2 	 global-step:17622	 l-p:0.1382196843624115
epoch£º881	 i:3 	 global-step:17623	 l-p:0.12676969170570374
epoch£º881	 i:4 	 global-step:17624	 l-p:0.1111476719379425
epoch£º881	 i:5 	 global-step:17625	 l-p:0.14802061021327972
epoch£º881	 i:6 	 global-step:17626	 l-p:0.12608158588409424
epoch£º881	 i:7 	 global-step:17627	 l-p:0.13096946477890015
epoch£º881	 i:8 	 global-step:17628	 l-p:0.1313885748386383
epoch£º881	 i:9 	 global-step:17629	 l-p:0.10276956111192703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1216, 4.8807, 4.8321],
        [5.1216, 4.8672, 4.6414],
        [5.1216, 5.1216, 5.1216],
        [5.1216, 5.1177, 5.1214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.09150254726409912 
model_pd.l_d.mean(): -20.42568016052246 
model_pd.lagr.mean(): -20.334177017211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4812], device='cuda:0')), ('power', tensor([-21.1404], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.09150254726409912
epoch£º882	 i:1 	 global-step:17641	 l-p:0.15060311555862427
epoch£º882	 i:2 	 global-step:17642	 l-p:0.18485255539417267
epoch£º882	 i:3 	 global-step:17643	 l-p:0.09020718187093735
epoch£º882	 i:4 	 global-step:17644	 l-p:0.2679649889469147
epoch£º882	 i:5 	 global-step:17645	 l-p:0.150312602519989
epoch£º882	 i:6 	 global-step:17646	 l-p:0.1474473625421524
epoch£º882	 i:7 	 global-step:17647	 l-p:0.14248615503311157
epoch£º882	 i:8 	 global-step:17648	 l-p:0.12072917073965073
epoch£º882	 i:9 	 global-step:17649	 l-p:0.12616439163684845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0998, 5.0970, 5.0997],
        [5.0998, 5.0993, 5.0998],
        [5.0998, 5.0870, 5.0986],
        [5.0998, 4.9368, 4.9816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.13058963418006897 
model_pd.l_d.mean(): -20.674304962158203 
model_pd.lagr.mean(): -20.54371452331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4480], device='cuda:0')), ('power', tensor([-21.3578], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.13058963418006897
epoch£º883	 i:1 	 global-step:17661	 l-p:0.14469687640666962
epoch£º883	 i:2 	 global-step:17662	 l-p:0.1379358172416687
epoch£º883	 i:3 	 global-step:17663	 l-p:0.28131458163261414
epoch£º883	 i:4 	 global-step:17664	 l-p:0.15995298326015472
epoch£º883	 i:5 	 global-step:17665	 l-p:0.16321560740470886
epoch£º883	 i:6 	 global-step:17666	 l-p:0.19586597383022308
epoch£º883	 i:7 	 global-step:17667	 l-p:0.10399428755044937
epoch£º883	 i:8 	 global-step:17668	 l-p:0.11201123893260956
epoch£º883	 i:9 	 global-step:17669	 l-p:0.13841032981872559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.1141, 5.1141],
        [5.1141, 4.8523, 4.6500],
        [5.1141, 5.1141, 5.1141],
        [5.1141, 4.9136, 4.6175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.1108691543340683 
model_pd.l_d.mean(): -19.960447311401367 
model_pd.lagr.mean(): -19.849578857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.6738], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.1108691543340683
epoch£º884	 i:1 	 global-step:17681	 l-p:0.09609370678663254
epoch£º884	 i:2 	 global-step:17682	 l-p:0.15691117942333221
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12874823808670044
epoch£º884	 i:4 	 global-step:17684	 l-p:0.14885975420475006
epoch£º884	 i:5 	 global-step:17685	 l-p:0.08803364634513855
epoch£º884	 i:6 	 global-step:17686	 l-p:0.1541648954153061
epoch£º884	 i:7 	 global-step:17687	 l-p:0.15753161907196045
epoch£º884	 i:8 	 global-step:17688	 l-p:0.20994482934474945
epoch£º884	 i:9 	 global-step:17689	 l-p:0.1351487785577774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1448, 4.9826, 4.6755],
        [5.1448, 5.1448, 5.1448],
        [5.1448, 4.8942, 4.8144],
        [5.1448, 5.1694, 4.8905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.13896475732326508 
model_pd.l_d.mean(): -20.379276275634766 
model_pd.lagr.mean(): -20.240310668945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-21.0902], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.13896475732326508
epoch£º885	 i:1 	 global-step:17701	 l-p:0.19935904443264008
epoch£º885	 i:2 	 global-step:17702	 l-p:0.07979665696620941
epoch£º885	 i:3 	 global-step:17703	 l-p:0.1376066505908966
epoch£º885	 i:4 	 global-step:17704	 l-p:0.13671864569187164
epoch£º885	 i:5 	 global-step:17705	 l-p:0.12141746282577515
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12084715813398361
epoch£º885	 i:7 	 global-step:17707	 l-p:0.12636592984199524
epoch£º885	 i:8 	 global-step:17708	 l-p:0.1241169199347496
epoch£º885	 i:9 	 global-step:17709	 l-p:0.140438050031662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1387, 5.1370, 5.1387],
        [5.1387, 4.9838, 5.0310],
        [5.1387, 5.1311, 5.1382],
        [5.1387, 5.0456, 5.0987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14831778407096863 
model_pd.l_d.mean(): -19.381860733032227 
model_pd.lagr.mean(): -19.233543395996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5725], device='cuda:0')), ('power', tensor([-20.1784], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14831778407096863
epoch£º886	 i:1 	 global-step:17721	 l-p:0.10112294554710388
epoch£º886	 i:2 	 global-step:17722	 l-p:0.18057769536972046
epoch£º886	 i:3 	 global-step:17723	 l-p:0.14855295419692993
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11801580339670181
epoch£º886	 i:5 	 global-step:17725	 l-p:0.23784980177879333
epoch£º886	 i:6 	 global-step:17726	 l-p:0.161443293094635
epoch£º886	 i:7 	 global-step:17727	 l-p:0.09821617603302002
epoch£º886	 i:8 	 global-step:17728	 l-p:0.1443828046321869
epoch£º886	 i:9 	 global-step:17729	 l-p:0.08538564294576645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1128, 5.0902, 5.1096],
        [5.1128, 4.8488, 4.7219],
        [5.1128, 4.9961, 5.0517],
        [5.1128, 5.0195, 5.0728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.12105851620435715 
model_pd.l_d.mean(): -20.989030838012695 
model_pd.lagr.mean(): -20.867971420288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3905], device='cuda:0')), ('power', tensor([-21.6172], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.12105851620435715
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11673843860626221
epoch£º887	 i:2 	 global-step:17742	 l-p:0.1511951982975006
epoch£º887	 i:3 	 global-step:17743	 l-p:0.12153638899326324
epoch£º887	 i:4 	 global-step:17744	 l-p:0.2302006483078003
epoch£º887	 i:5 	 global-step:17745	 l-p:0.12200912088155746
epoch£º887	 i:6 	 global-step:17746	 l-p:0.22516025602817535
epoch£º887	 i:7 	 global-step:17747	 l-p:0.12211280316114426
epoch£º887	 i:8 	 global-step:17748	 l-p:0.2169390618801117
epoch£º887	 i:9 	 global-step:17749	 l-p:0.08671572804450989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1077, 5.4041, 5.2680],
        [5.1077, 4.8626, 4.8090],
        [5.1077, 5.3814, 5.2314],
        [5.1077, 4.8646, 4.8159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.07716476172208786 
model_pd.l_d.mean(): -19.447399139404297 
model_pd.lagr.mean(): -19.3702335357666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5880], device='cuda:0')), ('power', tensor([-20.2606], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.07716476172208786
epoch£º888	 i:1 	 global-step:17761	 l-p:0.2726944386959076
epoch£º888	 i:2 	 global-step:17762	 l-p:0.13562284409999847
epoch£º888	 i:3 	 global-step:17763	 l-p:0.11750082671642303
epoch£º888	 i:4 	 global-step:17764	 l-p:0.17939582467079163
epoch£º888	 i:5 	 global-step:17765	 l-p:0.1244807094335556
epoch£º888	 i:6 	 global-step:17766	 l-p:0.1073005273938179
epoch£º888	 i:7 	 global-step:17767	 l-p:0.1211552768945694
epoch£º888	 i:8 	 global-step:17768	 l-p:0.20101036131381989
epoch£º888	 i:9 	 global-step:17769	 l-p:0.19894452393054962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0981, 5.0981, 5.0981],
        [5.0981, 5.0981, 5.0981],
        [5.0981, 4.9279, 4.9695],
        [5.0981, 5.0355, 5.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.16558313369750977 
model_pd.l_d.mean(): -20.905860900878906 
model_pd.lagr.mean(): -20.740278244018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4275], device='cuda:0')), ('power', tensor([-21.5709], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.16558313369750977
epoch£º889	 i:1 	 global-step:17781	 l-p:0.18916358053684235
epoch£º889	 i:2 	 global-step:17782	 l-p:0.13046786189079285
epoch£º889	 i:3 	 global-step:17783	 l-p:0.15283584594726562
epoch£º889	 i:4 	 global-step:17784	 l-p:0.12324047088623047
epoch£º889	 i:5 	 global-step:17785	 l-p:0.15243786573410034
epoch£º889	 i:6 	 global-step:17786	 l-p:0.12949207425117493
epoch£º889	 i:7 	 global-step:17787	 l-p:0.1689509153366089
epoch£º889	 i:8 	 global-step:17788	 l-p:0.10860807448625565
epoch£º889	 i:9 	 global-step:17789	 l-p:0.17785154283046722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1251, 5.1251, 5.1252],
        [5.1251, 5.1252, 5.1252],
        [5.1251, 4.9157, 4.9217],
        [5.1251, 4.9075, 4.9028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.11535421013832092 
model_pd.l_d.mean(): -20.465253829956055 
model_pd.lagr.mean(): -20.349899291992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-21.1689], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.11535421013832092
epoch£º890	 i:1 	 global-step:17801	 l-p:0.13141614198684692
epoch£º890	 i:2 	 global-step:17802	 l-p:0.09887333959341049
epoch£º890	 i:3 	 global-step:17803	 l-p:0.13134096562862396
epoch£º890	 i:4 	 global-step:17804	 l-p:0.20629936456680298
epoch£º890	 i:5 	 global-step:17805	 l-p:0.1411643922328949
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13963961601257324
epoch£º890	 i:7 	 global-step:17807	 l-p:0.14575615525245667
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10791335999965668
epoch£º890	 i:9 	 global-step:17809	 l-p:0.13206347823143005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 5.1359, 5.1539],
        [5.1567, 5.2605, 5.0155],
        [5.1567, 5.0981, 5.1396],
        [5.1567, 5.1543, 5.1567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.1300482451915741 
model_pd.l_d.mean(): -19.120254516601562 
model_pd.lagr.mean(): -18.990205764770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5528], device='cuda:0')), ('power', tensor([-19.8939], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.1300482451915741
epoch£º891	 i:1 	 global-step:17821	 l-p:0.10686855763196945
epoch£º891	 i:2 	 global-step:17822	 l-p:0.1372307389974594
epoch£º891	 i:3 	 global-step:17823	 l-p:0.15910041332244873
epoch£º891	 i:4 	 global-step:17824	 l-p:0.11346542090177536
epoch£º891	 i:5 	 global-step:17825	 l-p:0.12266696989536285
epoch£º891	 i:6 	 global-step:17826	 l-p:0.10403601080179214
epoch£º891	 i:7 	 global-step:17827	 l-p:0.14662793278694153
epoch£º891	 i:8 	 global-step:17828	 l-p:0.16213366389274597
epoch£º891	 i:9 	 global-step:17829	 l-p:0.10605254769325256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1639, 4.9287, 4.8891],
        [5.1639, 5.1639, 5.1639],
        [5.1639, 5.0378, 4.7263],
        [5.1639, 5.1595, 5.1637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.13525651395320892 
model_pd.l_d.mean(): -20.003074645996094 
model_pd.lagr.mean(): -19.86781883239746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5274], device='cuda:0')), ('power', tensor([-20.7604], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.13525651395320892
epoch£º892	 i:1 	 global-step:17841	 l-p:0.13875681161880493
epoch£º892	 i:2 	 global-step:17842	 l-p:0.18171527981758118
epoch£º892	 i:3 	 global-step:17843	 l-p:0.04656011983752251
epoch£º892	 i:4 	 global-step:17844	 l-p:0.16340816020965576
epoch£º892	 i:5 	 global-step:17845	 l-p:0.0943523570895195
epoch£º892	 i:6 	 global-step:17846	 l-p:0.09947170317173004
epoch£º892	 i:7 	 global-step:17847	 l-p:0.11658986657857895
epoch£º892	 i:8 	 global-step:17848	 l-p:0.1455114781856537
epoch£º892	 i:9 	 global-step:17849	 l-p:0.14121736586093903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1584, 5.2296, 4.9692],
        [5.1584, 5.5563, 5.4839],
        [5.1584, 5.1451, 5.1571],
        [5.1584, 4.9554, 4.9669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.07164564728736877 
model_pd.l_d.mean(): -19.394559860229492 
model_pd.lagr.mean(): -19.322914123535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5874], device='cuda:0')), ('power', tensor([-20.2065], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.07164564728736877
epoch£º893	 i:1 	 global-step:17861	 l-p:0.1696784943342209
epoch£º893	 i:2 	 global-step:17862	 l-p:0.13635993003845215
epoch£º893	 i:3 	 global-step:17863	 l-p:0.12427791208028793
epoch£º893	 i:4 	 global-step:17864	 l-p:0.20565451681613922
epoch£º893	 i:5 	 global-step:17865	 l-p:0.12002915889024734
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11656554043292999
epoch£º893	 i:7 	 global-step:17867	 l-p:0.1574229747056961
epoch£º893	 i:8 	 global-step:17868	 l-p:0.10958632081747055
epoch£º893	 i:9 	 global-step:17869	 l-p:0.13211940228939056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1269, 5.1264, 5.1269],
        [5.1269, 5.1210, 5.1265],
        [5.1269, 4.9139, 4.6245],
        [5.1269, 4.8614, 4.7198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.12502369284629822 
model_pd.l_d.mean(): -20.052570343017578 
model_pd.lagr.mean(): -19.927547454833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-20.7629], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.12502369284629822
epoch£º894	 i:1 	 global-step:17881	 l-p:0.07415038347244263
epoch£º894	 i:2 	 global-step:17882	 l-p:0.1324448138475418
epoch£º894	 i:3 	 global-step:17883	 l-p:0.12421783804893494
epoch£º894	 i:4 	 global-step:17884	 l-p:0.09471645206212997
epoch£º894	 i:5 	 global-step:17885	 l-p:0.12314437329769135
epoch£º894	 i:6 	 global-step:17886	 l-p:0.11002167314291
epoch£º894	 i:7 	 global-step:17887	 l-p:0.24096056818962097
epoch£º894	 i:8 	 global-step:17888	 l-p:0.3486090898513794
epoch£º894	 i:9 	 global-step:17889	 l-p:0.23960824310779572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 5.0903, 5.0903],
        [5.0903, 5.0774, 5.0891],
        [5.0903, 5.3788, 5.2376],
        [5.0903, 4.8209, 4.6795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.1290682703256607 
model_pd.l_d.mean(): -18.938051223754883 
model_pd.lagr.mean(): -18.808982849121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6028], device='cuda:0')), ('power', tensor([-19.7607], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.1290682703256607
epoch£º895	 i:1 	 global-step:17901	 l-p:0.1798912137746811
epoch£º895	 i:2 	 global-step:17902	 l-p:0.16189533472061157
epoch£º895	 i:3 	 global-step:17903	 l-p:0.16408683359622955
epoch£º895	 i:4 	 global-step:17904	 l-p:0.1399211436510086
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11936106532812119
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13684973120689392
epoch£º895	 i:7 	 global-step:17907	 l-p:0.18612165749073029
epoch£º895	 i:8 	 global-step:17908	 l-p:0.24761275947093964
epoch£º895	 i:9 	 global-step:17909	 l-p:0.09384500235319138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1116, 5.4977, 5.4186],
        [5.1116, 5.0620, 5.0990],
        [5.1116, 5.1116, 5.1116],
        [5.1116, 4.8457, 4.7173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.1350330412387848 
model_pd.l_d.mean(): -20.26854705810547 
model_pd.lagr.mean(): -20.133514404296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.9892], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.1350330412387848
epoch£º896	 i:1 	 global-step:17921	 l-p:0.19197599589824677
epoch£º896	 i:2 	 global-step:17922	 l-p:0.11650696396827698
epoch£º896	 i:3 	 global-step:17923	 l-p:0.17965154349803925
epoch£º896	 i:4 	 global-step:17924	 l-p:0.1737835705280304
epoch£º896	 i:5 	 global-step:17925	 l-p:0.12831071019172668
epoch£º896	 i:6 	 global-step:17926	 l-p:0.11920491605997086
epoch£º896	 i:7 	 global-step:17927	 l-p:0.1527085304260254
epoch£º896	 i:8 	 global-step:17928	 l-p:0.11347588896751404
epoch£º896	 i:9 	 global-step:17929	 l-p:0.1316867172718048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1422, 5.0981, 5.1319],
        [5.1422, 5.1377, 5.1420],
        [5.1422, 5.0966, 5.1313],
        [5.1422, 4.8983, 4.8440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.14077739417552948 
model_pd.l_d.mean(): -20.5526180267334 
model_pd.lagr.mean(): -20.411840438842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4269], device='cuda:0')), ('power', tensor([-21.2132], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.14077739417552948
epoch£º897	 i:1 	 global-step:17941	 l-p:0.07676689326763153
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11876752227544785
epoch£º897	 i:3 	 global-step:17943	 l-p:0.13773413002490997
epoch£º897	 i:4 	 global-step:17944	 l-p:0.15843556821346283
epoch£º897	 i:5 	 global-step:17945	 l-p:0.14030881226062775
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14827710390090942
epoch£º897	 i:7 	 global-step:17947	 l-p:0.07081568241119385
epoch£º897	 i:8 	 global-step:17948	 l-p:0.2165503203868866
epoch£º897	 i:9 	 global-step:17949	 l-p:0.14272259175777435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 4.9059, 4.8566],
        [5.1475, 5.1468, 5.1475],
        [5.1475, 5.1034, 5.1372],
        [5.1475, 5.6121, 5.5852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.15601326525211334 
model_pd.l_d.mean(): -19.659181594848633 
model_pd.lagr.mean(): -19.5031681060791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.3991], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.15601326525211334
epoch£º898	 i:1 	 global-step:17961	 l-p:0.16095592081546783
epoch£º898	 i:2 	 global-step:17962	 l-p:0.15454603731632233
epoch£º898	 i:3 	 global-step:17963	 l-p:0.1421324461698532
epoch£º898	 i:4 	 global-step:17964	 l-p:0.0636071264743805
epoch£º898	 i:5 	 global-step:17965	 l-p:0.08627913892269135
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10654779523611069
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14460362493991852
epoch£º898	 i:8 	 global-step:17968	 l-p:0.1446799635887146
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10837589204311371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1871, 5.0917, 4.7817],
        [5.1871, 4.9396, 4.7089],
        [5.1871, 4.9484, 4.8988],
        [5.1871, 5.1860, 5.1870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.15434370934963226 
model_pd.l_d.mean(): -20.101472854614258 
model_pd.lagr.mean(): -19.947128295898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-20.7786], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.15434370934963226
epoch£º899	 i:1 	 global-step:17981	 l-p:0.0740227997303009
epoch£º899	 i:2 	 global-step:17982	 l-p:0.09446413815021515
epoch£º899	 i:3 	 global-step:17983	 l-p:0.11985445767641068
epoch£º899	 i:4 	 global-step:17984	 l-p:0.13061308860778809
epoch£º899	 i:5 	 global-step:17985	 l-p:0.18482017517089844
epoch£º899	 i:6 	 global-step:17986	 l-p:0.15002387762069702
epoch£º899	 i:7 	 global-step:17987	 l-p:0.028973804786801338
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12494386732578278
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12329740077257156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1857, 4.9265, 4.7956],
        [5.1857, 5.0784, 5.1331],
        [5.1857, 5.1857, 5.1857],
        [5.1857, 5.0852, 5.1393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.14668285846710205 
model_pd.l_d.mean(): -20.645660400390625 
model_pd.lagr.mean(): -20.498977661132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4186], device='cuda:0')), ('power', tensor([-21.2988], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.14668285846710205
epoch£º900	 i:1 	 global-step:18001	 l-p:0.054520051926374435
epoch£º900	 i:2 	 global-step:18002	 l-p:0.15180161595344543
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15704403817653656
epoch£º900	 i:4 	 global-step:18004	 l-p:0.1288433074951172
epoch£º900	 i:5 	 global-step:18005	 l-p:0.1332578957080841
epoch£º900	 i:6 	 global-step:18006	 l-p:0.1005084216594696
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12024230509996414
epoch£º900	 i:8 	 global-step:18008	 l-p:0.12937654554843903
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11696027219295502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1472, 5.1051, 5.1378],
        [5.1472, 5.1429, 5.1470],
        [5.1472, 5.2655, 5.0269],
        [5.1472, 4.9182, 4.8949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.183231920003891 
model_pd.l_d.mean(): -20.459758758544922 
model_pd.lagr.mean(): -20.276527404785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4761], device='cuda:0')), ('power', tensor([-21.1696], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.183231920003891
epoch£º901	 i:1 	 global-step:18021	 l-p:0.1425579935312271
epoch£º901	 i:2 	 global-step:18022	 l-p:0.1240869089961052
epoch£º901	 i:3 	 global-step:18023	 l-p:0.0950862318277359
epoch£º901	 i:4 	 global-step:18024	 l-p:0.1264253407716751
epoch£º901	 i:5 	 global-step:18025	 l-p:0.1508658528327942
epoch£º901	 i:6 	 global-step:18026	 l-p:0.18372869491577148
epoch£º901	 i:7 	 global-step:18027	 l-p:0.10224737972021103
epoch£º901	 i:8 	 global-step:18028	 l-p:0.21650439500808716
epoch£º901	 i:9 	 global-step:18029	 l-p:0.11870948225259781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1140, 5.1110, 5.1139],
        [5.1140, 4.8793, 4.8505],
        [5.1140, 4.9764, 4.6602],
        [5.1140, 5.0413, 5.0890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.13302281498908997 
model_pd.l_d.mean(): -19.581029891967773 
model_pd.lagr.mean(): -19.448007583618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4909], device='cuda:0')), ('power', tensor([-20.2964], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.13302281498908997
epoch£º902	 i:1 	 global-step:18041	 l-p:0.16319556534290314
epoch£º902	 i:2 	 global-step:18042	 l-p:0.19618095457553864
epoch£º902	 i:3 	 global-step:18043	 l-p:0.19330191612243652
epoch£º902	 i:4 	 global-step:18044	 l-p:0.11166422814130783
epoch£º902	 i:5 	 global-step:18045	 l-p:0.1744067221879959
epoch£º902	 i:6 	 global-step:18046	 l-p:0.11611396819353104
epoch£º902	 i:7 	 global-step:18047	 l-p:0.10808585584163666
epoch£º902	 i:8 	 global-step:18048	 l-p:0.1378212571144104
epoch£º902	 i:9 	 global-step:18049	 l-p:0.11047786474227905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.0879, 5.1235],
        [5.1350, 5.4373, 5.3032],
        [5.1350, 4.9521, 4.9847],
        [5.1350, 4.9896, 5.0412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.14397242665290833 
model_pd.l_d.mean(): -20.270151138305664 
model_pd.lagr.mean(): -20.126178741455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5129], device='cuda:0')), ('power', tensor([-21.0155], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.14397242665290833
epoch£º903	 i:1 	 global-step:18061	 l-p:0.1385618895292282
epoch£º903	 i:2 	 global-step:18062	 l-p:0.14926885068416595
epoch£º903	 i:3 	 global-step:18063	 l-p:0.13375282287597656
epoch£º903	 i:4 	 global-step:18064	 l-p:0.09952031075954437
epoch£º903	 i:5 	 global-step:18065	 l-p:0.15563948452472687
epoch£º903	 i:6 	 global-step:18066	 l-p:0.13256987929344177
epoch£º903	 i:7 	 global-step:18067	 l-p:0.12736688554286957
epoch£º903	 i:8 	 global-step:18068	 l-p:0.12662680447101593
epoch£º903	 i:9 	 global-step:18069	 l-p:0.1433326154947281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 5.1202, 5.1415],
        [5.1453, 4.9658, 5.0003],
        [5.1453, 5.1453, 5.1453],
        [5.1453, 5.1411, 5.1451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11693448573350906 
model_pd.l_d.mean(): -19.3597469329834 
model_pd.lagr.mean(): -19.242813110351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-20.1188], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11693448573350906
epoch£º904	 i:1 	 global-step:18081	 l-p:0.1525837630033493
epoch£º904	 i:2 	 global-step:18082	 l-p:0.15208926796913147
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14553436636924744
epoch£º904	 i:4 	 global-step:18084	 l-p:0.1271018087863922
epoch£º904	 i:5 	 global-step:18085	 l-p:0.1906714141368866
epoch£º904	 i:6 	 global-step:18086	 l-p:0.12257702648639679
epoch£º904	 i:7 	 global-step:18087	 l-p:0.1921340972185135
epoch£º904	 i:8 	 global-step:18088	 l-p:0.07988452911376953
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12814711034297943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1168, 5.1160, 5.1168],
        [5.1168, 5.0079, 5.0636],
        [5.1168, 4.8514, 4.6479],
        [5.1168, 4.8663, 4.8022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.13851457834243774 
model_pd.l_d.mean(): -20.577884674072266 
model_pd.lagr.mean(): -20.439369201660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4221], device='cuda:0')), ('power', tensor([-21.2338], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.13851457834243774
epoch£º905	 i:1 	 global-step:18101	 l-p:0.12449727952480316
epoch£º905	 i:2 	 global-step:18102	 l-p:0.13210061192512512
epoch£º905	 i:3 	 global-step:18103	 l-p:0.123013935983181
epoch£º905	 i:4 	 global-step:18104	 l-p:0.0914454236626625
epoch£º905	 i:5 	 global-step:18105	 l-p:0.19931630790233612
epoch£º905	 i:6 	 global-step:18106	 l-p:0.19874370098114014
epoch£º905	 i:7 	 global-step:18107	 l-p:0.24115392565727234
epoch£º905	 i:8 	 global-step:18108	 l-p:0.1367247998714447
epoch£º905	 i:9 	 global-step:18109	 l-p:0.17753304541110992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1095, 4.8784, 4.5991],
        [5.1095, 4.8897, 4.6001],
        [5.1095, 5.1970, 4.9436],
        [5.1095, 5.1091, 5.1095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.1261778026819229 
model_pd.l_d.mean(): -20.238462448120117 
model_pd.lagr.mean(): -20.11228370666504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.9483], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.1261778026819229
epoch£º906	 i:1 	 global-step:18121	 l-p:0.09907575696706772
epoch£º906	 i:2 	 global-step:18122	 l-p:0.1326427310705185
epoch£º906	 i:3 	 global-step:18123	 l-p:0.23559492826461792
epoch£º906	 i:4 	 global-step:18124	 l-p:0.14742422103881836
epoch£º906	 i:5 	 global-step:18125	 l-p:0.18796969950199127
epoch£º906	 i:6 	 global-step:18126	 l-p:0.15039145946502686
epoch£º906	 i:7 	 global-step:18127	 l-p:0.12160930037498474
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11678409576416016
epoch£º906	 i:9 	 global-step:18129	 l-p:0.15303540229797363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.1079, 5.1306],
        [5.1350, 5.0105, 5.0660],
        [5.1350, 5.0570, 5.1064],
        [5.1350, 4.9024, 4.6300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.21862976253032684 
model_pd.l_d.mean(): -19.623884201049805 
model_pd.lagr.mean(): -19.405254364013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-20.3274], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.21862976253032684
epoch£º907	 i:1 	 global-step:18141	 l-p:0.1285855770111084
epoch£º907	 i:2 	 global-step:18142	 l-p:0.05840946361422539
epoch£º907	 i:3 	 global-step:18143	 l-p:0.15879817306995392
epoch£º907	 i:4 	 global-step:18144	 l-p:0.13370832800865173
epoch£º907	 i:5 	 global-step:18145	 l-p:0.11312057077884674
epoch£º907	 i:6 	 global-step:18146	 l-p:0.13404658436775208
epoch£º907	 i:7 	 global-step:18147	 l-p:0.11815275251865387
epoch£º907	 i:8 	 global-step:18148	 l-p:0.17785170674324036
epoch£º907	 i:9 	 global-step:18149	 l-p:0.1252455711364746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[5.1458, 5.4279, 5.2807],
        [5.1458, 4.9589, 4.9882],
        [5.1458, 4.9485, 4.6497],
        [5.1458, 4.9395, 4.6459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.11676471680402756 
model_pd.l_d.mean(): -19.34123992919922 
model_pd.lagr.mean(): -19.224475860595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5876], device='cuda:0')), ('power', tensor([-20.1528], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.11676471680402756
epoch£º908	 i:1 	 global-step:18161	 l-p:0.13524484634399414
epoch£º908	 i:2 	 global-step:18162	 l-p:0.11821990460157394
epoch£º908	 i:3 	 global-step:18163	 l-p:0.1289432942867279
epoch£º908	 i:4 	 global-step:18164	 l-p:0.09339412301778793
epoch£º908	 i:5 	 global-step:18165	 l-p:0.13473105430603027
epoch£º908	 i:6 	 global-step:18166	 l-p:0.1843002289533615
epoch£º908	 i:7 	 global-step:18167	 l-p:0.13004039227962494
epoch£º908	 i:8 	 global-step:18168	 l-p:0.1356976479291916
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15008504688739777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1558, 5.1419, 5.1544],
        [5.1558, 5.1558, 5.1558],
        [5.1558, 5.0502, 4.7365],
        [5.1558, 5.1558, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.13755933940410614 
model_pd.l_d.mean(): -20.4072322845459 
model_pd.lagr.mean(): -20.269672393798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4468], device='cuda:0')), ('power', tensor([-21.0866], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.13755933940410614
epoch£º909	 i:1 	 global-step:18181	 l-p:0.13254185020923615
epoch£º909	 i:2 	 global-step:18182	 l-p:0.1181933730840683
epoch£º909	 i:3 	 global-step:18183	 l-p:0.10683320462703705
epoch£º909	 i:4 	 global-step:18184	 l-p:0.13189658522605896
epoch£º909	 i:5 	 global-step:18185	 l-p:0.12243914604187012
epoch£º909	 i:6 	 global-step:18186	 l-p:0.21634380519390106
epoch£º909	 i:7 	 global-step:18187	 l-p:0.09368734061717987
epoch£º909	 i:8 	 global-step:18188	 l-p:0.10064119845628738
epoch£º909	 i:9 	 global-step:18189	 l-p:0.14992448687553406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.1456, 5.1456],
        [5.1456, 5.1452, 5.1456],
        [5.1456, 5.1456, 5.1456],
        [5.1456, 4.9213, 4.6415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.17891353368759155 
model_pd.l_d.mean(): -19.319881439208984 
model_pd.lagr.mean(): -19.140968322753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-20.0798], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.17891353368759155
epoch£º910	 i:1 	 global-step:18201	 l-p:0.15321587026119232
epoch£º910	 i:2 	 global-step:18202	 l-p:0.06029323488473892
epoch£º910	 i:3 	 global-step:18203	 l-p:0.150246724486351
epoch£º910	 i:4 	 global-step:18204	 l-p:0.1680649220943451
epoch£º910	 i:5 	 global-step:18205	 l-p:0.12468139082193375
epoch£º910	 i:6 	 global-step:18206	 l-p:0.1138664036989212
epoch£º910	 i:7 	 global-step:18207	 l-p:0.18767207860946655
epoch£º910	 i:8 	 global-step:18208	 l-p:0.10502125322818756
epoch£º910	 i:9 	 global-step:18209	 l-p:0.14347706735134125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1195, 4.9202, 4.9402],
        [5.1195, 5.1195, 5.1195],
        [5.1195, 4.9801, 4.6633],
        [5.1195, 4.8935, 4.8803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.15920428931713104 
model_pd.l_d.mean(): -20.74068832397461 
model_pd.lagr.mean(): -20.581483840942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.4134], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.15920428931713104
epoch£º911	 i:1 	 global-step:18221	 l-p:0.2675187885761261
epoch£º911	 i:2 	 global-step:18222	 l-p:0.1338038444519043
epoch£º911	 i:3 	 global-step:18223	 l-p:0.18415649235248566
epoch£º911	 i:4 	 global-step:18224	 l-p:0.13315002620220184
epoch£º911	 i:5 	 global-step:18225	 l-p:0.15172766149044037
epoch£º911	 i:6 	 global-step:18226	 l-p:0.09168332070112228
epoch£º911	 i:7 	 global-step:18227	 l-p:0.13143688440322876
epoch£º911	 i:8 	 global-step:18228	 l-p:0.1135215163230896
epoch£º911	 i:9 	 global-step:18229	 l-p:0.09500085562467575
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1156, 5.2249, 4.9816],
        [5.1156, 5.0407, 5.0892],
        [5.1156, 4.8581, 4.6209],
        [5.1156, 5.1104, 5.1153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.15664230287075043 
model_pd.l_d.mean(): -19.419002532958984 
model_pd.lagr.mean(): -19.262359619140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5873], device='cuda:0')), ('power', tensor([-20.2311], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.15664230287075043
epoch£º912	 i:1 	 global-step:18241	 l-p:0.1132478192448616
epoch£º912	 i:2 	 global-step:18242	 l-p:0.172312393784523
epoch£º912	 i:3 	 global-step:18243	 l-p:0.1738145351409912
epoch£º912	 i:4 	 global-step:18244	 l-p:0.10832197219133377
epoch£º912	 i:5 	 global-step:18245	 l-p:0.1686318963766098
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13975557684898376
epoch£º912	 i:7 	 global-step:18247	 l-p:0.13902176916599274
epoch£º912	 i:8 	 global-step:18248	 l-p:0.16313207149505615
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12789061665534973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1197, 5.1197, 5.1197],
        [5.1197, 4.8627, 4.6255],
        [5.1197, 5.1193, 5.1197],
        [5.1197, 4.9079, 4.6127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.11026063561439514 
model_pd.l_d.mean(): -20.149675369262695 
model_pd.lagr.mean(): -20.03941535949707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5136], device='cuda:0')), ('power', tensor([-20.8945], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.11026063561439514
epoch£º913	 i:1 	 global-step:18261	 l-p:0.11567749083042145
epoch£º913	 i:2 	 global-step:18262	 l-p:0.1269366294145584
epoch£º913	 i:3 	 global-step:18263	 l-p:0.14152657985687256
epoch£º913	 i:4 	 global-step:18264	 l-p:0.141310915350914
epoch£º913	 i:5 	 global-step:18265	 l-p:0.18183019757270813
epoch£º913	 i:6 	 global-step:18266	 l-p:0.17756019532680511
epoch£º913	 i:7 	 global-step:18267	 l-p:0.18500763177871704
epoch£º913	 i:8 	 global-step:18268	 l-p:0.23192766308784485
epoch£º913	 i:9 	 global-step:18269	 l-p:0.12277007848024368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 4.8546, 4.7609],
        [5.1155, 5.1154, 5.1155],
        [5.1155, 5.0657, 5.1029],
        [5.1155, 4.8455, 4.6869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.1283850520849228 
model_pd.l_d.mean(): -19.955001831054688 
model_pd.lagr.mean(): -19.826616287231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.6831], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.1283850520849228
epoch£º914	 i:1 	 global-step:18281	 l-p:0.10429707914590836
epoch£º914	 i:2 	 global-step:18282	 l-p:0.09297725558280945
epoch£º914	 i:3 	 global-step:18283	 l-p:0.1632942408323288
epoch£º914	 i:4 	 global-step:18284	 l-p:0.14073263108730316
epoch£º914	 i:5 	 global-step:18285	 l-p:0.16304312646389008
epoch£º914	 i:6 	 global-step:18286	 l-p:0.19214031100273132
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12424148619174957
epoch£º914	 i:8 	 global-step:18288	 l-p:0.10482696443796158
epoch£º914	 i:9 	 global-step:18289	 l-p:0.16453075408935547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.0863, 5.1343],
        [5.1601, 5.1580, 5.1601],
        [5.1601, 5.1601, 5.1601],
        [5.1601, 5.1557, 5.1599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.11211387068033218 
model_pd.l_d.mean(): -20.656347274780273 
model_pd.lagr.mean(): -20.544233322143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-21.3381], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.11211387068033218
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12363989651203156
epoch£º915	 i:2 	 global-step:18302	 l-p:0.09167492389678955
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13436640799045563
epoch£º915	 i:4 	 global-step:18304	 l-p:0.18990041315555573
epoch£º915	 i:5 	 global-step:18305	 l-p:0.11279872059822083
epoch£º915	 i:6 	 global-step:18306	 l-p:0.18799762427806854
epoch£º915	 i:7 	 global-step:18307	 l-p:0.1060512512922287
epoch£º915	 i:8 	 global-step:18308	 l-p:0.11631589382886887
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12286277860403061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1676, 5.6432, 5.6223],
        [5.1676, 5.1676, 5.1676],
        [5.1676, 5.1424, 5.1637],
        [5.1676, 4.9412, 4.6659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13490508496761322 
model_pd.l_d.mean(): -19.936662673950195 
model_pd.lagr.mean(): -19.8017578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4465], device='cuda:0')), ('power', tensor([-20.6105], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13490508496761322
epoch£º916	 i:1 	 global-step:18321	 l-p:0.10110838711261749
epoch£º916	 i:2 	 global-step:18322	 l-p:0.19757932424545288
epoch£º916	 i:3 	 global-step:18323	 l-p:0.10401391983032227
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12324585020542145
epoch£º916	 i:5 	 global-step:18325	 l-p:0.10476592928171158
epoch£º916	 i:6 	 global-step:18326	 l-p:0.1328517645597458
epoch£º916	 i:7 	 global-step:18327	 l-p:0.15420374274253845
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12930156290531158
epoch£º916	 i:9 	 global-step:18329	 l-p:0.1266888678073883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1416, 5.0992, 5.1320],
        [5.1416, 5.1416, 5.1416],
        [5.1416, 5.4163, 5.2642],
        [5.1416, 5.0728, 5.1189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12917351722717285 
model_pd.l_d.mean(): -20.920652389526367 
model_pd.lagr.mean(): -20.791479110717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3890], device='cuda:0')), ('power', tensor([-21.5466], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12917351722717285
epoch£º917	 i:1 	 global-step:18341	 l-p:0.13147728145122528
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12405959516763687
epoch£º917	 i:3 	 global-step:18343	 l-p:0.16958987712860107
epoch£º917	 i:4 	 global-step:18344	 l-p:0.16310884058475494
epoch£º917	 i:5 	 global-step:18345	 l-p:0.12258540838956833
epoch£º917	 i:6 	 global-step:18346	 l-p:0.17397290468215942
epoch£º917	 i:7 	 global-step:18347	 l-p:0.20643077790737152
epoch£º917	 i:8 	 global-step:18348	 l-p:0.14995329082012177
epoch£º917	 i:9 	 global-step:18349	 l-p:0.09817152470350266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1118, 5.1118, 5.1118],
        [5.1118, 5.0981, 5.1105],
        [5.1118, 5.1118, 5.1118],
        [5.1118, 4.9634, 4.6454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.17268365621566772 
model_pd.l_d.mean(): -18.373319625854492 
model_pd.lagr.mean(): -18.20063591003418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6183], device='cuda:0')), ('power', tensor([-19.2057], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.17268365621566772
epoch£º918	 i:1 	 global-step:18361	 l-p:0.16561256349086761
epoch£º918	 i:2 	 global-step:18362	 l-p:0.07861624658107758
epoch£º918	 i:3 	 global-step:18363	 l-p:0.16470973193645477
epoch£º918	 i:4 	 global-step:18364	 l-p:0.16715145111083984
epoch£º918	 i:5 	 global-step:18365	 l-p:0.1332114189863205
epoch£º918	 i:6 	 global-step:18366	 l-p:0.13276508450508118
epoch£º918	 i:7 	 global-step:18367	 l-p:0.20843978226184845
epoch£º918	 i:8 	 global-step:18368	 l-p:0.11354906111955643
epoch£º918	 i:9 	 global-step:18369	 l-p:0.1217808946967125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[5.1259, 5.5072, 5.4229],
        [5.1259, 5.0733, 4.7657],
        [5.1259, 5.1831, 4.9147],
        [5.1259, 4.9236, 4.9411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.14154492318630219 
model_pd.l_d.mean(): -19.46387481689453 
model_pd.lagr.mean(): -19.322330474853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5169], device='cuda:0')), ('power', tensor([-20.2045], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.14154492318630219
epoch£º919	 i:1 	 global-step:18381	 l-p:0.15534478425979614
epoch£º919	 i:2 	 global-step:18382	 l-p:0.16659800708293915
epoch£º919	 i:3 	 global-step:18383	 l-p:0.1139446422457695
epoch£º919	 i:4 	 global-step:18384	 l-p:0.18034754693508148
epoch£º919	 i:5 	 global-step:18385	 l-p:0.15012840926647186
epoch£º919	 i:6 	 global-step:18386	 l-p:0.1305721402168274
epoch£º919	 i:7 	 global-step:18387	 l-p:0.13823744654655457
epoch£º919	 i:8 	 global-step:18388	 l-p:0.11933505535125732
epoch£º919	 i:9 	 global-step:18389	 l-p:0.13125988841056824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1338, 4.9229, 4.6274],
        [5.1338, 5.1337, 5.1338],
        [5.1338, 4.9271, 4.9396],
        [5.1338, 4.8888, 4.8397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.1500672698020935 
model_pd.l_d.mean(): -19.87708854675293 
model_pd.lagr.mean(): -19.727022171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-20.6152], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.1500672698020935
epoch£º920	 i:1 	 global-step:18401	 l-p:0.11200042814016342
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14566785097122192
epoch£º920	 i:3 	 global-step:18403	 l-p:0.11450685560703278
epoch£º920	 i:4 	 global-step:18404	 l-p:0.118233323097229
epoch£º920	 i:5 	 global-step:18405	 l-p:0.16220659017562866
epoch£º920	 i:6 	 global-step:18406	 l-p:0.14445483684539795
epoch£º920	 i:7 	 global-step:18407	 l-p:0.1090843454003334
epoch£º920	 i:8 	 global-step:18408	 l-p:0.20317427814006805
epoch£º920	 i:9 	 global-step:18409	 l-p:0.15196944773197174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1217, 5.1217, 5.1217],
        [5.1217, 5.0025, 4.6842],
        [5.1217, 5.0463, 5.0951],
        [5.1217, 5.1217, 5.1217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.15943527221679688 
model_pd.l_d.mean(): -19.294374465942383 
model_pd.lagr.mean(): -19.134939193725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5744], device='cuda:0')), ('power', tensor([-20.0920], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.15943527221679688
epoch£º921	 i:1 	 global-step:18421	 l-p:0.13500410318374634
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15358062088489532
epoch£º921	 i:3 	 global-step:18423	 l-p:0.1572207361459732
epoch£º921	 i:4 	 global-step:18424	 l-p:0.10425139218568802
epoch£º921	 i:5 	 global-step:18425	 l-p:0.10767829418182373
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12700927257537842
epoch£º921	 i:7 	 global-step:18427	 l-p:0.14210298657417297
epoch£º921	 i:8 	 global-step:18428	 l-p:0.15700751543045044
epoch£º921	 i:9 	 global-step:18429	 l-p:0.1519744098186493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1534, 5.1534, 5.1534],
        [5.1534, 4.9656, 4.9951],
        [5.1534, 5.4578, 5.3233],
        [5.1534, 5.2645, 5.0209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.1703871488571167 
model_pd.l_d.mean(): -20.526012420654297 
model_pd.lagr.mean(): -20.35562515258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.2183], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.1703871488571167
epoch£º922	 i:1 	 global-step:18441	 l-p:0.11453759670257568
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13517628610134125
epoch£º922	 i:3 	 global-step:18443	 l-p:0.1205211877822876
epoch£º922	 i:4 	 global-step:18444	 l-p:0.1152820810675621
epoch£º922	 i:5 	 global-step:18445	 l-p:0.09813345968723297
epoch£º922	 i:6 	 global-step:18446	 l-p:0.09045881032943726
epoch£º922	 i:7 	 global-step:18447	 l-p:0.16872040927410126
epoch£º922	 i:8 	 global-step:18448	 l-p:0.19257469475269318
epoch£º922	 i:9 	 global-step:18449	 l-p:0.09906436502933502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 4.9473, 4.6631],
        [5.1668, 4.9365, 4.9129],
        [5.1668, 5.0190, 5.0700],
        [5.1668, 5.1644, 5.1667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.12055172026157379 
model_pd.l_d.mean(): -20.48068618774414 
model_pd.lagr.mean(): -20.36013412475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4404], device='cuda:0')), ('power', tensor([-21.1543], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.12055172026157379
epoch£º923	 i:1 	 global-step:18461	 l-p:0.17899325489997864
epoch£º923	 i:2 	 global-step:18462	 l-p:0.11724380403757095
epoch£º923	 i:3 	 global-step:18463	 l-p:0.1107916459441185
epoch£º923	 i:4 	 global-step:18464	 l-p:0.1263667792081833
epoch£º923	 i:5 	 global-step:18465	 l-p:0.1258746236562729
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10671190172433853
epoch£º923	 i:7 	 global-step:18467	 l-p:0.15686900913715363
epoch£º923	 i:8 	 global-step:18468	 l-p:0.13557063043117523
epoch£º923	 i:9 	 global-step:18469	 l-p:0.09671861678361893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 4.9579, 4.9741],
        [5.1599, 4.9469, 4.6560],
        [5.1599, 5.1007, 5.1427],
        [5.1599, 5.0109, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.08585844933986664 
model_pd.l_d.mean(): -20.249095916748047 
model_pd.lagr.mean(): -20.163236618041992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.9720], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.08585844933986664
epoch£º924	 i:1 	 global-step:18481	 l-p:0.1815580576658249
epoch£º924	 i:2 	 global-step:18482	 l-p:0.14242149889469147
epoch£º924	 i:3 	 global-step:18483	 l-p:0.13336396217346191
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12461662292480469
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12479006499052048
epoch£º924	 i:6 	 global-step:18486	 l-p:0.20490296185016632
epoch£º924	 i:7 	 global-step:18487	 l-p:0.1073140874505043
epoch£º924	 i:8 	 global-step:18488	 l-p:0.08977524191141129
epoch£º924	 i:9 	 global-step:18489	 l-p:0.1130981519818306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1437, 5.1436, 5.1437],
        [5.1437, 5.1181, 4.8179],
        [5.1437, 4.9851, 4.6701],
        [5.1437, 4.9200, 4.6347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.15708990395069122 
model_pd.l_d.mean(): -20.450550079345703 
model_pd.lagr.mean(): -20.293460845947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4526], device='cuda:0')), ('power', tensor([-21.1363], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.15708990395069122
epoch£º925	 i:1 	 global-step:18501	 l-p:0.08445682376623154
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12495960295200348
epoch£º925	 i:3 	 global-step:18503	 l-p:0.1798437386751175
epoch£º925	 i:4 	 global-step:18504	 l-p:0.11439630389213562
epoch£º925	 i:5 	 global-step:18505	 l-p:0.13279472291469574
epoch£º925	 i:6 	 global-step:18506	 l-p:0.15288260579109192
epoch£º925	 i:7 	 global-step:18507	 l-p:0.1614719033241272
epoch£º925	 i:8 	 global-step:18508	 l-p:0.1299511045217514
epoch£º925	 i:9 	 global-step:18509	 l-p:0.20223122835159302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 5.1211, 5.1212],
        [5.1212, 4.8521, 4.7229],
        [5.1212, 4.8505, 4.7071],
        [5.1212, 5.0282, 4.7115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.13153314590454102 
model_pd.l_d.mean(): -19.938011169433594 
model_pd.lagr.mean(): -19.80647850036621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4579], device='cuda:0')), ('power', tensor([-20.6236], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.13153314590454102
epoch£º926	 i:1 	 global-step:18521	 l-p:0.20464982092380524
epoch£º926	 i:2 	 global-step:18522	 l-p:0.11507478356361389
epoch£º926	 i:3 	 global-step:18523	 l-p:0.12375632673501968
epoch£º926	 i:4 	 global-step:18524	 l-p:0.10098034888505936
epoch£º926	 i:5 	 global-step:18525	 l-p:0.16198696196079254
epoch£º926	 i:6 	 global-step:18526	 l-p:0.17694245278835297
epoch£º926	 i:7 	 global-step:18527	 l-p:0.11263319104909897
epoch£º926	 i:8 	 global-step:18528	 l-p:0.23159602284431458
epoch£º926	 i:9 	 global-step:18529	 l-p:0.12668249011039734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1106, 5.1099, 5.1106],
        [5.1106, 5.0357, 5.0844],
        [5.1106, 5.1065, 5.1104],
        [5.1106, 5.0235, 5.0760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.1351606249809265 
model_pd.l_d.mean(): -18.621110916137695 
model_pd.lagr.mean(): -18.485950469970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5512], device='cuda:0')), ('power', tensor([-19.3877], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.1351606249809265
epoch£º927	 i:1 	 global-step:18541	 l-p:0.16148123145103455
epoch£º927	 i:2 	 global-step:18542	 l-p:0.1294442117214203
epoch£º927	 i:3 	 global-step:18543	 l-p:0.134946808218956
epoch£º927	 i:4 	 global-step:18544	 l-p:0.1704210489988327
epoch£º927	 i:5 	 global-step:18545	 l-p:0.11623328924179077
epoch£º927	 i:6 	 global-step:18546	 l-p:0.11914033442735672
epoch£º927	 i:7 	 global-step:18547	 l-p:0.27799999713897705
epoch£º927	 i:8 	 global-step:18548	 l-p:0.12220647186040878
epoch£º927	 i:9 	 global-step:18549	 l-p:0.1614179164171219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 5.1089, 5.1089],
        [5.1089, 5.0003, 5.0567],
        [5.1089, 5.1089, 5.1089],
        [5.1089, 5.1071, 5.1088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.11105527728796005 
model_pd.l_d.mean(): -20.422183990478516 
model_pd.lagr.mean(): -20.311128616333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498], device='cuda:0')), ('power', tensor([-21.1047], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.11105527728796005
epoch£º928	 i:1 	 global-step:18561	 l-p:0.11823277175426483
epoch£º928	 i:2 	 global-step:18562	 l-p:0.15772008895874023
epoch£º928	 i:3 	 global-step:18563	 l-p:0.13859820365905762
epoch£º928	 i:4 	 global-step:18564	 l-p:0.18027830123901367
epoch£º928	 i:5 	 global-step:18565	 l-p:0.1318279206752777
epoch£º928	 i:6 	 global-step:18566	 l-p:0.18147388100624084
epoch£º928	 i:7 	 global-step:18567	 l-p:0.12069663405418396
epoch£º928	 i:8 	 global-step:18568	 l-p:0.17520815134048462
epoch£º928	 i:9 	 global-step:18569	 l-p:0.5594210624694824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0888, 5.0888, 5.0888],
        [5.0888, 5.0399, 5.0767],
        [5.0888, 4.8129, 4.6416],
        [5.0888, 5.0888, 5.0888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.09714457392692566 
model_pd.l_d.mean(): -20.64594268798828 
model_pd.lagr.mean(): -20.548797607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-21.3294], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.09714457392692566
epoch£º929	 i:1 	 global-step:18581	 l-p:0.5997700095176697
epoch£º929	 i:2 	 global-step:18582	 l-p:0.22021064162254333
epoch£º929	 i:3 	 global-step:18583	 l-p:0.13429073989391327
epoch£º929	 i:4 	 global-step:18584	 l-p:0.09843877702951431
epoch£º929	 i:5 	 global-step:18585	 l-p:0.1390492469072342
epoch£º929	 i:6 	 global-step:18586	 l-p:0.15910124778747559
epoch£º929	 i:7 	 global-step:18587	 l-p:0.1347523331642151
epoch£º929	 i:8 	 global-step:18588	 l-p:0.19493906199932098
epoch£º929	 i:9 	 global-step:18589	 l-p:0.136434406042099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 5.0821, 5.0897],
        [5.0903, 4.9402, 4.6185],
        [5.0903, 5.0703, 5.0877],
        [5.0903, 5.0891, 5.0903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.10750989615917206 
model_pd.l_d.mean(): -20.56441307067871 
model_pd.lagr.mean(): -20.4569034576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.2651], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.10750989615917206
epoch£º930	 i:1 	 global-step:18601	 l-p:0.14028871059417725
epoch£º930	 i:2 	 global-step:18602	 l-p:0.18105025589466095
epoch£º930	 i:3 	 global-step:18603	 l-p:0.8159958720207214
epoch£º930	 i:4 	 global-step:18604	 l-p:0.2169102430343628
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13781139254570007
epoch£º930	 i:6 	 global-step:18606	 l-p:0.18546539545059204
epoch£º930	 i:7 	 global-step:18607	 l-p:0.2537066638469696
epoch£º930	 i:8 	 global-step:18608	 l-p:0.09709140658378601
epoch£º930	 i:9 	 global-step:18609	 l-p:0.08611992001533508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0987, 5.0965, 5.0987],
        [5.0987, 4.8252, 4.6865],
        [5.0987, 5.0987, 5.0987],
        [5.0987, 4.9881, 5.0449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.16636499762535095 
model_pd.l_d.mean(): -20.29556655883789 
model_pd.lagr.mean(): -20.129201889038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-21.0049], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.16636499762535095
epoch£º931	 i:1 	 global-step:18621	 l-p:0.1441979557275772
epoch£º931	 i:2 	 global-step:18622	 l-p:0.16327960789203644
epoch£º931	 i:3 	 global-step:18623	 l-p:0.20376275479793549
epoch£º931	 i:4 	 global-step:18624	 l-p:0.12092239409685135
epoch£º931	 i:5 	 global-step:18625	 l-p:0.1326795518398285
epoch£º931	 i:6 	 global-step:18626	 l-p:0.1092807725071907
epoch£º931	 i:7 	 global-step:18627	 l-p:0.1448744386434555
epoch£º931	 i:8 	 global-step:18628	 l-p:0.21871238946914673
epoch£º931	 i:9 	 global-step:18629	 l-p:0.11860178411006927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1222, 4.9660, 4.6472],
        [5.1222, 5.0378, 5.0896],
        [5.1222, 4.8983, 4.8920],
        [5.1222, 5.1085, 5.1208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.20327500998973846 
model_pd.l_d.mean(): -19.04265022277832 
model_pd.lagr.mean(): -18.839374542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.8349], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.20327500998973846
epoch£º932	 i:1 	 global-step:18641	 l-p:0.16587182879447937
epoch£º932	 i:2 	 global-step:18642	 l-p:0.12279405444860458
epoch£º932	 i:3 	 global-step:18643	 l-p:0.202798530459404
epoch£º932	 i:4 	 global-step:18644	 l-p:0.09649398922920227
epoch£º932	 i:5 	 global-step:18645	 l-p:0.10576356947422028
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12361489236354828
epoch£º932	 i:7 	 global-step:18647	 l-p:0.08294372260570526
epoch£º932	 i:8 	 global-step:18648	 l-p:0.13189655542373657
epoch£º932	 i:9 	 global-step:18649	 l-p:0.2033226490020752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1296, 4.9875, 4.6679],
        [5.1296, 4.8820, 4.8324],
        [5.1296, 4.8581, 4.6733],
        [5.1296, 5.1296, 5.1296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.1302194446325302 
model_pd.l_d.mean(): -20.613269805908203 
model_pd.lagr.mean(): -20.483051300048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.3052], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.1302194446325302
epoch£º933	 i:1 	 global-step:18661	 l-p:0.0986703559756279
epoch£º933	 i:2 	 global-step:18662	 l-p:0.15772223472595215
epoch£º933	 i:3 	 global-step:18663	 l-p:0.1258922666311264
epoch£º933	 i:4 	 global-step:18664	 l-p:0.15685071051120758
epoch£º933	 i:5 	 global-step:18665	 l-p:0.12246422469615936
epoch£º933	 i:6 	 global-step:18666	 l-p:0.20615990459918976
epoch£º933	 i:7 	 global-step:18667	 l-p:0.17673808336257935
epoch£º933	 i:8 	 global-step:18668	 l-p:0.1041770651936531
epoch£º933	 i:9 	 global-step:18669	 l-p:0.15482637286186218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1368, 5.1368, 5.1368],
        [5.1368, 5.1368, 5.1368],
        [5.1368, 4.9825, 5.0328],
        [5.1368, 5.0210, 5.0775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.18908847868442535 
model_pd.l_d.mean(): -20.05306053161621 
model_pd.lagr.mean(): -19.863971710205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-20.7746], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.18908847868442535
epoch£º934	 i:1 	 global-step:18681	 l-p:0.13120992481708527
epoch£º934	 i:2 	 global-step:18682	 l-p:0.09452211111783981
epoch£º934	 i:3 	 global-step:18683	 l-p:0.12023824453353882
epoch£º934	 i:4 	 global-step:18684	 l-p:0.15353332459926605
epoch£º934	 i:5 	 global-step:18685	 l-p:0.15787923336029053
epoch£º934	 i:6 	 global-step:18686	 l-p:0.14370961487293243
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11765814572572708
epoch£º934	 i:8 	 global-step:18688	 l-p:0.12816965579986572
epoch£º934	 i:9 	 global-step:18689	 l-p:0.13276897370815277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 4.9244, 4.6314],
        [5.1429, 5.1285, 5.1414],
        [5.1429, 5.1351, 5.1424],
        [5.1429, 4.9925, 5.0440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.1594887524843216 
model_pd.l_d.mean(): -20.48012351989746 
model_pd.lagr.mean(): -20.320634841918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4708], device='cuda:0')), ('power', tensor([-21.1848], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.1594887524843216
epoch£º935	 i:1 	 global-step:18701	 l-p:0.12582536041736603
epoch£º935	 i:2 	 global-step:18702	 l-p:0.14193814992904663
epoch£º935	 i:3 	 global-step:18703	 l-p:0.15070760250091553
epoch£º935	 i:4 	 global-step:18704	 l-p:0.11079360544681549
epoch£º935	 i:5 	 global-step:18705	 l-p:0.13002125918865204
epoch£º935	 i:6 	 global-step:18706	 l-p:0.15573462843894958
epoch£º935	 i:7 	 global-step:18707	 l-p:0.13604840636253357
epoch£º935	 i:8 	 global-step:18708	 l-p:0.17344000935554504
epoch£º935	 i:9 	 global-step:18709	 l-p:0.13113710284233093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1275, 5.0172, 5.0737],
        [5.1275, 5.0945, 5.1214],
        [5.1275, 4.8619, 4.7589],
        [5.1275, 5.2666, 5.0362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16567383706569672 
model_pd.l_d.mean(): -19.760120391845703 
model_pd.lagr.mean(): -19.594446182250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-20.4748], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16567383706569672
epoch£º936	 i:1 	 global-step:18721	 l-p:0.1350446343421936
epoch£º936	 i:2 	 global-step:18722	 l-p:0.19705601036548615
epoch£º936	 i:3 	 global-step:18723	 l-p:0.13735198974609375
epoch£º936	 i:4 	 global-step:18724	 l-p:0.13556481897830963
epoch£º936	 i:5 	 global-step:18725	 l-p:0.1384964883327484
epoch£º936	 i:6 	 global-step:18726	 l-p:0.11846217513084412
epoch£º936	 i:7 	 global-step:18727	 l-p:0.12249093502759933
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12174990028142929
epoch£º936	 i:9 	 global-step:18729	 l-p:0.11485234647989273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1485, 4.8976, 4.8374],
        [5.1485, 5.1693, 4.8841],
        [5.1485, 4.9116, 4.8823],
        [5.1485, 5.1481, 5.1485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.08657503873109818 
model_pd.l_d.mean(): -20.73984146118164 
model_pd.lagr.mean(): -20.65326690673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4270], device='cuda:0')), ('power', tensor([-21.4026], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.08657503873109818
epoch£º937	 i:1 	 global-step:18741	 l-p:0.12112264335155487
epoch£º937	 i:2 	 global-step:18742	 l-p:0.1359279304742813
epoch£º937	 i:3 	 global-step:18743	 l-p:0.13441987335681915
epoch£º937	 i:4 	 global-step:18744	 l-p:0.12773364782333374
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12518231570720673
epoch£º937	 i:6 	 global-step:18746	 l-p:0.15725994110107422
epoch£º937	 i:7 	 global-step:18747	 l-p:0.2202662229537964
epoch£º937	 i:8 	 global-step:18748	 l-p:0.12061117589473724
epoch£º937	 i:9 	 global-step:18749	 l-p:0.15944108366966248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1394, 5.1394, 5.1394],
        [5.1394, 5.1093, 5.1342],
        [5.1394, 4.8675, 4.7101],
        [5.1394, 5.0098, 5.0658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.13185684382915497 
model_pd.l_d.mean(): -19.435598373413086 
model_pd.lagr.mean(): -19.303741455078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.1426], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.13185684382915497
epoch£º938	 i:1 	 global-step:18761	 l-p:0.16747115552425385
epoch£º938	 i:2 	 global-step:18762	 l-p:0.17013393342494965
epoch£º938	 i:3 	 global-step:18763	 l-p:0.10011343657970428
epoch£º938	 i:4 	 global-step:18764	 l-p:0.15637312829494476
epoch£º938	 i:5 	 global-step:18765	 l-p:0.16463948786258698
epoch£º938	 i:6 	 global-step:18766	 l-p:0.11680528521537781
epoch£º938	 i:7 	 global-step:18767	 l-p:0.05366089567542076
epoch£º938	 i:8 	 global-step:18768	 l-p:0.12085101753473282
epoch£º938	 i:9 	 global-step:18769	 l-p:0.18929919600486755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[5.1432, 5.3962, 5.2290],
        [5.1432, 4.9401, 4.6362],
        [5.1432, 4.8735, 4.7417],
        [5.1432, 5.0516, 4.7343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.08915749192237854 
model_pd.l_d.mean(): -20.113893508911133 
model_pd.lagr.mean(): -20.024736404418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4913], device='cuda:0')), ('power', tensor([-20.8355], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.08915749192237854
epoch£º939	 i:1 	 global-step:18781	 l-p:0.11637565493583679
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12366501241922379
epoch£º939	 i:3 	 global-step:18783	 l-p:0.16818144917488098
epoch£º939	 i:4 	 global-step:18784	 l-p:0.08761127293109894
epoch£º939	 i:5 	 global-step:18785	 l-p:0.18808957934379578
epoch£º939	 i:6 	 global-step:18786	 l-p:0.11464504152536392
epoch£º939	 i:7 	 global-step:18787	 l-p:0.1632826030254364
epoch£º939	 i:8 	 global-step:18788	 l-p:0.1791943460702896
epoch£º939	 i:9 	 global-step:18789	 l-p:0.1313074678182602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1493, 5.1482, 5.1493],
        [5.1493, 4.9471, 4.6433],
        [5.1493, 5.1349, 5.1478],
        [5.1493, 5.0739, 5.1228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.13388077914714813 
model_pd.l_d.mean(): -19.646629333496094 
model_pd.lagr.mean(): -19.51274871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5400], device='cuda:0')), ('power', tensor([-20.4129], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.13388077914714813
epoch£º940	 i:1 	 global-step:18801	 l-p:0.08673262596130371
epoch£º940	 i:2 	 global-step:18802	 l-p:0.14859335124492645
epoch£º940	 i:3 	 global-step:18803	 l-p:0.11227653175592422
epoch£º940	 i:4 	 global-step:18804	 l-p:0.17311228811740875
epoch£º940	 i:5 	 global-step:18805	 l-p:0.10903821885585785
epoch£º940	 i:6 	 global-step:18806	 l-p:0.16289737820625305
epoch£º940	 i:7 	 global-step:18807	 l-p:0.17116300761699677
epoch£º940	 i:8 	 global-step:18808	 l-p:0.08375491201877594
epoch£º940	 i:9 	 global-step:18809	 l-p:0.16587619483470917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.0238, 4.7050],
        [5.1613, 5.1611, 5.1613],
        [5.1613, 4.9039, 4.8233],
        [5.1613, 4.9173, 4.6530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.14384035766124725 
model_pd.l_d.mean(): -19.543865203857422 
model_pd.lagr.mean(): -19.4000244140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4975], device='cuda:0')), ('power', tensor([-20.2655], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.14384035766124725
epoch£º941	 i:1 	 global-step:18821	 l-p:0.12995974719524384
epoch£º941	 i:2 	 global-step:18822	 l-p:0.15014015138149261
epoch£º941	 i:3 	 global-step:18823	 l-p:0.0956321582198143
epoch£º941	 i:4 	 global-step:18824	 l-p:0.15140944719314575
epoch£º941	 i:5 	 global-step:18825	 l-p:0.1636151522397995
epoch£º941	 i:6 	 global-step:18826	 l-p:0.11006549745798111
epoch£º941	 i:7 	 global-step:18827	 l-p:0.12879639863967896
epoch£º941	 i:8 	 global-step:18828	 l-p:0.08177798986434937
epoch£º941	 i:9 	 global-step:18829	 l-p:0.12290307134389877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1652, 4.9715, 4.9977],
        [5.1652, 5.5742, 5.5055],
        [5.1652, 4.9489, 4.6557],
        [5.1652, 5.1600, 5.1649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12138383835554123 
model_pd.l_d.mean(): -20.325916290283203 
model_pd.lagr.mean(): -20.204532623291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-20.9986], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12138383835554123
epoch£º942	 i:1 	 global-step:18841	 l-p:0.1549709290266037
epoch£º942	 i:2 	 global-step:18842	 l-p:0.12485170364379883
epoch£º942	 i:3 	 global-step:18843	 l-p:0.17273195087909698
epoch£º942	 i:4 	 global-step:18844	 l-p:0.13118065893650055
epoch£º942	 i:5 	 global-step:18845	 l-p:0.11657382547855377
epoch£º942	 i:6 	 global-step:18846	 l-p:0.1265384405851364
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12406599521636963
epoch£º942	 i:8 	 global-step:18848	 l-p:0.17945833504199982
epoch£º942	 i:9 	 global-step:18849	 l-p:0.057845428586006165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1673, 5.1673],
        [5.1673, 4.9099, 4.8291],
        [5.1673, 5.1672, 5.1673],
        [5.1673, 5.1672, 5.1673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12128268927335739 
model_pd.l_d.mean(): -20.382434844970703 
model_pd.lagr.mean(): -20.261152267456055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4185], device='cuda:0')), ('power', tensor([-21.0326], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12128268927335739
epoch£º943	 i:1 	 global-step:18861	 l-p:0.1371556669473648
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13370275497436523
epoch£º943	 i:3 	 global-step:18863	 l-p:0.09500466287136078
epoch£º943	 i:4 	 global-step:18864	 l-p:0.1426752209663391
epoch£º943	 i:5 	 global-step:18865	 l-p:0.08806271106004715
epoch£º943	 i:6 	 global-step:18866	 l-p:0.15598064661026
epoch£º943	 i:7 	 global-step:18867	 l-p:0.130057230591774
epoch£º943	 i:8 	 global-step:18868	 l-p:0.11717204749584198
epoch£º943	 i:9 	 global-step:18869	 l-p:0.1616438776254654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[5.1786, 4.9294, 4.6769],
        [5.1786, 5.1052, 4.7916],
        [5.1786, 4.9379, 4.8983],
        [5.1786, 4.9955, 5.0296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.11379335075616837 
model_pd.l_d.mean(): -20.981477737426758 
model_pd.lagr.mean(): -20.867685317993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3761], device='cuda:0')), ('power', tensor([-21.5949], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.11379335075616837
epoch£º944	 i:1 	 global-step:18881	 l-p:0.10439491271972656
epoch£º944	 i:2 	 global-step:18882	 l-p:0.11996851861476898
epoch£º944	 i:3 	 global-step:18883	 l-p:0.14323805272579193
epoch£º944	 i:4 	 global-step:18884	 l-p:0.1153545007109642
epoch£º944	 i:5 	 global-step:18885	 l-p:0.11420039087533951
epoch£º944	 i:6 	 global-step:18886	 l-p:0.14335325360298157
epoch£º944	 i:7 	 global-step:18887	 l-p:0.15912599861621857
epoch£º944	 i:8 	 global-step:18888	 l-p:0.11760301142930984
epoch£º944	 i:9 	 global-step:18889	 l-p:0.1304103434085846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1745, 5.0444, 4.7256],
        [5.1745, 5.1704, 5.1744],
        [5.1745, 5.0795, 5.1337],
        [5.1745, 5.1745, 5.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.09865762293338776 
model_pd.l_d.mean(): -19.634830474853516 
model_pd.lagr.mean(): -19.53617286682129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-20.3098], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.09865762293338776
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12364808470010757
epoch£º945	 i:2 	 global-step:18902	 l-p:0.12616567313671112
epoch£º945	 i:3 	 global-step:18903	 l-p:0.06492849439382553
epoch£º945	 i:4 	 global-step:18904	 l-p:0.11320047080516815
epoch£º945	 i:5 	 global-step:18905	 l-p:0.15084539353847504
epoch£º945	 i:6 	 global-step:18906	 l-p:0.10394880920648575
epoch£º945	 i:7 	 global-step:18907	 l-p:0.1400405764579773
epoch£º945	 i:8 	 global-step:18908	 l-p:0.15549245476722717
epoch£º945	 i:9 	 global-step:18909	 l-p:0.27560287714004517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1426, 5.0052, 4.6838],
        [5.1426, 5.1308, 4.8325],
        [5.1426, 5.1425, 5.1426],
        [5.1426, 5.5878, 5.5437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.20826871693134308 
model_pd.l_d.mean(): -19.690576553344727 
model_pd.lagr.mean(): -19.48230743408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-20.4483], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.20826871693134308
epoch£º946	 i:1 	 global-step:18921	 l-p:0.12266460061073303
epoch£º946	 i:2 	 global-step:18922	 l-p:0.1269410103559494
epoch£º946	 i:3 	 global-step:18923	 l-p:0.10012909770011902
epoch£º946	 i:4 	 global-step:18924	 l-p:0.12886063754558563
epoch£º946	 i:5 	 global-step:18925	 l-p:0.1297355592250824
epoch£º946	 i:6 	 global-step:18926	 l-p:0.19638609886169434
epoch£º946	 i:7 	 global-step:18927	 l-p:0.09828061610460281
epoch£º946	 i:8 	 global-step:18928	 l-p:0.14062651991844177
epoch£º946	 i:9 	 global-step:18929	 l-p:0.12047802656888962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1438, 5.1303, 5.1425],
        [5.1438, 5.1417, 5.1438],
        [5.1438, 5.1438, 5.1438],
        [5.1438, 5.1438, 5.1438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.20006652176380157 
model_pd.l_d.mean(): -20.842817306518555 
model_pd.lagr.mean(): -20.642751693725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.4987], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.20006652176380157
epoch£º947	 i:1 	 global-step:18941	 l-p:0.15619182586669922
epoch£º947	 i:2 	 global-step:18942	 l-p:0.1275685727596283
epoch£º947	 i:3 	 global-step:18943	 l-p:0.12084882706403732
epoch£º947	 i:4 	 global-step:18944	 l-p:0.1007666364312172
epoch£º947	 i:5 	 global-step:18945	 l-p:0.14861565828323364
epoch£º947	 i:6 	 global-step:18946	 l-p:0.1423940509557724
epoch£º947	 i:7 	 global-step:18947	 l-p:0.09926638752222061
epoch£º947	 i:8 	 global-step:18948	 l-p:0.1465924084186554
epoch£º947	 i:9 	 global-step:18949	 l-p:0.17166726291179657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1311, 5.1589, 4.8750],
        [5.1311, 5.0570, 5.1056],
        [5.1311, 5.1310, 5.1311],
        [5.1311, 4.9968, 5.0529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11733454465866089 
model_pd.l_d.mean(): -19.347211837768555 
model_pd.lagr.mean(): -19.229877471923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5824], device='cuda:0')), ('power', tensor([-20.1536], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11733454465866089
epoch£º948	 i:1 	 global-step:18961	 l-p:0.11880838125944138
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11809679120779037
epoch£º948	 i:3 	 global-step:18963	 l-p:0.15052728354930878
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14662687480449677
epoch£º948	 i:5 	 global-step:18965	 l-p:0.17346785962581635
epoch£º948	 i:6 	 global-step:18966	 l-p:0.16129903495311737
epoch£º948	 i:7 	 global-step:18967	 l-p:0.07029402256011963
epoch£º948	 i:8 	 global-step:18968	 l-p:0.14248664677143097
epoch£º948	 i:9 	 global-step:18969	 l-p:0.20767860114574432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.1348, 5.1359],
        [5.1359, 5.0182, 5.0753],
        [5.1359, 5.0596, 5.1090],
        [5.1359, 5.0818, 5.1215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.12675200402736664 
model_pd.l_d.mean(): -20.786226272583008 
model_pd.lagr.mean(): -20.659473419189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4076], device='cuda:0')), ('power', tensor([-21.4297], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.12675200402736664
epoch£º949	 i:1 	 global-step:18981	 l-p:0.10170644521713257
epoch£º949	 i:2 	 global-step:18982	 l-p:0.15635816752910614
epoch£º949	 i:3 	 global-step:18983	 l-p:0.14951518177986145
epoch£º949	 i:4 	 global-step:18984	 l-p:0.20552781224250793
epoch£º949	 i:5 	 global-step:18985	 l-p:0.17949114739894867
epoch£º949	 i:6 	 global-step:18986	 l-p:0.12805883586406708
epoch£º949	 i:7 	 global-step:18987	 l-p:0.08441890776157379
epoch£º949	 i:8 	 global-step:18988	 l-p:0.15061728656291962
epoch£º949	 i:9 	 global-step:18989	 l-p:0.12344358116388321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1509, 5.1431, 5.1504],
        [5.1509, 4.8776, 4.7196],
        [5.1509, 4.8881, 4.6529],
        [5.1509, 5.1188, 5.1451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.11615294963121414 
model_pd.l_d.mean(): -19.729413986206055 
model_pd.lagr.mean(): -19.61326026916504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5004], device='cuda:0')), ('power', tensor([-20.4561], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.11615294963121414
epoch£º950	 i:1 	 global-step:19001	 l-p:0.1934288740158081
epoch£º950	 i:2 	 global-step:19002	 l-p:0.15474599599838257
epoch£º950	 i:3 	 global-step:19003	 l-p:0.13569626212120056
epoch£º950	 i:4 	 global-step:19004	 l-p:0.11284855008125305
epoch£º950	 i:5 	 global-step:19005	 l-p:0.14904342591762543
epoch£º950	 i:6 	 global-step:19006	 l-p:0.1685079038143158
epoch£º950	 i:7 	 global-step:19007	 l-p:0.11320951581001282
epoch£º950	 i:8 	 global-step:19008	 l-p:0.13443021476268768
epoch£º950	 i:9 	 global-step:19009	 l-p:0.09216826409101486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1378, 4.9635, 5.0058],
        [5.1378, 5.1378, 5.1378],
        [5.1378, 5.1378, 5.1378],
        [5.1378, 4.8629, 4.7049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.21639707684516907 
model_pd.l_d.mean(): -19.06140899658203 
model_pd.lagr.mean(): -18.845012664794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5334], device='cuda:0')), ('power', tensor([-19.8145], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.21639707684516907
epoch£º951	 i:1 	 global-step:19021	 l-p:0.13876444101333618
epoch£º951	 i:2 	 global-step:19022	 l-p:0.13205678761005402
epoch£º951	 i:3 	 global-step:19023	 l-p:0.0891449898481369
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12849129736423492
epoch£º951	 i:5 	 global-step:19025	 l-p:0.15986734628677368
epoch£º951	 i:6 	 global-step:19026	 l-p:0.16371966898441315
epoch£º951	 i:7 	 global-step:19027	 l-p:0.1389864832162857
epoch£º951	 i:8 	 global-step:19028	 l-p:0.11270475387573242
epoch£º951	 i:9 	 global-step:19029	 l-p:0.12670119106769562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1331, 5.1270, 5.1327],
        [5.1331, 4.9857, 4.6628],
        [5.1331, 5.0917, 5.1241],
        [5.1331, 5.1099, 5.1298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.11907079070806503 
model_pd.l_d.mean(): -20.34836196899414 
model_pd.lagr.mean(): -20.229291915893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-21.0440], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.11907079070806503
epoch£º952	 i:1 	 global-step:19041	 l-p:0.15366926789283752
epoch£º952	 i:2 	 global-step:19042	 l-p:0.11498405039310455
epoch£º952	 i:3 	 global-step:19043	 l-p:0.16222858428955078
epoch£º952	 i:4 	 global-step:19044	 l-p:0.19014222919940948
epoch£º952	 i:5 	 global-step:19045	 l-p:0.13976682722568512
epoch£º952	 i:6 	 global-step:19046	 l-p:0.118058942258358
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12740395963191986
epoch£º952	 i:8 	 global-step:19048	 l-p:0.13320398330688477
epoch£º952	 i:9 	 global-step:19049	 l-p:0.19309575855731964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1169, 4.8398, 4.6502],
        [5.1169, 5.1168, 5.1169],
        [5.1169, 5.1168, 5.1169],
        [5.1169, 5.1131, 5.1167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.10216155648231506 
model_pd.l_d.mean(): -20.451261520385742 
model_pd.lagr.mean(): -20.34910011291504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-21.1654], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.10216155648231506
epoch£º953	 i:1 	 global-step:19061	 l-p:0.23537179827690125
epoch£º953	 i:2 	 global-step:19062	 l-p:0.1731598675251007
epoch£º953	 i:3 	 global-step:19063	 l-p:0.18171454966068268
epoch£º953	 i:4 	 global-step:19064	 l-p:0.1592620611190796
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13415485620498657
epoch£º953	 i:6 	 global-step:19066	 l-p:0.09909971058368683
epoch£º953	 i:7 	 global-step:19067	 l-p:0.10964633524417877
epoch£º953	 i:8 	 global-step:19068	 l-p:0.2155548632144928
epoch£º953	 i:9 	 global-step:19069	 l-p:0.16847577691078186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1008, 5.0333, 5.0795],
        [5.1008, 5.4740, 5.3818],
        [5.1008, 5.0978, 5.1007],
        [5.1008, 4.9132, 4.9491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.1478247195482254 
model_pd.l_d.mean(): -19.84178352355957 
model_pd.lagr.mean(): -19.693958282470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-20.6344], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.1478247195482254
epoch£º954	 i:1 	 global-step:19081	 l-p:0.11083941906690598
epoch£º954	 i:2 	 global-step:19082	 l-p:0.3301442563533783
epoch£º954	 i:3 	 global-step:19083	 l-p:0.21690472960472107
epoch£º954	 i:4 	 global-step:19084	 l-p:0.09855882078409195
epoch£º954	 i:5 	 global-step:19085	 l-p:0.12358039617538452
epoch£º954	 i:6 	 global-step:19086	 l-p:0.25388839840888977
epoch£º954	 i:7 	 global-step:19087	 l-p:0.1063181683421135
epoch£º954	 i:8 	 global-step:19088	 l-p:0.1631653606891632
epoch£º954	 i:9 	 global-step:19089	 l-p:0.1299516260623932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0988, 5.0988, 5.0988],
        [5.0988, 5.4711, 5.3783],
        [5.0988, 5.0205, 5.0709],
        [5.0988, 5.0574, 5.0899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.09710858762264252 
model_pd.l_d.mean(): -20.198198318481445 
model_pd.lagr.mean(): -20.101089477539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5178], device='cuda:0')), ('power', tensor([-20.9478], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.09710858762264252
epoch£º955	 i:1 	 global-step:19101	 l-p:0.11804183572530746
epoch£º955	 i:2 	 global-step:19102	 l-p:0.09501342475414276
epoch£º955	 i:3 	 global-step:19103	 l-p:-0.6670807003974915
epoch£º955	 i:4 	 global-step:19104	 l-p:0.21272403001785278
epoch£º955	 i:5 	 global-step:19105	 l-p:0.13351507484912872
epoch£º955	 i:6 	 global-step:19106	 l-p:0.19031354784965515
epoch£º955	 i:7 	 global-step:19107	 l-p:0.20756421983242035
epoch£º955	 i:8 	 global-step:19108	 l-p:0.1527160108089447
epoch£º955	 i:9 	 global-step:19109	 l-p:0.7152421474456787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0664, 5.0327, 4.7247],
        [5.0664, 5.0657, 5.0664],
        [5.0664, 5.0652, 5.0664],
        [5.0664, 4.7830, 4.6251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.1492026150226593 
model_pd.l_d.mean(): -20.99897575378418 
model_pd.lagr.mean(): -20.849773406982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-21.6575], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.1492026150226593
epoch£º956	 i:1 	 global-step:19121	 l-p:0.1617497354745865
epoch£º956	 i:2 	 global-step:19122	 l-p:0.123598113656044
epoch£º956	 i:3 	 global-step:19123	 l-p:0.18866263329982758
epoch£º956	 i:4 	 global-step:19124	 l-p:0.12682101130485535
epoch£º956	 i:5 	 global-step:19125	 l-p:0.16828584671020508
epoch£º956	 i:6 	 global-step:19126	 l-p:1.1044366359710693
epoch£º956	 i:7 	 global-step:19127	 l-p:0.12825119495391846
epoch£º956	 i:8 	 global-step:19128	 l-p:-0.0027102374006062746
epoch£º956	 i:9 	 global-step:19129	 l-p:0.2389621138572693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0790, 5.0786, 5.0790],
        [5.0790, 4.9019, 4.9455],
        [5.0790, 5.0573, 5.0761],
        [5.0790, 5.0556, 5.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.1651778370141983 
model_pd.l_d.mean(): -20.730077743530273 
model_pd.lagr.mean(): -20.564899444580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.4233], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.1651778370141983
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13551338016986847
epoch£º957	 i:2 	 global-step:19142	 l-p:0.21130092442035675
epoch£º957	 i:3 	 global-step:19143	 l-p:0.11197943240404129
epoch£º957	 i:4 	 global-step:19144	 l-p:0.09688575565814972
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11967622488737106
epoch£º957	 i:6 	 global-step:19146	 l-p:0.15396562218666077
epoch£º957	 i:7 	 global-step:19147	 l-p:0.2151176780462265
epoch£º957	 i:8 	 global-step:19148	 l-p:0.3284517228603363
epoch£º957	 i:9 	 global-step:19149	 l-p:0.19790656864643097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1079, 5.0846, 5.1046],
        [5.1079, 5.0373, 5.0847],
        [5.1079, 5.0296, 5.0799],
        [5.1079, 5.0055, 4.6820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.1399761587381363 
model_pd.l_d.mean(): -20.846336364746094 
model_pd.lagr.mean(): -20.70635986328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-21.5013], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.1399761587381363
epoch£º958	 i:1 	 global-step:19161	 l-p:0.24725066125392914
epoch£º958	 i:2 	 global-step:19162	 l-p:0.12306609749794006
epoch£º958	 i:3 	 global-step:19163	 l-p:0.1731603443622589
epoch£º958	 i:4 	 global-step:19164	 l-p:0.15879136323928833
epoch£º958	 i:5 	 global-step:19165	 l-p:0.10235221683979034
epoch£º958	 i:6 	 global-step:19166	 l-p:0.10953889787197113
epoch£º958	 i:7 	 global-step:19167	 l-p:0.16514253616333008
epoch£º958	 i:8 	 global-step:19168	 l-p:0.1273571103811264
epoch£º958	 i:9 	 global-step:19169	 l-p:0.13846632838249207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 4.9927, 4.6763],
        [5.1678, 5.1678, 5.1678],
        [5.1678, 4.9357, 4.6509],
        [5.1678, 5.1649, 5.1677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05549977347254753 
model_pd.l_d.mean(): -20.57269859313965 
model_pd.lagr.mean(): -20.51719856262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4379], device='cuda:0')), ('power', tensor([-21.2448], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.05549977347254753
epoch£º959	 i:1 	 global-step:19181	 l-p:0.15044522285461426
epoch£º959	 i:2 	 global-step:19182	 l-p:0.16576679050922394
epoch£º959	 i:3 	 global-step:19183	 l-p:0.13690781593322754
epoch£º959	 i:4 	 global-step:19184	 l-p:0.12207290530204773
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12592700123786926
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13849472999572754
epoch£º959	 i:7 	 global-step:19187	 l-p:0.1388908177614212
epoch£º959	 i:8 	 global-step:19188	 l-p:0.11587236821651459
epoch£º959	 i:9 	 global-step:19189	 l-p:0.09983552992343903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1962, 5.1962, 5.1962],
        [5.1962, 5.1927, 5.1961],
        [5.1962, 5.5033, 5.3661],
        [5.1962, 5.4920, 5.3477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.10662161558866501 
model_pd.l_d.mean(): -19.578529357910156 
model_pd.lagr.mean(): -19.471908569335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.3054], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.10662161558866501
epoch£º960	 i:1 	 global-step:19201	 l-p:0.1644190400838852
epoch£º960	 i:2 	 global-step:19202	 l-p:0.1298360973596573
epoch£º960	 i:3 	 global-step:19203	 l-p:0.12046591192483902
epoch£º960	 i:4 	 global-step:19204	 l-p:0.1368253529071808
epoch£º960	 i:5 	 global-step:19205	 l-p:0.10911230742931366
epoch£º960	 i:6 	 global-step:19206	 l-p:0.11773887276649475
epoch£º960	 i:7 	 global-step:19207	 l-p:0.1053122878074646
epoch£º960	 i:8 	 global-step:19208	 l-p:0.12505526840686798
epoch£º960	 i:9 	 global-step:19209	 l-p:0.11682645231485367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1768, 5.0365, 4.7153],
        [5.1768, 5.1038, 5.1519],
        [5.1768, 5.0153, 4.6965],
        [5.1768, 5.1719, 5.1766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.12776923179626465 
model_pd.l_d.mean(): -19.57793617248535 
model_pd.lagr.mean(): -19.450166702270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5758], device='cuda:0')), ('power', tensor([-20.3800], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.12776923179626465
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12170823663473129
epoch£º961	 i:2 	 global-step:19222	 l-p:0.16892218589782715
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13304142653942108
epoch£º961	 i:4 	 global-step:19224	 l-p:0.16704654693603516
epoch£º961	 i:5 	 global-step:19225	 l-p:0.12367058545351028
epoch£º961	 i:6 	 global-step:19226	 l-p:0.10704866796731949
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13555730879306793
epoch£º961	 i:8 	 global-step:19228	 l-p:0.11992490291595459
epoch£º961	 i:9 	 global-step:19229	 l-p:0.12120184302330017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1529, 5.1046, 5.1411],
        [5.1529, 5.1529, 5.1529],
        [5.1529, 5.1515, 5.1528],
        [5.1529, 5.0529, 5.1086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.11386361718177795 
model_pd.l_d.mean(): -20.837574005126953 
model_pd.lagr.mean(): -20.723711013793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.4761], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.11386361718177795
epoch£º962	 i:1 	 global-step:19241	 l-p:0.09614236652851105
epoch£º962	 i:2 	 global-step:19242	 l-p:0.12824225425720215
epoch£º962	 i:3 	 global-step:19243	 l-p:0.1451643854379654
epoch£º962	 i:4 	 global-step:19244	 l-p:0.10695666819810867
epoch£º962	 i:5 	 global-step:19245	 l-p:0.23310436308383942
epoch£º962	 i:6 	 global-step:19246	 l-p:0.10988461226224899
epoch£º962	 i:7 	 global-step:19247	 l-p:0.2933163046836853
epoch£º962	 i:8 	 global-step:19248	 l-p:0.1572534292936325
epoch£º962	 i:9 	 global-step:19249	 l-p:0.17365702986717224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1046, 5.1021, 5.1045],
        [5.1046, 5.1030, 5.1045],
        [5.1046, 5.0496, 5.0899],
        [5.1046, 5.0610, 5.0948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.12256977707147598 
model_pd.l_d.mean(): -20.071186065673828 
model_pd.lagr.mean(): -19.94861602783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-20.7983], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.12256977707147598
epoch£º963	 i:1 	 global-step:19261	 l-p:0.13948559761047363
epoch£º963	 i:2 	 global-step:19262	 l-p:0.13047762215137482
epoch£º963	 i:3 	 global-step:19263	 l-p:0.08774338662624359
epoch£º963	 i:4 	 global-step:19264	 l-p:0.18868720531463623
epoch£º963	 i:5 	 global-step:19265	 l-p:0.27339664101600647
epoch£º963	 i:6 	 global-step:19266	 l-p:-2.0953760147094727
epoch£º963	 i:7 	 global-step:19267	 l-p:0.156972736120224
epoch£º963	 i:8 	 global-step:19268	 l-p:0.2506537437438965
epoch£º963	 i:9 	 global-step:19269	 l-p:0.1179843321442604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0845, 5.0829, 5.0845],
        [5.0845, 5.0842, 5.0845],
        [5.0845, 5.5096, 5.4516],
        [5.0845, 4.8719, 4.5578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.21045054495334625 
model_pd.l_d.mean(): -18.608348846435547 
model_pd.lagr.mean(): -18.397897720336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6014], device='cuda:0')), ('power', tensor([-19.4260], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.21045054495334625
epoch£º964	 i:1 	 global-step:19281	 l-p:0.2079300582408905
epoch£º964	 i:2 	 global-step:19282	 l-p:0.1570013016462326
epoch£º964	 i:3 	 global-step:19283	 l-p:0.2658032476902008
epoch£º964	 i:4 	 global-step:19284	 l-p:0.15006086230278015
epoch£º964	 i:5 	 global-step:19285	 l-p:1.0974427461624146
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13448737561702728
epoch£º964	 i:7 	 global-step:19287	 l-p:0.16075025498867035
epoch£º964	 i:8 	 global-step:19288	 l-p:0.10682912915945053
epoch£º964	 i:9 	 global-step:19289	 l-p:0.0737454742193222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.0971, 4.8616, 4.5644],
        [5.0971, 4.8260, 4.7316],
        [5.0971, 4.8635, 4.5647],
        [5.0971, 4.8426, 4.7942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.18850696086883545 
model_pd.l_d.mean(): -20.531848907470703 
model_pd.lagr.mean(): -20.343341827392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4568], device='cuda:0')), ('power', tensor([-21.2228], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.18850696086883545
epoch£º965	 i:1 	 global-step:19301	 l-p:0.14558938145637512
epoch£º965	 i:2 	 global-step:19302	 l-p:0.14408797025680542
epoch£º965	 i:3 	 global-step:19303	 l-p:0.09673316776752472
epoch£º965	 i:4 	 global-step:19304	 l-p:0.1315116435289383
epoch£º965	 i:5 	 global-step:19305	 l-p:0.23234900832176208
epoch£º965	 i:6 	 global-step:19306	 l-p:0.2359798699617386
epoch£º965	 i:7 	 global-step:19307	 l-p:0.16906501352787018
epoch£º965	 i:8 	 global-step:19308	 l-p:0.130323126912117
epoch£º965	 i:9 	 global-step:19309	 l-p:0.07808098196983337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.2444, 4.9956],
        [5.1359, 5.1354, 5.1359],
        [5.1359, 5.2145, 4.9513],
        [5.1359, 4.8890, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.22166453301906586 
model_pd.l_d.mean(): -19.51426124572754 
model_pd.lagr.mean(): -19.2925968170166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5476], device='cuda:0')), ('power', tensor([-20.2868], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.22166453301906586
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11328347772359848
epoch£º966	 i:2 	 global-step:19322	 l-p:0.07504792511463165
epoch£º966	 i:3 	 global-step:19323	 l-p:0.12485044449567795
epoch£º966	 i:4 	 global-step:19324	 l-p:0.1330847442150116
epoch£º966	 i:5 	 global-step:19325	 l-p:0.14639487862586975
epoch£º966	 i:6 	 global-step:19326	 l-p:0.14075011014938354
epoch£º966	 i:7 	 global-step:19327	 l-p:0.12021037191152573
epoch£º966	 i:8 	 global-step:19328	 l-p:0.1409611999988556
epoch£º966	 i:9 	 global-step:19329	 l-p:0.18042872846126556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 5.1433, 5.1435],
        [5.1435, 5.1435, 5.1435],
        [5.1435, 5.1435, 5.1435],
        [5.1435, 4.8763, 4.6397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.16339430212974548 
model_pd.l_d.mean(): -20.839494705200195 
model_pd.lagr.mean(): -20.67609977722168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4363], device='cuda:0')), ('power', tensor([-21.5128], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.16339430212974548
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14193414151668549
epoch£º967	 i:2 	 global-step:19342	 l-p:0.15917624533176422
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12306054681539536
epoch£º967	 i:4 	 global-step:19344	 l-p:0.11287754774093628
epoch£º967	 i:5 	 global-step:19345	 l-p:0.13070951402187347
epoch£º967	 i:6 	 global-step:19346	 l-p:0.16579899191856384
epoch£º967	 i:7 	 global-step:19347	 l-p:0.1648704558610916
epoch£º967	 i:8 	 global-step:19348	 l-p:0.165334090590477
epoch£º967	 i:9 	 global-step:19349	 l-p:0.12741465866565704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1314, 5.0671, 4.7506],
        [5.1314, 4.9800, 4.6546],
        [5.1314, 5.0457, 5.0983],
        [5.1314, 4.8640, 4.6222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.09054890275001526 
model_pd.l_d.mean(): -20.287399291992188 
model_pd.lagr.mean(): -20.196849822998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5098], device='cuda:0')), ('power', tensor([-21.0298], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:0.09054890275001526
epoch£º968	 i:1 	 global-step:19361	 l-p:0.1900108903646469
epoch£º968	 i:2 	 global-step:19362	 l-p:0.11256290972232819
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15418624877929688
epoch£º968	 i:4 	 global-step:19364	 l-p:0.09543237835168839
epoch£º968	 i:5 	 global-step:19365	 l-p:0.16535326838493347
epoch£º968	 i:6 	 global-step:19366	 l-p:0.10621500015258789
epoch£º968	 i:7 	 global-step:19367	 l-p:0.15008601546287537
epoch£º968	 i:8 	 global-step:19368	 l-p:0.2035427838563919
epoch£º968	 i:9 	 global-step:19369	 l-p:0.14205776154994965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1424, 4.8890, 4.6212],
        [5.1424, 5.2113, 4.9432],
        [5.1424, 4.9903, 4.6656],
        [5.1424, 5.0199, 5.0774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.1553981453180313 
model_pd.l_d.mean(): -20.95016098022461 
model_pd.lagr.mean(): -20.794763565063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3921], device='cuda:0')), ('power', tensor([-21.5796], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.1553981453180313
epoch£º969	 i:1 	 global-step:19381	 l-p:0.13262714445590973
epoch£º969	 i:2 	 global-step:19382	 l-p:0.11779826134443283
epoch£º969	 i:3 	 global-step:19383	 l-p:0.12027816474437714
epoch£º969	 i:4 	 global-step:19384	 l-p:0.14383690059185028
epoch£º969	 i:5 	 global-step:19385	 l-p:0.20303528010845184
epoch£º969	 i:6 	 global-step:19386	 l-p:0.12424060702323914
epoch£º969	 i:7 	 global-step:19387	 l-p:0.1359456479549408
epoch£º969	 i:8 	 global-step:19388	 l-p:0.19971957802772522
epoch£º969	 i:9 	 global-step:19389	 l-p:0.0984523817896843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1410, 5.1409, 5.1410],
        [5.1410, 4.9125, 4.9057],
        [5.1410, 4.9886, 5.0416],
        [5.1410, 4.8699, 4.7634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.12217036634683609 
model_pd.l_d.mean(): -20.658185958862305 
model_pd.lagr.mean(): -20.5360164642334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.3345], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.12217036634683609
epoch£º970	 i:1 	 global-step:19401	 l-p:0.116269052028656
epoch£º970	 i:2 	 global-step:19402	 l-p:0.18642058968544006
epoch£º970	 i:3 	 global-step:19403	 l-p:0.1904069036245346
epoch£º970	 i:4 	 global-step:19404	 l-p:0.1362656056880951
epoch£º970	 i:5 	 global-step:19405	 l-p:0.10932125151157379
epoch£º970	 i:6 	 global-step:19406	 l-p:0.12026709318161011
epoch£º970	 i:7 	 global-step:19407	 l-p:0.12678897380828857
epoch£º970	 i:8 	 global-step:19408	 l-p:0.13736644387245178
epoch£º970	 i:9 	 global-step:19409	 l-p:0.11136471480131149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 5.0704, 5.1247],
        [5.1630, 5.0749, 5.1281],
        [5.1630, 5.1622, 5.1630],
        [5.1630, 4.8902, 4.6815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.10282374918460846 
model_pd.l_d.mean(): -20.756542205810547 
model_pd.lagr.mean(): -20.653718948364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150], device='cuda:0')), ('power', tensor([-21.4072], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.10282374918460846
epoch£º971	 i:1 	 global-step:19421	 l-p:0.1375054270029068
epoch£º971	 i:2 	 global-step:19422	 l-p:0.09130407124757767
epoch£º971	 i:3 	 global-step:19423	 l-p:0.1408693939447403
epoch£º971	 i:4 	 global-step:19424	 l-p:0.14652065932750702
epoch£º971	 i:5 	 global-step:19425	 l-p:0.16055503487586975
epoch£º971	 i:6 	 global-step:19426	 l-p:0.18295902013778687
epoch£º971	 i:7 	 global-step:19427	 l-p:0.1086231991648674
epoch£º971	 i:8 	 global-step:19428	 l-p:0.15084493160247803
epoch£º971	 i:9 	 global-step:19429	 l-p:0.14022406935691833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1505, 5.0830, 5.1291],
        [5.1505, 5.1489, 5.1505],
        [5.1505, 4.8987, 4.8494],
        [5.1505, 5.1476, 5.1504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.15696951746940613 
model_pd.l_d.mean(): -19.964580535888672 
model_pd.lagr.mean(): -19.8076114654541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5259], device='cuda:0')), ('power', tensor([-20.7200], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.15696951746940613
epoch£º972	 i:1 	 global-step:19441	 l-p:0.11905214190483093
epoch£º972	 i:2 	 global-step:19442	 l-p:0.14033493399620056
epoch£º972	 i:3 	 global-step:19443	 l-p:0.1451536864042282
epoch£º972	 i:4 	 global-step:19444	 l-p:0.16129274666309357
epoch£º972	 i:5 	 global-step:19445	 l-p:0.14643648266792297
epoch£º972	 i:6 	 global-step:19446	 l-p:0.19302675127983093
epoch£º972	 i:7 	 global-step:19447	 l-p:0.08323892205953598
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10889926552772522
epoch£º972	 i:9 	 global-step:19449	 l-p:0.10781785845756531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1558, 5.1422, 5.1544],
        [5.1558, 4.9430, 4.9553],
        [5.1558, 5.0446, 5.1018],
        [5.1558, 5.1198, 4.8108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.18167640268802643 
model_pd.l_d.mean(): -19.989503860473633 
model_pd.lagr.mean(): -19.80782699584961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4463], device='cuda:0')), ('power', tensor([-20.6638], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.18167640268802643
epoch£º973	 i:1 	 global-step:19461	 l-p:0.13087700307369232
epoch£º973	 i:2 	 global-step:19462	 l-p:0.07067888230085373
epoch£º973	 i:3 	 global-step:19463	 l-p:0.13444408774375916
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16250836849212646
epoch£º973	 i:5 	 global-step:19465	 l-p:0.12301213294267654
epoch£º973	 i:6 	 global-step:19466	 l-p:0.15262112021446228
epoch£º973	 i:7 	 global-step:19467	 l-p:0.1691950559616089
epoch£º973	 i:8 	 global-step:19468	 l-p:0.113986536860466
epoch£º973	 i:9 	 global-step:19469	 l-p:0.12101074308156967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1400, 4.8746, 4.7922],
        [5.1400, 5.0866, 5.1260],
        [5.1400, 4.9218, 4.9293],
        [5.1400, 5.1785, 4.8964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.14177638292312622 
model_pd.l_d.mean(): -19.24020767211914 
model_pd.lagr.mean(): -19.098430633544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5089], device='cuda:0')), ('power', tensor([-19.9703], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.14177638292312622
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12154483795166016
epoch£º974	 i:2 	 global-step:19482	 l-p:0.13463325798511505
epoch£º974	 i:3 	 global-step:19483	 l-p:0.1423078179359436
epoch£º974	 i:4 	 global-step:19484	 l-p:0.23343266546726227
epoch£º974	 i:5 	 global-step:19485	 l-p:0.13758905231952667
epoch£º974	 i:6 	 global-step:19486	 l-p:0.1320788860321045
epoch£º974	 i:7 	 global-step:19487	 l-p:0.11341163516044617
epoch£º974	 i:8 	 global-step:19488	 l-p:0.1375022828578949
epoch£º974	 i:9 	 global-step:19489	 l-p:0.17799846827983856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1265, 5.1265, 5.1265],
        [5.1265, 5.1265, 5.1265],
        [5.1265, 4.8463, 4.6531],
        [5.1265, 4.9940, 5.0515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.10938778519630432 
model_pd.l_d.mean(): -19.541278839111328 
model_pd.lagr.mean(): -19.4318904876709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-20.2499], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.10938778519630432
epoch£º975	 i:1 	 global-step:19501	 l-p:0.19876813888549805
epoch£º975	 i:2 	 global-step:19502	 l-p:0.16392908990383148
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13356660306453705
epoch£º975	 i:4 	 global-step:19504	 l-p:0.10293669253587723
epoch£º975	 i:5 	 global-step:19505	 l-p:0.15020303428173065
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10738067328929901
epoch£º975	 i:7 	 global-step:19507	 l-p:0.3274829089641571
epoch£º975	 i:8 	 global-step:19508	 l-p:0.1121448278427124
epoch£º975	 i:9 	 global-step:19509	 l-p:0.1477879136800766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 4.8412, 4.7719],
        [5.1054, 5.0793, 5.1014],
        [5.1054, 5.1413, 4.8578],
        [5.1054, 5.1055, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.24935081601142883 
model_pd.l_d.mean(): -20.543739318847656 
model_pd.lagr.mean(): -20.294387817382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-21.2541], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.24935081601142883
epoch£º976	 i:1 	 global-step:19521	 l-p:0.37886562943458557
epoch£º976	 i:2 	 global-step:19522	 l-p:0.1543235033750534
epoch£º976	 i:3 	 global-step:19523	 l-p:0.157892107963562
epoch£º976	 i:4 	 global-step:19524	 l-p:0.16325922310352325
epoch£º976	 i:5 	 global-step:19525	 l-p:0.1276215761899948
epoch£º976	 i:6 	 global-step:19526	 l-p:0.11123225092887878
epoch£º976	 i:7 	 global-step:19527	 l-p:0.1282956898212433
epoch£º976	 i:8 	 global-step:19528	 l-p:0.12253537774085999
epoch£º976	 i:9 	 global-step:19529	 l-p:0.14485369622707367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1140, 5.1140, 5.1140],
        [5.1140, 5.1140, 5.1140],
        [5.1140, 5.0180, 5.0736],
        [5.1140, 4.8589, 4.8105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.17052599787712097 
model_pd.l_d.mean(): -20.669992446899414 
model_pd.lagr.mean(): -20.499465942382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4615], device='cuda:0')), ('power', tensor([-21.3673], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.17052599787712097
epoch£º977	 i:1 	 global-step:19541	 l-p:0.18011511862277985
epoch£º977	 i:2 	 global-step:19542	 l-p:0.11505451053380966
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12406275421380997
epoch£º977	 i:4 	 global-step:19544	 l-p:0.1676628589630127
epoch£º977	 i:5 	 global-step:19545	 l-p:0.08944740891456604
epoch£º977	 i:6 	 global-step:19546	 l-p:0.11346372216939926
epoch£º977	 i:7 	 global-step:19547	 l-p:0.21563121676445007
epoch£º977	 i:8 	 global-step:19548	 l-p:0.4324895739555359
epoch£º977	 i:9 	 global-step:19549	 l-p:0.20175334811210632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1069, 5.1062, 5.1069],
        [5.1069, 5.0253, 5.0770],
        [5.1069, 5.1069, 5.1069],
        [5.1069, 4.8229, 4.6501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.13355788588523865 
model_pd.l_d.mean(): -20.315908432006836 
model_pd.lagr.mean(): -20.182350158691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-21.0389], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.13355788588523865
epoch£º978	 i:1 	 global-step:19561	 l-p:0.1670667678117752
epoch£º978	 i:2 	 global-step:19562	 l-p:0.25254660844802856
epoch£º978	 i:3 	 global-step:19563	 l-p:0.11941676586866379
epoch£º978	 i:4 	 global-step:19564	 l-p:0.14936457574367523
epoch£º978	 i:5 	 global-step:19565	 l-p:0.13267087936401367
epoch£º978	 i:6 	 global-step:19566	 l-p:0.19285644590854645
epoch£º978	 i:7 	 global-step:19567	 l-p:0.2627701163291931
epoch£º978	 i:8 	 global-step:19568	 l-p:0.08654609322547913
epoch£º978	 i:9 	 global-step:19569	 l-p:0.12165091931819916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[5.1194, 5.5254, 5.4522],
        [5.1194, 4.8419, 4.6180],
        [5.1194, 4.8675, 4.5865],
        [5.1194, 4.8453, 4.7416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.1695026010274887 
model_pd.l_d.mean(): -19.832693099975586 
model_pd.lagr.mean(): -19.663190841674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-20.6067], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.1695026010274887
epoch£º979	 i:1 	 global-step:19581	 l-p:0.11883337050676346
epoch£º979	 i:2 	 global-step:19582	 l-p:0.18382374942302704
epoch£º979	 i:3 	 global-step:19583	 l-p:0.14732956886291504
epoch£º979	 i:4 	 global-step:19584	 l-p:0.1155448853969574
epoch£º979	 i:5 	 global-step:19585	 l-p:0.12294486910104752
epoch£º979	 i:6 	 global-step:19586	 l-p:0.12298267334699631
epoch£º979	 i:7 	 global-step:19587	 l-p:0.17051145434379578
epoch£º979	 i:8 	 global-step:19588	 l-p:0.16420131921768188
epoch£º979	 i:9 	 global-step:19589	 l-p:0.12387910485267639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1496, 5.1496, 5.1496],
        [5.1496, 5.1466, 5.1495],
        [5.1496, 5.1038, 5.1390],
        [5.1496, 5.1492, 5.1496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.18955443799495697 
model_pd.l_d.mean(): -20.676332473754883 
model_pd.lagr.mean(): -20.486778259277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4326], device='cuda:0')), ('power', tensor([-21.3442], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.18955443799495697
epoch£º980	 i:1 	 global-step:19601	 l-p:0.1479811668395996
epoch£º980	 i:2 	 global-step:19602	 l-p:0.13966698944568634
epoch£º980	 i:3 	 global-step:19603	 l-p:0.0799933448433876
epoch£º980	 i:4 	 global-step:19604	 l-p:0.10599223524332047
epoch£º980	 i:5 	 global-step:19605	 l-p:0.19273631274700165
epoch£º980	 i:6 	 global-step:19606	 l-p:0.1163344532251358
epoch£º980	 i:7 	 global-step:19607	 l-p:0.14878974854946136
epoch£º980	 i:8 	 global-step:19608	 l-p:0.1261148601770401
epoch£º980	 i:9 	 global-step:19609	 l-p:0.11051946133375168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 5.0426, 5.1003],
        [5.1654, 5.1654, 5.1654],
        [5.1654, 5.1632, 5.1653],
        [5.1654, 4.9191, 4.6412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13981328904628754 
model_pd.l_d.mean(): -19.75338363647461 
model_pd.lagr.mean(): -19.613571166992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-20.4640], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13981328904628754
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13148117065429688
epoch£º981	 i:2 	 global-step:19622	 l-p:0.13235966861248016
epoch£º981	 i:3 	 global-step:19623	 l-p:0.13970954716205597
epoch£º981	 i:4 	 global-step:19624	 l-p:0.1303180307149887
epoch£º981	 i:5 	 global-step:19625	 l-p:0.1616942435503006
epoch£º981	 i:6 	 global-step:19626	 l-p:0.1563258171081543
epoch£º981	 i:7 	 global-step:19627	 l-p:0.1657988727092743
epoch£º981	 i:8 	 global-step:19628	 l-p:0.09048629552125931
epoch£º981	 i:9 	 global-step:19629	 l-p:0.1901572346687317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1276, 4.8776, 4.8390],
        [5.1276, 5.2796, 5.0517],
        [5.1276, 5.1276, 5.1276],
        [5.1276, 5.0408, 5.0940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.15057788789272308 
model_pd.l_d.mean(): -20.515331268310547 
model_pd.lagr.mean(): -20.36475372314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-21.2208], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.15057788789272308
epoch£º982	 i:1 	 global-step:19641	 l-p:0.16218630969524384
epoch£º982	 i:2 	 global-step:19642	 l-p:0.1764972060918808
epoch£º982	 i:3 	 global-step:19643	 l-p:0.15088246762752533
epoch£º982	 i:4 	 global-step:19644	 l-p:0.15418264269828796
epoch£º982	 i:5 	 global-step:19645	 l-p:0.07080309092998505
epoch£º982	 i:6 	 global-step:19646	 l-p:0.14787204563617706
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12412171065807343
epoch£º982	 i:8 	 global-step:19648	 l-p:0.19155272841453552
epoch£º982	 i:9 	 global-step:19649	 l-p:0.16951902210712433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1294, 4.9592, 5.0064],
        [5.1294, 4.9964, 5.0542],
        [5.1294, 5.0426, 5.0958],
        [5.1294, 5.1264, 5.1293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.07461729645729065 
model_pd.l_d.mean(): -20.68769073486328 
model_pd.lagr.mean(): -20.613073348999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-21.3778], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.07461729645729065
epoch£º983	 i:1 	 global-step:19661	 l-p:0.1190609335899353
epoch£º983	 i:2 	 global-step:19662	 l-p:0.14676344394683838
epoch£º983	 i:3 	 global-step:19663	 l-p:0.13413192331790924
epoch£º983	 i:4 	 global-step:19664	 l-p:0.1306082159280777
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11817654222249985
epoch£º983	 i:6 	 global-step:19666	 l-p:0.1495550274848938
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14522910118103027
epoch£º983	 i:8 	 global-step:19668	 l-p:0.24417836964130402
epoch£º983	 i:9 	 global-step:19669	 l-p:0.14807061851024628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 4.9635, 4.6419],
        [5.1490, 5.1429, 5.1487],
        [5.1490, 5.1489, 5.1490],
        [5.1490, 5.1439, 5.1488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.07898514717817307 
model_pd.l_d.mean(): -20.199962615966797 
model_pd.lagr.mean(): -20.1209774017334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.9301], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.07898514717817307
epoch£º984	 i:1 	 global-step:19681	 l-p:0.1841699331998825
epoch£º984	 i:2 	 global-step:19682	 l-p:0.11861716210842133
epoch£º984	 i:3 	 global-step:19683	 l-p:0.1383431851863861
epoch£º984	 i:4 	 global-step:19684	 l-p:0.12083423137664795
epoch£º984	 i:5 	 global-step:19685	 l-p:0.11535297334194183
epoch£º984	 i:6 	 global-step:19686	 l-p:0.11441605538129807
epoch£º984	 i:7 	 global-step:19687	 l-p:0.1799386739730835
epoch£º984	 i:8 	 global-step:19688	 l-p:0.20423468947410583
epoch£º984	 i:9 	 global-step:19689	 l-p:0.14326414465904236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1382, 5.5255, 5.4388],
        [5.1382, 4.9680, 5.0152],
        [5.1382, 5.1773, 4.8943],
        [5.1382, 5.1382, 5.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.18652690947055817 
model_pd.l_d.mean(): -20.447893142700195 
model_pd.lagr.mean(): -20.26136589050293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4666], device='cuda:0')), ('power', tensor([-21.1479], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.18652690947055817
epoch£º985	 i:1 	 global-step:19701	 l-p:0.12820757925510406
epoch£º985	 i:2 	 global-step:19702	 l-p:0.14608438313007355
epoch£º985	 i:3 	 global-step:19703	 l-p:0.15914037823677063
epoch£º985	 i:4 	 global-step:19704	 l-p:0.16129814088344574
epoch£º985	 i:5 	 global-step:19705	 l-p:0.11381092667579651
epoch£º985	 i:6 	 global-step:19706	 l-p:0.13383489847183228
epoch£º985	 i:7 	 global-step:19707	 l-p:0.12929058074951172
epoch£º985	 i:8 	 global-step:19708	 l-p:0.1867314726114273
epoch£º985	 i:9 	 global-step:19709	 l-p:0.08840609341859818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 5.1453, 5.1453],
        [5.1453, 5.0533, 5.1079],
        [5.1453, 5.1409, 5.1451],
        [5.1453, 5.0157, 5.0736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.09699798375368118 
model_pd.l_d.mean(): -20.517024993896484 
model_pd.lagr.mean(): -20.420026779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-21.2116], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.09699798375368118
epoch£º986	 i:1 	 global-step:19721	 l-p:0.16286788880825043
epoch£º986	 i:2 	 global-step:19722	 l-p:0.11866021156311035
epoch£º986	 i:3 	 global-step:19723	 l-p:0.09192842245101929
epoch£º986	 i:4 	 global-step:19724	 l-p:0.15715640783309937
epoch£º986	 i:5 	 global-step:19725	 l-p:0.2106492519378662
epoch£º986	 i:6 	 global-step:19726	 l-p:0.17883075773715973
epoch£º986	 i:7 	 global-step:19727	 l-p:0.13622981309890747
epoch£º986	 i:8 	 global-step:19728	 l-p:0.10316972434520721
epoch£º986	 i:9 	 global-step:19729	 l-p:0.10740400850772858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 5.1715, 5.1715],
        [5.1715, 5.1715, 5.1715],
        [5.1715, 5.1228, 5.1596],
        [5.1715, 5.0851, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11545506864786148 
model_pd.l_d.mean(): -19.377094268798828 
model_pd.lagr.mean(): -19.261638641357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.0843], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11545506864786148
epoch£º987	 i:1 	 global-step:19741	 l-p:0.15505561232566833
epoch£º987	 i:2 	 global-step:19742	 l-p:0.039117518812417984
epoch£º987	 i:3 	 global-step:19743	 l-p:0.0938907116651535
epoch£º987	 i:4 	 global-step:19744	 l-p:0.1523861289024353
epoch£º987	 i:5 	 global-step:19745	 l-p:0.14408904314041138
epoch£º987	 i:6 	 global-step:19746	 l-p:0.1325518935918808
epoch£º987	 i:7 	 global-step:19747	 l-p:0.17346526682376862
epoch£º987	 i:8 	 global-step:19748	 l-p:0.16134783625602722
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14501088857650757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 4.8893, 4.6747],
        [5.1651, 4.8880, 4.7545],
        [5.1651, 5.1647, 5.1651],
        [5.1651, 5.1651, 5.1651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.1667872965335846 
model_pd.l_d.mean(): -20.750755310058594 
model_pd.lagr.mean(): -20.583967208862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4121], device='cuda:0')), ('power', tensor([-21.3984], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.1667872965335846
epoch£º988	 i:1 	 global-step:19761	 l-p:0.11093162000179291
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13079428672790527
epoch£º988	 i:3 	 global-step:19763	 l-p:0.14944706857204437
epoch£º988	 i:4 	 global-step:19764	 l-p:0.17139792442321777
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11664973944425583
epoch£º988	 i:6 	 global-step:19766	 l-p:0.0954466238617897
epoch£º988	 i:7 	 global-step:19767	 l-p:0.183211088180542
epoch£º988	 i:8 	 global-step:19768	 l-p:0.158328115940094
epoch£º988	 i:9 	 global-step:19769	 l-p:0.1110481545329094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1423, 4.9317, 4.9498],
        [5.1423, 5.1391, 5.1422],
        [5.1423, 5.0124, 5.0704],
        [5.1423, 5.1423, 5.1423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.1319424957036972 
model_pd.l_d.mean(): -20.77396583557129 
model_pd.lagr.mean(): -20.64202308654785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4124], device='cuda:0')), ('power', tensor([-21.4222], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.1319424957036972
epoch£º989	 i:1 	 global-step:19781	 l-p:0.20026789605617523
epoch£º989	 i:2 	 global-step:19782	 l-p:0.1445745825767517
epoch£º989	 i:3 	 global-step:19783	 l-p:0.07610857486724854
epoch£º989	 i:4 	 global-step:19784	 l-p:0.17356643080711365
epoch£º989	 i:5 	 global-step:19785	 l-p:0.17278456687927246
epoch£º989	 i:6 	 global-step:19786	 l-p:0.13043944537639618
epoch£º989	 i:7 	 global-step:19787	 l-p:0.16229285299777985
epoch£º989	 i:8 	 global-step:19788	 l-p:0.07715766131877899
epoch£º989	 i:9 	 global-step:19789	 l-p:0.13537299633026123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[5.1651, 5.4543, 5.3039],
        [5.1651, 4.8953, 4.7997],
        [5.1651, 5.0019, 4.6759],
        [5.1651, 5.4798, 5.3453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.1016705334186554 
model_pd.l_d.mean(): -19.173385620117188 
model_pd.lagr.mean(): -19.071714401245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5905], device='cuda:0')), ('power', tensor([-19.9861], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:0.1016705334186554
epoch£º990	 i:1 	 global-step:19801	 l-p:0.13819999992847443
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12101166695356369
epoch£º990	 i:3 	 global-step:19803	 l-p:0.131954625248909
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11125129461288452
epoch£º990	 i:5 	 global-step:19805	 l-p:0.15191854536533356
epoch£º990	 i:6 	 global-step:19806	 l-p:0.1447071135044098
epoch£º990	 i:7 	 global-step:19807	 l-p:0.1411392241716385
epoch£º990	 i:8 	 global-step:19808	 l-p:0.14353391528129578
epoch£º990	 i:9 	 global-step:19809	 l-p:0.0725051686167717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1952, 5.1452, 5.1827],
        [5.1952, 4.9194, 4.7740],
        [5.1952, 5.1153, 5.1662],
        [5.1952, 5.1911, 5.1950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.132228821516037 
model_pd.l_d.mean(): -20.904199600219727 
model_pd.lagr.mean(): -20.771970748901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3825], device='cuda:0')), ('power', tensor([-21.5233], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.132228821516037
epoch£º991	 i:1 	 global-step:19821	 l-p:0.05265476182103157
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13343694806098938
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15614274144172668
epoch£º991	 i:4 	 global-step:19824	 l-p:0.1275663524866104
epoch£º991	 i:5 	 global-step:19825	 l-p:0.11900173127651215
epoch£º991	 i:6 	 global-step:19826	 l-p:0.14136245846748352
epoch£º991	 i:7 	 global-step:19827	 l-p:0.16663482785224915
epoch£º991	 i:8 	 global-step:19828	 l-p:0.10746145248413086
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10920173674821854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1759, 5.1758, 5.1759],
        [5.1759, 5.1612, 5.1744],
        [5.1759, 5.2262, 4.9472],
        [5.1759, 5.0267, 4.6998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.11617062985897064 
model_pd.l_d.mean(): -20.66840362548828 
model_pd.lagr.mean(): -20.55223274230957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194], device='cuda:0')), ('power', tensor([-21.3226], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.11617062985897064
epoch£º992	 i:1 	 global-step:19841	 l-p:0.15476982295513153
epoch£º992	 i:2 	 global-step:19842	 l-p:0.13361044228076935
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10765004903078079
epoch£º992	 i:4 	 global-step:19844	 l-p:0.16494250297546387
epoch£º992	 i:5 	 global-step:19845	 l-p:0.10486762970685959
epoch£º992	 i:6 	 global-step:19846	 l-p:0.1620296686887741
epoch£º992	 i:7 	 global-step:19847	 l-p:0.12974794209003448
epoch£º992	 i:8 	 global-step:19848	 l-p:0.11078678071498871
epoch£º992	 i:9 	 global-step:19849	 l-p:0.14110492169857025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.2631, 5.0102],
        [5.1568, 4.8759, 4.6840],
        [5.1568, 5.1567, 5.1568],
        [5.1568, 5.1458, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.2544146180152893 
model_pd.l_d.mean(): -20.50183868408203 
model_pd.lagr.mean(): -20.24742317199707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.2057], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.2544146180152893
epoch£º993	 i:1 	 global-step:19861	 l-p:0.15104304254055023
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12219409644603729
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15201129019260406
epoch£º993	 i:4 	 global-step:19864	 l-p:0.09713198244571686
epoch£º993	 i:5 	 global-step:19865	 l-p:0.09867950528860092
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12213700264692307
epoch£º993	 i:7 	 global-step:19867	 l-p:0.13122329115867615
epoch£º993	 i:8 	 global-step:19868	 l-p:0.1378566175699234
epoch£º993	 i:9 	 global-step:19869	 l-p:0.13090617954730988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1269, 5.1220, 5.1266],
        [5.1269, 5.1542, 4.8653],
        [5.1269, 4.8887, 4.5887],
        [5.1269, 5.1267, 5.1269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13306023180484772 
model_pd.l_d.mean(): -20.369945526123047 
model_pd.lagr.mean(): -20.23688507080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.0786], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13306023180484772
epoch£º994	 i:1 	 global-step:19881	 l-p:0.11240784078836441
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11689607053995132
epoch£º994	 i:3 	 global-step:19883	 l-p:0.14688822627067566
epoch£º994	 i:4 	 global-step:19884	 l-p:0.1248619332909584
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13185136020183563
epoch£º994	 i:6 	 global-step:19886	 l-p:0.6408470869064331
epoch£º994	 i:7 	 global-step:19887	 l-p:0.201161190867424
epoch£º994	 i:8 	 global-step:19888	 l-p:0.1878579705953598
epoch£º994	 i:9 	 global-step:19889	 l-p:0.17525771260261536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1049, 4.9846, 5.0437],
        [5.1049, 4.9057, 4.5805],
        [5.1049, 4.9072, 4.9397],
        [5.1049, 5.1028, 5.1049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.17619766294956207 
model_pd.l_d.mean(): -20.727426528930664 
model_pd.lagr.mean(): -20.55122947692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.4142], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.17619766294956207
epoch£º995	 i:1 	 global-step:19901	 l-p:0.11298149824142456
epoch£º995	 i:2 	 global-step:19902	 l-p:0.12785398960113525
epoch£º995	 i:3 	 global-step:19903	 l-p:0.2697378993034363
epoch£º995	 i:4 	 global-step:19904	 l-p:0.15667571127414703
epoch£º995	 i:5 	 global-step:19905	 l-p:0.30131518840789795
epoch£º995	 i:6 	 global-step:19906	 l-p:0.11961852014064789
epoch£º995	 i:7 	 global-step:19907	 l-p:0.08660300821065903
epoch£º995	 i:8 	 global-step:19908	 l-p:0.1539047807455063
epoch£º995	 i:9 	 global-step:19909	 l-p:0.12331220507621765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1233, 4.9019, 4.9107],
        [5.1233, 5.4373, 5.3027],
        [5.1233, 4.8378, 4.6492],
        [5.1233, 5.1226, 5.1233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.11328071355819702 
model_pd.l_d.mean(): -18.666675567626953 
model_pd.lagr.mean(): -18.553394317626953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5984], device='cuda:0')), ('power', tensor([-19.4819], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.11328071355819702
epoch£º996	 i:1 	 global-step:19921	 l-p:0.2522633373737335
epoch£º996	 i:2 	 global-step:19922	 l-p:0.14037546515464783
epoch£º996	 i:3 	 global-step:19923	 l-p:0.14995336532592773
epoch£º996	 i:4 	 global-step:19924	 l-p:0.17481562495231628
epoch£º996	 i:5 	 global-step:19925	 l-p:0.14341911673545837
epoch£º996	 i:6 	 global-step:19926	 l-p:0.12892499566078186
epoch£º996	 i:7 	 global-step:19927	 l-p:0.13988302648067474
epoch£º996	 i:8 	 global-step:19928	 l-p:0.1280684471130371
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11917903274297714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1461, 5.0682, 5.1187],
        [5.1461, 5.1461, 5.1461],
        [5.1461, 4.8656, 4.7319],
        [5.1461, 5.2185, 4.9490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.1521589308977127 
model_pd.l_d.mean(): -19.938447952270508 
model_pd.lagr.mean(): -19.78628921508789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-20.7087], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.1521589308977127
epoch£º997	 i:1 	 global-step:19941	 l-p:0.1400430053472519
epoch£º997	 i:2 	 global-step:19942	 l-p:0.11582314223051071
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11019115895032883
epoch£º997	 i:4 	 global-step:19944	 l-p:0.16991133987903595
epoch£º997	 i:5 	 global-step:19945	 l-p:0.19092096388339996
epoch£º997	 i:6 	 global-step:19946	 l-p:0.14234067499637604
epoch£º997	 i:7 	 global-step:19947	 l-p:0.1310427188873291
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10947597026824951
epoch£º997	 i:9 	 global-step:19949	 l-p:0.1960211545228958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1358, 4.9574, 4.6295],
        [5.1358, 5.1341, 5.1357],
        [5.1358, 5.1358, 5.1358],
        [5.1358, 5.0978, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.13573120534420013 
model_pd.l_d.mean(): -21.030546188354492 
model_pd.lagr.mean(): -20.89481544494629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3795], device='cuda:0')), ('power', tensor([-21.6479], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.13573120534420013
epoch£º998	 i:1 	 global-step:19961	 l-p:0.08202379941940308
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15670783817768097
epoch£º998	 i:3 	 global-step:19963	 l-p:0.21024377644062042
epoch£º998	 i:4 	 global-step:19964	 l-p:0.12840591371059418
epoch£º998	 i:5 	 global-step:19965	 l-p:0.14692430198192596
epoch£º998	 i:6 	 global-step:19966	 l-p:0.16173815727233887
epoch£º998	 i:7 	 global-step:19967	 l-p:0.10403855890035629
epoch£º998	 i:8 	 global-step:19968	 l-p:0.16620244085788727
epoch£º998	 i:9 	 global-step:19969	 l-p:0.1483619660139084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1494, 5.1492, 5.1494],
        [5.1494, 4.9502, 4.9796],
        [5.1494, 5.1465, 5.1493],
        [5.1494, 5.1449, 5.1492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.15124228596687317 
model_pd.l_d.mean(): -18.051849365234375 
model_pd.lagr.mean(): -17.900606155395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6050], device='cuda:0')), ('power', tensor([-18.8671], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.15124228596687317
epoch£º999	 i:1 	 global-step:19981	 l-p:0.0996624082326889
epoch£º999	 i:2 	 global-step:19982	 l-p:0.1783340722322464
epoch£º999	 i:3 	 global-step:19983	 l-p:0.12592686712741852
epoch£º999	 i:4 	 global-step:19984	 l-p:0.12161126732826233
epoch£º999	 i:5 	 global-step:19985	 l-p:0.1466679871082306
epoch£º999	 i:6 	 global-step:19986	 l-p:0.12981289625167847
epoch£º999	 i:7 	 global-step:19987	 l-p:0.12682890892028809
epoch£º999	 i:8 	 global-step:19988	 l-p:0.17816562950611115
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11465569585561752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1583, 5.0711, 5.1246],
        [5.1583, 4.9313, 4.6260],
        [5.1583, 5.0659, 5.1207],
        [5.1583, 5.1942, 4.9083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.12314590811729431 
model_pd.l_d.mean(): -20.521060943603516 
model_pd.lagr.mean(): -20.39791488647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.1959], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.12314590811729431
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12425035983324051
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.13568519055843353
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.1637338250875473
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.13575628399848938
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.131236732006073
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13882742822170258
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.09382545202970505
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.21531325578689575
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.1424478441476822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1440, 5.1429, 5.1440],
        [5.1440, 4.8928, 4.8550],
        [5.1440, 4.9806, 5.0316],
        [5.1440, 5.1440, 5.1440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.10539910197257996 
model_pd.l_d.mean(): -19.466094970703125 
model_pd.lagr.mean(): -19.36069679260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-20.1713], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.10539910197257996
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.19420625269412994
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.1420966237783432
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.21478775143623352
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.08842513710260391
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.08499858528375626
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.13112203776836395
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.17266501486301422
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.19037950038909912
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.10811225324869156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1390, 5.1173, 5.1361],
        [5.1390, 4.9174, 4.9255],
        [5.1390, 5.5081, 5.4079],
        [5.1390, 5.0050, 5.0632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.10561338067054749 
model_pd.l_d.mean(): -20.509733200073242 
model_pd.lagr.mean(): -20.40411949157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.1995], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.10561338067054749
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.22033649682998657
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.1313643902540207
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.11197710782289505
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.10729780048131943
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.20753227174282074
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.1490321010351181
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.13253732025623322
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.1406634896993637
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.12994354963302612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1447, 4.9751, 4.6459],
        [5.1447, 5.1447, 5.1447],
        [5.1447, 5.1447, 5.1447],
        [5.1447, 5.1025, 5.1355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.12635456025600433 
model_pd.l_d.mean(): -19.74818229675293 
model_pd.lagr.mean(): -19.621828079223633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5522], device='cuda:0')), ('power', tensor([-20.5281], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.12635456025600433
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.14532619714736938
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.15936654806137085
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.2072920799255371
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.12833955883979797
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.1437963992357254
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.1156410202383995
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.10925135016441345
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.1400313377380371
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.1539149433374405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1340, 4.8495, 4.6480],
        [5.1340, 5.1251, 4.8213],
        [5.1340, 4.8886, 4.8636],
        [5.1340, 5.1337, 5.1340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.17705745995044708 
model_pd.l_d.mean(): -20.308259963989258 
model_pd.lagr.mean(): -20.131202697753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5050], device='cuda:0')), ('power', tensor([-21.0460], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.17705745995044708
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.17401939630508423
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.13574762642383575
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.13671229779720306
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.13084274530410767
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.12145337462425232
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.10628636926412582
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.14155834913253784
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.16401736438274384
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.23458518087863922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.0118, 5.0704],
        [5.1253, 4.8775, 4.8494],
        [5.1253, 5.1239, 5.1253],
        [5.1253, 4.9532, 5.0011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.1289510279893875 
model_pd.l_d.mean(): -20.521909713745117 
model_pd.lagr.mean(): -20.392959594726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-21.1927], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.1289510279893875
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11528970301151276
epoch£º1005	 i:2 	 global-step:20102	 l-p:0.1452120691537857
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.1861196905374527
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.09907987713813782
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.20497936010360718
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.1865207999944687
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.15178817510604858
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.10941897332668304
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.225508913397789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[5.1242, 5.4141, 5.2638],
        [5.1242, 4.9502, 4.6194],
        [5.1242, 5.2660, 5.0308],
        [5.1242, 5.2087, 4.9447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.16297268867492676 
model_pd.l_d.mean(): -19.133468627929688 
model_pd.lagr.mean(): -18.970495223999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6090], device='cuda:0')), ('power', tensor([-19.9646], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.16297268867492676
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.10698252171278
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.1676914542913437
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.10646125674247742
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.1686786264181137
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.1727021187543869
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.12152467668056488
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.1476891040802002
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.14912189543247223
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.1941836178302765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1355, 5.0800, 5.1207],
        [5.1355, 5.1348, 5.1355],
        [5.1355, 5.1354, 5.1355],
        [5.1355, 5.1135, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.14790686964988708 
model_pd.l_d.mean(): -20.030733108520508 
model_pd.lagr.mean(): -19.88282585144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5352], device='cuda:0')), ('power', tensor([-20.7963], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.14790686964988708
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.23084144294261932
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.13002346456050873
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.1178799495100975
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.14358578622341156
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11607485264539719
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.16471366584300995
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.118284210562706
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13304591178894043
epoch£º1007	 i:9 	 global-step:20149	 l-p:0.15103432536125183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1406, 4.9436, 4.9757],
        [5.1406, 5.1066, 5.1343],
        [5.1406, 4.9484, 4.6234],
        [5.1406, 4.8916, 4.6020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.0921783447265625 
model_pd.l_d.mean(): -20.67404556274414 
model_pd.lagr.mean(): -20.581867218017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-21.3688], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.0921783447265625
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.1773516982793808
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.10601251572370529
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.18278566002845764
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.15346957743167877
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.17689678072929382
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.10556035488843918
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.14058427512645721
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.1397075653076172
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.14284977316856384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 5.1528, 5.1556],
        [5.1557, 5.1557, 5.1557],
        [5.1557, 4.9670, 5.0043],
        [5.1557, 5.0352, 5.0938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.17776091396808624 
model_pd.l_d.mean(): -20.763017654418945 
model_pd.lagr.mean(): -20.585256576538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4314], device='cuda:0')), ('power', tensor([-21.4306], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.17776091396808624
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.1174311414361
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.15225204825401306
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.1063411682844162
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.1809423416852951
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11407796293497086
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.1378805935382843
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12303812056779861
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.08670016378164291
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.16163325309753418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 5.0097, 4.6800],
        [5.1636, 5.1635, 5.1636],
        [5.1636, 5.1636, 5.1636],
        [5.1636, 5.1417, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.1248532235622406 
model_pd.l_d.mean(): -20.134071350097656 
model_pd.lagr.mean(): -20.009218215942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.8516], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.1248532235622406
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.1521773487329483
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.06900104880332947
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.1847410500049591
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.18981273472309113
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12480434030294418
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.11390233039855957
epoch£º1010	 i:7 	 global-step:20207	 l-p:0.11389293521642685
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.0975276529788971
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.185940220952034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1573, 5.2310, 4.9614],
        [5.1573, 5.2610, 5.0059],
        [5.1573, 5.2510, 4.9909],
        [5.1573, 5.1568, 5.1573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.2190295308828354 
model_pd.l_d.mean(): -18.419795989990234 
model_pd.lagr.mean(): -18.20076560974121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6154], device='cuda:0')), ('power', tensor([-19.2496], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.2190295308828354
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.09241607785224915
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.14593130350112915
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.11724918335676193
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.10419301688671112
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.14750327169895172
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.13371406495571136
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.1411886364221573
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12447283416986465
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.13991300761699677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 5.0033, 5.0572],
        [5.1579, 5.1532, 5.1576],
        [5.1579, 4.8779, 4.6621],
        [5.1579, 5.0419, 5.1004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.13138872385025024 
model_pd.l_d.mean(): -20.757822036743164 
model_pd.lagr.mean(): -20.626432418823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-21.4159], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.13138872385025024
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.14350932836532593
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.18494001030921936
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.1875135451555252
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.1246475800871849
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.11339816451072693
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.13700167834758759
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.1474030315876007
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.13187581300735474
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.08522593975067139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.0374, 4.7075],
        [5.1490, 5.1490, 5.1490],
        [5.1490, 4.8648, 4.7043],
        [5.1490, 4.8655, 4.6666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.1484231948852539 
model_pd.l_d.mean(): -20.617568969726562 
model_pd.lagr.mean(): -20.469146728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4573], device='cuda:0')), ('power', tensor([-21.3100], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.1484231948852539
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.13306723535060883
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12553292512893677
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.22822095453739166
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.10846830159425735
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.11816847324371338
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.10371245443820953
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.153483584523201
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.16835102438926697
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.11695916205644608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1430, 5.5787, 5.5225],
        [5.1430, 5.1426, 5.1430],
        [5.1430, 5.1430, 5.1430],
        [5.1430, 4.8582, 4.6657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.14294122159481049 
model_pd.l_d.mean(): -19.904132843017578 
model_pd.lagr.mean(): -19.761192321777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-20.6178], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.14294122159481049
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.20319339632987976
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.11740940064191818
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.09878982603549957
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.12728850543498993
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.20126298069953918
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12372100353240967
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.20306624472141266
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.09844222664833069
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.15891189873218536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1260, 4.9694, 4.6364],
        [5.1260, 5.1200, 5.1256],
        [5.1260, 5.1240, 5.1259],
        [5.1260, 5.4944, 5.3937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.1548592895269394 
model_pd.l_d.mean(): -20.198266983032227 
model_pd.lagr.mean(): -20.043407440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5142], device='cuda:0')), ('power', tensor([-20.9442], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.1548592895269394
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.13960877060890198
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.12185899168252945
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.15751080214977264
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.103573739528656
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.23646321892738342
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.16276365518569946
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.1528758704662323
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.31062179803848267
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.12328948825597763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1129, 5.1125, 5.1129],
        [5.1129, 5.1129, 5.1129],
        [5.1129, 5.0568, 5.0979],
        [5.1129, 5.1113, 5.1128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.145308718085289 
model_pd.l_d.mean(): -19.087158203125 
model_pd.lagr.mean(): -18.941848754882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4962], device='cuda:0')), ('power', tensor([-19.8026], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.145308718085289
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.2539292871952057
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.10903781652450562
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.1452103704214096
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.10481604933738708
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.10660915821790695
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.30523669719696045
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.15746816992759705
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.213003471493721
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.10497260838747025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1166, 5.1166, 5.1166],
        [5.1166, 5.0249, 5.0799],
        [5.1166, 4.8826, 4.8776],
        [5.1166, 5.1166, 5.1166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.1164148673415184 
model_pd.l_d.mean(): -20.14793586730957 
model_pd.lagr.mean(): -20.03152084350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-20.8841], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.1164148673415184
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.13037726283073425
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.13079532980918884
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.3528567850589752
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.1474684178829193
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.11780929565429688
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.13073740899562836
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.1766408085823059
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.21055136620998383
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.16312400996685028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1109, 5.1062, 5.1106],
        [5.1109, 5.0286, 5.0809],
        [5.1109, 5.0797, 4.7674],
        [5.1109, 4.8341, 4.7403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.1215929314494133 
model_pd.l_d.mean(): -19.655139923095703 
model_pd.lagr.mean(): -19.533546447753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5111], device='cuda:0')), ('power', tensor([-20.3920], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.1215929314494133
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.4536148011684418
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.14521001279354095
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.17064103484153748
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.1342547982931137
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.09759335219860077
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.19552834331989288
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.13871580362319946
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.12441829591989517
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.12905225157737732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1165, 5.1058, 5.1156],
        [5.1165, 4.8503, 4.7862],
        [5.1165, 5.1378, 4.8451],
        [5.1165, 5.5142, 5.4329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13998232781887054 
model_pd.l_d.mean(): -20.54731559753418 
model_pd.lagr.mean(): -20.407333374023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-21.2183], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13998232781887054
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.2010422796010971
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.2011466920375824
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.14193879067897797
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.1330244392156601
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.26713472604751587
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.12250934541225433
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.1804424375295639
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.09068894386291504
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.11889990419149399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1235, 4.8580, 4.5856],
        [5.1235, 5.0331, 5.0878],
        [5.1235, 4.8360, 4.6433],
        [5.1235, 4.9851, 4.6513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.137735053896904 
model_pd.l_d.mean(): -20.482912063598633 
model_pd.lagr.mean(): -20.345176696777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-21.1849], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.137735053896904
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.23738542199134827
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.16831178963184357
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.2589889466762543
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.10105033963918686
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.07406110316514969
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.12838442623615265
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.16938458383083344
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.12755155563354492
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.13146604597568512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1291, 4.8435, 4.6968],
        [5.1291, 5.1291, 5.1291],
        [5.1291, 4.8542, 4.6030],
        [5.1291, 4.8703, 4.8214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12954413890838623 
model_pd.l_d.mean(): -20.746732711791992 
model_pd.lagr.mean(): -20.617189407348633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4399], device='cuda:0')), ('power', tensor([-21.4228], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12954413890838623
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.12393278628587723
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.1096385046839714
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.23530806601047516
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.1290847808122635
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.1474069505929947
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.2159581333398819
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.23805111646652222
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.10499101132154465
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.14547738432884216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1167, 5.1166, 5.1167],
        [5.1167, 4.8500, 4.5772],
        [5.1167, 4.9143, 4.9438],
        [5.1167, 5.1167, 5.1167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.14253175258636475 
model_pd.l_d.mean(): -20.69556999206543 
model_pd.lagr.mean(): -20.553037643432617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-21.3691], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.14253175258636475
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.16586217284202576
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.17498543858528137
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.1352720856666565
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.11368419229984283
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.11090467125177383
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.20731574296951294
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.15111660957336426
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.3423442244529724
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.11921656131744385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1160, 5.1154, 5.1160],
        [5.1160, 4.8354, 4.5947],
        [5.1160, 5.1160, 5.1160],
        [5.1160, 4.9485, 4.9993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.14607827365398407 
model_pd.l_d.mean(): -21.067567825317383 
model_pd.lagr.mean(): -20.921489715576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3756], device='cuda:0')), ('power', tensor([-21.6814], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.14607827365398407
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.26651880145072937
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11932429671287537
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.3198954463005066
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.17978104948997498
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.1355026662349701
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.1224055290222168
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.08914965391159058
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.12327376753091812
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.13738837838172913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1201, 5.1201, 5.1201],
        [5.1201, 5.0906, 4.7787],
        [5.1201, 5.0654, 4.7457],
        [5.1201, 5.1201, 5.1201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.21357688307762146 
model_pd.l_d.mean(): -19.72064781188965 
model_pd.lagr.mean(): -19.507070541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5445], device='cuda:0')), ('power', tensor([-20.4923], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.21357688307762146
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.13881276547908783
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.11685550212860107
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.26648855209350586
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.1294133961200714
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.09538686275482178
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.18060271441936493
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.12591354548931122
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.12180115282535553
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.17707443237304688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1266, 5.1252, 5.1266],
        [5.1266, 5.1212, 5.1263],
        [5.1266, 5.1266, 5.1266],
        [5.1266, 5.1257, 5.1266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.21420195698738098 
model_pd.l_d.mean(): -20.743715286254883 
model_pd.lagr.mean(): -20.529512405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.3973], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.21420195698738098
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.15156136453151703
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13459256291389465
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.1297684758901596
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.17503219842910767
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.12874288856983185
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.1271207630634308
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.12029580771923065
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.17457911372184753
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.1630290448665619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[5.1323, 5.5349, 5.4563],
        [5.1323, 4.8472, 4.6355],
        [5.1323, 5.1631, 4.8741],
        [5.1323, 4.8738, 4.5916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.07956968247890472 
model_pd.l_d.mean(): -19.944068908691406 
model_pd.lagr.mean(): -19.864500045776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-20.7127], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.07956968247890472
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.13045065104961395
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.1655910313129425
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.2139384001493454
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.15857447683811188
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.14324414730072021
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.14425845444202423
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.15739677846431732
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.1405474692583084
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.15694937109947205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 4.8479, 4.6730],
        [5.1351, 4.9736, 4.6409],
        [5.1351, 5.0457, 5.1000],
        [5.1351, 4.8761, 4.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.1382581740617752 
model_pd.l_d.mean(): -19.981037139892578 
model_pd.lagr.mean(): -19.8427791595459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-20.7576], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.1382581740617752
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.1357468068599701
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.15401101112365723
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.13944962620735168
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.09840467572212219
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.24246029555797577
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.1619071215391159
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.1266428679227829
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.13233156502246857
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.14386186003684998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1371, 5.1364, 5.1371],
        [5.1371, 5.1371, 5.1371],
        [5.1371, 5.1325, 5.1369],
        [5.1371, 5.1153, 5.1342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.18135683238506317 
model_pd.l_d.mean(): -20.98036003112793 
model_pd.lagr.mean(): -20.79900360107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3790], device='cuda:0')), ('power', tensor([-21.5968], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.18135683238506317
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1889926642179489
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.1126650720834732
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.14960499107837677
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.20243923366069794
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.15723279118537903
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.10571510344743729
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.12915590405464172
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.10710059851408005
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.11584862321615219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 4.8869, 4.6109],
        [5.1473, 5.1473, 5.1473],
        [5.1473, 5.1473, 5.1473],
        [5.1473, 4.9382, 4.9602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.13006733357906342 
model_pd.l_d.mean(): -20.83648681640625 
model_pd.lagr.mean(): -20.706418991088867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4025], device='cuda:0')), ('power', tensor([-21.4752], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.13006733357906342
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12841521203517914
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.08733197301626205
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.1653195172548294
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.2509768605232239
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.10682131350040436
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.1239859014749527
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.13213983178138733
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.10606339573860168
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.20842674374580383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1401, 5.1273, 5.1389],
        [5.1401, 5.1028, 5.1328],
        [5.1401, 5.2240, 4.9587],
        [5.1401, 4.9124, 4.9146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.15628120303153992 
model_pd.l_d.mean(): -20.25428581237793 
model_pd.lagr.mean(): -20.098005294799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.9760], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.15628120303153992
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.129930317401886
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.1680627465248108
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.0998394638299942
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.11055483669042587
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.16672345995903015
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12745481729507446
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.25142085552215576
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.08365853130817413
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.1677805632352829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1383, 5.4005, 5.2325],
        [5.1383, 4.9980, 4.6646],
        [5.1383, 4.8671, 4.7861],
        [5.1383, 5.1163, 5.1353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.13071933388710022 
model_pd.l_d.mean(): -19.927001953125 
model_pd.lagr.mean(): -19.796281814575195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-20.6539], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.13071933388710022
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.1649797260761261
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.1447533518075943
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.1786535233259201
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12517815828323364
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.13075067102909088
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.13188093900680542
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.0883721336722374
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.10401899367570877
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.2448095828294754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 4.9055, 4.8933],
        [5.1434, 5.0672, 5.1172],
        [5.1434, 5.1430, 5.1434],
        [5.1434, 5.0015, 5.0590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.18429702520370483 
model_pd.l_d.mean(): -20.243270874023438 
model_pd.lagr.mean(): -20.05897331237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4751], device='cuda:0')), ('power', tensor([-20.9498], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.18429702520370483
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.13130296766757965
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17508895695209503
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.14875555038452148
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.12915989756584167
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.1466042548418045
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.10521210730075836
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.17105503380298615
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.08011103421449661
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.13528606295585632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.1563, 5.1563],
        [5.1563, 4.8716, 4.7109],
        [5.1563, 4.9399, 4.9541],
        [5.1563, 5.1552, 5.1563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.07778362929821014 
model_pd.l_d.mean(): -20.55478286743164 
model_pd.lagr.mean(): -20.476999282836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4609], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.07778362929821014
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.1402120739221573
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.2043989896774292
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.16155582666397095
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.13160790503025055
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.1529032438993454
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.1297319531440735
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.1238570362329483
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.1544334888458252
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.1014135330915451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 4.9132, 4.6174],
        [5.1564, 5.1453, 5.1555],
        [5.1564, 5.1009, 5.1416],
        [5.1564, 5.1535, 5.1563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.22775991261005402 
model_pd.l_d.mean(): -20.912322998046875 
model_pd.lagr.mean(): -20.68456268310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3875], device='cuda:0')), ('power', tensor([-21.5366], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.22775991261005402
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.1375989019870758
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.1150953397154808
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.1397656798362732
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.20163528621196747
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.17980729043483734
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.062123365700244904
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.10487999022006989
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.08462096750736237
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.11490336805582047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.1514, 5.1632],
        [5.1644, 5.1557, 5.1638],
        [5.1644, 4.9038, 4.6318],
        [5.1644, 5.1459, 5.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.08629857748746872 
model_pd.l_d.mean(): -19.692384719848633 
model_pd.lagr.mean(): -19.60608673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.4031], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.08629857748746872
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.14945177733898163
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.1736203283071518
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.11658477783203125
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.10012725740671158
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.1393749713897705
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.13990673422813416
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.12571366131305695
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.1966898888349533
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11661885678768158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.1385, 5.1588],
        [5.1621, 5.6028, 5.5488],
        [5.1621, 4.9712, 5.0077],
        [5.1621, 4.9885, 5.0352]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.14349298179149628 
model_pd.l_d.mean(): -19.426517486572266 
model_pd.lagr.mean(): -19.283023834228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-20.1997], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.14349298179149628
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.15365013480186462
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.16917169094085693
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.10110212117433548
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.11494798213243484
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.08817929774522781
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.16290315985679626
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.15260015428066254
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.1304100602865219
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.15564601123332977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.1374, 5.1553],
        [5.1580, 4.9792, 5.0234],
        [5.1580, 4.8732, 4.7148],
        [5.1580, 5.1538, 5.1578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.12421766668558121 
model_pd.l_d.mean(): -18.97802734375 
model_pd.lagr.mean(): -18.853809356689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5499], device='cuda:0')), ('power', tensor([-19.7471], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.12421766668558121
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.16517461836338043
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.15617762506008148
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.0711512491106987
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.148856982588768
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.11918739974498749
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.1390431523323059
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.1742841899394989
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12784133851528168
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.1320577710866928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 4.9158, 4.8815],
        [5.1655, 5.0609, 4.7314],
        [5.1655, 4.9206, 4.8951],
        [5.1655, 5.1655, 5.1655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.1394234150648117 
model_pd.l_d.mean(): -19.810396194458008 
model_pd.lagr.mean(): -19.67097282409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4403], device='cuda:0')), ('power', tensor([-20.4766], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.1394234150648117
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.09151128679513931
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.12369956076145172
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.19847504794597626
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.08166036754846573
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.1755589246749878
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.15695731341838837
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.10957231372594833
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.16799037158489227
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.12006136029958725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.0696, 5.1241],
        [5.1597, 4.9947, 5.0454],
        [5.1597, 5.1055, 5.1456],
        [5.1597, 4.9702, 4.6442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.14680907130241394 
model_pd.l_d.mean(): -20.596282958984375 
model_pd.lagr.mean(): -20.449474334716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.2952], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.14680907130241394
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.12064065039157867
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.12050192803144455
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.1157045066356659
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.13034473359584808
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12689368426799774
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.15539036691188812
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12310583144426346
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.28616082668304443
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.09888633340597153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1409, 5.1409, 5.1409],
        [5.1409, 5.1062, 5.1344],
        [5.1409, 4.8661, 4.6145],
        [5.1409, 5.1379, 5.1408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.09285911172628403 
model_pd.l_d.mean(): -20.11309814453125 
model_pd.lagr.mean(): -20.020238876342773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5290], device='cuda:0')), ('power', tensor([-20.8733], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.09285911172628403
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13300243020057678
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.15299633145332336
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.1434870809316635
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.1131269633769989
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.1849246770143509
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.17435511946678162
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.1840514838695526
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.1386861354112625
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.1494792103767395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1406, 5.1405, 5.1406],
        [5.1406, 4.8866, 4.8480],
        [5.1406, 5.1363, 5.1404],
        [5.1406, 5.1168, 5.1372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11875508725643158 
model_pd.l_d.mean(): -18.894983291625977 
model_pd.lagr.mean(): -18.776227951049805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5570], device='cuda:0')), ('power', tensor([-19.6704], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11875508725643158
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.13489054143428802
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.11268752813339233
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.11994560062885284
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.15668784081935883
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.20929378271102905
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.13141824305057526
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.2075192928314209
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.16292662918567657
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.06921760737895966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 5.1520, 5.1521],
        [5.1521, 4.9798, 5.0277],
        [5.1521, 5.1519, 5.1521],
        [5.1521, 4.9570, 4.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.15084566175937653 
model_pd.l_d.mean(): -20.4260196685791 
model_pd.lagr.mean(): -20.27517318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.1172], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.15084566175937653
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.12191066145896912
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.1338762789964676
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11780475080013275
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.15208588540554047
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.09841789305210114
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.11463729292154312
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.1581980586051941
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.17971055209636688
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.1348487287759781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.1696, 5.1696],
        [5.1696, 5.0831, 5.1365],
        [5.1696, 5.5102, 5.3895],
        [5.1696, 4.9392, 4.9358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.16975177824497223 
model_pd.l_d.mean(): -20.596590042114258 
model_pd.lagr.mean(): -20.426837921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.2796], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.16975177824497223
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11320693790912628
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.16437587141990662
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.13534553349018097
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.1349460780620575
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.11233195662498474
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.0676773339509964
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.16014353930950165
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11431258916854858
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.14769235253334045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1734, 5.1724, 5.1734],
        [5.1734, 5.1733, 5.1734],
        [5.1734, 5.1177, 5.1585],
        [5.1734, 5.0874, 4.7609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.19757117331027985 
model_pd.l_d.mean(): -20.310590744018555 
model_pd.lagr.mean(): -20.113019943237305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4592], device='cuda:0')), ('power', tensor([-21.0016], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.19757117331027985
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.1459869146347046
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.04051589220762253
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.13434243202209473
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12401475757360458
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.1332644820213318
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.1739945411682129
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.11308203637599945
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.10663922876119614
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.14877533912658691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1698, 5.4737, 5.3299],
        [5.1698, 5.0789, 5.1335],
        [5.1698, 5.1637, 5.1694],
        [5.1698, 4.8907, 4.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.15126000344753265 
model_pd.l_d.mean(): -19.026655197143555 
model_pd.lagr.mean(): -18.875394821166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5330], device='cuda:0')), ('power', tensor([-19.7790], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.15126000344753265
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.17099496722221375
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.11971279233694077
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.1999925822019577
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11349750310182571
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.14530712366104126
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.08450338244438171
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.13829992711544037
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.09142705798149109
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.11269983649253845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 4.8951, 4.7872],
        [5.1717, 5.1710, 5.1717],
        [5.1717, 5.0030, 4.6732],
        [5.1717, 5.1717, 5.1717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.10094558447599411 
model_pd.l_d.mean(): -20.341962814331055 
model_pd.lagr.mean(): -20.241016387939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4656], device='cuda:0')), ('power', tensor([-21.0398], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.10094558447599411
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.11599395424127579
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.09107417613267899
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.1597876250743866
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.12428441643714905
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.11540677398443222
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.14039941132068634
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.15430931746959686
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.21066203713417053
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12936022877693176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.0150, 5.0714],
        [5.1621, 5.1356, 5.1580],
        [5.1621, 4.8770, 4.7185],
        [5.1621, 5.0933, 5.1403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.10845372080802917 
model_pd.l_d.mean(): -19.975086212158203 
model_pd.lagr.mean(): -19.86663246154785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4654], device='cuda:0')), ('power', tensor([-20.6688], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.10845372080802917
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.2008119523525238
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.1300218105316162
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.1420741081237793
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.1130467876791954
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.20315951108932495
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.11672500520944595
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.144880011677742
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.09885936230421066
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.11418040841817856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1577, 5.3088, 5.0767],
        [5.1577, 4.9779, 5.0220],
        [5.1577, 5.1577, 5.1577],
        [5.1577, 4.8898, 4.6263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.08620958775281906 
model_pd.l_d.mean(): -19.883142471313477 
model_pd.lagr.mean(): -19.796932220458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5215], device='cuda:0')), ('power', tensor([-20.6331], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.08620958775281906
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.08265583217144012
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14693205058574677
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.1315089762210846
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.12032222747802734
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.2317875623703003
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11356071382761002
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.19963672757148743
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.12613138556480408
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.15818066895008087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1500, 5.0746, 5.1244],
        [5.1500, 4.9740, 4.6431],
        [5.1500, 5.5318, 5.4383],
        [5.1500, 4.9581, 4.9949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12861473858356476 
model_pd.l_d.mean(): -19.475934982299805 
model_pd.lagr.mean(): -19.347320556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5213], device='cuda:0')), ('power', tensor([-20.2212], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12861473858356476
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.118891142308712
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.12866462767124176
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.1577465981245041
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.18412373960018158
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.1081242635846138
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.15066911280155182
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.15583883225917816
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.13313525915145874
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.1416986584663391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1515, 4.8802, 4.7988],
        [5.1515, 5.1061, 5.1412],
        [5.1515, 5.1501, 5.1515],
        [5.1515, 5.1141, 5.1442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.13262853026390076 
model_pd.l_d.mean(): -20.877822875976562 
model_pd.lagr.mean(): -20.745193481445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-21.5299], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.13262853026390076
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.11085334420204163
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11334560066461563
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.14739882946014404
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.14916075766086578
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.1157374382019043
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.22776168584823608
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.1578616499900818
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.12085461616516113
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.17754295468330383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1384, 4.8499, 4.6750],
        [5.1384, 5.1373, 5.1384],
        [5.1384, 4.8501, 4.6603],
        [5.1384, 4.8536, 4.7212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.15723511576652527 
model_pd.l_d.mean(): -19.060020446777344 
model_pd.lagr.mean(): -18.902786254882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5574], device='cuda:0')), ('power', tensor([-19.8376], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.15723511576652527
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.1577766239643097
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.20989762246608734
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.10734817385673523
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.1483553797006607
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.09134654700756073
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.12663739919662476
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.12527857720851898
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.14521823823451996
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.18049174547195435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1470, 5.1470, 5.1470],
        [5.1470, 5.1470, 5.1470],
        [5.1470, 5.0083, 5.0665],
        [5.1470, 4.9002, 4.8751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.10001657158136368 
model_pd.l_d.mean(): -20.677358627319336 
model_pd.lagr.mean(): -20.577342987060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4311], device='cuda:0')), ('power', tensor([-21.3437], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.10001657158136368
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.14110851287841797
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.13983486592769623
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13443246483802795
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.19349418580532074
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.0995934009552002
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.08445766568183899
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.23377807438373566
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.1306421160697937
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.1378941684961319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[5.1588, 4.9361, 4.6225],
        [5.1588, 5.4413, 5.2843],
        [5.1588, 4.9000, 4.8500],
        [5.1588, 4.9571, 4.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.1589195281267166 
model_pd.l_d.mean(): -20.538610458374023 
model_pd.lagr.mean(): -20.379690170288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4611], device='cuda:0')), ('power', tensor([-21.2340], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.1589195281267166
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.11787790060043335
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.17570053040981293
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.14178338646888733
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.10629573464393616
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.11158227920532227
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.11551722884178162
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.1513250470161438
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.16891753673553467
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.11982469260692596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 5.1603, 5.1632],
        [5.1633, 5.1633, 5.1633],
        [5.1633, 5.1630, 5.1633],
        [5.1633, 5.1591, 5.1631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.1201128363609314 
model_pd.l_d.mean(): -17.695011138916016 
model_pd.lagr.mean(): -17.57489776611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6184], device='cuda:0')), ('power', tensor([-18.5201], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.1201128363609314
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12042616307735443
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.14498938620090485
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.12116654217243195
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.19197231531143188
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.09401105344295502
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.1409282088279724
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.14072179794311523
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.17250888049602509
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.09705933183431625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1700, 5.1641, 5.1697],
        [5.1700, 5.1231, 5.1590],
        [5.1700, 4.8900, 4.6634],
        [5.1700, 5.0033, 5.0535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.17602580785751343 
model_pd.l_d.mean(): -19.481504440307617 
model_pd.lagr.mean(): -19.305479049682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5830], device='cuda:0')), ('power', tensor([-20.2899], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.17602580785751343
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.06340979784727097
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.13922061026096344
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.16263288259506226
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.1484854519367218
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.1372835487127304
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.11285233497619629
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.08191315829753876
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.16023437678813934
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13214904069900513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1833, 5.1833, 5.1833],
        [5.1833, 5.1827, 5.1832],
        [5.1833, 5.1757, 5.1828],
        [5.1833, 5.1832, 5.1833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.18186978995800018 
model_pd.l_d.mean(): -21.000822067260742 
model_pd.lagr.mean(): -20.818952560424805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3678], device='cuda:0')), ('power', tensor([-21.6059], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.18186978995800018
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12305092811584473
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.16268791258335114
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.09662185609340668
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.121985524892807
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.08478040248155594
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.10797841846942902
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.16284450888633728
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.09608306735754013
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.15748167037963867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1779, 5.1160, 5.1599],
        [5.1779, 5.2532, 4.9826],
        [5.1779, 5.1779, 5.1779],
        [5.1779, 5.0877, 5.1421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.16851764917373657 
model_pd.l_d.mean(): -20.510772705078125 
model_pd.lagr.mean(): -20.342254638671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4458], device='cuda:0')), ('power', tensor([-21.1902], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.16851764917373657
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.10012384504079819
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.11482814699411392
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.10005934536457062
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.1335318386554718
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.12933677434921265
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.1142164096236229
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.16242285072803497
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.1526184380054474
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.1294786036014557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1759, 5.1573, 5.1736],
        [5.1759, 4.9449, 4.9416],
        [5.1759, 4.9147, 4.8567],
        [5.1759, 5.1200, 5.1609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13451378047466278 
model_pd.l_d.mean(): -20.769498825073242 
model_pd.lagr.mean(): -20.634984970092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4084], device='cuda:0')), ('power', tensor([-21.4136], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13451378047466278
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10893911868333817
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.11291339993476868
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.12772589921951294
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.16671350598335266
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.17850133776664734
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.15732020139694214
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.07526350021362305
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.12609563767910004
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.19300776720046997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1486, 5.1449, 5.1485],
        [5.1486, 4.9576, 4.9956],
        [5.1486, 5.1299, 5.1464],
        [5.1486, 5.1265, 5.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.1243518516421318 
model_pd.l_d.mean(): -20.59002113342285 
model_pd.lagr.mean(): -20.465669631958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4448], device='cuda:0')), ('power', tensor([-21.2694], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.1243518516421318
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.11209016293287277
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.09780135750770569
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.1837405413389206
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.28319883346557617
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.14327263832092285
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.1456276923418045
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.11729871481657028
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.1378462016582489
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.1299426257610321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1300, 5.1270, 5.1299],
        [5.1300, 4.8517, 4.5991],
        [5.1300, 4.9304, 4.6018],
        [5.1300, 5.1295, 5.1300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13399161398410797 
model_pd.l_d.mean(): -19.929027557373047 
model_pd.lagr.mean(): -19.79503631591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4549], device='cuda:0')), ('power', tensor([-20.6114], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13399161398410797
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.17986039817333221
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.1290871798992157
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.16689880192279816
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.11502189189195633
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.14039626717567444
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.14570705592632294
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.19628864526748657
epoch£º1060	 i:8 	 global-step:21208	 l-p:0.1089896634221077
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.26216888427734375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1246, 5.1246, 5.1246],
        [5.1246, 4.9310, 4.9685],
        [5.1246, 5.4877, 5.3819],
        [5.1246, 4.8502, 4.7693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.12799429893493652 
model_pd.l_d.mean(): -19.712095260620117 
model_pd.lagr.mean(): -19.5841007232666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5297], device='cuda:0')), ('power', tensor([-20.4686], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.12799429893493652
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.15928158164024353
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.21174713969230652
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.21256795525550842
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.09503461420536041
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.13855598866939545
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.1342615932226181
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.1253819614648819
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.1816025823354721
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.16951076686382294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 5.1328, 5.1332],
        [5.1333, 5.0876, 5.1229],
        [5.1333, 4.9576, 5.0052],
        [5.1333, 4.9902, 5.0483]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.0990588515996933 
model_pd.l_d.mean(): -20.226858139038086 
model_pd.lagr.mean(): -20.12779998779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4931], device='cuda:0')), ('power', tensor([-20.9515], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.0990588515996933
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.13618260622024536
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.1327029913663864
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.21170446276664734
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.11393096297979355
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.1168588250875473
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.13965165615081787
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12326870858669281
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.12178575247526169
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.31050628423690796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1328, 5.1325, 5.1328],
        [5.1328, 4.8769, 4.8385],
        [5.1328, 5.1328, 5.1328],
        [5.1328, 4.8550, 4.7606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.1409667432308197 
model_pd.l_d.mean(): -20.652679443359375 
model_pd.lagr.mean(): -20.5117130279541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4307], device='cuda:0')), ('power', tensor([-21.3183], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.1409667432308197
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.06192166730761528
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.16755196452140808
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.14590807259082794
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.13044269382953644
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.2066146731376648
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.1675310879945755
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.2438621073961258
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11826711148023605
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.166167214512825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 5.1279, 5.1281],
        [5.1281, 4.8377, 4.6740],
        [5.1281, 4.9078, 4.9218],
        [5.1281, 5.0512, 5.1018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.3087231516838074 
model_pd.l_d.mean(): -20.609174728393555 
model_pd.lagr.mean(): -20.300451278686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.2773], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.3087231516838074
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.16138970851898193
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.16726890206336975
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.13044390082359314
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.13423006236553192
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.12322495877742767
epoch£º1064	 i:6 	 global-step:21286	 l-p:0.0943368524312973
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.1190590187907219
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.16346797347068787
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1308099627494812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1344, 5.1344, 5.1345],
        [5.1344, 5.1344, 5.1345],
        [5.1344, 5.0136, 4.6788],
        [5.1344, 5.1338, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.1358218789100647 
model_pd.l_d.mean(): -19.869037628173828 
model_pd.lagr.mean(): -19.73321533203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.5954], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.1358218789100647
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.2546457052230835
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.1484927088022232
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.14195135235786438
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.23424582183361053
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.11682578921318054
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.1259557455778122
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.0627557709813118
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12304963916540146
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.15950866043567657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 5.4991, 5.3948],
        [5.1333, 4.9023, 4.5884],
        [5.1333, 5.0198, 5.0789],
        [5.1333, 4.8889, 4.5853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.22181260585784912 
model_pd.l_d.mean(): -20.528217315673828 
model_pd.lagr.mean(): -20.30640411376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4482], device='cuda:0')), ('power', tensor([-21.2103], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.22181260585784912
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11607478559017181
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.12698908150196075
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.20447948575019836
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.17246997356414795
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.13484840095043182
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.09691561758518219
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12862835824489594
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.1616571992635727
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.13756883144378662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1317, 5.1317, 5.1317],
        [5.1317, 5.1305, 5.1317],
        [5.1317, 5.1265, 5.1314],
        [5.1317, 5.4564, 5.3258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.18148143589496613 
model_pd.l_d.mean(): -20.486745834350586 
model_pd.lagr.mean(): -20.30526351928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.1898], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.18148143589496613
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.1352539360523224
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.1725543588399887
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.08584227412939072
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.15001139044761658
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.16896167397499084
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.16231992840766907
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.16981112957000732
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11186317354440689
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.16806508600711823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1383, 5.1383, 5.1383],
        [5.1383, 5.1378, 5.1383],
        [5.1383, 5.1383, 5.1383],
        [5.1383, 5.1383, 5.1383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.21421658992767334 
model_pd.l_d.mean(): -19.527488708496094 
model_pd.lagr.mean(): -19.31327247619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.2461], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.21421658992767334
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.17360541224479675
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12724991142749786
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.1398085504770279
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.07293058931827545
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.17234863340854645
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.1164771318435669
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.1445591300725937
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.17711496353149414
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.10549989342689514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 4.9082, 4.8837],
        [5.1550, 5.1415, 5.1537],
        [5.1550, 5.1342, 5.1523],
        [5.1550, 5.0177, 5.0763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.11206037551164627 
model_pd.l_d.mean(): -20.739200592041016 
model_pd.lagr.mean(): -20.627140045166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4091], device='cuda:0')), ('power', tensor([-21.3837], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.11206037551164627
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.0940653458237648
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.16468995809555054
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.14797154068946838
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.12951062619686127
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.1510893553495407
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.11905532330274582
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.13068479299545288
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.17260566353797913
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.14385250210762024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1756, 5.3169, 5.0786],
        [5.1756, 5.1742, 5.1756],
        [5.1756, 5.1383, 5.1683],
        [5.1756, 5.2483, 4.9761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.09941330552101135 
model_pd.l_d.mean(): -19.490270614624023 
model_pd.lagr.mean(): -19.390857696533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5522], device='cuda:0')), ('power', tensor([-20.2673], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.09941330552101135
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.14329972863197327
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.1410684883594513
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.13653817772865295
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.137228325009346
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.12906812131404877
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.07867530733346939
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.15991172194480896
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.15698640048503876
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11191446334123611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1900, 5.1900, 5.1900],
        [5.1900, 5.1720, 5.1879],
        [5.1900, 5.0369, 5.0917],
        [5.1900, 4.9467, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.10869977623224258 
model_pd.l_d.mean(): -20.028688430786133 
model_pd.lagr.mean(): -19.91998863220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.7355], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.10869977623224258
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.11191478371620178
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.14013487100601196
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.14661508798599243
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.15051715075969696
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.08713620901107788
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.09569089859724045
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.12354278564453125
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.1703558713197708
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.1391911655664444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1889, 5.1889, 5.1889],
        [5.1889, 5.5823, 5.4946],
        [5.1889, 5.1880, 5.1889],
        [5.1889, 4.9058, 4.7031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.14933863282203674 
model_pd.l_d.mean(): -19.533639907836914 
model_pd.lagr.mean(): -19.384302139282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5473], device='cuda:0')), ('power', tensor([-20.3062], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.14933863282203674
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.09916721284389496
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.10063399374485016
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.10458448529243469
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.11955432593822479
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15873686969280243
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.14939875900745392
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.1216694787144661
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12110669910907745
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.14461649954319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1880, 5.1662, 5.1851],
        [5.1880, 5.1812, 5.1876],
        [5.1880, 5.1195, 4.7957],
        [5.1880, 5.1722, 5.1863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.09758159518241882 
model_pd.l_d.mean(): -19.677907943725586 
model_pd.lagr.mean(): -19.580326080322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5337], device='cuda:0')), ('power', tensor([-20.4381], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.09758159518241882
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.10625577718019485
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.1231340542435646
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.13151799142360687
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11106231808662415
epoch£º1073	 i:5 	 global-step:21465	 l-p:0.12225066870450974
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.17477984726428986
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.14577320218086243
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.13709929585456848
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.15230481326580048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1714, 5.1714, 5.1714],
        [5.1714, 4.8883, 4.7567],
        [5.1714, 5.0796, 5.1346],
        [5.1714, 4.8878, 4.7528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.0972774550318718 
model_pd.l_d.mean(): -19.831192016601562 
model_pd.lagr.mean(): -19.733915328979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4959], device='cuda:0')), ('power', tensor([-20.5545], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.0972774550318718
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12352170795202255
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.1342451423406601
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.15251155197620392
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.16607360541820526
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.1673923283815384
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.1221218854188919
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.15067631006240845
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.16817159950733185
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.0954475924372673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.0565, 5.1130],
        [5.1542, 5.1504, 5.1540],
        [5.1542, 5.1539, 5.1542],
        [5.1542, 4.9212, 4.9184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.09375467151403427 
model_pd.l_d.mean(): -20.80877113342285 
model_pd.lagr.mean(): -20.715017318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4049], device='cuda:0')), ('power', tensor([-21.4497], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.09375467151403427
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.16203971207141876
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.1341208964586258
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.13374006748199463
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.16694791615009308
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12877804040908813
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.13211345672607422
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.1391504853963852
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.19154223799705505
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12647856771945953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 5.1288, 5.1455],
        [5.1478, 5.3942, 5.2148],
        [5.1478, 5.4242, 5.2628],
        [5.1478, 5.1129, 5.1413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.1122940331697464 
model_pd.l_d.mean(): -20.37099838256836 
model_pd.lagr.mean(): -20.258705139160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-21.0945], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.1122940331697464
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.1580912321805954
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.1768834888935089
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.13364599645137787
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.13197200000286102
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10653367638587952
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.19283287227153778
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.08254610002040863
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.2049703299999237
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13165955245494843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 4.8676, 4.7681],
        [5.1466, 4.8835, 4.8293],
        [5.1466, 5.1415, 5.1464],
        [5.1466, 5.1459, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.1679106056690216 
model_pd.l_d.mean(): -20.059188842773438 
model_pd.lagr.mean(): -19.891277313232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4315], device='cuda:0')), ('power', tensor([-20.7191], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.1679106056690216
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.10755448043346405
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.12671907246112823
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.19704745709896088
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.1989506632089615
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.11910350620746613
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.12691926956176758
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.15763653814792633
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.10261575132608414
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12245477735996246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1499, 4.8641, 4.7314],
        [5.1499, 4.9614, 5.0017],
        [5.1499, 4.8998, 4.8712],
        [5.1499, 5.1499, 5.1499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.1601676344871521 
model_pd.l_d.mean(): -19.36153221130371 
model_pd.lagr.mean(): -19.201364517211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.1252], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.1601676344871521
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13716135919094086
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.1262856125831604
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.15387892723083496
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.1248866617679596
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.10058867931365967
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.12048260122537613
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.1508941650390625
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.22357486188411713
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.14407794177532196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1441, 5.1049, 5.1361],
        [5.1441, 5.0219, 4.6866],
        [5.1441, 5.1394, 5.1438],
        [5.1441, 4.8542, 4.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.27936407923698425 
model_pd.l_d.mean(): -20.17292022705078 
model_pd.lagr.mean(): -19.893556594848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.9109], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.27936407923698425
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.11890897154808044
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.06504005938768387
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13521018624305725
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.12245287001132965
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.14065919816493988
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.13290901482105255
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.1413078010082245
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.17048412561416626
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.1375764012336731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1462, 5.1181, 5.1418],
        [5.1462, 5.0466, 5.1037],
        [5.1462, 5.4058, 5.2341],
        [5.1462, 4.9421, 4.6151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11430079489946365 
model_pd.l_d.mean(): -20.252355575561523 
model_pd.lagr.mean(): -20.13805389404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.9757], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11430079489946365
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.11500515043735504
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.11771448701620102
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.1374877393245697
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.22589972615242004
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.10879484564065933
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13468049466609955
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.19490930438041687
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.16068042814731598
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.15885044634342194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1404, 5.1403, 5.1404],
        [5.1404, 5.1404, 5.1404],
        [5.1404, 5.1390, 5.1404],
        [5.1404, 5.1319, 5.1398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11350741982460022 
model_pd.l_d.mean(): -19.34447479248047 
model_pd.lagr.mean(): -19.230966567993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5316], device='cuda:0')), ('power', tensor([-20.0988], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11350741982460022
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.22032085061073303
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.19061587750911713
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.13029971718788147
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.23029406368732452
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.12629443407058716
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.10373890399932861
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.08903589844703674
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.10937794297933578
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14755186438560486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1452, 5.1452, 5.1452],
        [5.1452, 5.0228, 5.0824],
        [5.1452, 4.8901, 4.5973],
        [5.1452, 4.9808, 4.6451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.11225490272045135 
model_pd.l_d.mean(): -19.370901107788086 
model_pd.lagr.mean(): -19.25864601135254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5471], device='cuda:0')), ('power', tensor([-20.1414], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.11225490272045135
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.11048096418380737
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.1347750723361969
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13843362033367157
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.11834049224853516
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.18972976505756378
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.27875450253486633
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.14108337461948395
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.14683747291564941
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.10878662019968033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1329, 5.1300, 5.1328],
        [5.1329, 4.8813, 4.8531],
        [5.1329, 5.4179, 5.2618],
        [5.1329, 4.9898, 4.6523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.11988230794668198 
model_pd.l_d.mean(): -20.18245506286621 
model_pd.lagr.mean(): -20.062572479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5200], device='cuda:0')), ('power', tensor([-20.9341], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.11988230794668198
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.2354699671268463
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.15279075503349304
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.1285271793603897
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.09966997057199478
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.11503754556179047
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.24078421294689178
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.1609029322862625
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.10168617963790894
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.2071639597415924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1230, 5.4232, 5.2767],
        [5.1230, 5.1088, 5.1216],
        [5.1230, 4.8319, 4.6185],
        [5.1230, 4.9776, 4.6392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.12875278294086456 
model_pd.l_d.mean(): -20.227983474731445 
model_pd.lagr.mean(): -20.09922981262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5197], device='cuda:0')), ('power', tensor([-20.9798], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.12875278294086456
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.17697376012802124
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.12619702517986298
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.1552325189113617
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.10317319631576538
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.12199106067419052
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.13975055515766144
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.11163688451051712
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.24270953238010406
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.4389271140098572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1147, 4.9962, 5.0561],
        [5.1147, 4.8396, 4.5682],
        [5.1147, 4.8770, 4.8726],
        [5.1147, 5.1119, 5.1146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.39067506790161133 
model_pd.l_d.mean(): -19.67009162902832 
model_pd.lagr.mean(): -19.279417037963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-20.3992], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.39067506790161133
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.13797272741794586
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.14819841086864471
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.1325429081916809
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.08704708516597748
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.19851945340633392
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.13646601140499115
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.11201649159193039
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.1092090904712677
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.33584707975387573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1104, 5.0195, 5.0748],
        [5.1104, 5.1882, 4.9180],
        [5.1104, 5.1104, 5.1104],
        [5.1104, 5.1103, 5.1104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.22370238602161407 
model_pd.l_d.mean(): -19.87508201599121 
model_pd.lagr.mean(): -19.65138053894043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345], device='cuda:0')), ('power', tensor([-20.5361], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.22370238602161407
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.13441412150859833
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.07210712134838104
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.136664479970932
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.11330434679985046
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.17674079537391663
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.26606276631355286
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.11311505734920502
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.1457941234111786
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.40424856543540955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1163, 5.1161, 5.1163],
        [5.1163, 5.0829, 5.1103],
        [5.1163, 5.0971, 5.1139],
        [5.1163, 4.9412, 4.6036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.1871614009141922 
model_pd.l_d.mean(): -19.563764572143555 
model_pd.lagr.mean(): -19.376604080200195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-20.3251], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.1871614009141922
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.1328427642583847
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.17939268052577972
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.14970603585243225
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.1207214817404747
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.3254571855068207
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.12258537113666534
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.134215846657753
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.10440536588430405
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.17889370024204254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1257, 5.1149, 5.1248],
        [5.1257, 5.1059, 4.7944],
        [5.1257, 5.3887, 5.2191],
        [5.1257, 5.0619, 4.7364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.14313803613185883 
model_pd.l_d.mean(): -20.13782501220703 
model_pd.lagr.mean(): -19.994686126708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5104], device='cuda:0')), ('power', tensor([-20.8792], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.14313803613185883
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.13361510634422302
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.18230271339416504
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.1471800059080124
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.07433633506298065
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.19996649026870728
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.10882476717233658
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.2925640344619751
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.1334616243839264
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.14931800961494446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 5.0437, 5.0990],
        [5.1351, 5.0390, 5.0955],
        [5.1351, 4.8626, 4.5915],
        [5.1351, 5.1351, 5.1351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.15289464592933655 
model_pd.l_d.mean(): -20.708778381347656 
model_pd.lagr.mean(): -20.555883407592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4436], device='cuda:0')), ('power', tensor([-21.3882], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.15289464592933655
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.2059701830148697
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.1641075611114502
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14300666749477386
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.11827575415372849
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11851022392511368
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12502793967723846
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.10301470756530762
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.15965454280376434
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.18056128919124603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1496, 4.8613, 4.7178],
        [5.1496, 4.8708, 4.6188],
        [5.1496, 5.2127, 4.9352],
        [5.1496, 5.1482, 5.1496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.2026626467704773 
model_pd.l_d.mean(): -20.682615280151367 
model_pd.lagr.mean(): -20.479951858520508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4318], device='cuda:0')), ('power', tensor([-21.3497], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.2026626467704773
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.1523769199848175
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.10807821899652481
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11966103315353394
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.14260976016521454
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.15614503622055054
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.16800709068775177
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.12849515676498413
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.12001591175794601
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11923205107450485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1549, 5.0445, 4.7104],
        [5.1549, 4.8751, 4.6276],
        [5.1549, 4.9786, 4.6444],
        [5.1549, 5.1547, 5.1549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.15371102094650269 
model_pd.l_d.mean(): -20.311439514160156 
model_pd.lagr.mean(): -20.15772819519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-20.9786], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.15371102094650269
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.14062242209911346
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.18537597358226776
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.1428826004266739
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.15991942584514618
epoch£º1091	 i:5 	 global-step:21825	 l-p:0.15909025073051453
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.12494339793920517
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.09122814238071442
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.13970792293548584
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.12409179657697678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1495, 5.0024, 5.0600],
        [5.1495, 5.1307, 5.1472],
        [5.1495, 5.0910, 4.7675],
        [5.1495, 4.8985, 4.8703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.13157397508621216 
model_pd.l_d.mean(): -20.513803482055664 
model_pd.lagr.mean(): -20.38222885131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-21.2221], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.13157397508621216
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.12316238880157471
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.13966017961502075
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.13215596973896027
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.26631900668144226
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.14213325083255768
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.08098796010017395
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.17535753548145294
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.12977640330791473
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12634345889091492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1403, 5.0794, 4.7549],
        [5.1403, 5.1402, 5.1403],
        [5.1403, 5.1403, 5.1403],
        [5.1403, 5.1403, 5.1403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.17984187602996826 
model_pd.l_d.mean(): -20.492691040039062 
model_pd.lagr.mean(): -20.312849044799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.1896], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.17984187602996826
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.12258875370025635
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.11111603677272797
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.11366565525531769
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.15037775039672852
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.10616252571344376
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.16562342643737793
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.10730315744876862
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.2129746526479721
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.2302413433790207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1363, 5.0553, 5.1074],
        [5.1363, 5.0323, 5.0904],
        [5.1363, 5.1363, 5.1363],
        [5.1363, 4.8879, 4.8662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.16122767329216003 
model_pd.l_d.mean(): -20.849323272705078 
model_pd.lagr.mean(): -20.688095092773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4110], device='cuda:0')), ('power', tensor([-21.4970], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.16122767329216003
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.12788312137126923
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.11403404176235199
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.1312446892261505
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.17303241789340973
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.1271241307258606
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.11066586524248123
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.18032033741474152
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.23546351492404938
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.1744339019060135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1331, 5.1308, 5.1330],
        [5.1331, 4.9755, 5.0310],
        [5.1331, 4.8808, 4.8526],
        [5.1331, 5.1331, 5.1331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.14033107459545135 
model_pd.l_d.mean(): -20.379045486450195 
model_pd.lagr.mean(): -20.23871421813965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4729], device='cuda:0')), ('power', tensor([-21.0848], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.14033107459545135
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.10211069136857986
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.21605975925922394
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.12851601839065552
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.15575161576271057
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.10181600600481033
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.10938388854265213
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.25764206051826477
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.19114014506340027
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.1349816918373108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1300, 4.8812, 4.8595],
        [5.1300, 4.8773, 4.8489],
        [5.1300, 5.1299, 5.1300],
        [5.1300, 4.9142, 4.9346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.12350137531757355 
model_pd.l_d.mean(): -17.802602767944336 
model_pd.lagr.mean(): -17.679101943969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6462], device='cuda:0')), ('power', tensor([-18.6572], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.12350137531757355
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.13894568383693695
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.2527182698249817
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.09852921962738037
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.18931256234645844
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.12541034817695618
epoch£º1096	 i:6 	 global-step:21926	 l-p:0.12920986115932465
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.14912249147891998
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.17012092471122742
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.152351513504982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1436, 5.1436, 5.1436],
        [5.1436, 5.1420, 5.1435],
        [5.1436, 5.1436, 5.1436],
        [5.1436, 5.2656, 5.0166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.10440711677074432 
model_pd.l_d.mean(): -19.591245651245117 
model_pd.lagr.mean(): -19.486839294433594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.3003], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.10440711677074432
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.12995754182338715
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.13801246881484985
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12709148228168488
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.13305211067199707
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.1321062445640564
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.20711444318294525
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.13411147892475128
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.160857155919075
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.18622872233390808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1457, 5.1410, 5.1455],
        [5.1457, 5.1329, 5.1445],
        [5.1457, 4.8544, 4.6900],
        [5.1457, 4.9272, 4.9438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.18646235764026642 
model_pd.l_d.mean(): -20.84602928161621 
model_pd.lagr.mean(): -20.65956687927246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4069], device='cuda:0')), ('power', tensor([-21.4895], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.18646235764026642
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.2043805718421936
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.17591892182826996
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.16686058044433594
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.1659325808286667
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11564937233924866
epoch£º1098	 i:6 	 global-step:21966	 l-p:0.07366330176591873
epoch£º1098	 i:7 	 global-step:21967	 l-p:0.11078645288944244
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11700073629617691
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.1020224466919899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.1545, 4.8462],
        [5.1653, 5.1651, 5.1653],
        [5.1653, 5.0883, 5.1389],
        [5.1653, 5.1653, 5.1653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.13152194023132324 
model_pd.l_d.mean(): -20.575044631958008 
model_pd.lagr.mean(): -20.443523406982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4517], device='cuda:0')), ('power', tensor([-21.2613], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.13152194023132324
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.20887935161590576
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.11648820340633392
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.10900767147541046
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.09384623914957047
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.19898444414138794
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.08449453115463257
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.17968635261058807
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.1172967329621315
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11460788547992706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1740, 5.1740],
        [5.1740, 5.5141, 5.3910],
        [5.1740, 5.2629, 4.9970],
        [5.1740, 4.9212, 4.8870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.1098947674036026 
model_pd.l_d.mean(): -20.40739631652832 
model_pd.lagr.mean(): -20.297500610351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4719], device='cuda:0')), ('power', tensor([-21.1124], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:0.1098947674036026
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.17210844159126282
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10550782084465027
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.15188628435134888
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.12887077033519745
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.10158658027648926
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.10056475549936295
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.17442184686660767
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.14033399522304535
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.13442665338516235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1788, 4.9326, 4.6342],
        [5.1788, 5.1522, 5.1747],
        [5.1788, 5.1737, 5.1786],
        [5.1788, 4.9547, 4.6391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.11672786623239517 
model_pd.l_d.mean(): -20.347204208374023 
model_pd.lagr.mean(): -20.23047637939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4782], device='cuda:0')), ('power', tensor([-21.0580], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.11672786623239517
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.12286126613616943
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.15977531671524048
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.07916372269392014
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.13874594867229462
epoch£º1101	 i:5 	 global-step:22025	 l-p:0.12255529314279556
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.11598175019025803
epoch£º1101	 i:7 	 global-step:22027	 l-p:0.11261468380689621
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.1159425899386406
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.22044318914413452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1828, 4.9229, 4.8733],
        [5.1828, 5.0077, 4.6751],
        [5.1828, 5.1742, 5.1822],
        [5.1828, 4.8952, 4.7319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12934596836566925 
model_pd.l_d.mean(): -19.668716430664062 
model_pd.lagr.mean(): -19.539369583129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-20.3620], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12934596836566925
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.15722282230854034
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.15020519495010376
epoch£º1102	 i:3 	 global-step:22043	 l-p:0.15436595678329468
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.14257486164569855
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.10015290975570679
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.1299680471420288
epoch£º1102	 i:7 	 global-step:22047	 l-p:0.11392522603273392
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.1682138442993164
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.07188880443572998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1713, 5.1494, 5.1684],
        [5.1713, 5.0093, 5.0626],
        [5.1713, 5.1695, 4.8644],
        [5.1713, 5.1713, 5.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.14001844823360443 
model_pd.l_d.mean(): -20.479785919189453 
model_pd.lagr.mean(): -20.339767456054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4614], device='cuda:0')), ('power', tensor([-21.1749], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.14001844823360443
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.14721308648586273
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.19722075760364532
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.1367393583059311
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.14608125388622284
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.05727963149547577
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.11519989371299744
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.13133348524570465
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.10900086909532547
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.16706542670726776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1706, 5.4660, 5.3147],
        [5.1706, 5.0809, 5.1356],
        [5.1706, 5.1706, 5.1706],
        [5.1706, 5.1663, 5.1704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.11706472188234329 
model_pd.l_d.mean(): -20.52997589111328 
model_pd.lagr.mean(): -20.41291046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4591], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.11706472188234329
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.148795485496521
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.07097900658845901
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.1324968934059143
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.12259475141763687
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.14582762122154236
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.1642461121082306
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11510717123746872
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.20733867585659027
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12696585059165955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1670, 4.9717, 4.6418],
        [5.1670, 5.1593, 5.1664],
        [5.1670, 5.0725, 4.7406],
        [5.1670, 4.9929, 5.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.16218151152133942 
model_pd.l_d.mean(): -20.41874122619629 
model_pd.lagr.mean(): -20.256559371948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1204], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.16218151152133942
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.13030387461185455
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.13314279913902283
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.17347030341625214
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.15321104228496552
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.16213126480579376
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.09357544779777527
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.11735383421182632
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.09384093433618546
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.15057094395160675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1577, 5.2435, 4.9760],
        [5.1577, 5.1546, 5.1575],
        [5.1577, 5.1337, 5.1543],
        [5.1577, 5.1242, 5.1516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.13596773147583008 
model_pd.l_d.mean(): -20.300594329833984 
model_pd.lagr.mean(): -20.164627075195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.9942], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.13596773147583008
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.1058746948838234
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.1254432499408722
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.14512208104133606
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.11281362175941467
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.2186008095741272
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.13436855375766754
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.16784639656543732
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.12351459264755249
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.14817918837070465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.2886, 5.0478],
        [5.1498, 5.1498, 5.1498],
        [5.1498, 5.0507, 5.1078],
        [5.1498, 5.1467, 5.1496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.10877306014299393 
model_pd.l_d.mean(): -20.503509521484375 
model_pd.lagr.mean(): -20.394737243652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4720], device='cuda:0')), ('power', tensor([-21.2097], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.10877306014299393
epoch£º1107	 i:1 	 global-step:22141	 l-p:0.13545651733875275
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.16907766461372375
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.12014400213956833
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.19195029139518738
epoch£º1107	 i:5 	 global-step:22145	 l-p:0.1616450399160385
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12668605148792267
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.1315191686153412
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.11455070227384567
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.17195506393909454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1511, 4.8652, 4.7438],
        [5.1511, 5.1511, 5.1511],
        [5.1511, 4.9144, 4.9095],
        [5.1511, 4.8664, 4.6280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13901139795780182 
model_pd.l_d.mean(): -20.237863540649414 
model_pd.lagr.mean(): -20.098852157592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-20.9527], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13901139795780182
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.19337384402751923
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.1757909208536148
epoch£º1108	 i:3 	 global-step:22163	 l-p:0.1198534443974495
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.17572011053562164
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.12286494672298431
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.09540341049432755
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.09973696619272232
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.16539348661899567
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.1327856481075287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 5.2453, 4.9791],
        [5.1567, 5.1566, 5.1567],
        [5.1567, 4.9626, 5.0001],
        [5.1567, 4.9354, 4.9490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.1596660315990448 
model_pd.l_d.mean(): -20.33203887939453 
model_pd.lagr.mean(): -20.172372817993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4722], device='cuda:0')), ('power', tensor([-21.0365], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.1596660315990448
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.19121363759040833
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.1388222724199295
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.14600491523742676
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.15727517008781433
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.126288503408432
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.10425305366516113
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.08695272356271744
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.1403592973947525
epoch£º1109	 i:9 	 global-step:22189	 l-p:0.128695547580719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[5.1718, 4.9908, 5.0357],
        [5.1718, 5.0092, 4.6734],
        [5.1718, 5.2072, 4.9167],
        [5.1718, 5.2559, 4.9874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.17244142293930054 
model_pd.l_d.mean(): -20.21469497680664 
model_pd.lagr.mean(): -20.042253494262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.9520], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.17244142293930054
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.1209009513258934
epoch£º1110	 i:2 	 global-step:22202	 l-p:0.1072031557559967
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.17444351315498352
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.1579740345478058
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.1351659893989563
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12690110504627228
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.08908554166555405
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11749693006277084
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.1325976848602295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1789, 5.1550, 4.8419],
        [5.1789, 5.1404, 5.1712],
        [5.1789, 5.1790, 5.1790],
        [5.1789, 4.8903, 4.7255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13258960843086243 
model_pd.l_d.mean(): -18.863235473632812 
model_pd.lagr.mean(): -18.73064613342285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5372], device='cuda:0')), ('power', tensor([-19.6181], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13258960843086243
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.0941740944981575
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.14838288724422455
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.15826855599880219
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.13309478759765625
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.18726159632205963
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.12940539419651031
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.10841720551252365
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.06238919496536255
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.20464031398296356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1611, 4.9185, 4.6119],
        [5.1611, 4.9420, 4.6200],
        [5.1611, 4.8988, 4.8488],
        [5.1611, 4.9186, 4.6119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.13384689390659332 
model_pd.l_d.mean(): -19.84554672241211 
model_pd.lagr.mean(): -19.711700439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-20.5664], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.13384689390659332
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.19880717992782593
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.07705192267894745
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.1885053664445877
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.16791686415672302
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.11748593300580978
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.09543302655220032
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.13478431105613708
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.13232305645942688
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.13155922293663025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1658, 5.1646, 5.1658],
        [5.1658, 5.1652, 5.1658],
        [5.1658, 5.1658, 5.1658],
        [5.1658, 4.9173, 4.8931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.16599252820014954 
model_pd.l_d.mean(): -19.41986656188965 
model_pd.lagr.mean(): -19.253873825073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5307], device='cuda:0')), ('power', tensor([-20.1741], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.16599252820014954
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.16786330938339233
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12630338966846466
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.13239574432373047
epoch£º1113	 i:4 	 global-step:22264	 l-p:0.1082019954919815
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.15764498710632324
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.12820371985435486
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.11184179037809372
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.16548143327236176
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.11005692929029465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.0785, 5.1310],
        [5.1609, 4.8697, 4.6995],
        [5.1609, 4.8872, 4.6210],
        [5.1609, 5.1579, 5.1608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.13112491369247437 
model_pd.l_d.mean(): -18.504331588745117 
model_pd.lagr.mean(): -18.373207092285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5415], device='cuda:0')), ('power', tensor([-19.2597], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.13112491369247437
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.15387055277824402
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.155512273311615
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.15202739834785461
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12453063577413559
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.15814121067523956
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.1712990552186966
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13888052105903625
epoch£º1114	 i:8 	 global-step:22288	 l-p:0.10403565317392349
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.09448825567960739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 4.9800, 4.6452],
        [5.1609, 5.1541, 5.1605],
        [5.1609, 5.2413, 4.9708],
        [5.1609, 5.1465, 5.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.12192867696285248 
model_pd.l_d.mean(): -20.14432144165039 
model_pd.lagr.mean(): -20.02239227294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4941], device='cuda:0')), ('power', tensor([-20.8691], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.12192867696285248
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.13926348090171814
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.12990161776542664
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.12568998336791992
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.22045151889324188
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.15307924151420593
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.1420959085226059
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.1198788583278656
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12490001320838928
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.10830161720514297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 4.9338, 4.9424],
        [5.1596, 4.9116, 4.8895],
        [5.1596, 4.9088, 4.6089],
        [5.1596, 5.1596, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.13937531411647797 
model_pd.l_d.mean(): -20.51175880432129 
model_pd.lagr.mean(): -20.37238311767578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4441], device='cuda:0')), ('power', tensor([-21.1895], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.13937531411647797
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.18025809526443481
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12335484474897385
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.18040727078914642
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.1517503708600998
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.12811152637004852
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.11080170422792435
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.10655960440635681
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13968564569950104
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.1408616155385971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1569, 5.0942, 5.1387],
        [5.1569, 5.1487, 5.1563],
        [5.1569, 4.8999, 4.8623],
        [5.1569, 5.1568, 5.1569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.13486649096012115 
model_pd.l_d.mean(): -20.714757919311523 
model_pd.lagr.mean(): -20.579891204833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.3693], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.13486649096012115
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.12559351325035095
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.2176772654056549
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.05346374213695526
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.17716084420681
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.17167693376541138
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.10851524770259857
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.1048521175980568
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.18262982368469238
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.12992523610591888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1573, 5.1519, 5.1571],
        [5.1573, 4.9825, 5.0313],
        [5.1573, 5.0672, 5.1222],
        [5.1573, 5.1546, 5.1572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.12022604048252106 
model_pd.l_d.mean(): -19.26190757751465 
model_pd.lagr.mean(): -19.141681671142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-19.9898], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.12022604048252106
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.12839065492153168
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.10159047693014145
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12587779760360718
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.1466543823480606
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.1739221215248108
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.2041381448507309
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.18646597862243652
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12385719269514084
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.11278907209634781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 4.9102, 4.8983],
        [5.1523, 5.2809, 5.0343],
        [5.1523, 5.1470, 5.1520],
        [5.1523, 5.5140, 5.4045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.10877980291843414 
model_pd.l_d.mean(): -19.274621963500977 
model_pd.lagr.mean(): -19.165842056274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5678], device='cuda:0')), ('power', tensor([-20.0652], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.10877980291843414
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.12533937394618988
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.16111424565315247
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.16611279547214508
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.15484383702278137
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.11196747422218323
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.11893214285373688
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.14663466811180115
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.15027232468128204
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.15917834639549255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1649, 4.8845, 4.6344],
        [5.1649, 5.0401, 5.0999],
        [5.1649, 5.1152, 5.1529],
        [5.1649, 5.2591, 4.9950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.14710742235183716 
model_pd.l_d.mean(): -19.43842887878418 
model_pd.lagr.mean(): -19.29132080078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5480], device='cuda:0')), ('power', tensor([-20.2105], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.14710742235183716
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.18234431743621826
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.07667919248342514
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12143536657094955
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.14987273514270782
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.12656225264072418
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.14195013046264648
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.14056725800037384
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.1324203461408615
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.1329474002122879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1805, 5.0638, 4.7285],
        [5.1805, 5.2097, 4.9161],
        [5.1805, 5.1697, 5.1796],
        [5.1805, 5.1802, 5.1805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.12188075482845306 
model_pd.l_d.mean(): -20.90907859802246 
model_pd.lagr.mean(): -20.78719711303711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3960], device='cuda:0')), ('power', tensor([-21.5421], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.12188075482845306
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.1503053456544876
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.11329130828380585
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.137582927942276
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.13033170998096466
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.09944777935743332
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.1703334003686905
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.10486610233783722
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.12578748166561127
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.18028007447719574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1707, 5.1278, 5.1614],
        [5.1707, 5.1374, 5.1648],
        [5.1707, 5.0822, 5.1366],
        [5.1707, 5.4337, 5.2621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.16045810282230377 
model_pd.l_d.mean(): -20.017152786254883 
model_pd.lagr.mean(): -19.8566951751709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5276], device='cuda:0')), ('power', tensor([-20.7748], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.16045810282230377
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.16333122551441193
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.10696472972631454
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.1077788695693016
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13057295978069305
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1404927372932434
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.18722224235534668
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.1332879364490509
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.11743846535682678
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.0959244966506958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1732, 4.8931, 4.6446],
        [5.1732, 4.9738, 4.6442],
        [5.1732, 5.0846, 5.1390],
        [5.1732, 5.1677, 5.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.12014731764793396 
model_pd.l_d.mean(): -20.374956130981445 
model_pd.lagr.mean(): -20.25480842590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4817], device='cuda:0')), ('power', tensor([-21.0896], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.12014731764793396
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.11065062880516052
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.16617879271507263
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.11127100884914398
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.1194317489862442
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.1581237018108368
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.1019987240433693
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14931894838809967
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.17816774547100067
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12821698188781738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1704, 4.9997, 4.6635],
        [5.1704, 5.0522, 5.1118],
        [5.1704, 4.9034, 4.8426],
        [5.1704, 5.0664, 5.1245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.16187384724617004 
model_pd.l_d.mean(): -20.68064308166504 
model_pd.lagr.mean(): -20.518768310546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-21.3390], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.16187384724617004
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.10903966426849365
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13053171336650848
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.11503469944000244
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13737590610980988
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.08973260968923569
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.15176236629486084
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.1277417093515396
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.17272230982780457
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.18575936555862427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.0674, 5.1231],
        [5.1603, 4.8685, 4.6734],
        [5.1603, 4.8686, 4.7049],
        [5.1603, 5.1514, 5.1597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15850014984607697 
model_pd.l_d.mean(): -20.892915725708008 
model_pd.lagr.mean(): -20.73441505432129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.5467], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15850014984607697
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.1553635150194168
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.128993958234787
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.11126062273979187
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.14001134037971497
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.14436255395412445
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.15537488460540771
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.08758709579706192
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14209435880184174
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.16028979420661926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.1653, 5.1653],
        [5.1653, 5.0762, 5.1309],
        [5.1653, 4.9985, 5.0507],
        [5.1653, 4.8794, 4.7576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.10699934512376785 
model_pd.l_d.mean(): -20.743356704711914 
model_pd.lagr.mean(): -20.6363582611084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4343], device='cuda:0')), ('power', tensor([-21.4136], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.10699934512376785
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.21120120584964752
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.174824520945549
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.1097024530172348
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.1183658093214035
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12967412173748016
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.16117940843105316
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.11867864429950714
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.12974077463150024
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.09332583844661713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1745, 4.8854, 4.7369],
        [5.1745, 5.1740, 5.1745],
        [5.1745, 5.1743, 5.1745],
        [5.1745, 4.9951, 4.6604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.17787007987499237 
model_pd.l_d.mean(): -20.701662063598633 
model_pd.lagr.mean(): -20.523792266845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4531], device='cuda:0')), ('power', tensor([-21.3906], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.17787007987499237
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.08023965358734131
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.1339668482542038
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.112095408141613
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.11446154117584229
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.15385667979717255
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.12968458235263824
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.1484086662530899
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.1108943521976471
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.181391179561615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1704, 5.1704, 5.1704],
        [5.1704, 5.1704, 5.1704],
        [5.1704, 5.1153, 5.1560],
        [5.1704, 4.8790, 4.7020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.18734273314476013 
model_pd.l_d.mean(): -18.802955627441406 
model_pd.lagr.mean(): -18.615612030029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5549], device='cuda:0')), ('power', tensor([-19.5752], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.18734273314476013
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.14873704314231873
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.11866363137960434
epoch£º1128	 i:3 	 global-step:22563	 l-p:0.11046390235424042
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12310947477817535
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.16029490530490875
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.10047522932291031
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.120921790599823
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.16091075539588928
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12949316203594208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.1635, 5.1635],
        [5.1635, 5.0719, 5.1273],
        [5.1635, 5.1512, 5.1624],
        [5.1635, 5.1475, 5.1618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.11563713103532791 
model_pd.l_d.mean(): -19.714284896850586 
model_pd.lagr.mean(): -19.598648071289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4611], device='cuda:0')), ('power', tensor([-20.4007], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.11563713103532791
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.18617649376392365
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.11351822316646576
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12423942238092422
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.13790304958820343
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.15140917897224426
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.1857355386018753
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.1977131962776184
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.08711795508861542
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.09032489359378815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.4039, 5.2209],
        [5.1597, 5.1597, 5.1597],
        [5.1597, 5.4340, 5.2690],
        [5.1597, 4.9111, 4.8893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.1736970990896225 
model_pd.l_d.mean(): -19.218814849853516 
model_pd.lagr.mean(): -19.04511833190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-20.0023], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.1736970990896225
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.14234741032123566
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.11523294448852539
epoch£º1130	 i:3 	 global-step:22603	 l-p:0.1159413531422615
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.1675035059452057
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.18196086585521698
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.11813068389892578
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.0959383174777031
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.1243426576256752
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.14725229144096375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1648, 5.1648],
        [5.1648, 5.1217, 5.1555],
        [5.1648, 5.2284, 4.9495],
        [5.1648, 5.0703, 5.1264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.16242705285549164 
model_pd.l_d.mean(): -19.977432250976562 
model_pd.lagr.mean(): -19.815004348754883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-20.7007], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.16242705285549164
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.14309042692184448
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.13289640843868256
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.1081755980849266
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.12939554452896118
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.19108465313911438
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.10060719400644302
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.09680674970149994
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.18999041616916656
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12329412251710892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[5.1657, 5.4258, 5.2522],
        [5.1657, 4.9134, 4.8848],
        [5.1657, 4.9089, 4.6144],
        [5.1657, 4.8893, 4.8057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.1295628398656845 
model_pd.l_d.mean(): -20.587785720825195 
model_pd.lagr.mean(): -20.458223342895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-21.2660], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.1295628398656845
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.1751834601163864
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.16007807850837708
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.1686912328004837
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.07042482495307922
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.11525975912809372
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.1308666616678238
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.13796348869800568
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.11456526815891266
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.15773189067840576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1740, 5.1740],
        [5.1740, 4.9551, 4.6324],
        [5.1740, 4.9587, 4.9786],
        [5.1740, 5.1253, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.0835585743188858 
model_pd.l_d.mean(): -20.02350616455078 
model_pd.lagr.mean(): -19.9399471282959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4947], device='cuda:0')), ('power', tensor([-20.7476], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:0.0835585743188858
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.13217662274837494
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.14465828239917755
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.12464122474193573
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.15216514468193054
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.12643752992153168
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.11234179139137268
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.16933444142341614
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11610116064548492
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.16552968323230743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1823, 4.9348, 4.9123],
        [5.1823, 4.9572, 4.9662],
        [5.1823, 5.0923, 5.1471],
        [5.1823, 5.0058, 5.0535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.16058239340782166 
model_pd.l_d.mean(): -20.307157516479492 
model_pd.lagr.mean(): -20.146575927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867], device='cuda:0')), ('power', tensor([-21.0262], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.16058239340782166
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.15268495678901672
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13692770898342133
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.10652032494544983
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.17094959318637848
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.14387576282024384
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.10743669420480728
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.12092523276805878
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.12323560565710068
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.09115009009838104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1821, 5.4632, 5.3017],
        [5.1821, 4.9079, 4.6446],
        [5.1821, 5.1310, 5.1695],
        [5.1821, 4.8917, 4.7265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.13989757001399994 
model_pd.l_d.mean(): -20.239444732666016 
model_pd.lagr.mean(): -20.099546432495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5024], device='cuda:0')), ('power', tensor([-20.9738], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.13989757001399994
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.13119371235370636
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.12681196630001068
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.12124218046665192
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.14344245195388794
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.06554453074932098
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.13149844110012054
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.11345907300710678
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.17892403900623322
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.14697758853435516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1907, 5.1625, 5.1862],
        [5.1907, 5.1888, 5.1907],
        [5.1907, 5.0219, 5.0728],
        [5.1907, 4.9014, 4.7386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.1073845848441124 
model_pd.l_d.mean(): -20.8193359375 
model_pd.lagr.mean(): -20.711952209472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3847], device='cuda:0')), ('power', tensor([-21.4398], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.1073845848441124
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.09725267440080643
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.15015879273414612
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.13537457585334778
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.12851132452487946
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.10077095776796341
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14211416244506836
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.13180984556674957
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.10965228825807571
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.16929396986961365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1967, 5.1963, 5.1967],
        [5.1967, 5.1967, 5.1967],
        [5.1967, 5.1916, 5.1965],
        [5.1967, 4.9088, 4.7043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.0933736190199852 
model_pd.l_d.mean(): -20.307193756103516 
model_pd.lagr.mean(): -20.21381950378418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-21.0066], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.0933736190199852
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.10443603247404099
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.08744340389966965
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.13688014447689056
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.15475834906101227
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.1727348417043686
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.150814488530159
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11636985838413239
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.16823644936084747
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.08801134675741196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1862, 5.1819, 5.1860],
        [5.1862, 5.1862, 5.1862],
        [5.1862, 5.1318, 5.1721],
        [5.1862, 5.1353, 4.8125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.1131725162267685 
model_pd.l_d.mean(): -19.465534210205078 
model_pd.lagr.mean(): -19.35236167907715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-20.1770], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.1131725162267685
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.1497577279806137
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14951449632644653
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.1292211264371872
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.18190868198871613
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.08531588315963745
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.11419633775949478
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.17362180352210999
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.13655774295330048
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.10250551253557205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.1712, 5.1712],
        [5.1712, 5.1575, 5.1698],
        [5.1712, 4.9159, 4.6195],
        [5.1712, 5.1551, 5.1694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.1339377909898758 
model_pd.l_d.mean(): -20.17797088623047 
model_pd.lagr.mean(): -20.04403305053711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4885], device='cuda:0')), ('power', tensor([-20.8975], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.1339377909898758
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.12051764875650406
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14547030627727509
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.13449759781360626
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.15199051797389984
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.16426719725131989
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.2662271559238434
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.09041869640350342
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.11352352052927017
epoch£º1139	 i:9 	 global-step:22789	 l-p:0.08160322159528732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 5.1513, 5.1513],
        [5.1513, 5.1008, 5.1390],
        [5.1513, 5.1502, 5.1512],
        [5.1513, 5.1512, 5.1513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.1704886257648468 
model_pd.l_d.mean(): -20.951494216918945 
model_pd.lagr.mean(): -20.781005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3884], device='cuda:0')), ('power', tensor([-21.5771], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.1704886257648468
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.1429230123758316
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.06570277363061905
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.22919140756130219
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.13097620010375977
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.1427459716796875
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.1427803933620453
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.13135208189487457
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.1894192397594452
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12871403992176056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1422, 5.0116, 5.0720],
        [5.1422, 5.1415, 5.1422],
        [5.1422, 5.0299, 4.6917],
        [5.1422, 4.9142, 4.9234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.12364664673805237 
model_pd.l_d.mean(): -19.086170196533203 
model_pd.lagr.mean(): -18.9625244140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6100], device='cuda:0')), ('power', tensor([-19.9178], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:0.12364664673805237
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.21661508083343506
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.14774271845817566
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.11146945506334305
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12653635442256927
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.18867672979831696
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.20677921175956726
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.09373990446329117
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.14315493404865265
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.11570301651954651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 5.1509, 5.1513],
        [5.1513, 5.5243, 5.4214],
        [5.1513, 5.1513, 5.1513],
        [5.1513, 5.0878, 4.7604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.1266653835773468 
model_pd.l_d.mean(): -19.79300308227539 
model_pd.lagr.mean(): -19.666337966918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-20.4732], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.1266653835773468
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.2554723024368286
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.1377619206905365
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.13390326499938965
epoch£º1142	 i:4 	 global-step:22844	 l-p:0.15035197138786316
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.13099466264247894
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13304109871387482
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.1333429217338562
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.11387696862220764
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11277446150779724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1575, 5.4263, 5.2576],
        [5.1575, 5.1238, 5.1514],
        [5.1575, 4.9620, 4.9998],
        [5.1575, 5.1562, 5.1575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.10602321475744247 
model_pd.l_d.mean(): -20.134096145629883 
model_pd.lagr.mean(): -20.028072357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328], device='cuda:0')), ('power', tensor([-20.7962], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.10602321475744247
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.1319185346364975
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.17998939752578735
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.14331994950771332
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.1251000612974167
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.1318288892507553
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.10554181039333344
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.13252858817577362
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.22231560945510864
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.14811865985393524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1543, 5.1543, 5.1543],
        [5.1543, 5.1543, 5.1543],
        [5.1543, 5.1541, 5.1543],
        [5.1543, 4.8600, 4.6892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.09819183498620987 
model_pd.l_d.mean(): -20.43545913696289 
model_pd.lagr.mean(): -20.33726692199707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.1287], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.09819183498620987
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15446414053440094
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.11514606326818466
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.15435253083705902
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.1097743809223175
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.09510721266269684
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.18300017714500427
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.12309201806783676
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.2551455795764923
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13688130676746368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.0659, 5.1209],
        [5.1554, 5.0143, 5.0736],
        [5.1554, 5.1491, 5.1551],
        [5.1554, 5.1445, 5.1545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.11872462928295135 
model_pd.l_d.mean(): -19.946043014526367 
model_pd.lagr.mean(): -19.82731819152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5154], device='cuda:0')), ('power', tensor([-20.6904], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.11872462928295135
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.13075797259807587
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.12163081765174866
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.17323599755764008
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.13427449762821198
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.14595924317836761
epoch£º1145	 i:6 	 global-step:22906	 l-p:0.06883058696985245
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.1791967898607254
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.15763691067695618
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.19026491045951843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 5.1574, 5.1574],
        [5.1574, 5.5564, 5.4703],
        [5.1574, 4.8640, 4.7037],
        [5.1574, 4.8656, 4.7212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.17970463633537292 
model_pd.l_d.mean(): -19.87959861755371 
model_pd.lagr.mean(): -19.699893951416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5511], device='cuda:0')), ('power', tensor([-20.6598], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.17970463633537292
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.15817484259605408
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.10852115601301193
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.11015379428863525
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.14957809448242188
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.1106913611292839
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.17171502113342285
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.14768855273723602
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.12791936099529266
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.13264022767543793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.2906, 5.0428],
        [5.1621, 5.1616, 5.1621],
        [5.1621, 5.1610, 5.1621],
        [5.1621, 4.9782, 4.6414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.08834811300039291 
model_pd.l_d.mean(): -20.25202751159668 
model_pd.lagr.mean(): -20.163679122924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4997], device='cuda:0')), ('power', tensor([-20.9837], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.08834811300039291
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.15080997347831726
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.133047416806221
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.14949819445610046
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.1304568350315094
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.15233327448368073
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.14360253512859344
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.1364687830209732
epoch£º1147	 i:8 	 global-step:22948	 l-p:0.12658672034740448
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.18441051244735718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1588, 5.4150, 5.2386],
        [5.1588, 5.0774, 5.1298],
        [5.1588, 5.0429, 5.1028],
        [5.1588, 5.1545, 5.1586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11975269764661789 
model_pd.l_d.mean(): -19.760515213012695 
model_pd.lagr.mean(): -19.640762329101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5193], device='cuda:0')), ('power', tensor([-20.5069], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11975269764661789
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.16268965601921082
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.1539737731218338
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.1512300670146942
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.14005860686302185
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.14129413664340973
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.13087156414985657
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11675106734037399
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.13135987520217896
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.17837412655353546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 5.0024, 4.6615],
        [5.1523, 4.8974, 4.5954],
        [5.1523, 5.0185, 5.0787],
        [5.1523, 5.0357, 5.0957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.18171019852161407 
model_pd.l_d.mean(): -19.877016067504883 
model_pd.lagr.mean(): -19.6953067779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5280], device='cuda:0')), ('power', tensor([-20.6336], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.18171019852161407
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.0734255313873291
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.1885833740234375
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.15464459359645844
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.10001575201749802
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12319864332675934
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.13852719962596893
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.17501677572727203
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.1919800341129303
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08213508129119873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 5.1633, 5.1633],
        [5.1633, 5.1629, 5.1633],
        [5.1633, 5.1630, 5.1633],
        [5.1633, 5.1628, 5.1633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11518224328756332 
model_pd.l_d.mean(): -20.519248962402344 
model_pd.lagr.mean(): -20.40406608581543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4186], device='cuda:0')), ('power', tensor([-21.1711], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11518224328756332
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.11727674305438995
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.09121864289045334
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14109496772289276
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.21966423094272614
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.1455734372138977
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.10664304345846176
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.15404054522514343
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.1557893306016922
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.14544077217578888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 5.1656, 5.1656],
        [5.1656, 5.1655, 5.1656],
        [5.1656, 5.0764, 5.1313],
        [5.1656, 4.8991, 4.8443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.18625079095363617 
model_pd.l_d.mean(): -20.683300018310547 
model_pd.lagr.mean(): -20.49704933166504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4271], device='cuda:0')), ('power', tensor([-21.3455], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.18625079095363617
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.10303938388824463
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.1258024275302887
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.14107976853847504
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12130638211965561
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.15504001080989838
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.1059206947684288
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.12957462668418884
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.1291830837726593
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.1562255322933197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1797, 5.1790, 5.1797],
        [5.1797, 4.8875, 4.7092],
        [5.1797, 5.1797, 5.1797],
        [5.1797, 4.9894, 5.0301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.143606036901474 
model_pd.l_d.mean(): -20.90116310119629 
model_pd.lagr.mean(): -20.757556915283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3767], device='cuda:0')), ('power', tensor([-21.5143], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.143606036901474
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.0987781509757042
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.130708247423172
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.05404740199446678
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.163787841796875
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.13477590680122375
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11401297152042389
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.14314252138137817
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.1708431988954544
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.1698828488588333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1830, 5.1819, 5.1829],
        [5.1830, 5.1305, 5.1697],
        [5.1830, 5.0606, 5.1206],
        [5.1830, 5.1688, 5.1816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.1646462082862854 
model_pd.l_d.mean(): -20.469085693359375 
model_pd.lagr.mean(): -20.304439544677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4651], device='cuda:0')), ('power', tensor([-21.1678], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.1646462082862854
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.1512736678123474
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.1202857494354248
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.11915126442909241
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11998409032821655
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.08126558363437653
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12827709317207336
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.10806193202733994
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.15997545421123505
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.17474620044231415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1724, 5.1329, 5.1644],
        [5.1724, 5.0448, 5.1050],
        [5.1724, 5.1724, 5.1724],
        [5.1724, 5.1387, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13789482414722443 
model_pd.l_d.mean(): -20.679269790649414 
model_pd.lagr.mean(): -20.54137420654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-21.3526], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13789482414722443
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.15864711999893188
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.12189340591430664
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.14057785272598267
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.17194905877113342
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.1370905637741089
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.06477823853492737
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.15952709317207336
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.13612207770347595
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.1312333345413208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1718, 5.1670, 5.1715],
        [5.1718, 5.1718, 5.1718],
        [5.1718, 5.1718, 5.1718],
        [5.1718, 5.4851, 5.3430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.09481874108314514 
model_pd.l_d.mean(): -19.341222763061523 
model_pd.lagr.mean(): -19.24640464782715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5409], device='cuda:0')), ('power', tensor([-20.1051], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.09481874108314514
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12465676665306091
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.10093067586421967
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.2153938263654709
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.17589171230793
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.14910803735256195
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.15041515231132507
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.11298605054616928
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.11351044476032257
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12780457735061646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1679, 5.1229, 5.1578],
        [5.1679, 5.1679, 5.1679],
        [5.1679, 5.1283, 5.1599],
        [5.1679, 4.8963, 4.6192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.1602032631635666 
model_pd.l_d.mean(): -21.00722885131836 
model_pd.lagr.mean(): -20.84702491760254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3804], device='cuda:0')), ('power', tensor([-21.6253], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.1602032631635666
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.13296286761760712
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.10964716970920563
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.19459015130996704
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.17490196228027344
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14143845438957214
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.15624989569187164
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.11650998145341873
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.09700589627027512
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.12333844602108002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.1546, 5.1546],
        [5.1546, 4.8972, 4.8635],
        [5.1546, 5.1516, 5.1545],
        [5.1546, 5.0540, 5.1118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.1286829113960266 
model_pd.l_d.mean(): -20.229251861572266 
model_pd.lagr.mean(): -20.100568771362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977], device='cuda:0')), ('power', tensor([-20.9586], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.1286829113960266
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.17990978062152863
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.14262501895427704
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.17377956211566925
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.12955176830291748
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12878116965293884
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.19312293827533722
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.08799079805612564
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12399027496576309
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.11412729322910309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1674, 5.0514, 5.1113],
        [5.1674, 5.1673, 5.1674],
        [5.1674, 5.1674, 5.1674],
        [5.1674, 4.8981, 4.8373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.15314319729804993 
model_pd.l_d.mean(): -20.057823181152344 
model_pd.lagr.mean(): -19.904680252075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-20.7856], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.15314319729804993
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12318544089794159
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.14094291627407074
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.15381956100463867
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.1187758818268776
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.13102179765701294
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.13170473277568817
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.19146564602851868
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.11347566545009613
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.1320505440235138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 4.8615, 4.6462],
        [5.1550, 5.1549, 5.1550],
        [5.1550, 5.1546, 5.1550],
        [5.1550, 5.0982, 5.1399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.19498419761657715 
model_pd.l_d.mean(): -20.55335807800293 
model_pd.lagr.mean(): -20.358373641967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4474], device='cuda:0')), ('power', tensor([-21.2350], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.19498419761657715
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.15137338638305664
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.12083133310079575
epoch£º1159	 i:3 	 global-step:23183	 l-p:0.12561242282390594
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.1328030526638031
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10995461046695709
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.14774571359157562
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14766664803028107
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.14282099902629852
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.16360388696193695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 4.8789, 4.5974],
        [5.1512, 5.1457, 5.1509],
        [5.1512, 4.9137, 4.5953],
        [5.1512, 5.1511, 5.1512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.1299636960029602 
model_pd.l_d.mean(): -20.112354278564453 
model_pd.lagr.mean(): -19.982391357421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4556], device='cuda:0')), ('power', tensor([-20.7975], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.1299636960029602
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.17630736529827118
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.15395388007164001
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.1249442920088768
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.16227921843528748
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.0959603488445282
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.13063623011112213
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.10739880055189133
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.248975932598114
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.13945317268371582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 4.9512, 4.6137],
        [5.1458, 5.1458, 5.1459],
        [5.1458, 5.0446, 5.1027],
        [5.1458, 4.9243, 4.9416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.08756395429372787 
model_pd.l_d.mean(): -18.62885093688965 
model_pd.lagr.mean(): -18.54128646850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6387], device='cuda:0')), ('power', tensor([-19.4849], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.08756395429372787
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.1369675248861313
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.11306265741586685
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.1359393149614334
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.10111066699028015
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.2206081748008728
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.1897260844707489
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.1934603601694107
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.11907532066106796
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.14885175228118896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[5.1585, 5.0982, 4.7709],
        [5.1585, 5.0187, 5.0785],
        [5.1585, 4.9039, 4.8752],
        [5.1585, 4.8950, 4.6021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.21993602812290192 
model_pd.l_d.mean(): -20.217758178710938 
model_pd.lagr.mean(): -19.997821807861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5203], device='cuda:0')), ('power', tensor([-20.9702], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.21993602812290192
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.09513656049966812
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12471716850996017
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.15115351974964142
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12199021130800247
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.08242886513471603
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.19307899475097656
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.11588741093873978
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.13821522891521454
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.14308315515518188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.0339, 4.6938],
        [5.1750, 5.1750, 5.1750],
        [5.1750, 5.1750, 5.1750],
        [5.1750, 5.5049, 5.3729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.11679786443710327 
model_pd.l_d.mean(): -19.854454040527344 
model_pd.lagr.mean(): -19.737655639648438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4880], device='cuda:0')), ('power', tensor([-20.5699], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.11679786443710327
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.10212995111942291
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.19255216419696808
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11221784353256226
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.16095943748950958
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.1452297568321228
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.09757015109062195
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.08384796231985092
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.16062785685062408
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.1604919731616974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[5.1788, 5.0454, 5.1054],
        [5.1788, 5.0150, 5.0689],
        [5.1788, 4.9150, 4.8657],
        [5.1788, 5.5583, 5.4581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.15687964856624603 
model_pd.l_d.mean(): -20.091176986694336 
model_pd.lagr.mean(): -19.934297561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-20.8209], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.15687964856624603
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.1059083566069603
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.192209854722023
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.046694252640008926
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13197170197963715
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.1573609858751297
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13119040429592133
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.14246217906475067
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.1499081701040268
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.11541208624839783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1781, 4.9330, 4.6242],
        [5.1781, 5.0540, 4.7149],
        [5.1781, 4.9580, 4.9747],
        [5.1781, 5.1781, 5.1781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.1174916923046112 
model_pd.l_d.mean(): -19.958446502685547 
model_pd.lagr.mean(): -19.84095573425293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5164], device='cuda:0')), ('power', tensor([-20.7040], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.1174916923046112
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.07929055392742157
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.15222109854221344
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.1756095141172409
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.10572833567857742
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.10646089911460876
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.09129815548658371
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.22215978801250458
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.19772320985794067
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.11438802629709244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.1667, 5.1667],
        [5.1667, 5.0252, 5.0847],
        [5.1667, 5.1523, 5.1653],
        [5.1667, 5.0330, 5.0932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.18102574348449707 
model_pd.l_d.mean(): -20.525312423706055 
model_pd.lagr.mean(): -20.34428596496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.2115], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.18102574348449707
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.1569262146949768
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.15202070772647858
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.10871941596269608
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.14249981939792633
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.1080729141831398
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.1229386180639267
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.1191418394446373
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.14672048389911652
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.14186647534370422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.0546, 5.1133],
        [5.1597, 5.1374, 5.1567],
        [5.1597, 4.8942, 4.8453],
        [5.1597, 4.9369, 4.9524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.1447620838880539 
model_pd.l_d.mean(): -20.459197998046875 
model_pd.lagr.mean(): -20.314435958862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-21.1912], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.1447620838880539
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.08323130756616592
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.18387657403945923
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.20857587456703186
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.08755293488502502
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12735724449157715
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.15654850006103516
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.1316128820180893
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.12706269323825836
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.18746815621852875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.1434, 5.1478],
        [5.1480, 5.1629, 4.8613],
        [5.1480, 5.1420, 5.1476],
        [5.1480, 5.5813, 5.5175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.17749148607254028 
model_pd.l_d.mean(): -17.400495529174805 
model_pd.lagr.mean(): -17.223003387451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6642], device='cuda:0')), ('power', tensor([-18.2691], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.17749148607254028
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11937684565782547
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.13179920613765717
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.2744710147380829
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.12685108184814453
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.15906350314617157
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.11676233261823654
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.12828685343265533
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.1307084560394287
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.11253061890602112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.0009, 4.6582],
        [5.1498, 5.1475, 5.1497],
        [5.1498, 5.0828, 4.7529],
        [5.1498, 5.0446, 5.1034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.1514662653207779 
model_pd.l_d.mean(): -20.666709899902344 
model_pd.lagr.mean(): -20.515243530273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4277], device='cuda:0')), ('power', tensor([-21.3294], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.1514662653207779
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.14951413869857788
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.14149504899978638
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.2033337503671646
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13512586057186127
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.1166708916425705
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.10182181745767593
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.16680696606636047
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.1721268743276596
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11951083689928055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1549, 5.1549, 5.1549],
        [5.1549, 4.8817, 4.8164],
        [5.1549, 4.9150, 4.9105],
        [5.1549, 4.9053, 4.5953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.14337791502475739 
model_pd.l_d.mean(): -20.06188201904297 
model_pd.lagr.mean(): -19.91850471496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-20.8046], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.14337791502475739
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.1286754608154297
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12621043622493744
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.09283072501420975
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12412374466657639
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.17095525562763214
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.18094785511493683
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.20307086408138275
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.15527090430259705
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.09649896621704102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.1593, 5.1600],
        [5.1600, 5.0420, 4.7021],
        [5.1600, 5.0312, 4.6901],
        [5.1600, 5.0338, 5.0944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.1336604505777359 
model_pd.l_d.mean(): -20.419034957885742 
model_pd.lagr.mean(): -20.28537368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.1025], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.1336604505777359
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.19428248703479767
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.11177530139684677
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.1368197798728943
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.16795887053012848
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.08659469336271286
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.19930297136306763
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.12066221237182617
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.11817159503698349
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.14583206176757812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.1533, 5.1562],
        [5.1563, 5.1532, 5.1562],
        [5.1563, 5.1523, 5.1561],
        [5.1563, 5.0144, 5.0741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.13141579926013947 
model_pd.l_d.mean(): -20.80585479736328 
model_pd.lagr.mean(): -20.6744384765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-21.4570], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.13141579926013947
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.1794956922531128
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.14678026735782623
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.09076462686061859
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.1464032679796219
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.14625492691993713
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.13266617059707642
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.21279345452785492
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.13656243681907654
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.11641935259103775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 4.8560, 4.6632],
        [5.1530, 5.1530, 5.1530],
        [5.1530, 4.8728, 4.7894],
        [5.1530, 5.1530, 5.1530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.13288450241088867 
model_pd.l_d.mean(): -20.105987548828125 
model_pd.lagr.mean(): -19.973102569580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4653], device='cuda:0')), ('power', tensor([-20.8010], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.13288450241088867
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.10361194610595703
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.1611689031124115
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12851808965206146
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.11599193513393402
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.11489781737327576
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.15841206908226013
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.15024669468402863
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.20132434368133545
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.1631573885679245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 4.9901, 5.0427],
        [5.1599, 5.0315, 5.0921],
        [5.1599, 4.8732, 4.6227],
        [5.1599, 5.2162, 4.9322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.16464057564735413 
model_pd.l_d.mean(): -19.68216323852539 
model_pd.lagr.mean(): -19.51752281188965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5099], device='cuda:0')), ('power', tensor([-20.4180], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.16464057564735413
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.183492049574852
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.1297513097524643
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.13294750452041626
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.15783855319023132
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11909571290016174
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.11328871548175812
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.10807531327009201
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.133203387260437
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.15801717340946198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.5358, 5.4316],
        [5.1624, 4.9998, 5.0549],
        [5.1624, 4.8684, 4.7202],
        [5.1624, 5.1623, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.11791692674160004 
model_pd.l_d.mean(): -19.022048950195312 
model_pd.lagr.mean(): -18.904132843017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5915], device='cuda:0')), ('power', tensor([-19.8341], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.11791692674160004
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.21442389488220215
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.13269968330860138
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.12825395166873932
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.08312559127807617
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.11062978208065033
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.1293472945690155
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.15356485545635223
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.1968340277671814
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.13648204505443573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 4.9055, 4.8768],
        [5.1608, 4.8644, 4.6856],
        [5.1608, 5.1608, 5.1608],
        [5.1608, 4.9077, 4.8828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12353827059268951 
model_pd.l_d.mean(): -19.953506469726562 
model_pd.lagr.mean(): -19.829967498779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-20.6178], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12353827059268951
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.1240568682551384
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.18343375623226166
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.11108403652906418
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.09180557727813721
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.12237536907196045
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.18154136836528778
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.14471440017223358
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.16413143277168274
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14782704412937164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1660, 5.1369, 5.1613],
        [5.1660, 5.1091, 5.1509],
        [5.1660, 5.1634, 5.1659],
        [5.1660, 5.2440, 4.9699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.18805146217346191 
model_pd.l_d.mean(): -20.390668869018555 
model_pd.lagr.mean(): -20.202617645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4689], device='cuda:0')), ('power', tensor([-21.0925], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.18805146217346191
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.08783140033483505
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.15052954852581024
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.1364310085773468
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.10745345801115036
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.12420472502708435
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.14567232131958008
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.1722206026315689
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.1382238268852234
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13707870244979858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.1653, 5.1653],
        [5.1653, 5.1383, 5.1612],
        [5.1653, 4.8731, 4.7367],
        [5.1653, 5.1653, 5.1653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.19377395510673523 
model_pd.l_d.mean(): -20.941186904907227 
model_pd.lagr.mean(): -20.747413635253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3917], device='cuda:0')), ('power', tensor([-21.5701], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.19377395510673523
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.10131676495075226
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.12150213122367859
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.14899559319019318
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.11103469878435135
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.11156671494245529
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.16900430619716644
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.12118203938007355
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.16327138245105743
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13864871859550476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 4.9958, 5.0451],
        [5.1723, 5.1530, 5.1700],
        [5.1723, 5.1099, 4.7814],
        [5.1723, 5.1385, 5.1663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.13571365177631378 
model_pd.l_d.mean(): -20.69606590270996 
model_pd.lagr.mean(): -20.560352325439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([-21.3564], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.13571365177631378
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11213317513465881
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.15628767013549805
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.12654222548007965
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.1279439777135849
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.13007479906082153
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.14554449915885925
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.18231739103794098
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.1280849426984787
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.11321409046649933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1753, 5.1664, 5.1747],
        [5.1753, 5.1184, 5.1602],
        [5.1753, 5.1017, 5.1512],
        [5.1753, 5.1558, 5.1730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.1410083770751953 
model_pd.l_d.mean(): -19.73017692565918 
model_pd.lagr.mean(): -19.589168548583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-20.4248], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.1410083770751953
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12412092089653015
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.13228215277194977
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.14894254505634308
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13508424162864685
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.1579497903585434
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.09036437422037125
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.1164385974407196
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.1557948887348175
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.20096832513809204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.4575, 5.3070],
        [5.1566, 5.1480, 5.1560],
        [5.1566, 5.1343, 5.1536],
        [5.1566, 5.4082, 5.2277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.12524311244487762 
model_pd.l_d.mean(): -20.022579193115234 
model_pd.lagr.mean(): -19.897336959838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-20.7868], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.12524311244487762
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.18859203159809113
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.12633435428142548
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.2019580453634262
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.16819819808006287
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.09460245817899704
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11649299412965775
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.08641529083251953
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.18085680902004242
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.153026282787323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 5.0240, 4.6819],
        [5.1503, 4.9728, 5.0226],
        [5.1503, 4.9465, 4.9803],
        [5.1503, 5.1503, 5.1503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.12750424444675446 
model_pd.l_d.mean(): -20.6362361907959 
model_pd.lagr.mean(): -20.508731842041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4305], device='cuda:0')), ('power', tensor([-21.3015], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.12750424444675446
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.15068472921848297
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.20235344767570496
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12823016941547394
epoch£º1182	 i:4 	 global-step:23644	 l-p:0.1082935705780983
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.22450706362724304
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.0817074105143547
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.16922476887702942
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13032124936580658
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.17012622952461243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1471, 5.4066, 5.2310],
        [5.1471, 5.3038, 5.0696],
        [5.1471, 5.1471, 5.1471],
        [5.1471, 4.8578, 4.6067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.101964071393013 
model_pd.l_d.mean(): -20.256011962890625 
model_pd.lagr.mean(): -20.1540470123291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5097], device='cuda:0')), ('power', tensor([-20.9980], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.101964071393013
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.1423252671957016
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.09314486384391785
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.14671912789344788
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.20115624368190765
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.2213549166917801
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.1410476565361023
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.1361498385667801
epoch£º1183	 i:8 	 global-step:23668	 l-p:0.11882922798395157
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.16162024438381195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1518, 5.0873, 4.7574],
        [5.1518, 5.1435, 5.1512],
        [5.1518, 5.1941, 4.9034],
        [5.1518, 4.9110, 4.9066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.17272032797336578 
model_pd.l_d.mean(): -20.63779640197754 
model_pd.lagr.mean(): -20.465076446533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415], device='cuda:0')), ('power', tensor([-21.3143], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.17272032797336578
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.17125123739242554
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.17781805992126465
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.11701120436191559
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.10238318890333176
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.14960236847400665
epoch£º1184	 i:6 	 global-step:23686	 l-p:0.1415826380252838
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.07591290026903152
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.20065170526504517
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.11531293392181396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.8740, 4.7397],
        [5.1664, 4.9421, 4.9565],
        [5.1664, 4.9387, 4.9494],
        [5.1664, 5.0428, 5.1035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.15362843871116638 
model_pd.l_d.mean(): -20.508499145507812 
model_pd.lagr.mean(): -20.354869842529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4502], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.15362843871116638
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12697921693325043
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.1589995175600052
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12169153243303299
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.12527529895305634
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.189168319106102
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.1350717395544052
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.14046552777290344
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.11141999065876007
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.10879293084144592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1305, 5.1645],
        [5.1739, 5.1218, 5.1610],
        [5.1739, 4.9059, 4.8512],
        [5.1739, 5.1348, 4.8129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.08886934816837311 
model_pd.l_d.mean(): -20.49295997619629 
model_pd.lagr.mean(): -20.404090881347656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4556], device='cuda:0')), ('power', tensor([-21.1822], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.08886934816837311
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.11991992592811584
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.13800229132175446
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14785221219062805
epoch£º1186	 i:4 	 global-step:23724	 l-p:0.13763001561164856
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.18990428745746613
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.11073436588048935
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13798083364963531
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.11795642226934433
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.15756331384181976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 5.0257, 5.0834],
        [5.1778, 5.3063, 5.0568],
        [5.1778, 5.0960, 5.1486],
        [5.1778, 5.1763, 5.1777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.18110011518001556 
model_pd.l_d.mean(): -20.482982635498047 
model_pd.lagr.mean(): -20.301881790161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4537], device='cuda:0')), ('power', tensor([-21.1703], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.18110011518001556
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.1471765637397766
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11548309028148651
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.15293170511722565
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12187448143959045
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.1446245312690735
epoch£º1187	 i:6 	 global-step:23746	 l-p:0.09616773575544357
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.15629367530345917
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.07054921239614487
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15661132335662842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1808, 5.0884, 5.1443],
        [5.1808, 5.1808, 5.1808],
        [5.1808, 5.6056, 5.5345],
        [5.1808, 5.1356, 5.1707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12492978572845459 
model_pd.l_d.mean(): -20.43494987487793 
model_pd.lagr.mean(): -20.310020446777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4005], device='cuda:0')), ('power', tensor([-21.0674], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12492978572845459
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.16346244513988495
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11230400949716568
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.12320958822965622
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.11033638566732407
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.18963490426540375
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.0640888586640358
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.18291926383972168
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.13639861345291138
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12058825045824051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1859, 5.0460, 5.1057],
        [5.1859, 4.9650, 4.9818],
        [5.1859, 5.1859, 5.1859],
        [5.1859, 5.1840, 5.1858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.11269199848175049 
model_pd.l_d.mean(): -20.9989070892334 
model_pd.lagr.mean(): -20.886215209960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3639], device='cuda:0')), ('power', tensor([-21.6000], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.11269199848175049
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.1992747038602829
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.1692037135362625
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.05559083819389343
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.13954390585422516
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12783630192279816
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.19334270060062408
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.09218888729810715
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.12163131684064865
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.10143107920885086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1862, 5.1862, 5.1862],
        [5.1862, 5.1672, 5.1839],
        [5.1862, 5.1862, 5.1862],
        [5.1862, 5.1701, 5.1845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.0868552103638649 
model_pd.l_d.mean(): -19.695524215698242 
model_pd.lagr.mean(): -19.60866928100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4933], device='cuda:0')), ('power', tensor([-20.4146], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.0868552103638649
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.16617903113365173
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.0725073367357254
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.1621306836605072
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.15802550315856934
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.1937769055366516
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.13378769159317017
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.12910869717597961
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.08922074735164642
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.12030798941850662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1884, 5.5550, 5.4451],
        [5.1884, 5.1884, 5.1884],
        [5.1884, 5.1450, 5.1790],
        [5.1884, 4.9660, 4.9811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.07378046214580536 
model_pd.l_d.mean(): -19.26618194580078 
model_pd.lagr.mean(): -19.192401885986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5709], device='cuda:0')), ('power', tensor([-20.0598], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.07378046214580536
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.12385687232017517
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.10648535192012787
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.1504838764667511
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.09046244621276855
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.1560746133327484
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.15130282938480377
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.1276506781578064
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.14112146198749542
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.19161728024482727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1860, 5.1079, 5.1592],
        [5.1860, 5.0950, 5.1505],
        [5.1860, 5.0257, 5.0812],
        [5.1860, 5.1810, 5.1858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.14259961247444153 
model_pd.l_d.mean(): -19.759288787841797 
model_pd.lagr.mean(): -19.616689682006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-20.5039], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.14259961247444153
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.1865440011024475
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.1351100206375122
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.08672285079956055
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.13350337743759155
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.15429770946502686
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.13288705050945282
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.1091926246881485
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.14719852805137634
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.0936669334769249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1820, 4.8949, 4.6492],
        [5.1820, 5.1263, 5.1674],
        [5.1820, 5.1772, 5.1817],
        [5.1820, 5.1594, 5.1789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.1411682665348053 
model_pd.l_d.mean(): -20.167238235473633 
model_pd.lagr.mean(): -20.02606964111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5161], device='cuda:0')), ('power', tensor([-20.9147], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.1411682665348053
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.1004338338971138
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.1099279597401619
epoch£º1193	 i:3 	 global-step:23863	 l-p:0.1468285620212555
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.14256292581558228
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14853814244270325
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.1322629749774933
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.15251922607421875
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.1273694485425949
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.13298964500427246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1782, 4.9302, 4.6198],
        [5.1782, 5.1771, 5.1781],
        [5.1782, 5.0327, 4.6904],
        [5.1782, 4.8841, 4.6692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.1199134886264801 
model_pd.l_d.mean(): -20.500850677490234 
model_pd.lagr.mean(): -20.380937576293945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-21.1825], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.1199134886264801
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.1365395188331604
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.13002856075763702
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.16070319712162018
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.14152154326438904
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.16278937458992004
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.1095837652683258
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.1241755485534668
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.14994443953037262
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.17187993228435516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1578, 5.1526, 5.1575],
        [5.1578, 5.1573, 5.1577],
        [5.1578, 5.0230, 5.0838],
        [5.1578, 4.8852, 4.8245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.14179310202598572 
model_pd.l_d.mean(): -20.509647369384766 
model_pd.lagr.mean(): -20.367855072021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4532], device='cuda:0')), ('power', tensor([-21.1967], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.14179310202598572
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.1187395229935646
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.17189383506774902
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.16881832480430603
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12435328960418701
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.08798541128635406
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.1350642442703247
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.13980022072792053
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.15643948316574097
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.17926284670829773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 5.0786, 5.1315],
        [5.1608, 5.1295, 5.1555],
        [5.1608, 5.3809, 5.1814],
        [5.1608, 5.2373, 4.9617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.13529466092586517 
model_pd.l_d.mean(): -20.30022430419922 
model_pd.lagr.mean(): -20.16493034362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-21.0187], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.13529466092586517
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.14683887362480164
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.18918415904045105
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.1498323678970337
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.10160444676876068
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.14468418061733246
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.1401224434375763
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12305326014757156
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.13525591790676117
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.13716904819011688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1631, 4.8821, 4.7982],
        [5.1631, 5.0794, 5.1328],
        [5.1631, 5.4850, 5.3469],
        [5.1631, 4.9557, 4.6202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.14796769618988037 
model_pd.l_d.mean(): -20.15070343017578 
model_pd.lagr.mean(): -20.002735137939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5094], device='cuda:0')), ('power', tensor([-20.8912], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.14796769618988037
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.14052504301071167
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.11647629737854004
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.14379145205020905
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.15002672374248505
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.20910584926605225
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13865578174591064
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.16301707923412323
epoch£º1197	 i:8 	 global-step:23948	 l-p:0.1283862441778183
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.07356598973274231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.1421, 4.8251],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 5.1598, 5.1640],
        [5.1642, 5.1474, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.10651061683893204 
model_pd.l_d.mean(): -20.04910659790039 
model_pd.lagr.mean(): -19.942596435546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-20.7760], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:0.10651061683893204
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.12888671457767487
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.09611345082521439
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.1671779602766037
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.14248888194561005
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12046432495117188
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.1365823894739151
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.14169913530349731
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.21544751524925232
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.14626367390155792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1634, 5.1648],
        [5.1648, 5.1648, 5.1648],
        [5.1648, 5.1640, 5.1648],
        [5.1648, 5.1648, 5.1648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.10654757916927338 
model_pd.l_d.mean(): -19.27239227294922 
model_pd.lagr.mean(): -19.165843963623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5539], device='cuda:0')), ('power', tensor([-20.0488], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.10654757916927338
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.1893337070941925
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.09351585805416107
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14319464564323425
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.13185027241706848
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.15061500668525696
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.09286996722221375
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.18859420716762543
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13402941823005676
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.16337820887565613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.0844, 5.1364],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 4.9344, 4.9437],
        [5.1642, 4.9292, 4.9323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05070394277572632 
model_pd.l_d.mean(): -20.29601287841797 
model_pd.lagr.mean(): -20.245309829711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-20.9953], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.05070394277572632
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.12023328244686127
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.1125398576259613
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.13600243628025055
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.16928265988826752
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.13015876710414886
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.1670687198638916
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.21944421529769897
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.11337611079216003
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.18440648913383484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.1633, 5.1632],
        [5.1632, 5.1388, 5.1598],
        [5.1632, 4.8730, 4.6268],
        [5.1632, 4.8676, 4.6464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.13749676942825317 
model_pd.l_d.mean(): -19.02437973022461 
model_pd.lagr.mean(): -18.886882781982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5395], device='cuda:0')), ('power', tensor([-19.7834], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.13749676942825317
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.1401907056570053
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.18846385180950165
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10712747275829315
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.1326969712972641
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.1224483847618103
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.13435791432857513
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.14504499733448029
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.18903414905071259
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.1126699298620224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.0773, 5.1308],
        [5.1611, 4.8760, 4.7804],
        [5.1611, 5.1611, 5.1611],
        [5.1611, 5.4425, 5.2791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.0878671258687973 
model_pd.l_d.mean(): -19.823747634887695 
model_pd.lagr.mean(): -19.73587989807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5133], device='cuda:0')), ('power', tensor([-20.5647], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.0878671258687973
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.14910295605659485
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14251337945461273
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.14336441457271576
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.09406933933496475
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10306064039468765
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.15065284073352814
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.2748255729675293
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.1524515151977539
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.14020520448684692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1540, 5.1302, 5.1507],
        [5.1540, 5.1536, 5.1540],
        [5.1540, 4.9772, 4.6338],
        [5.1540, 4.9011, 4.5895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.22940956056118011 
model_pd.l_d.mean(): -20.437082290649414 
model_pd.lagr.mean(): -20.207672119140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4944], device='cuda:0')), ('power', tensor([-21.1654], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.22940956056118011
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.15814349055290222
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.11434093117713928
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.17247430980205536
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12213384360074997
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.1539766788482666
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.10625676065683365
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13591302931308746
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.13243554532527924
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.1270279735326767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 5.1433, 5.1518],
        [5.1524, 5.1231, 5.1477],
        [5.1524, 4.8635, 4.7574],
        [5.1524, 4.9920, 5.0486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.16746985912322998 
model_pd.l_d.mean(): -20.730043411254883 
model_pd.lagr.mean(): -20.56257438659668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4293], device='cuda:0')), ('power', tensor([-21.3951], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.16746985912322998
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.1421034187078476
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.12900039553642273
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.124119333922863
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12984682619571686
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.2677503228187561
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.08255261182785034
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.14453336596488953
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.1129324734210968
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.1608545482158661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 4.8972, 4.5871],
        [5.1522, 5.0990, 5.1388],
        [5.1522, 5.2714, 5.0167],
        [5.1522, 4.9221, 4.5942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.1166185736656189 
model_pd.l_d.mean(): -20.545364379882812 
model_pd.lagr.mean(): -20.42874526977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.2483], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.1166185736656189
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.15051132440567017
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12728199362754822
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.2235068827867508
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.10351362824440002
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.14755779504776
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.16528844833374023
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12986968457698822
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.19427044689655304
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.1172243282198906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1486, 5.1486, 5.1486],
        [5.1486, 4.8487, 4.6515],
        [5.1486, 5.0975, 5.1362],
        [5.1486, 4.8730, 4.8081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.16964390873908997 
model_pd.l_d.mean(): -20.764657974243164 
model_pd.lagr.mean(): -20.595014572143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.4197], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.16964390873908997
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12705571949481964
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.07409337162971497
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.14218585193157196
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.16337989270687103
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.18491338193416595
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.17506594955921173
epoch£º1206	 i:7 	 global-step:24127	 l-p:0.09274140000343323
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.127533420920372
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.21510332822799683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1528, 4.9727, 5.0218],
        [5.1528, 4.8766, 4.5931],
        [5.1528, 5.1516, 5.1528],
        [5.1528, 4.8541, 4.6903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.17246459424495697 
model_pd.l_d.mean(): -20.204925537109375 
model_pd.lagr.mean(): -20.032461166381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.9448], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.17246459424495697
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.13330350816249847
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11844906210899353
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.1706622987985611
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.17338882386684418
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.1288481503725052
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.14721998572349548
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.15366563200950623
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.12456085532903671
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11885639280080795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 4.9835, 4.6403],
        [5.1589, 5.0271, 5.0881],
        [5.1589, 5.1589, 5.1589],
        [5.1589, 5.0087, 5.0677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.12961313128471375 
model_pd.l_d.mean(): -20.089113235473633 
model_pd.lagr.mean(): -19.95949935913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-20.8331], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.12961313128471375
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.20810429751873016
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.17728836834430695
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.12406391650438309
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.1362891048192978
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.18551774322986603
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.13866370916366577
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.08502944558858871
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.13217772543430328
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.1089383065700531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.0636, 5.1203],
        [5.1580, 5.1456, 5.1569],
        [5.1580, 5.1580, 5.1580],
        [5.1580, 5.4372, 5.2725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.16635502874851227 
model_pd.l_d.mean(): -19.659833908081055 
model_pd.lagr.mean(): -19.493478775024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.3596], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.16635502874851227
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.17288827896118164
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.16177771985530853
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.10487353801727295
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.1404303014278412
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11487843841314316
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.1166883260011673
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.21426421403884888
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.12198200821876526
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.1242673471570015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1525, 5.1542],
        [5.1542, 5.3715, 5.1701],
        [5.1542, 5.1489, 5.1539],
        [5.1542, 5.1023, 5.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.1541319191455841 
model_pd.l_d.mean(): -20.059940338134766 
model_pd.lagr.mean(): -19.905807495117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5288], device='cuda:0')), ('power', tensor([-20.8193], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.1541319191455841
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.1264037787914276
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12304763495922089
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.17186568677425385
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.10263439267873764
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.1026785671710968
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.14201219379901886
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.16285625100135803
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.25610074400901794
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.1210787370800972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 5.1512, 5.1512],
        [5.1512, 5.1369, 5.1498],
        [5.1512, 5.1512, 5.1512],
        [5.1512, 5.1512, 5.1512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.19419632852077484 
model_pd.l_d.mean(): -20.592670440673828 
model_pd.lagr.mean(): -20.398473739624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-21.2882], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.19419632852077484
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.11876533925533295
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.15175841748714447
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.1190306544303894
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.24292664229869843
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13717958331108093
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12784098088741302
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.0768706277012825
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.15791016817092896
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.14035947620868683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1517, 5.1455, 5.1513],
        [5.1517, 5.1498, 5.1516],
        [5.1517, 4.9811, 5.0345],
        [5.1517, 5.1516, 5.1517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.18966159224510193 
model_pd.l_d.mean(): -19.302061080932617 
model_pd.lagr.mean(): -19.11240005493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5565], device='cuda:0')), ('power', tensor([-20.0814], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.18966159224510193
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.09392047673463821
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13939635455608368
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.08447851240634918
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.160795658826828
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.21660995483398438
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.19691401720046997
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.12424816191196442
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.1099294126033783
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.14032745361328125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.1563, 5.1565],
        [5.1565, 5.1564, 5.1565],
        [5.1565, 5.0719, 5.1257],
        [5.1565, 5.0272, 5.0883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.15540610253810883 
model_pd.l_d.mean(): -20.399112701416016 
model_pd.lagr.mean(): -20.24370574951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4729], device='cuda:0')), ('power', tensor([-21.1050], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.15540610253810883
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.1377522200345993
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.11718515306711197
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.10055495798587799
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.10208230465650558
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.13082730770111084
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.1521003097295761
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.13254566490650177
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.1960235983133316
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.209077849984169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.0060, 5.0646],
        [5.1580, 4.9489, 4.6126],
        [5.1580, 5.0939, 5.1394],
        [5.1580, 5.0804, 5.1317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.13832199573516846 
model_pd.l_d.mean(): -20.473552703857422 
model_pd.lagr.mean(): -20.335229873657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.1652], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.13832199573516846
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.12100940197706223
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.18428291380405426
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.1430719792842865
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.1502663493156433
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.1292150616645813
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.1161440759897232
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.16194990277290344
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11588859558105469
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.17292636632919312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 4.9625, 4.6224],
        [5.1581, 4.9816, 4.6383],
        [5.1581, 4.9184, 4.9164],
        [5.1581, 5.1581, 5.1581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.13138699531555176 
model_pd.l_d.mean(): -20.672687530517578 
model_pd.lagr.mean(): -20.54129981994629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.3491], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.13138699531555176
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.15107372403144836
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.14289450645446777
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.15909501910209656
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.1710723489522934
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.11382237076759338
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.1497466117143631
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.16662070155143738
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.10448361188173294
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.1374560296535492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 4.9841, 4.6409],
        [5.1601, 5.1154, 4.7905],
        [5.1601, 5.0657, 5.1224],
        [5.1601, 5.1597, 5.1601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12212469428777695 
model_pd.l_d.mean(): -19.46112632751465 
model_pd.lagr.mean(): -19.339000701904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-20.1615], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12212469428777695
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.14491066336631775
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.1475413292646408
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.13339826464653015
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.1112293228507042
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.08759697526693344
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.1239931657910347
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.1513044387102127
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.24516253173351288
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.16793686151504517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 5.1543, 5.1573],
        [5.1574, 4.8626, 4.7281],
        [5.1574, 5.0068, 4.6621],
        [5.1574, 4.9716, 5.0177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.1552998274564743 
model_pd.l_d.mean(): -19.48999786376953 
model_pd.lagr.mean(): -19.334697723388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5410], device='cuda:0')), ('power', tensor([-20.2556], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.1552998274564743
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.1306832730770111
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.14201053977012634
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.17936348915100098
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.11319908499717712
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.21360060572624207
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.14973092079162598
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12916076183319092
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.05647191032767296
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.1560399830341339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 4.9662, 4.6262],
        [5.1613, 4.8624, 4.6838],
        [5.1613, 5.1612, 5.1613],
        [5.1613, 5.0452, 5.1057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.16990362107753754 
model_pd.l_d.mean(): -19.898841857910156 
model_pd.lagr.mean(): -19.728939056396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5008], device='cuda:0')), ('power', tensor([-20.6279], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.16990362107753754
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.16102895140647888
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.17247119545936584
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.1423618048429489
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.17496736347675323
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.07356147468090057
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.15016421675682068
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.10269764810800552
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.13073022663593292
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.13156352937221527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1638, 4.8661, 4.6596],
        [5.1638, 5.0800, 5.1335],
        [5.1638, 5.1246, 5.1560],
        [5.1638, 5.1638, 5.1638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.1319739818572998 
model_pd.l_d.mean(): -19.337879180908203 
model_pd.lagr.mean(): -19.20590591430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5073], device='cuda:0')), ('power', tensor([-20.0674], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.1319739818572998
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13258013129234314
epoch£º1219	 i:2 	 global-step:24382	 l-p:0.07110698521137238
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.17444075644016266
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.11252536624670029
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.17707961797714233
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.16713544726371765
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.12124103307723999
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.2077801525592804
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.1104116439819336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1643, 5.1642, 5.1643],
        [5.1643, 5.3845, 5.1846],
        [5.1643, 5.0762, 4.7396],
        [5.1643, 4.9904, 4.6472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.1780383288860321 
model_pd.l_d.mean(): -20.754501342773438 
model_pd.lagr.mean(): -20.57646369934082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4276], device='cuda:0')), ('power', tensor([-21.4180], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.1780383288860321
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13716964423656464
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.12889578938484192
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.12296918779611588
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.18802331387996674
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12111692875623703
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.07564889639616013
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.14268319308757782
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.13297545909881592
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.17938552796840668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.1610, 5.1621],
        [5.1621, 5.1621, 5.1621],
        [5.1621, 5.1384, 4.8205],
        [5.1621, 4.8636, 4.6638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.11045200377702713 
model_pd.l_d.mean(): -20.42298698425293 
model_pd.lagr.mean(): -20.31253433227539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4855], device='cuda:0')), ('power', tensor([-21.1421], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:0.11045200377702713
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.10978072881698608
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.1543540507555008
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.17600440979003906
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.1654784232378006
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.1341658979654312
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.15343455970287323
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.18247367441654205
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.11012450605630875
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.12400591373443604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1594, 5.1594, 5.1594],
        [5.1594, 5.1035, 5.1448],
        [5.1594, 5.1426, 5.1576],
        [5.1594, 5.1589, 5.1594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.13509222865104675 
model_pd.l_d.mean(): -20.799123764038086 
model_pd.lagr.mean(): -20.664031982421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4130], device='cuda:0')), ('power', tensor([-21.4482], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.13509222865104675
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.1325303018093109
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.13903509080410004
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.10834591835737228
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.11531767249107361
epoch£º1222	 i:5 	 global-step:24445	 l-p:0.15168020129203796
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.14033789932727814
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.16420714557170868
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.24076297879219055
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.1300191879272461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1520, 5.1014, 5.1398],
        [5.1520, 5.1381, 5.1506],
        [5.1520, 5.0718, 5.1241],
        [5.1520, 4.9311, 4.9514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11754938215017319 
model_pd.l_d.mean(): -19.445011138916016 
model_pd.lagr.mean(): -19.32746124267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5048], device='cuda:0')), ('power', tensor([-20.1731], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11754938215017319
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19682982563972473
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.13954240083694458
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.14844386279582977
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.20226344466209412
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.1655714511871338
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.12065809220075607
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10808389633893967
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.139787957072258
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.13121940195560455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1506, 5.0695, 5.1221],
        [5.1506, 5.1506, 5.1506],
        [5.1506, 5.5151, 5.4040],
        [5.1506, 4.8520, 4.6353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.21466994285583496 
model_pd.l_d.mean(): -19.178312301635742 
model_pd.lagr.mean(): -18.963642120361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-19.9238], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.21466994285583496
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.16894939541816711
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13261432945728302
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.10798240453004837
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.20343372225761414
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.13029125332832336
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13530310988426208
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.15014757215976715
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.09408483654260635
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.13399381935596466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1505, 5.1278, 5.1474],
        [5.1505, 5.1503, 5.1505],
        [5.1505, 5.1320, 5.1483],
        [5.1505, 5.1505, 5.1505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.11014917492866516 
model_pd.l_d.mean(): -20.466936111450195 
model_pd.lagr.mean(): -20.356786727905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.1422], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.11014917492866516
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.22645388543605804
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.1361265480518341
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12553004920482635
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.12344487011432648
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12958961725234985
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.16933605074882507
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.20428626239299774
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.11417995393276215
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.15101578831672668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1469, 5.0146, 5.0759],
        [5.1469, 4.8769, 4.5813],
        [5.1469, 5.1469, 5.1469],
        [5.1469, 5.1468, 5.1469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.12719443440437317 
model_pd.l_d.mean(): -19.130281448364258 
model_pd.lagr.mean(): -19.00308609008789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5735], device='cuda:0')), ('power', tensor([-19.9252], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.12719443440437317
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.1672385334968567
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.11697576940059662
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.1785617470741272
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.20686347782611847
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.16615460813045502
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.11774596571922302
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.09790321439504623
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.1600651890039444
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.15721267461776733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1418, 5.1470],
        [5.1473, 5.1461, 5.1473],
        [5.1473, 5.0565, 5.1123],
        [5.1473, 5.1473, 5.1473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.1005120798945427 
model_pd.l_d.mean(): -20.19131088256836 
model_pd.lagr.mean(): -20.09079933166504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5036], device='cuda:0')), ('power', tensor([-20.9264], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.1005120798945427
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.18453556299209595
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.12251174449920654
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.14554305374622345
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.09481434524059296
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.1433788388967514
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.16956011950969696
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.2110535353422165
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.1411990523338318
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.1727508008480072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1502, 4.9903, 4.6448],
        [5.1502, 5.1392, 5.1493],
        [5.1502, 5.1471, 5.1501],
        [5.1502, 4.9716, 4.6275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.13189448416233063 
model_pd.l_d.mean(): -20.51203727722168 
model_pd.lagr.mean(): -20.380142211914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4513], device='cuda:0')), ('power', tensor([-21.1971], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.13189448416233063
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.14693643152713776
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.15018144249916077
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12411919981241226
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.1311027556657791
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.16998934745788574
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.09592299908399582
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.22436393797397614
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.1330696940422058
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.15029078722000122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.1048, 5.1440],
        [5.1568, 5.1490, 5.1563],
        [5.1568, 4.8571, 4.6789],
        [5.1568, 4.8616, 4.7270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.09396356344223022 
model_pd.l_d.mean(): -19.384279251098633 
model_pd.lagr.mean(): -19.290315628051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4932], device='cuda:0')), ('power', tensor([-20.0999], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.09396356344223022
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.20033103227615356
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12559783458709717
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.13858991861343384
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.1923004537820816
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11639061570167542
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.1304500699043274
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.16137923300266266
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13480545580387115
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.12877418100833893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1623, 5.1071, 5.1480],
        [5.1623, 5.1536, 5.1617],
        [5.1623, 5.1618, 5.1623],
        [5.1623, 5.0357, 4.6924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.12705384194850922 
model_pd.l_d.mean(): -20.926250457763672 
model_pd.lagr.mean(): -20.799196243286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3968], device='cuda:0')), ('power', tensor([-21.5602], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.12705384194850922
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14346523582935333
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.2023625671863556
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12355479598045349
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.12363824248313904
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.10226734727621078
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13335458934307098
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.190907821059227
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.11808766424655914
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.14529597759246826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1619, 4.9148, 4.5984],
        [5.1619, 5.1618, 5.1619],
        [5.1619, 5.1619, 5.1619],
        [5.1619, 5.0310, 4.6872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.09235532581806183 
model_pd.l_d.mean(): -20.651947021484375 
model_pd.lagr.mean(): -20.55959129333496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350], device='cuda:0')), ('power', tensor([-21.3219], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.09235532581806183
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13636258244514465
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.20560477674007416
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.1510075181722641
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.2590353190898895
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14809513092041016
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12220222502946854
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.11797012388706207
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.12707337737083435
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.06878678500652313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1532, 4.8921, 4.5868],
        [5.1532, 5.1369, 5.1514],
        [5.1532, 4.8717, 4.5959],
        [5.1532, 4.8819, 4.8273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.1742001622915268 
model_pd.l_d.mean(): -20.79436683654785 
model_pd.lagr.mean(): -20.620166778564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4264], device='cuda:0')), ('power', tensor([-21.4571], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.1742001622915268
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.07139557600021362
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13170523941516876
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.1574726700782776
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.1511641889810562
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.137177512049675
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.252198725938797
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.1401844024658203
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.14204101264476776
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.12469898164272308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1457, 4.8624, 4.7788],
        [5.1457, 5.0259, 4.6825],
        [5.1457, 5.1457, 5.1457],
        [5.1457, 5.1367, 5.1451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.10909933596849442 
model_pd.l_d.mean(): -20.611480712890625 
model_pd.lagr.mean(): -20.502382278442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-21.2794], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.10909933596849442
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11758967489004135
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.09770814329385757
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.1293577402830124
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.1956029236316681
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.2645089328289032
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.25379303097724915
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.1169920489192009
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.1571677029132843
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.10362127423286438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1397, 5.1380, 5.1396],
        [5.1397, 5.1397, 5.1397],
        [5.1397, 5.1397, 5.1397],
        [5.1397, 4.8700, 4.8214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.12715421617031097 
model_pd.l_d.mean(): -20.77014923095703 
model_pd.lagr.mean(): -20.642995834350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4052], device='cuda:0')), ('power', tensor([-21.4109], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.12715421617031097
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.11829137802124023
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11980342864990234
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.1571442037820816
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10910753160715103
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.2587873637676239
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.351040780544281
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.11001051962375641
epoch£º1234	 i:8 	 global-step:24688	 l-p:0.10583382099866867
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.12268136441707611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.0913, 5.1256],
        [5.1350, 5.1319, 5.1349],
        [5.1350, 5.0997, 5.1285],
        [5.1350, 5.1266, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13985395431518555 
model_pd.l_d.mean(): -20.714584350585938 
model_pd.lagr.mean(): -20.574729919433594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.3687], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13985395431518555
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.19336308538913727
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.21250292658805847
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.2886832654476166
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.1090596541762352
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.11919180303812027
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.0933656096458435
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.09694131463766098
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.13256573677062988
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.23170073330402374
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 4.8596, 4.8053],
        [5.1325, 4.9615, 5.0155],
        [5.1325, 4.9676, 5.0237],
        [5.1325, 5.1325, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.09360429644584656 
model_pd.l_d.mean(): -20.319351196289062 
model_pd.lagr.mean(): -20.225746154785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4990], device='cuda:0')), ('power', tensor([-21.0511], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.09360429644584656
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13519109785556793
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.2077709585428238
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12314202636480331
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.1733633130788803
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.31315311789512634
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.14026358723640442
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.1388491839170456
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.1972128450870514
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.10646963119506836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1362, 5.1362, 5.1362],
        [5.1362, 5.1362, 5.1362],
        [5.1362, 5.1358, 5.1362],
        [5.1362, 5.0906, 5.1261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.1225496456027031 
model_pd.l_d.mean(): -20.04279899597168 
model_pd.lagr.mean(): -19.920249938964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-20.7346], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.1225496456027031
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.2433374673128128
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13487058877944946
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.10219970345497131
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.14417143166065216
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.1689520925283432
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.1498192548751831
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.09646689891815186
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.10500578582286835
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.30600425601005554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1390, 4.8370, 4.6575],
        [5.1390, 4.8692, 4.8207],
        [5.1390, 5.0746, 5.1203],
        [5.1390, 5.0931, 5.1288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.12947581708431244 
model_pd.l_d.mean(): -21.109439849853516 
model_pd.lagr.mean(): -20.979963302612305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3617], device='cuda:0')), ('power', tensor([-21.7096], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.12947581708431244
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.10926534235477448
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.15097269415855408
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.15652623772621155
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.1579076498746872
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.1299869865179062
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.3065057098865509
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.14032438397407532
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.1402256190776825
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.11664087325334549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 5.1446, 5.1451],
        [5.1451, 5.1373, 5.1446],
        [5.1451, 5.2544, 4.9943],
        [5.1451, 5.1318, 5.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.12011675536632538 
model_pd.l_d.mean(): -20.97162437438965 
model_pd.lagr.mean(): -20.85150718688965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3917], device='cuda:0')), ('power', tensor([-21.6008], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.12011675536632538
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13792160153388977
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.1249799132347107
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.13033650815486908
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10791786015033722
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.3379947543144226
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.11866632103919983
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.18728217482566833
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.12708528339862823
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.12846794724464417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1431, 5.5293, 5.4324],
        [5.1431, 5.4246, 5.2611],
        [5.1431, 5.0222, 5.0834],
        [5.1431, 4.9242, 4.5886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12162066996097565 
model_pd.l_d.mean(): -20.133874893188477 
model_pd.lagr.mean(): -20.01225471496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.8639], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12162066996097565
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.2328740507364273
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.1832393854856491
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.13241524994373322
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.13820016384124756
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.11616303771734238
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.12975069880485535
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.1687113344669342
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.15956175327301025
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13980571925640106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 5.0174, 5.0788],
        [5.1451, 4.8577, 4.7622],
        [5.1451, 4.9376, 4.9704],
        [5.1451, 5.0772, 5.1245]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.14079411327838898 
model_pd.l_d.mean(): -18.59880256652832 
model_pd.lagr.mean(): -18.4580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6012], device='cuda:0')), ('power', tensor([-19.4162], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.14079411327838898
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.16212083399295807
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.2636028230190277
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12880215048789978
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.1190822497010231
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.18814392387866974
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12371603399515152
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.09485002607107162
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.10397868603467941
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.1829010397195816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1463, 5.0267, 4.6831],
        [5.1463, 5.1402, 4.8282],
        [5.1463, 5.1463, 5.1463],
        [5.1463, 5.1420, 5.1461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.1824358105659485 
model_pd.l_d.mean(): -20.151885986328125 
model_pd.lagr.mean(): -19.969449996948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5114], device='cuda:0')), ('power', tensor([-20.8944], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.1824358105659485
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.11214084178209305
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.14834310114383698
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.09566900879144669
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.1425468772649765
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.18067346513271332
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.2147703468799591
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.154370978474617
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.11565802246332169
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.14632785320281982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1509, 5.1505, 5.1509],
        [5.1509, 5.0697, 5.1224],
        [5.1509, 5.0212, 5.0825],
        [5.1509, 4.8584, 4.7418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.18369919061660767 
model_pd.l_d.mean(): -20.34986114501953 
model_pd.lagr.mean(): -20.166162490844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4776], device='cuda:0')), ('power', tensor([-21.0601], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.18369919061660767
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.14052577316761017
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.12775570154190063
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.12309185415506363
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.1723262369632721
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.1274515837430954
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13468509912490845
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.1582944542169571
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.13305391371250153
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.16478082537651062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1555, 5.0843, 5.1330],
        [5.1555, 5.0048, 5.0640],
        [5.1555, 4.9249, 4.9350],
        [5.1555, 5.1555, 5.1555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.17863398790359497 
model_pd.l_d.mean(): -20.31220245361328 
model_pd.lagr.mean(): -20.133567810058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-20.9742], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.17863398790359497
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.1396029144525528
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.24677537381649017
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.1341734528541565
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.12114616483449936
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.11614375561475754
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13669653236865997
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.14427965879440308
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.07858126610517502
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13970473408699036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1615, 5.0389, 4.6957],
        [5.1615, 5.0117, 4.6665],
        [5.1615, 5.2137, 4.9261],
        [5.1615, 4.9085, 4.5963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.16431011259555817 
model_pd.l_d.mean(): -20.588773727416992 
model_pd.lagr.mean(): -20.424463272094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4621], device='cuda:0')), ('power', tensor([-21.2858], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.16431011259555817
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.22984245419502258
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.11425263434648514
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.06303349137306213
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.16413265466690063
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.16511456668376923
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.1288192719221115
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.10318596661090851
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.1584748476743698
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12388679385185242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 4.9082, 4.5961],
        [5.1614, 5.1614, 5.1614],
        [5.1614, 5.1176, 5.1519],
        [5.1614, 5.1614, 5.1614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.21702073514461517 
model_pd.l_d.mean(): -20.13846778869629 
model_pd.lagr.mean(): -19.92144775390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-20.8997], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.21702073514461517
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.118364617228508
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.14538069069385529
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.1343996524810791
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.13245895504951477
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.11553512513637543
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.1175856962800026
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.11912042647600174
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.15413817763328552
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.17553025484085083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1561, 4.8582, 4.7093],
        [5.1561, 5.1561, 5.1561],
        [5.1561, 4.9833, 5.0359],
        [5.1561, 5.2685, 5.0097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.15595504641532898 
model_pd.l_d.mean(): -20.066368103027344 
model_pd.lagr.mean(): -19.91041374206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4868], device='cuda:0')), ('power', tensor([-20.7828], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.15595504641532898
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.11899121850728989
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.10782572627067566
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.1526925265789032
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.15867042541503906
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12160412222146988
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.1697208732366562
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.1383865624666214
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.14402583241462708
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.1774747222661972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 5.0656, 5.1214],
        [5.1564, 5.1564, 5.1564],
        [5.1564, 4.8741, 4.7921],
        [5.1564, 4.8567, 4.6900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.13426457345485687 
model_pd.l_d.mean(): -21.03473663330078 
model_pd.lagr.mean(): -20.90047264099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3808], device='cuda:0')), ('power', tensor([-21.6536], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.13426457345485687
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.170930877327919
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12796960771083832
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.13423103094100952
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.21528585255146027
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.17424923181533813
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.1327257752418518
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.1556253731250763
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.07898163795471191
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.12096504122018814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 4.8585, 4.6428],
        [5.1572, 5.1572, 5.1572],
        [5.1572, 4.8802, 4.5962],
        [5.1572, 5.1572, 5.1572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.17733582854270935 
model_pd.l_d.mean(): -20.321552276611328 
model_pd.lagr.mean(): -20.144216537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-21.0398], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.17733582854270935
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.089304618537426
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.1863626092672348
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.16460734605789185
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.10123009234666824
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.1753898411989212
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.12422624230384827
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.16452087461948395
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.1096869632601738
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.14544421434402466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[5.1591, 4.9521, 4.9846],
        [5.1591, 4.9281, 4.9377],
        [5.1591, 5.0055, 5.0640],
        [5.1591, 5.0123, 5.0721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.13334889709949493 
model_pd.l_d.mean(): -20.393396377563477 
model_pd.lagr.mean(): -20.260047912597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.0781], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.13334889709949493
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.1736343502998352
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13610254228115082
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.12863409519195557
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.12881320714950562
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.22562892735004425
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.07644803076982498
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.12255523353815079
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.11103951930999756
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.20412848889827728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1569, 4.8746, 4.7923],
        [5.1569, 5.1560, 5.1569],
        [5.1569, 5.1569, 5.1569],
        [5.1569, 4.9498, 4.9824]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.09599750488996506 
model_pd.l_d.mean(): -20.09178352355957 
model_pd.lagr.mean(): -19.995786666870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.8198], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.09599750488996506
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.1329662799835205
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.1901639848947525
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.13261447846889496
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.17833220958709717
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.11394251883029938
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.11363054066896439
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.2345181554555893
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.12219344824552536
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.12649644911289215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 5.0110, 5.0708],
        [5.1579, 4.9806, 4.6363],
        [5.1579, 5.1578, 5.1579],
        [5.1579, 4.8848, 4.5945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.16039881110191345 
model_pd.l_d.mean(): -20.099042892456055 
model_pd.lagr.mean(): -19.938644409179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5087], device='cuda:0')), ('power', tensor([-20.8383], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.16039881110191345
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.21201497316360474
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.1387762427330017
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.12415381520986557
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.11090373992919922
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.15117253363132477
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.10036401450634003
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.1662720888853073
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.14445433020591736
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.12221252173185349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 5.1596, 5.1596],
        [5.1596, 5.1590, 5.1596],
        [5.1596, 5.1596, 5.1596],
        [5.1596, 5.1596, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12517254054546356 
model_pd.l_d.mean(): -20.21561050415039 
model_pd.lagr.mean(): -20.090438842773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-20.8776], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12517254054546356
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.13579106330871582
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.15241502225399017
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.16490016877651215
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.16312329471111298
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.13392344117164612
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.09828551113605499
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.09289249032735825
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.11359832435846329
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.26303422451019287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1558, 5.4143, 5.2365],
        [5.1558, 4.9829, 5.0356],
        [5.1558, 5.1527, 5.1557],
        [5.1558, 4.8561, 4.6920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.12364687770605087 
model_pd.l_d.mean(): -19.302242279052734 
model_pd.lagr.mean(): -19.1785945892334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5424], device='cuda:0')), ('power', tensor([-20.0672], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.12364687770605087
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.10938321053981781
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.11172890663146973
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.11630155146121979
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.14219234883785248
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.12269458174705505
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.2643534243106842
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.12320117652416229
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.21570132672786713
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.13136398792266846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1527, 4.9719, 5.0213],
        [5.1527, 4.8563, 4.7217],
        [5.1527, 5.0203, 5.0816],
        [5.1527, 5.1527, 5.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.11596940457820892 
model_pd.l_d.mean(): -19.216339111328125 
model_pd.lagr.mean(): -19.100370407104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5672], device='cuda:0')), ('power', tensor([-20.0057], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.11596940457820892
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.18469306826591492
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.12007471174001694
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.1845221221446991
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.11742658168077469
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.20758506655693054
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.16175046563148499
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.12055858224630356
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.14392970502376556
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.10655894130468369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1541, 5.0522, 5.1108],
        [5.1541, 4.8779, 4.8128],
        [5.1541, 5.4302, 5.2630],
        [5.1541, 5.1455, 5.1535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.08768045902252197 
model_pd.l_d.mean(): -20.55815887451172 
model_pd.lagr.mean(): -20.470478057861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-21.2360], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.08768045902252197
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.14988449215888977
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.16779518127441406
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.1864466369152069
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.17407332360744476
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.15252871811389923
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.10235631465911865
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.13230597972869873
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.16360847651958466
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.14468321204185486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 4.8603, 4.7437],
        [5.1530, 5.0364, 4.6933],
        [5.1530, 5.1526, 5.1530],
        [5.1530, 5.1530, 5.1530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.11623083055019379 
model_pd.l_d.mean(): -20.428762435913086 
model_pd.lagr.mean(): -20.312532424926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4738], device='cuda:0')), ('power', tensor([-21.1360], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.11623083055019379
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.2189239263534546
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.11926944553852081
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.23321911692619324
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.10337115824222565
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.11183248460292816
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.14971180260181427
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.10656514763832092
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.15103232860565186
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.15661604702472687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1520, 5.1516, 5.1520],
        [5.1520, 5.1520, 5.1520],
        [5.1520, 4.8936, 4.8656],
        [5.1520, 4.8669, 4.5961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.135543093085289 
model_pd.l_d.mean(): -20.837602615356445 
model_pd.lagr.mean(): -20.702058792114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-21.4923], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.135543093085289
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.1926829069852829
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.2008432149887085
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.1291523277759552
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.12146171182394028
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.23018044233322144
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.06664439290761948
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.12232540547847748
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.14515261352062225
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12273073196411133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 5.1518, 5.1560],
        [5.1562, 4.8799, 4.8145],
        [5.1562, 5.1520, 5.1560],
        [5.1562, 4.9791, 4.6344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.14540044963359833 
model_pd.l_d.mean(): -20.005691528320312 
model_pd.lagr.mean(): -19.86029052734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5159], device='cuda:0')), ('power', tensor([-20.7513], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.14540044963359833
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.16426429152488708
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.1347387135028839
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.12106535583734512
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.07908778637647629
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.17730551958084106
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.13101233541965485
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13801629841327667
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.20105457305908203
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.14532513916492462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 5.1552, 5.1594],
        [5.1596, 4.8965, 4.8591],
        [5.1596, 5.1596, 5.1596],
        [5.1596, 5.0736, 5.1279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.16484610736370087 
model_pd.l_d.mean(): -20.49888038635254 
model_pd.lagr.mean(): -20.334033966064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-21.1932], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.16484610736370087
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.1932685673236847
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.15987154841423035
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.1176256462931633
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.16676244139671326
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.12905552983283997
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.10237595438957214
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.18840943276882172
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.06361053138971329
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.1352950930595398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 5.1636, 5.1636],
        [5.1636, 5.1634, 5.1636],
        [5.1636, 5.1467, 5.1617],
        [5.1636, 5.1587, 5.1633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.17238685488700867 
model_pd.l_d.mean(): -20.820987701416016 
model_pd.lagr.mean(): -20.648601531982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154], device='cuda:0')), ('power', tensor([-21.4728], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.17238685488700867
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.16071312129497528
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.21633118391036987
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.11577154695987701
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.10411107540130615
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.09680874645709991
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.11491121351718903
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.1259719282388687
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.18417775630950928
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.11452581733465195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 5.1787, 4.8744],
        [5.1651, 5.1406, 5.1616],
        [5.1651, 5.1770, 4.8719],
        [5.1651, 5.0487, 4.7061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.1827922910451889 
model_pd.l_d.mean(): -19.626785278320312 
model_pd.lagr.mean(): -19.443992614746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.3441], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.1827922910451889
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.1638304889202118
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.16323356330394745
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.18364225327968597
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.1162782609462738
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.11551763862371445
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.1497061848640442
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.08262505382299423
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.10177644342184067
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.14497917890548706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 5.2981, 5.0494],
        [5.1654, 5.1170, 5.1541],
        [5.1654, 5.0692, 5.1264],
        [5.1654, 5.0034, 5.0597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.08172450959682465 
model_pd.l_d.mean(): -18.96877098083496 
model_pd.lagr.mean(): -18.887046813964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5344], device='cuda:0')), ('power', tensor([-19.7219], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.08172450959682465
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.14126300811767578
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.214157372713089
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.1172887310385704
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.11639208346605301
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.19830222427845
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.13865447044372559
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.10513884574174881
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.13085755705833435
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.15770645439624786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1663, 4.8692, 4.6474],
        [5.1663, 4.9493, 4.9730],
        [5.1663, 5.1663, 5.1663],
        [5.1663, 5.1663, 5.1663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.10532283782958984 
model_pd.l_d.mean(): -19.193775177001953 
model_pd.lagr.mean(): -19.088451385498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5227], device='cuda:0')), ('power', tensor([-19.9375], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.10532283782958984
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.21962516009807587
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.14410482347011566
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.16386720538139343
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.1246238499879837
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.10422264784574509
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.1939646452665329
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.10270272940397263
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.13340850174427032
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.10877078026533127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.1592, 5.1641],
        [5.1644, 4.9547, 4.9850],
        [5.1644, 5.1108, 5.1509],
        [5.1644, 5.1644, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10757730901241302 
model_pd.l_d.mean(): -19.40266227722168 
model_pd.lagr.mean(): -19.295085906982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.1562], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10757730901241302
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.1390061378479004
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.14412888884544373
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.1577100306749344
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.1263587921857834
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.11699771881103516
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.21668784320354462
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.19539746642112732
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.12012677639722824
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.1019282341003418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[5.1598, 4.9344, 4.9504],
        [5.1598, 5.0060, 5.0646],
        [5.1598, 4.9506, 4.9816],
        [5.1598, 4.9044, 4.8805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.20722560584545135 
model_pd.l_d.mean(): -19.862730026245117 
model_pd.lagr.mean(): -19.65550422668457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4759], device='cuda:0')), ('power', tensor([-20.5658], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.20722560584545135
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.15900863707065582
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.13134990632534027
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.1393173784017563
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.10714820772409439
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.195469930768013
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.10453784465789795
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.09008552134037018
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.13046374917030334
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.16491058468818665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 4.8880, 4.8336],
        [5.1595, 5.1589, 5.1595],
        [5.1595, 5.1530, 5.1591],
        [5.1595, 5.1595, 5.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.1363927274942398 
model_pd.l_d.mean(): -20.806489944458008 
model_pd.lagr.mean(): -20.67009735107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4246], device='cuda:0')), ('power', tensor([-21.4675], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.1363927274942398
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.12324859201908112
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12557795643806458
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.10105614364147186
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.168942391872406
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.1800297647714615
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.15634088218212128
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.1323244273662567
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.1842620074748993
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.13402046263217926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.1555, 5.1571],
        [5.1572, 4.9599, 4.9997],
        [5.1572, 5.1572, 5.1572],
        [5.1572, 4.9031, 4.8817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.13404785096645355 
model_pd.l_d.mean(): -19.552902221679688 
model_pd.lagr.mean(): -19.418853759765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-20.2480], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.13404785096645355
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.1268180012702942
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.15558701753616333
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.16961516439914703
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.13062958419322968
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.1283002346754074
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.20165997743606567
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.09100224077701569
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.1330529898405075
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.16860315203666687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 4.9809, 5.0311],
        [5.1596, 5.1517, 5.1591],
        [5.1596, 5.1596, 5.1596],
        [5.1596, 5.1595, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.07057934254407883 
model_pd.l_d.mean(): -20.286718368530273 
model_pd.lagr.mean(): -20.21613883972168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-20.9882], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.07057934254407883
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.171754390001297
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.12870950996875763
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.17841529846191406
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.11686985939741135
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.16566988825798035
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.15797656774520874
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.14378024637699127
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.1285843551158905
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.15362849831581116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1688, 5.2841, 5.0264],
        [5.1688, 5.1676, 5.1688],
        [5.1688, 4.8701, 4.6632],
        [5.1688, 4.8772, 4.6302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.13066530227661133 
model_pd.l_d.mean(): -20.495834350585938 
model_pd.lagr.mean(): -20.365169525146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.1829], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.13066530227661133
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.1383601725101471
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.048780765384435654
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.18968547880649567
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.16206620633602142
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.14724716544151306
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.1497412919998169
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.1417549103498459
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.14543403685092926
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.1241486519575119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1754, 5.1759, 4.8663],
        [5.1754, 4.9890, 4.6471],
        [5.1754, 5.1229, 5.1623],
        [5.1754, 4.8809, 4.7460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.1318054497241974 
model_pd.l_d.mean(): -19.428844451904297 
model_pd.lagr.mean(): -19.297039031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5118], device='cuda:0')), ('power', tensor([-20.1639], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.1318054497241974
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.15704791247844696
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.14991940557956696
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.12042226642370224
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.1384228616952896
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.1483161747455597
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.18826600909233093
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.12372640520334244
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.12455672770738602
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.08902929723262787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[5.1728, 4.9781, 4.6377],
        [5.1728, 4.8804, 4.6384],
        [5.1728, 4.9029, 4.6109],
        [5.1728, 5.3938, 5.1935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.13615520298480988 
model_pd.l_d.mean(): -20.067773818969727 
model_pd.lagr.mean(): -19.931617736816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5275], device='cuda:0')), ('power', tensor([-20.8259], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.13615520298480988
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.1318388134241104
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12198708951473236
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.13426624238491058
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.10051538050174713
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.1433744877576828
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.14497728645801544
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.1417761892080307
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.13275375962257385
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.17748379707336426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1773, 4.8794, 4.6751],
        [5.1773, 5.1220, 5.1630],
        [5.1773, 5.1769, 5.1773],
        [5.1773, 5.1727, 5.1771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.1574237197637558 
model_pd.l_d.mean(): -19.882522583007812 
model_pd.lagr.mean(): -19.725099563598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5381], device='cuda:0')), ('power', tensor([-20.6494], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.1574237197637558
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.19215065240859985
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.0851699560880661
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14718174934387207
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.11363357305526733
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.09981625527143478
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17816321551799774
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.1256994754076004
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.1086297482252121
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.14353053271770477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1784, 5.1784, 5.1784],
        [5.1784, 5.0003, 5.0502],
        [5.1784, 4.9692, 4.9992],
        [5.1784, 5.1762, 5.1783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.1492781788110733 
model_pd.l_d.mean(): -20.053627014160156 
model_pd.lagr.mean(): -19.904348373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4625], device='cuda:0')), ('power', tensor([-20.7452], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.1492781788110733
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.1274588704109192
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.14402030408382416
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.10878953337669373
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.17580832540988922
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12565846741199493
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.12135114520788193
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.08048146963119507
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.19944675266742706
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.13950367271900177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.0423, 5.1035],
        [5.1696, 5.1676, 5.1696],
        [5.1696, 5.1689, 5.1696],
        [5.1696, 5.5258, 5.4082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.09631140530109406 
model_pd.l_d.mean(): -20.242637634277344 
model_pd.lagr.mean(): -20.146326065063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.9647], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.09631140530109406
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.14716866612434387
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.10185596346855164
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.13687290251255035
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.12083136290311813
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.13690336048603058
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.17132112383842468
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.15590818226337433
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.13130630552768707
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.20048874616622925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1651, 5.1655],
        [5.1655, 4.9737, 4.6317],
        [5.1655, 5.0155, 4.6699],
        [5.1655, 4.8696, 4.7328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.1208663359284401 
model_pd.l_d.mean(): -20.519222259521484 
model_pd.lagr.mean(): -20.39835548400879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-21.2236], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.1208663359284401
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.14236794412136078
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.14802436530590057
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.17701053619384766
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.1318584382534027
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.12403939664363861
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.12168759107589722
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.11858134716749191
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.12591086328029633
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.19217655062675476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1665, 4.9273, 4.6040],
        [5.1665, 5.1650, 5.1665],
        [5.1665, 5.4322, 5.2583],
        [5.1665, 5.1660, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.18479490280151367 
model_pd.l_d.mean(): -20.399213790893555 
model_pd.lagr.mean(): -20.214418411254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-21.1416], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.18479490280151367
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11905837059020996
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.1444290727376938
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.12095528095960617
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.15326404571533203
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.11132887750864029
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.12434928864240646
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.11171749234199524
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.12650296092033386
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.19572429358959198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1701, 5.1701, 5.1701],
        [5.1701, 5.1827, 4.8777],
        [5.1701, 5.0800, 4.7421],
        [5.1701, 5.1554, 5.1686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.1441701054573059 
model_pd.l_d.mean(): -20.561492919921875 
model_pd.lagr.mean(): -20.417322158813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4384], device='cuda:0')), ('power', tensor([-21.2339], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.1441701054573059
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.1610851287841797
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.11375625431537628
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.16806864738464355
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13323566317558289
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.15948516130447388
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.12021253257989883
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.09476806223392487
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.15131749212741852
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.13341869413852692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1721, 5.0792, 5.1356],
        [5.1721, 5.1719, 5.1721],
        [5.1721, 4.9841, 4.6419],
        [5.1721, 5.1721, 5.1721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.13517950475215912 
model_pd.l_d.mean(): -18.078012466430664 
model_pd.lagr.mean(): -17.942832946777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6623], device='cuda:0')), ('power', tensor([-18.9521], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.13517950475215912
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.13086356222629547
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.15365995466709137
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.12049996852874756
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.17542433738708496
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.10713895410299301
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.15736691653728485
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.1302786022424698
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.14208638668060303
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12814126908779144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1694, 4.8713, 4.6554],
        [5.1694, 5.0158, 5.0742],
        [5.1694, 5.1665, 5.1693],
        [5.1694, 5.1677, 5.1693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.1472778171300888 
model_pd.l_d.mean(): -19.764799118041992 
model_pd.lagr.mean(): -19.617521286010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-20.4925], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.1472778171300888
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.14462700486183167
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.13385139405727386
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.14218047261238098
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.13013175129890442
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.12154954671859741
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.1559445559978485
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.1292402297258377
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.14998160302639008
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.13993334770202637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1663, 5.1450, 5.1635],
        [5.1663, 4.8950, 4.8405],
        [5.1663, 5.0304, 5.0914],
        [5.1663, 4.8840, 4.8016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.09006841480731964 
model_pd.l_d.mean(): -20.580745697021484 
model_pd.lagr.mean(): -20.490676879882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.2507], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.09006841480731964
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.1768934577703476
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.14309558272361755
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.16137845814228058
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.16866645216941833
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.12764889001846313
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.17355594038963318
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.12375049293041229
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.09298604726791382
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.13268622756004333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1722, 5.1721, 5.1722],
        [5.1722, 5.1699, 5.1721],
        [5.1722, 4.9296, 4.9243],
        [5.1722, 5.0425, 4.6983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.1057090163230896 
model_pd.l_d.mean(): -20.390060424804688 
model_pd.lagr.mean(): -20.284351348876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4708], device='cuda:0')), ('power', tensor([-21.0938], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.1057090163230896
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.08061760663986206
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.15090304613113403
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.16097213327884674
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.11061073839664459
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.1361515074968338
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.09956994652748108
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.1927545815706253
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.20912466943264008
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.14055135846138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1691, 4.8733, 4.7363],
        [5.1691, 5.1684, 5.1691],
        [5.1691, 5.1668, 5.1690],
        [5.1691, 5.1691, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.08133850246667862 
model_pd.l_d.mean(): -20.232486724853516 
model_pd.lagr.mean(): -20.151147842407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.9401], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.08133850246667862
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.13863277435302734
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.16447177529335022
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.1472010612487793
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.18604016304016113
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.08480893820524216
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12306148558855057
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.15492810308933258
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.1513550579547882
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.15045733749866486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1735, 5.2446, 4.9651],
        [5.1735, 5.1704, 5.1733],
        [5.1735, 5.0894, 5.1431],
        [5.1735, 5.1734, 5.1735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.12907278537750244 
model_pd.l_d.mean(): -20.82115936279297 
model_pd.lagr.mean(): -20.692087173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-21.4752], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.12907278537750244
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.15214386582374573
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12546303868293762
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12558522820472717
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.14960798621177673
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.19444569945335388
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.13838788866996765
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.1413636952638626
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.0953715592622757
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.13028092682361603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1693, 4.9594, 4.6224],
        [5.1693, 5.1638, 5.1690],
        [5.1693, 4.9007, 4.8517],
        [5.1693, 5.0530, 4.7101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.21242833137512207 
model_pd.l_d.mean(): -20.68511199951172 
model_pd.lagr.mean(): -20.47268295288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4425], device='cuda:0')), ('power', tensor([-21.3631], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.21242833137512207
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.1629352867603302
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.11274603009223938
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.09858228266239166
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.12558326125144958
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.13477593660354614
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.10443954914808273
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.16643302142620087
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.14139841496944427
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.13592460751533508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 4.8827, 4.8003],
        [5.1651, 5.1213, 5.1556],
        [5.1651, 5.4879, 5.3490],
        [5.1651, 5.2968, 5.0474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.12209115922451019 
model_pd.l_d.mean(): -19.569074630737305 
model_pd.lagr.mean(): -19.446983337402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5159], device='cuda:0')), ('power', tensor([-20.3099], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.12209115922451019
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.13007967174053192
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.12081592530012131
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.19567708671092987
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.16220882534980774
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.18938443064689636
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.144393652677536
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11862742900848389
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.12347070127725601
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.09868272393941879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.1010, 4.7687],
        [5.1668, 4.8674, 4.7030],
        [5.1668, 5.1668, 5.1668],
        [5.1668, 5.1668, 5.1668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.1264904886484146 
model_pd.l_d.mean(): -20.435527801513672 
model_pd.lagr.mean(): -20.309038162231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4940], device='cuda:0')), ('power', tensor([-21.1634], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.1264904886484146
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.1169213056564331
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.15213534235954285
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14677850902080536
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.1355455070734024
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.12438724935054779
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.1123274564743042
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12289060652256012
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.18491727113723755
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.18541675806045532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 5.1630, 5.1630],
        [5.1630, 5.1629, 5.1630],
        [5.1630, 4.8798, 4.7959],
        [5.1630, 5.0392, 4.6950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.17489121854305267 
model_pd.l_d.mean(): -19.949256896972656 
model_pd.lagr.mean(): -19.77436637878418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.6591], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.17489121854305267
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.1609172523021698
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.13271965086460114
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.19526726007461548
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.12082397192716599
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.08221237361431122
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.13241887092590332
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.13056659698486328
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.10526124387979507
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.1805499792098999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.2938, 5.0431],
        [5.1644, 5.0713, 5.1278],
        [5.1644, 5.1644, 5.1644],
        [5.1644, 5.1207, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.1072891429066658 
model_pd.l_d.mean(): -20.321044921875 
model_pd.lagr.mean(): -20.213756561279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-20.9882], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.1072891429066658
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.10694144666194916
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.11713778972625732
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.22637483477592468
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.13070854544639587
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.13295114040374756
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12294661998748779
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.21277230978012085
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13769696652889252
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.1180977001786232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1634, 5.1634, 5.1634],
        [5.1634, 4.9394, 4.9570],
        [5.1634, 4.9626, 5.0002],
        [5.1634, 5.0735, 5.1290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.10394345968961716 
model_pd.l_d.mean(): -19.642892837524414 
model_pd.lagr.mean(): -19.538949966430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5244], device='cuda:0')), ('power', tensor([-20.3932], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.10394345968961716
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.2323317527770996
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.14949730038642883
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12464895099401474
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.22994942963123322
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.13549834489822388
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.08202535659074783
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.09658771008253098
epoch£º1290	 i:8 	 global-step:25808	 l-p:0.09593461453914642
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.16740795969963074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1616, 4.9346, 4.9492],
        [5.1616, 4.9248, 4.9282],
        [5.1616, 4.8631, 4.7140],
        [5.1616, 5.1616, 5.1616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.12989915907382965 
model_pd.l_d.mean(): -20.897951126098633 
model_pd.lagr.mean(): -20.768051147460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.5371], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.12989915907382965
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.13715703785419464
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12688317894935608
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.23909899592399597
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.13666054606437683
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.17687399685382843
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.14117993414402008
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.0702337846159935
epoch£º1291	 i:8 	 global-step:25828	 l-p:0.12848694622516632
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.14591163396835327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1612, 5.1608, 5.1612],
        [5.1612, 5.5522, 5.4573],
        [5.1612, 5.1600, 5.1612],
        [5.1612, 5.0799, 5.1327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.121711865067482 
model_pd.l_d.mean(): -18.95985221862793 
model_pd.lagr.mean(): -18.8381404876709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5321], device='cuda:0')), ('power', tensor([-19.7106], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.121711865067482
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.19863292574882507
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.14030905067920685
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.12636473774909973
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.11428148299455643
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.14223262667655945
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.17384134232997894
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.13595542311668396
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.1396256983280182
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.1281711608171463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1583, 5.1623],
        [5.1625, 4.8645, 4.7188],
        [5.1625, 5.0011, 5.0578],
        [5.1625, 5.0447, 5.1057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.0996803417801857 
model_pd.l_d.mean(): -20.338706970214844 
model_pd.lagr.mean(): -20.23902702331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.0996803417801857
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.12063933908939362
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.25083962082862854
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.15155845880508423
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.15916115045547485
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.10731782764196396
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.12660567462444305
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.0826290100812912
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.20088878273963928
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.13257677853107452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.1577, 5.1586],
        [5.1586, 5.1089, 5.1468],
        [5.1586, 4.8584, 4.6965],
        [5.1586, 5.1582, 5.1586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.09488207846879959 
model_pd.l_d.mean(): -19.338237762451172 
model_pd.lagr.mean(): -19.24335479736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5358], device='cuda:0')), ('power', tensor([-20.0968], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.09488207846879959
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.15780195593833923
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11377046257257462
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.24403704702854156
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12611904740333557
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.14419518411159515
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.15561354160308838
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12388546019792557
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.14954113960266113
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.13178367912769318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1578, 5.1554, 5.1577],
        [5.1578, 5.1439, 5.1564],
        [5.1578, 5.1070, 5.1455],
        [5.1578, 5.2327, 4.9549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.1691480427980423 
model_pd.l_d.mean(): -20.4482479095459 
model_pd.lagr.mean(): -20.27910041809082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4622], device='cuda:0')), ('power', tensor([-21.1438], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.1691480427980423
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.07498753070831299
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.2094617784023285
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.126697838306427
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.12594223022460938
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10798241198062897
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.1657712459564209
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.1688300222158432
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.15214130282402039
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.1390237659215927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1590, 5.0727, 5.1272],
        [5.1590, 5.0909, 5.1383],
        [5.1590, 5.1590, 5.1590],
        [5.1590, 5.1392, 5.1566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.14634676277637482 
model_pd.l_d.mean(): -19.410526275634766 
model_pd.lagr.mean(): -19.264179229736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5222], device='cuda:0')), ('power', tensor([-20.1560], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.14634676277637482
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.10908631980419159
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.15804529190063477
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.1210629940032959
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.174236461520195
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.15817248821258545
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.1851477026939392
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.14094851911067963
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.12252141535282135
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11216161400079727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[5.1644, 4.9100, 4.5972],
        [5.1644, 4.9100, 4.8884],
        [5.1644, 4.9082, 4.8836],
        [5.1644, 4.8637, 4.6849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.12209262698888779 
model_pd.l_d.mean(): -18.86454200744629 
model_pd.lagr.mean(): -18.742448806762695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5387], device='cuda:0')), ('power', tensor([-19.6209], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.12209262698888779
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.12127307802438736
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.20307330787181854
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14767448604106903
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.19906440377235413
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.187920942902565
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.09849023818969727
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.08562341332435608
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.14760273694992065
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.1034238412976265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1634, 5.1632, 5.1634],
        [5.1634, 4.8858, 4.6012],
        [5.1634, 4.9703, 4.6277],
        [5.1634, 4.8723, 4.6179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.10747428983449936 
model_pd.l_d.mean(): -20.473133087158203 
model_pd.lagr.mean(): -20.365659713745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.1838], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.10747428983449936
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.17765536904335022
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.20666027069091797
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.08174269646406174
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.10732559859752655
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.12338309735059738
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.19229461252689362
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.14007925987243652
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.20694446563720703
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.08281514048576355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 5.1590, 5.1596],
        [5.1596, 4.8584, 4.6606],
        [5.1596, 5.1596, 5.1596],
        [5.1596, 5.1273, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.12374258786439896 
model_pd.l_d.mean(): -20.93133544921875 
model_pd.lagr.mean(): -20.807592391967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4029], device='cuda:0')), ('power', tensor([-21.5716], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.12374258786439896
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.12363503873348236
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.21672658622264862
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.1495886594057083
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.12800461053848267
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.12158752232789993
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12098971754312515
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.15852314233779907
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.1424141675233841
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.15707740187644958
