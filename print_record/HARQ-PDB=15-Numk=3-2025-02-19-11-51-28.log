
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0001], device='cuda:0')), ('power', tensor([0.9999], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.27698108553886414
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802981376648
epoch£º0	 i:4 	 global-step:4	 l-p:-0.7105808854103088
epoch£º0	 i:5 	 global-step:5	 l-p:-2.530949592590332
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505184412002563
epoch£º0	 i:7 	 global-step:7	 l-p:0.4916812479496002
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606582403182983
epoch£º0	 i:9 	 global-step:9	 l-p:0.05681001394987106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.37725746631622314 
model_pd.l_d.mean(): -18.863636016845703 
model_pd.lagr.mean(): -18.486377716064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0058], device='cuda:0')), ('power', tensor([0.9952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.37725746631622314
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797874927521
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878843903541565
epoch£º1	 i:3 	 global-step:23	 l-p:0.24289998412132263
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980881869792938
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688207864761353
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738257348537445
epoch£º1	 i:7 	 global-step:27	 l-p:0.21875017881393433
epoch£º1	 i:8 	 global-step:28	 l-p:0.22573958337306976
epoch£º1	 i:9 	 global-step:29	 l-p:0.11670548468828201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 5.9885, 6.5145],
        [4.9151, 5.0129, 4.9528],
        [4.9151, 5.7268, 5.9879],
        [4.9151, 4.9970, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13362069427967072 
model_pd.l_d.mean(): -20.28553581237793 
model_pd.lagr.mean(): -20.151914596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0098], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13362069427967072
epoch£º2	 i:1 	 global-step:41	 l-p:0.10827075690031052
epoch£º2	 i:2 	 global-step:42	 l-p:0.10365581512451172
epoch£º2	 i:3 	 global-step:43	 l-p:0.10691770166158676
epoch£º2	 i:4 	 global-step:44	 l-p:0.11889182776212692
epoch£º2	 i:5 	 global-step:45	 l-p:0.10603252053260803
epoch£º2	 i:6 	 global-step:46	 l-p:0.11256218701601028
epoch£º2	 i:7 	 global-step:47	 l-p:-0.5149641633033752
epoch£º2	 i:8 	 global-step:48	 l-p:0.10819238424301147
epoch£º2	 i:9 	 global-step:49	 l-p:0.11758995801210403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3948, 6.4842, 6.9566],
        [5.3948, 5.4458, 5.4069],
        [5.3948, 5.5008, 5.4351],
        [5.3948, 6.9666, 8.0014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08163349330425262 
model_pd.l_d.mean(): -18.719200134277344 
model_pd.lagr.mean(): -18.6375675201416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-19.3489], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08163349330425262
epoch£º3	 i:1 	 global-step:61	 l-p:0.08723415434360504
epoch£º3	 i:2 	 global-step:62	 l-p:0.0002457952359691262
epoch£º3	 i:3 	 global-step:63	 l-p:0.12233922630548477
epoch£º3	 i:4 	 global-step:64	 l-p:0.11886759847402573
epoch£º3	 i:5 	 global-step:65	 l-p:0.1169566735625267
epoch£º3	 i:6 	 global-step:66	 l-p:0.11346924304962158
epoch£º3	 i:7 	 global-step:67	 l-p:0.1185336485505104
epoch£º3	 i:8 	 global-step:68	 l-p:0.12619806826114655
epoch£º3	 i:9 	 global-step:69	 l-p:0.12450944632291794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9813, 5.7049, 5.8823],
        [4.9813, 5.8650, 6.1866],
        [4.9813, 6.5221, 7.6085],
        [4.9813, 5.0697, 5.0130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11540031433105469 
model_pd.l_d.mean(): -19.056425094604492 
model_pd.lagr.mean(): -18.941024780273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-19.7591], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11540031433105469
epoch£º4	 i:1 	 global-step:81	 l-p:0.1313871145248413
epoch£º4	 i:2 	 global-step:82	 l-p:0.16932238638401031
epoch£º4	 i:3 	 global-step:83	 l-p:0.13243122398853302
epoch£º4	 i:4 	 global-step:84	 l-p:0.15471573173999786
epoch£º4	 i:5 	 global-step:85	 l-p:0.1268613338470459
epoch£º4	 i:6 	 global-step:86	 l-p:0.1303672343492508
epoch£º4	 i:7 	 global-step:87	 l-p:0.13439373672008514
epoch£º4	 i:8 	 global-step:88	 l-p:0.14058850705623627
epoch£º4	 i:9 	 global-step:89	 l-p:0.16173113882541656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6544, 4.6546, 4.6544],
        [4.6544, 4.6605, 4.6548],
        [4.6544, 4.7478, 4.6909],
        [4.6544, 4.7884, 4.7204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18696849048137665 
model_pd.l_d.mean(): -20.10610580444336 
model_pd.lagr.mean(): -19.919137954711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.8780], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18696849048137665
epoch£º5	 i:1 	 global-step:101	 l-p:0.15168221294879913
epoch£º5	 i:2 	 global-step:102	 l-p:0.16870613396167755
epoch£º5	 i:3 	 global-step:103	 l-p:0.08483648300170898
epoch£º5	 i:4 	 global-step:104	 l-p:0.1332511156797409
epoch£º5	 i:5 	 global-step:105	 l-p:0.15382398664951324
epoch£º5	 i:6 	 global-step:106	 l-p:0.12070296704769135
epoch£º5	 i:7 	 global-step:107	 l-p:0.13906322419643402
epoch£º5	 i:8 	 global-step:108	 l-p:0.1369633972644806
epoch£º5	 i:9 	 global-step:109	 l-p:0.13650406897068024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8654, 4.8931, 4.8703],
        [4.8654, 4.8654, 4.8654],
        [4.8654, 6.2734, 7.2129],
        [4.8654, 4.8702, 4.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13271106779575348 
model_pd.l_d.mean(): -20.582923889160156 
model_pd.lagr.mean(): -20.450212478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.2438], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13271106779575348
epoch£º6	 i:1 	 global-step:121	 l-p:0.1321173459291458
epoch£º6	 i:2 	 global-step:122	 l-p:0.13903439044952393
epoch£º6	 i:3 	 global-step:123	 l-p:0.15074068307876587
epoch£º6	 i:4 	 global-step:124	 l-p:0.1295313686132431
epoch£º6	 i:5 	 global-step:125	 l-p:0.1523834615945816
epoch£º6	 i:6 	 global-step:126	 l-p:0.1375342160463333
epoch£º6	 i:7 	 global-step:127	 l-p:0.12962381541728973
epoch£º6	 i:8 	 global-step:128	 l-p:0.13109979033470154
epoch£º6	 i:9 	 global-step:129	 l-p:0.12418151646852493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 5.5407, 5.6706],
        [4.8894, 4.9519, 4.9077],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 4.9105, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.1277899593114853 
model_pd.l_d.mean(): -20.45469093322754 
model_pd.lagr.mean(): -20.326900482177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.1229], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.1277899593114853
epoch£º7	 i:1 	 global-step:141	 l-p:0.11821331828832626
epoch£º7	 i:2 	 global-step:142	 l-p:0.13609935343265533
epoch£º7	 i:3 	 global-step:143	 l-p:0.13799819350242615
epoch£º7	 i:4 	 global-step:144	 l-p:0.14886143803596497
epoch£º7	 i:5 	 global-step:145	 l-p:-0.24089446663856506
epoch£º7	 i:6 	 global-step:146	 l-p:0.12930408120155334
epoch£º7	 i:7 	 global-step:147	 l-p:0.13897578418254852
epoch£º7	 i:8 	 global-step:148	 l-p:0.16633009910583496
epoch£º7	 i:9 	 global-step:149	 l-p:0.14020609855651855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6980, 4.6980, 4.6980],
        [4.6980, 4.6981, 4.6980],
        [4.6980, 5.4962, 5.7742],
        [4.6980, 4.8410, 4.7713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1339498907327652 
model_pd.l_d.mean(): -18.755619049072266 
model_pd.lagr.mean(): -18.62166976928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-19.5433], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1339498907327652
epoch£º8	 i:1 	 global-step:161	 l-p:0.10937061160802841
epoch£º8	 i:2 	 global-step:162	 l-p:0.139433816075325
epoch£º8	 i:3 	 global-step:163	 l-p:0.14245149493217468
epoch£º8	 i:4 	 global-step:164	 l-p:0.12771061062812805
epoch£º8	 i:5 	 global-step:165	 l-p:0.14805713295936584
epoch£º8	 i:6 	 global-step:166	 l-p:0.12971782684326172
epoch£º8	 i:7 	 global-step:167	 l-p:0.13172125816345215
epoch£º8	 i:8 	 global-step:168	 l-p:0.14077118039131165
epoch£º8	 i:9 	 global-step:169	 l-p:0.1406712681055069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8656, 4.8674, 4.8657],
        [4.8656, 4.8676, 4.8657],
        [4.8656, 5.0135, 4.9413],
        [4.8656, 4.8864, 4.8687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.13530249893665314 
model_pd.l_d.mean(): -17.660247802734375 
model_pd.lagr.mean(): -17.524946212768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5548], device='cuda:0')), ('power', tensor([-18.4200], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.13530249893665314
epoch£º9	 i:1 	 global-step:181	 l-p:0.12824119627475739
epoch£º9	 i:2 	 global-step:182	 l-p:0.13838034868240356
epoch£º9	 i:3 	 global-step:183	 l-p:0.13025379180908203
epoch£º9	 i:4 	 global-step:184	 l-p:0.12077466398477554
epoch£º9	 i:5 	 global-step:185	 l-p:0.14665237069129944
epoch£º9	 i:6 	 global-step:186	 l-p:0.16524070501327515
epoch£º9	 i:7 	 global-step:187	 l-p:0.12909583747386932
epoch£º9	 i:8 	 global-step:188	 l-p:0.12787117063999176
epoch£º9	 i:9 	 global-step:189	 l-p:0.10969632863998413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8094, 6.0339, 6.7596],
        [4.8094, 5.6447, 5.9466],
        [4.8094, 4.8145, 4.8098],
        [4.8094, 4.8094, 4.8094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15206658840179443 
model_pd.l_d.mean(): -20.22431755065918 
model_pd.lagr.mean(): -20.072250366210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.9380], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15206658840179443
epoch£º10	 i:1 	 global-step:201	 l-p:0.13919971883296967
epoch£º10	 i:2 	 global-step:202	 l-p:0.11491084843873978
epoch£º10	 i:3 	 global-step:203	 l-p:0.13253667950630188
epoch£º10	 i:4 	 global-step:204	 l-p:0.13277125358581543
epoch£º10	 i:5 	 global-step:205	 l-p:0.13973146677017212
epoch£º10	 i:6 	 global-step:206	 l-p:0.14991764724254608
epoch£º10	 i:7 	 global-step:207	 l-p:0.1646253615617752
epoch£º10	 i:8 	 global-step:208	 l-p:-0.0572550967335701
epoch£º10	 i:9 	 global-step:209	 l-p:0.14185257256031036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7934, 5.3128, 5.3590],
        [4.7934, 6.1000, 6.9314],
        [4.7934, 5.5043, 5.6960],
        [4.7934, 4.8165, 4.7971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11840078234672546 
model_pd.l_d.mean(): -20.09127426147461 
model_pd.lagr.mean(): -19.97287368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.8149], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11840078234672546
epoch£º11	 i:1 	 global-step:221	 l-p:0.1498456597328186
epoch£º11	 i:2 	 global-step:222	 l-p:0.12453462928533554
epoch£º11	 i:3 	 global-step:223	 l-p:0.12744097411632538
epoch£º11	 i:4 	 global-step:224	 l-p:0.14308921992778778
epoch£º11	 i:5 	 global-step:225	 l-p:0.1384139209985733
epoch£º11	 i:6 	 global-step:226	 l-p:0.12569934129714966
epoch£º11	 i:7 	 global-step:227	 l-p:0.20616783201694489
epoch£º11	 i:8 	 global-step:228	 l-p:0.12325727939605713
epoch£º11	 i:9 	 global-step:229	 l-p:0.15525996685028076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9188, 5.3214, 5.2957],
        [4.9188, 4.9555, 4.9266],
        [4.9188, 4.9500, 4.9248],
        [4.9188, 5.0892, 5.0141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12307573109865189 
model_pd.l_d.mean(): -19.340229034423828 
model_pd.lagr.mean(): -19.217153549194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.0808], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12307573109865189
epoch£º12	 i:1 	 global-step:241	 l-p:0.1360073983669281
epoch£º12	 i:2 	 global-step:242	 l-p:0.12004424631595612
epoch£º12	 i:3 	 global-step:243	 l-p:0.13655410706996918
epoch£º12	 i:4 	 global-step:244	 l-p:0.13907752931118011
epoch£º12	 i:5 	 global-step:245	 l-p:0.1387525051832199
epoch£º12	 i:6 	 global-step:246	 l-p:0.1355053335428238
epoch£º12	 i:7 	 global-step:247	 l-p:0.1336917132139206
epoch£º12	 i:8 	 global-step:248	 l-p:0.13904359936714172
epoch£º12	 i:9 	 global-step:249	 l-p:0.09760922193527222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8531, 5.9949, 6.6194],
        [4.8531, 4.8531, 4.8531],
        [4.8531, 4.8564, 4.8532],
        [4.8531, 5.2476, 5.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12329305708408356 
model_pd.l_d.mean(): -18.70691680908203 
model_pd.lagr.mean(): -18.5836238861084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-19.4482], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12329305708408356
epoch£º13	 i:1 	 global-step:261	 l-p:0.1899624913930893
epoch£º13	 i:2 	 global-step:262	 l-p:0.12834058701992035
epoch£º13	 i:3 	 global-step:263	 l-p:0.130558043718338
epoch£º13	 i:4 	 global-step:264	 l-p:0.13821381330490112
epoch£º13	 i:5 	 global-step:265	 l-p:0.14281068742275238
epoch£º13	 i:6 	 global-step:266	 l-p:0.11233249306678772
epoch£º13	 i:7 	 global-step:267	 l-p:0.13484714925289154
epoch£º13	 i:8 	 global-step:268	 l-p:0.13868066668510437
epoch£º13	 i:9 	 global-step:269	 l-p:0.12354068458080292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8310, 4.8310, 4.8310],
        [4.8310, 5.7266, 6.0897],
        [4.8310, 4.8650, 4.8380],
        [4.8310, 5.6608, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1509464979171753 
model_pd.l_d.mean(): -18.57611083984375 
model_pd.lagr.mean(): -18.4251651763916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-19.3109], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.1509464979171753
epoch£º14	 i:1 	 global-step:281	 l-p:0.12489092350006104
epoch£º14	 i:2 	 global-step:282	 l-p:0.16706731915473938
epoch£º14	 i:3 	 global-step:283	 l-p:0.14096157252788544
epoch£º14	 i:4 	 global-step:284	 l-p:0.1355593502521515
epoch£º14	 i:5 	 global-step:285	 l-p:0.12305637449026108
epoch£º14	 i:6 	 global-step:286	 l-p:0.1179165467619896
epoch£º14	 i:7 	 global-step:287	 l-p:0.13151219487190247
epoch£º14	 i:8 	 global-step:288	 l-p:0.13156642019748688
epoch£º14	 i:9 	 global-step:289	 l-p:0.015596399083733559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7478, 4.7787, 4.7538],
        [4.7478, 4.7514, 4.7480],
        [4.7478, 4.9186, 4.8468],
        [4.7478, 4.7583, 4.7488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.1557748168706894 
model_pd.l_d.mean(): -20.549571990966797 
model_pd.lagr.mean(): -20.393796920776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2513], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.1557748168706894
epoch£º15	 i:1 	 global-step:301	 l-p:0.16217103600502014
epoch£º15	 i:2 	 global-step:302	 l-p:0.12419743090867996
epoch£º15	 i:3 	 global-step:303	 l-p:0.14429180324077606
epoch£º15	 i:4 	 global-step:304	 l-p:0.158433735370636
epoch£º15	 i:5 	 global-step:305	 l-p:0.14607146382331848
epoch£º15	 i:6 	 global-step:306	 l-p:0.11911483854055405
epoch£º15	 i:7 	 global-step:307	 l-p:0.14245370030403137
epoch£º15	 i:8 	 global-step:308	 l-p:0.13309895992279053
epoch£º15	 i:9 	 global-step:309	 l-p:0.20727179944515228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 4.8642, 4.8624],
        [4.8623, 4.8884, 4.8668],
        [4.8623, 4.8627, 4.8623],
        [4.8623, 5.2191, 5.1793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12061785161495209 
model_pd.l_d.mean(): -18.991748809814453 
model_pd.lagr.mean(): -18.871131896972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-19.6958], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12061785161495209
epoch£º16	 i:1 	 global-step:321	 l-p:0.11633118987083435
epoch£º16	 i:2 	 global-step:322	 l-p:0.1511351764202118
epoch£º16	 i:3 	 global-step:323	 l-p:0.1355067640542984
epoch£º16	 i:4 	 global-step:324	 l-p:0.12454656511545181
epoch£º16	 i:5 	 global-step:325	 l-p:0.11478568613529205
epoch£º16	 i:6 	 global-step:326	 l-p:0.13393905758857727
epoch£º16	 i:7 	 global-step:327	 l-p:0.1547187715768814
epoch£º16	 i:8 	 global-step:328	 l-p:0.13002590835094452
epoch£º16	 i:9 	 global-step:329	 l-p:0.12865795195102692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9091, 5.8365, 6.2249],
        [4.9091, 5.2229, 5.1669],
        [4.9091, 4.9091, 4.9090],
        [4.9091, 5.8243, 6.2004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.1605760008096695 
model_pd.l_d.mean(): -20.177288055419922 
model_pd.lagr.mean(): -20.016712188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.8588], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.1605760008096695
epoch£º17	 i:1 	 global-step:341	 l-p:0.12124790251255035
epoch£º17	 i:2 	 global-step:342	 l-p:0.13692818582057953
epoch£º17	 i:3 	 global-step:343	 l-p:0.13877204060554504
epoch£º17	 i:4 	 global-step:344	 l-p:0.13086794316768646
epoch£º17	 i:5 	 global-step:345	 l-p:0.1255854070186615
epoch£º17	 i:6 	 global-step:346	 l-p:0.13737832009792328
epoch£º17	 i:7 	 global-step:347	 l-p:0.1422727108001709
epoch£º17	 i:8 	 global-step:348	 l-p:0.13048997521400452
epoch£º17	 i:9 	 global-step:349	 l-p:0.1333596259355545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7509, 4.7988, 4.7634],
        [4.7509, 4.7511, 4.7509],
        [4.7509, 4.8621, 4.8004],
        [4.7509, 4.7632, 4.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14244742691516876 
model_pd.l_d.mean(): -20.058181762695312 
model_pd.lagr.mean(): -19.915735244750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8015], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14244742691516876
epoch£º18	 i:1 	 global-step:361	 l-p:0.19641028344631195
epoch£º18	 i:2 	 global-step:362	 l-p:0.1352490931749344
epoch£º18	 i:3 	 global-step:363	 l-p:0.1751853972673416
epoch£º18	 i:4 	 global-step:364	 l-p:0.13196001946926117
epoch£º18	 i:5 	 global-step:365	 l-p:0.13758906722068787
epoch£º18	 i:6 	 global-step:366	 l-p:0.13374996185302734
epoch£º18	 i:7 	 global-step:367	 l-p:0.055846650153398514
epoch£º18	 i:8 	 global-step:368	 l-p:0.1406228393316269
epoch£º18	 i:9 	 global-step:369	 l-p:0.14228111505508423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7707, 4.9428, 4.8715],
        [4.7707, 4.8001, 4.7764],
        [4.7707, 4.7707, 4.7707],
        [4.7707, 4.9752, 4.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.14762258529663086 
model_pd.l_d.mean(): -20.52219581604004 
model_pd.lagr.mean(): -20.37457275390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.2100], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.14762258529663086
epoch£º19	 i:1 	 global-step:381	 l-p:0.146769180893898
epoch£º19	 i:2 	 global-step:382	 l-p:0.1333903968334198
epoch£º19	 i:3 	 global-step:383	 l-p:0.1314684897661209
epoch£º19	 i:4 	 global-step:384	 l-p:0.12496525794267654
epoch£º19	 i:5 	 global-step:385	 l-p:0.14535675942897797
epoch£º19	 i:6 	 global-step:386	 l-p:0.12978975474834442
epoch£º19	 i:7 	 global-step:387	 l-p:0.12460935115814209
epoch£º19	 i:8 	 global-step:388	 l-p:0.10972108691930771
epoch£º19	 i:9 	 global-step:389	 l-p:0.17068137228488922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9244, 4.9271, 4.9245],
        [4.9244, 5.3463, 5.3350],
        [4.9244, 4.9666, 4.9344],
        [4.9244, 4.9853, 4.9425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12436496466398239 
model_pd.l_d.mean(): -20.474220275878906 
model_pd.lagr.mean(): -20.349855422973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4229], device='cuda:0')), ('power', tensor([-21.1300], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12436496466398239
epoch£º20	 i:1 	 global-step:401	 l-p:0.12764768302440643
epoch£º20	 i:2 	 global-step:402	 l-p:0.12204500287771225
epoch£º20	 i:3 	 global-step:403	 l-p:0.1946762353181839
epoch£º20	 i:4 	 global-step:404	 l-p:0.14041239023208618
epoch£º20	 i:5 	 global-step:405	 l-p:0.1392231285572052
epoch£º20	 i:6 	 global-step:406	 l-p:0.12473927438259125
epoch£º20	 i:7 	 global-step:407	 l-p:0.1312161386013031
epoch£º20	 i:8 	 global-step:408	 l-p:0.125510111451149
epoch£º20	 i:9 	 global-step:409	 l-p:0.12827454507350922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9206, 6.3323, 7.2835],
        [4.9206, 4.9821, 4.9391],
        [4.9206, 4.9206, 4.9206],
        [4.9206, 5.0206, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13278590142726898 
model_pd.l_d.mean(): -20.315526962280273 
model_pd.lagr.mean(): -20.182741165161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13278590142726898
epoch£º21	 i:1 	 global-step:421	 l-p:0.12889422476291656
epoch£º21	 i:2 	 global-step:422	 l-p:0.1364850252866745
epoch£º21	 i:3 	 global-step:423	 l-p:0.1306980699300766
epoch£º21	 i:4 	 global-step:424	 l-p:0.16297109425067902
epoch£º21	 i:5 	 global-step:425	 l-p:0.13519859313964844
epoch£º21	 i:6 	 global-step:426	 l-p:0.1579914689064026
epoch£º21	 i:7 	 global-step:427	 l-p:0.16641047596931458
epoch£º21	 i:8 	 global-step:428	 l-p:0.1887599378824234
epoch£º21	 i:9 	 global-step:429	 l-p:0.13467782735824585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7039, 4.8001, 4.7437],
        [4.7039, 4.9843, 4.9283],
        [4.7039, 4.7060, 4.7040],
        [4.7039, 4.7340, 4.7099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18139030039310455 
model_pd.l_d.mean(): -20.529512405395508 
model_pd.lagr.mean(): -20.348121643066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-21.2463], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18139030039310455
epoch£º22	 i:1 	 global-step:441	 l-p:0.13949239253997803
epoch£º22	 i:2 	 global-step:442	 l-p:0.13412991166114807
epoch£º22	 i:3 	 global-step:443	 l-p:0.13338463008403778
epoch£º22	 i:4 	 global-step:444	 l-p:0.1140412762761116
epoch£º22	 i:5 	 global-step:445	 l-p:0.14604128897190094
epoch£º22	 i:6 	 global-step:446	 l-p:0.11981366574764252
epoch£º22	 i:7 	 global-step:447	 l-p:0.1396036446094513
epoch£º22	 i:8 	 global-step:448	 l-p:0.13568787276744843
epoch£º22	 i:9 	 global-step:449	 l-p:0.12129499763250351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0005, 6.4325, 7.3950],
        [5.0005, 5.5550, 5.6192],
        [5.0005, 5.0011, 5.0005],
        [5.0005, 5.9316, 6.3177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10506945103406906 
model_pd.l_d.mean(): -19.999509811401367 
model_pd.lagr.mean(): -19.894439697265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-20.6707], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10506945103406906
epoch£º23	 i:1 	 global-step:461	 l-p:0.13177190721035004
epoch£º23	 i:2 	 global-step:462	 l-p:0.13836048543453217
epoch£º23	 i:3 	 global-step:463	 l-p:0.11578115075826645
epoch£º23	 i:4 	 global-step:464	 l-p:0.13819652795791626
epoch£º23	 i:5 	 global-step:465	 l-p:0.13182735443115234
epoch£º23	 i:6 	 global-step:466	 l-p:0.15629057586193085
epoch£º23	 i:7 	 global-step:467	 l-p:0.1282476931810379
epoch£º23	 i:8 	 global-step:468	 l-p:0.1431386023759842
epoch£º23	 i:9 	 global-step:469	 l-p:0.14557306468486786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8297, 5.2774, 5.2876],
        [4.8297, 5.1131, 5.0544],
        [4.8297, 4.9099, 4.8588],
        [4.8297, 4.8297, 4.8297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13809718191623688 
model_pd.l_d.mean(): -20.95712661743164 
model_pd.lagr.mean(): -20.819028854370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4010], device='cuda:0')), ('power', tensor([-21.5957], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.13809718191623688
epoch£º24	 i:1 	 global-step:481	 l-p:0.14638163149356842
epoch£º24	 i:2 	 global-step:482	 l-p:0.12133726477622986
epoch£º24	 i:3 	 global-step:483	 l-p:0.12021402269601822
epoch£º24	 i:4 	 global-step:484	 l-p:0.14195188879966736
epoch£º24	 i:5 	 global-step:485	 l-p:0.1167437881231308
epoch£º24	 i:6 	 global-step:486	 l-p:0.14352484047412872
epoch£º24	 i:7 	 global-step:487	 l-p:0.1408405303955078
epoch£º24	 i:8 	 global-step:488	 l-p:0.18710727989673615
epoch£º24	 i:9 	 global-step:489	 l-p:0.12445072829723358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 6.0146, 6.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13400287926197052 
model_pd.l_d.mean(): -18.35012435913086 
model_pd.lagr.mean(): -18.216121673583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5760], device='cuda:0')), ('power', tensor([-19.1391], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13400287926197052
epoch£º25	 i:1 	 global-step:501	 l-p:0.1348123848438263
epoch£º25	 i:2 	 global-step:502	 l-p:0.1424339860677719
epoch£º25	 i:3 	 global-step:503	 l-p:0.12410890311002731
epoch£º25	 i:4 	 global-step:504	 l-p:0.38650745153427124
epoch£º25	 i:5 	 global-step:505	 l-p:0.14746806025505066
epoch£º25	 i:6 	 global-step:506	 l-p:0.13004589080810547
epoch£º25	 i:7 	 global-step:507	 l-p:0.1287323534488678
epoch£º25	 i:8 	 global-step:508	 l-p:0.14900243282318115
epoch£º25	 i:9 	 global-step:509	 l-p:0.12858295440673828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8317, 5.0213, 4.9503],
        [4.8317, 5.4288, 5.5392],
        [4.8317, 4.8906, 4.8494],
        [4.8317, 5.3466, 5.3990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15572035312652588 
model_pd.l_d.mean(): -20.57039451599121 
model_pd.lagr.mean(): -20.414674758911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15572035312652588
epoch£º26	 i:1 	 global-step:521	 l-p:0.13857895135879517
epoch£º26	 i:2 	 global-step:522	 l-p:0.13958783447742462
epoch£º26	 i:3 	 global-step:523	 l-p:0.12689098715782166
epoch£º26	 i:4 	 global-step:524	 l-p:0.005108680576086044
epoch£º26	 i:5 	 global-step:525	 l-p:0.15632794797420502
epoch£º26	 i:6 	 global-step:526	 l-p:0.14128746092319489
epoch£º26	 i:7 	 global-step:527	 l-p:0.14351701736450195
epoch£º26	 i:8 	 global-step:528	 l-p:0.14050118625164032
epoch£º26	 i:9 	 global-step:529	 l-p:0.12953175604343414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8516, 4.9671, 4.9046],
        [4.8516, 5.1809, 5.1367],
        [4.8516, 4.8516, 4.8516],
        [4.8516, 4.8541, 4.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.1248527467250824 
model_pd.l_d.mean(): -18.258005142211914 
model_pd.lagr.mean(): -18.13315200805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5421], device='cuda:0')), ('power', tensor([-19.0114], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.1248527467250824
epoch£º27	 i:1 	 global-step:541	 l-p:0.13130122423171997
epoch£º27	 i:2 	 global-step:542	 l-p:0.11955448240041733
epoch£º27	 i:3 	 global-step:543	 l-p:0.1403065174818039
epoch£º27	 i:4 	 global-step:544	 l-p:0.13030590116977692
epoch£º27	 i:5 	 global-step:545	 l-p:0.1388091892004013
epoch£º27	 i:6 	 global-step:546	 l-p:0.13480255007743835
epoch£º27	 i:7 	 global-step:547	 l-p:0.14729154109954834
epoch£º27	 i:8 	 global-step:548	 l-p:0.379425048828125
epoch£º27	 i:9 	 global-step:549	 l-p:0.1356358379125595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8318, 5.0297, 4.9594],
        [4.8318, 5.4246, 5.5333],
        [4.8318, 4.8797, 4.8445],
        [4.8318, 6.1341, 6.9727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13328687846660614 
model_pd.l_d.mean(): -19.90778350830078 
model_pd.lagr.mean(): -19.77449607849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.6357], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13328687846660614
epoch£º28	 i:1 	 global-step:561	 l-p:0.16079837083816528
epoch£º28	 i:2 	 global-step:562	 l-p:0.12945255637168884
epoch£º28	 i:3 	 global-step:563	 l-p:0.12827634811401367
epoch£º28	 i:4 	 global-step:564	 l-p:0.13639064133167267
epoch£º28	 i:5 	 global-step:565	 l-p:0.1605297029018402
epoch£º28	 i:6 	 global-step:566	 l-p:0.12322260439395905
epoch£º28	 i:7 	 global-step:567	 l-p:0.12306933104991913
epoch£º28	 i:8 	 global-step:568	 l-p:0.12756720185279846
epoch£º28	 i:9 	 global-step:569	 l-p:0.13800181448459625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0150, 5.0150, 5.0150],
        [5.0150, 5.2199, 5.1464],
        [5.0150, 5.6643, 5.8007],
        [5.0150, 5.0193, 5.0153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.1361730545759201 
model_pd.l_d.mean(): -19.668298721313477 
model_pd.lagr.mean(): -19.53212547302246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-20.3400], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.1361730545759201
epoch£º29	 i:1 	 global-step:581	 l-p:0.107369065284729
epoch£º29	 i:2 	 global-step:582	 l-p:0.11962670087814331
epoch£º29	 i:3 	 global-step:583	 l-p:0.13835930824279785
epoch£º29	 i:4 	 global-step:584	 l-p:0.13205905258655548
epoch£º29	 i:5 	 global-step:585	 l-p:0.13992439210414886
epoch£º29	 i:6 	 global-step:586	 l-p:0.21212023496627808
epoch£º29	 i:7 	 global-step:587	 l-p:0.12483386695384979
epoch£º29	 i:8 	 global-step:588	 l-p:0.1286928355693817
epoch£º29	 i:9 	 global-step:589	 l-p:0.1323905736207962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 4.8350, 4.8335],
        [4.8334, 6.1193, 6.9392],
        [4.8334, 5.0449, 4.9758],
        [4.8334, 6.0008, 6.6780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.15086619555950165 
model_pd.l_d.mean(): -18.951658248901367 
model_pd.lagr.mean(): -18.800792694091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5613], device='cuda:0')), ('power', tensor([-19.7322], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.15086619555950165
epoch£º30	 i:1 	 global-step:601	 l-p:0.12216059118509293
epoch£º30	 i:2 	 global-step:602	 l-p:0.12749306857585907
epoch£º30	 i:3 	 global-step:603	 l-p:0.011759418994188309
epoch£º30	 i:4 	 global-step:604	 l-p:0.1311386376619339
epoch£º30	 i:5 	 global-step:605	 l-p:0.16878864169120789
epoch£º30	 i:6 	 global-step:606	 l-p:0.13794542849063873
epoch£º30	 i:7 	 global-step:607	 l-p:0.13072310388088226
epoch£º30	 i:8 	 global-step:608	 l-p:0.1297626793384552
epoch£º30	 i:9 	 global-step:609	 l-p:0.1551998108625412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7230, 4.7230, 4.7230],
        [4.7230, 4.7310, 4.7237],
        [4.7230, 6.0393, 6.9206],
        [4.7230, 4.7768, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.1340341567993164 
model_pd.l_d.mean(): -18.069137573242188 
model_pd.lagr.mean(): -17.935104370117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6331], device='cuda:0')), ('power', tensor([-18.9134], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.1340341567993164
epoch£º31	 i:1 	 global-step:621	 l-p:0.17097556591033936
epoch£º31	 i:2 	 global-step:622	 l-p:0.1206577867269516
epoch£º31	 i:3 	 global-step:623	 l-p:0.13284067809581757
epoch£º31	 i:4 	 global-step:624	 l-p:0.1317089945077896
epoch£º31	 i:5 	 global-step:625	 l-p:0.12651531398296356
epoch£º31	 i:6 	 global-step:626	 l-p:0.14885243773460388
epoch£º31	 i:7 	 global-step:627	 l-p:0.20637263357639313
epoch£º31	 i:8 	 global-step:628	 l-p:0.1471026986837387
epoch£º31	 i:9 	 global-step:629	 l-p:0.16361545026302338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 6.2454, 7.2552],
        [4.8148, 5.7434, 6.1608],
        [4.8148, 4.8712, 4.8316],
        [4.8148, 4.8163, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13486292958259583 
model_pd.l_d.mean(): -19.91737937927246 
model_pd.lagr.mean(): -19.782516479492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.6232], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13486292958259583
epoch£º32	 i:1 	 global-step:641	 l-p:0.14056624472141266
epoch£º32	 i:2 	 global-step:642	 l-p:0.15270277857780457
epoch£º32	 i:3 	 global-step:643	 l-p:0.12265060096979141
epoch£º32	 i:4 	 global-step:644	 l-p:0.17839182913303375
epoch£º32	 i:5 	 global-step:645	 l-p:0.1390889286994934
epoch£º32	 i:6 	 global-step:646	 l-p:0.15400075912475586
epoch£º32	 i:7 	 global-step:647	 l-p:0.2804555892944336
epoch£º32	 i:8 	 global-step:648	 l-p:0.11922265589237213
epoch£º32	 i:9 	 global-step:649	 l-p:0.1317528486251831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9064, 4.9066, 4.9064],
        [4.9064, 5.1508, 5.0847],
        [4.9064, 4.9071, 4.9064],
        [4.9064, 5.1511, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.1178995668888092 
model_pd.l_d.mean(): -20.350540161132812 
model_pd.lagr.mean(): -20.232641220092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4456], device='cuda:0')), ('power', tensor([-21.0282], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.1178995668888092
epoch£º33	 i:1 	 global-step:661	 l-p:0.20173382759094238
epoch£º33	 i:2 	 global-step:662	 l-p:0.13895785808563232
epoch£º33	 i:3 	 global-step:663	 l-p:0.11759902536869049
epoch£º33	 i:4 	 global-step:664	 l-p:0.14047014713287354
epoch£º33	 i:5 	 global-step:665	 l-p:0.13330426812171936
epoch£º33	 i:6 	 global-step:666	 l-p:0.1486964225769043
epoch£º33	 i:7 	 global-step:667	 l-p:0.15925198793411255
epoch£º33	 i:8 	 global-step:668	 l-p:0.13533395528793335
epoch£º33	 i:9 	 global-step:669	 l-p:0.19958755373954773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 5.3300, 5.4378],
        [4.7523, 4.7525, 4.7523],
        [4.7523, 5.0221, 4.9660],
        [4.7523, 4.8058, 4.7679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.15000680088996887 
model_pd.l_d.mean(): -20.41020965576172 
model_pd.lagr.mean(): -20.260202407836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-21.1281], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.15000680088996887
epoch£º34	 i:1 	 global-step:681	 l-p:0.14427055418491364
epoch£º34	 i:2 	 global-step:682	 l-p:0.13362862169742584
epoch£º34	 i:3 	 global-step:683	 l-p:0.12760525941848755
epoch£º34	 i:4 	 global-step:684	 l-p:0.15791381895542145
epoch£º34	 i:5 	 global-step:685	 l-p:0.14213165640830994
epoch£º34	 i:6 	 global-step:686	 l-p:0.14003978669643402
epoch£º34	 i:7 	 global-step:687	 l-p:0.136570543050766
epoch£º34	 i:8 	 global-step:688	 l-p:0.12412753701210022
epoch£º34	 i:9 	 global-step:689	 l-p:0.17167049646377563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.3872, 5.4008],
        [4.9326, 4.9326, 4.9326],
        [4.9326, 5.0646, 4.9986],
        [4.9326, 6.2882, 7.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.119378961622715 
model_pd.l_d.mean(): -19.90288543701172 
model_pd.lagr.mean(): -19.783506393432617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.6084], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.119378961622715
epoch£º35	 i:1 	 global-step:701	 l-p:0.13346019387245178
epoch£º35	 i:2 	 global-step:702	 l-p:0.15160037577152252
epoch£º35	 i:3 	 global-step:703	 l-p:0.12814588844776154
epoch£º35	 i:4 	 global-step:704	 l-p:0.1159839779138565
epoch£º35	 i:5 	 global-step:705	 l-p:0.1319238245487213
epoch£º35	 i:6 	 global-step:706	 l-p:0.14844465255737305
epoch£º35	 i:7 	 global-step:707	 l-p:0.12747281789779663
epoch£º35	 i:8 	 global-step:708	 l-p:0.1074044406414032
epoch£º35	 i:9 	 global-step:709	 l-p:0.12258652597665787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8829, 4.8829, 4.8829],
        [4.8829, 4.8829, 4.8829],
        [4.8829, 5.5178, 5.6593],
        [4.8829, 4.9286, 4.8948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.13556867837905884 
model_pd.l_d.mean(): -19.032873153686523 
model_pd.lagr.mean(): -18.89730453491211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-19.7650], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.13556867837905884
epoch£º36	 i:1 	 global-step:721	 l-p:0.11874749511480331
epoch£º36	 i:2 	 global-step:722	 l-p:0.14395715296268463
epoch£º36	 i:3 	 global-step:723	 l-p:0.1194489449262619
epoch£º36	 i:4 	 global-step:724	 l-p:0.13529375195503235
epoch£º36	 i:5 	 global-step:725	 l-p:0.15216568112373352
epoch£º36	 i:6 	 global-step:726	 l-p:0.15417829155921936
epoch£º36	 i:7 	 global-step:727	 l-p:0.13981586694717407
epoch£º36	 i:8 	 global-step:728	 l-p:-0.25122207403182983
epoch£º36	 i:9 	 global-step:729	 l-p:0.1316712498664856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8259, 4.8204],
        [4.8199, 6.0991, 6.9193],
        [4.8199, 5.2085, 5.1949],
        [4.8199, 5.3073, 5.3491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.12874586880207062 
model_pd.l_d.mean(): -18.53508186340332 
model_pd.lagr.mean(): -18.406335830688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.3218], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.12874586880207062
epoch£º37	 i:1 	 global-step:741	 l-p:0.12683431804180145
epoch£º37	 i:2 	 global-step:742	 l-p:0.23317851126194
epoch£º37	 i:3 	 global-step:743	 l-p:0.1261831372976303
epoch£º37	 i:4 	 global-step:744	 l-p:0.14028684794902802
epoch£º37	 i:5 	 global-step:745	 l-p:0.13209925591945648
epoch£º37	 i:6 	 global-step:746	 l-p:0.12678922712802887
epoch£º37	 i:7 	 global-step:747	 l-p:0.117988720536232
epoch£º37	 i:8 	 global-step:748	 l-p:0.1229647696018219
epoch£º37	 i:9 	 global-step:749	 l-p:0.11887933313846588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9075, 5.0171, 4.9567],
        [4.9075, 4.9725, 4.9286],
        [4.9075, 4.9326, 4.9120],
        [4.9075, 4.9696, 4.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.16295409202575684 
model_pd.l_d.mean(): -19.890724182128906 
model_pd.lagr.mean(): -19.72776985168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4836], device='cuda:0')), ('power', tensor([-20.6022], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.16295409202575684
epoch£º38	 i:1 	 global-step:761	 l-p:0.14156825840473175
epoch£º38	 i:2 	 global-step:762	 l-p:0.1292097568511963
epoch£º38	 i:3 	 global-step:763	 l-p:0.12469334155321121
epoch£º38	 i:4 	 global-step:764	 l-p:0.1333230435848236
epoch£º38	 i:5 	 global-step:765	 l-p:0.12368366122245789
epoch£º38	 i:6 	 global-step:766	 l-p:0.17688819766044617
epoch£º38	 i:7 	 global-step:767	 l-p:0.13974912464618683
epoch£º38	 i:8 	 global-step:768	 l-p:0.14742878079414368
epoch£º38	 i:9 	 global-step:769	 l-p:0.12311307340860367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7669, 4.7669, 4.7669],
        [4.7669, 5.8098, 6.3631],
        [4.7669, 4.7669, 4.7669],
        [4.7669, 4.7689, 4.7670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13791494071483612 
model_pd.l_d.mean(): -20.780826568603516 
model_pd.lagr.mean(): -20.642911911010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.4596], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13791494071483612
epoch£º39	 i:1 	 global-step:781	 l-p:0.13355517387390137
epoch£º39	 i:2 	 global-step:782	 l-p:0.10830505937337875
epoch£º39	 i:3 	 global-step:783	 l-p:0.13879543542861938
epoch£º39	 i:4 	 global-step:784	 l-p:0.1692054718732834
epoch£º39	 i:5 	 global-step:785	 l-p:0.19250816106796265
epoch£º39	 i:6 	 global-step:786	 l-p:0.14551080763339996
epoch£º39	 i:7 	 global-step:787	 l-p:0.1443931758403778
epoch£º39	 i:8 	 global-step:788	 l-p:0.13691237568855286
epoch£º39	 i:9 	 global-step:789	 l-p:0.12353095412254333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9624, 4.9626, 4.9624],
        [4.9624, 4.9809, 4.9652],
        [4.9624, 5.0748, 5.0134],
        [4.9624, 4.9699, 4.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.11194055527448654 
model_pd.l_d.mean(): -20.54973030090332 
model_pd.lagr.mean(): -20.437789916992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112], device='cuda:0')), ('power', tensor([-21.1944], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.11194055527448654
epoch£º40	 i:1 	 global-step:801	 l-p:0.13013163208961487
epoch£º40	 i:2 	 global-step:802	 l-p:0.11617657542228699
epoch£º40	 i:3 	 global-step:803	 l-p:0.1514817774295807
epoch£º40	 i:4 	 global-step:804	 l-p:0.1238432377576828
epoch£º40	 i:5 	 global-step:805	 l-p:0.11678903549909592
epoch£º40	 i:6 	 global-step:806	 l-p:0.12566934525966644
epoch£º40	 i:7 	 global-step:807	 l-p:0.19719910621643066
epoch£º40	 i:8 	 global-step:808	 l-p:0.12911377847194672
epoch£º40	 i:9 	 global-step:809	 l-p:0.14513464272022247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9346, 4.9372, 4.9348],
        [4.9346, 4.9349, 4.9346],
        [4.9346, 5.1178, 5.0478],
        [4.9346, 5.0490, 4.9875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.14238084852695465 
model_pd.l_d.mean(): -20.475343704223633 
model_pd.lagr.mean(): -20.332962036132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.1286], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.14238084852695465
epoch£º41	 i:1 	 global-step:821	 l-p:0.1218755692243576
epoch£º41	 i:2 	 global-step:822	 l-p:0.11777670681476593
epoch£º41	 i:3 	 global-step:823	 l-p:0.1327102929353714
epoch£º41	 i:4 	 global-step:824	 l-p:0.22756117582321167
epoch£º41	 i:5 	 global-step:825	 l-p:0.12361915409564972
epoch£º41	 i:6 	 global-step:826	 l-p:0.13086546957492828
epoch£º41	 i:7 	 global-step:827	 l-p:0.12329099327325821
epoch£º41	 i:8 	 global-step:828	 l-p:0.13106496632099152
epoch£º41	 i:9 	 global-step:829	 l-p:0.16321763396263123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 5.1117, 5.0574],
        [4.8334, 4.8334, 4.8334],
        [4.8334, 6.1941, 7.1180],
        [4.8334, 4.8490, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12238796055316925 
model_pd.l_d.mean(): -18.99122428894043 
model_pd.lagr.mean(): -18.86883544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5236], device='cuda:0')), ('power', tensor([-19.7336], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.12238796055316925
epoch£º42	 i:1 	 global-step:841	 l-p:0.13512760400772095
epoch£º42	 i:2 	 global-step:842	 l-p:0.13941630721092224
epoch£º42	 i:3 	 global-step:843	 l-p:0.21287322044372559
epoch£º42	 i:4 	 global-step:844	 l-p:0.15254874527454376
epoch£º42	 i:5 	 global-step:845	 l-p:0.147209033370018
epoch£º42	 i:6 	 global-step:846	 l-p:0.13498573005199432
epoch£º42	 i:7 	 global-step:847	 l-p:0.09509290754795074
epoch£º42	 i:8 	 global-step:848	 l-p:0.13840322196483612
epoch£º42	 i:9 	 global-step:849	 l-p:0.16125446557998657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7679, 5.9701, 6.7115],
        [4.7679, 5.3090, 5.3940],
        [4.7679, 5.0661, 5.0204],
        [4.7679, 5.9103, 6.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.062362585216760635 
model_pd.l_d.mean(): -19.928688049316406 
model_pd.lagr.mean(): -19.86632537841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.6763], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.062362585216760635
epoch£º43	 i:1 	 global-step:861	 l-p:0.14900274574756622
epoch£º43	 i:2 	 global-step:862	 l-p:0.1346110850572586
epoch£º43	 i:3 	 global-step:863	 l-p:0.13241024315357208
epoch£º43	 i:4 	 global-step:864	 l-p:0.1403081864118576
epoch£º43	 i:5 	 global-step:865	 l-p:0.12816762924194336
epoch£º43	 i:6 	 global-step:866	 l-p:0.13030381500720978
epoch£º43	 i:7 	 global-step:867	 l-p:0.14207206666469574
epoch£º43	 i:8 	 global-step:868	 l-p:0.12005419284105301
epoch£º43	 i:9 	 global-step:869	 l-p:0.14093877375125885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8878, 5.0136, 4.9502],
        [4.8878, 4.8948, 4.8884],
        [4.8878, 4.8902, 4.8880],
        [4.8878, 4.8879, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13652120530605316 
model_pd.l_d.mean(): -20.439321517944336 
model_pd.lagr.mean(): -20.30280113220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1045], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13652120530605316
epoch£º44	 i:1 	 global-step:881	 l-p:0.13807760179042816
epoch£º44	 i:2 	 global-step:882	 l-p:-0.03221297636628151
epoch£º44	 i:3 	 global-step:883	 l-p:0.1403733342885971
epoch£º44	 i:4 	 global-step:884	 l-p:0.13725225627422333
epoch£º44	 i:5 	 global-step:885	 l-p:0.12938447296619415
epoch£º44	 i:6 	 global-step:886	 l-p:0.1125863641500473
epoch£º44	 i:7 	 global-step:887	 l-p:0.1518799364566803
epoch£º44	 i:8 	 global-step:888	 l-p:0.16582359373569489
epoch£º44	 i:9 	 global-step:889	 l-p:0.12697505950927734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8619, 4.8619, 4.8619],
        [4.8619, 4.8629, 4.8619],
        [4.8619, 5.0938, 5.0297],
        [4.8619, 4.8619, 4.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13258911669254303 
model_pd.l_d.mean(): -20.82813262939453 
model_pd.lagr.mean(): -20.69554328918457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.4759], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13258911669254303
epoch£º45	 i:1 	 global-step:901	 l-p:0.13080714643001556
epoch£º45	 i:2 	 global-step:902	 l-p:0.11158215999603271
epoch£º45	 i:3 	 global-step:903	 l-p:0.12573997676372528
epoch£º45	 i:4 	 global-step:904	 l-p:0.14953848719596863
epoch£º45	 i:5 	 global-step:905	 l-p:-0.00768046360462904
epoch£º45	 i:6 	 global-step:906	 l-p:0.14163847267627716
epoch£º45	 i:7 	 global-step:907	 l-p:0.1417524814605713
epoch£º45	 i:8 	 global-step:908	 l-p:0.16078782081604004
epoch£º45	 i:9 	 global-step:909	 l-p:0.1286722719669342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8885, 4.8846],
        [4.8843, 4.9107, 4.8893],
        [4.8843, 4.8843, 4.8843],
        [4.8843, 4.9101, 4.8891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11818859726190567 
model_pd.l_d.mean(): -19.682453155517578 
model_pd.lagr.mean(): -19.56426429748535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.4005], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11818859726190567
epoch£º46	 i:1 	 global-step:921	 l-p:0.241484597325325
epoch£º46	 i:2 	 global-step:922	 l-p:0.12811945378780365
epoch£º46	 i:3 	 global-step:923	 l-p:0.12558235228061676
epoch£º46	 i:4 	 global-step:924	 l-p:0.14085659384727478
epoch£º46	 i:5 	 global-step:925	 l-p:0.1160445287823677
epoch£º46	 i:6 	 global-step:926	 l-p:0.14508891105651855
epoch£º46	 i:7 	 global-step:927	 l-p:0.13224764168262482
epoch£º46	 i:8 	 global-step:928	 l-p:0.12909680604934692
epoch£º46	 i:9 	 global-step:929	 l-p:0.15558671951293945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8284, 4.8840, 4.8452],
        [4.8284, 4.9346, 4.8765],
        [4.8284, 4.8301, 4.8284],
        [4.8284, 5.1694, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.12015225738286972 
model_pd.l_d.mean(): -18.4256534576416 
model_pd.lagr.mean(): -18.30550193786621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5323], device='cuda:0')), ('power', tensor([-19.1709], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.12015225738286972
epoch£º47	 i:1 	 global-step:941	 l-p:0.14286492764949799
epoch£º47	 i:2 	 global-step:942	 l-p:0.1649903506040573
epoch£º47	 i:3 	 global-step:943	 l-p:0.1309085637331009
epoch£º47	 i:4 	 global-step:944	 l-p:0.1378818154335022
epoch£º47	 i:5 	 global-step:945	 l-p:0.11365813761949539
epoch£º47	 i:6 	 global-step:946	 l-p:0.13483524322509766
epoch£º47	 i:7 	 global-step:947	 l-p:0.13506533205509186
epoch£º47	 i:8 	 global-step:948	 l-p:0.1337072104215622
epoch£º47	 i:9 	 global-step:949	 l-p:0.170115128159523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7862, 5.1243, 5.0940],
        [4.7862, 4.9667, 4.9006],
        [4.7862, 4.7862, 4.7862],
        [4.7862, 5.0162, 4.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17270880937576294 
model_pd.l_d.mean(): -20.85230255126953 
model_pd.lagr.mean(): -20.679594039916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.5241], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17270880937576294
epoch£º48	 i:1 	 global-step:961	 l-p:0.12782038748264313
epoch£º48	 i:2 	 global-step:962	 l-p:0.13549138605594635
epoch£º48	 i:3 	 global-step:963	 l-p:0.12013395875692368
epoch£º48	 i:4 	 global-step:964	 l-p:0.13807323575019836
epoch£º48	 i:5 	 global-step:965	 l-p:0.21216563880443573
epoch£º48	 i:6 	 global-step:966	 l-p:0.1240200474858284
epoch£º48	 i:7 	 global-step:967	 l-p:0.12953080236911774
epoch£º48	 i:8 	 global-step:968	 l-p:0.11850955337285995
epoch£º48	 i:9 	 global-step:969	 l-p:0.11744977533817291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9222, 4.9565, 4.9298],
        [4.9222, 4.9235, 4.9222],
        [4.9222, 5.0805, 5.0129],
        [4.9222, 4.9283, 4.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.1259399801492691 
model_pd.l_d.mean(): -20.845260620117188 
model_pd.lagr.mean(): -20.71932029724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3920], device='cuda:0')), ('power', tensor([-21.4735], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.1259399801492691
epoch£º49	 i:1 	 global-step:981	 l-p:0.13945183157920837
epoch£º49	 i:2 	 global-step:982	 l-p:0.1596713662147522
epoch£º49	 i:3 	 global-step:983	 l-p:0.1325751394033432
epoch£º49	 i:4 	 global-step:984	 l-p:0.14661987125873566
epoch£º49	 i:5 	 global-step:985	 l-p:0.1453665941953659
epoch£º49	 i:6 	 global-step:986	 l-p:0.1393745243549347
epoch£º49	 i:7 	 global-step:987	 l-p:0.1916542947292328
epoch£º49	 i:8 	 global-step:988	 l-p:0.11981222033500671
epoch£º49	 i:9 	 global-step:989	 l-p:0.14000074565410614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8121, 5.6421, 5.9736],
        [4.8121, 4.8122, 4.8121],
        [4.8121, 5.1292, 5.0903],
        [4.8121, 4.8138, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.1387050449848175 
model_pd.l_d.mean(): -20.09463119506836 
model_pd.lagr.mean(): -19.9559268951416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.8194], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.1387050449848175
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13915686309337616
epoch£º50	 i:2 	 global-step:1002	 l-p:0.14390888810157776
epoch£º50	 i:3 	 global-step:1003	 l-p:0.14031465351581573
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12623010575771332
epoch£º50	 i:5 	 global-step:1005	 l-p:0.12198720872402191
epoch£º50	 i:6 	 global-step:1006	 l-p:0.13390527665615082
epoch£º50	 i:7 	 global-step:1007	 l-p:0.14577431976795197
epoch£º50	 i:8 	 global-step:1008	 l-p:0.1240728348493576
epoch£º50	 i:9 	 global-step:1009	 l-p:0.1359298825263977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 5.5699, 5.6900],
        [4.9641, 6.2928, 7.1561],
        [4.9641, 4.9642, 4.9641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11433088034391403 
model_pd.l_d.mean(): -18.682893753051758 
model_pd.lagr.mean(): -18.56856346130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.4081], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11433088034391403
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10961992293596268
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11620552092790604
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12692876160144806
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12340954691171646
epoch£º51	 i:5 	 global-step:1025	 l-p:0.12480294704437256
epoch£º51	 i:6 	 global-step:1026	 l-p:0.13821417093276978
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18905329704284668
epoch£º51	 i:8 	 global-step:1028	 l-p:0.15352877974510193
epoch£º51	 i:9 	 global-step:1029	 l-p:0.1587277203798294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7054, 5.2840, 5.4071],
        [4.7054, 5.1215, 5.1349],
        [4.7054, 4.7539, 4.7193],
        [4.7054, 4.7054, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.14334239065647125 
model_pd.l_d.mean(): -20.545291900634766 
model_pd.lagr.mean(): -20.401948928833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.2487], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.14334239065647125
epoch£º52	 i:1 	 global-step:1041	 l-p:0.1566525548696518
epoch£º52	 i:2 	 global-step:1042	 l-p:0.139327272772789
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14809735119342804
epoch£º52	 i:4 	 global-step:1044	 l-p:0.040670957416296005
epoch£º52	 i:5 	 global-step:1045	 l-p:0.14601992070674896
epoch£º52	 i:6 	 global-step:1046	 l-p:0.16352465748786926
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.14917516708374023
epoch£º52	 i:8 	 global-step:1048	 l-p:0.19619335234165192
epoch£º52	 i:9 	 global-step:1049	 l-p:0.10795392096042633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6464, 4.6544, 4.6472],
        [4.6464, 4.6465, 4.6464],
        [4.6464, 4.6470, 4.6464],
        [4.6464, 4.6469, 4.6464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.15336965024471283 
model_pd.l_d.mean(): -19.0693359375 
model_pd.lagr.mean(): -18.915966033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-19.8512], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.15336965024471283
epoch£º53	 i:1 	 global-step:1061	 l-p:0.132614403963089
epoch£º53	 i:2 	 global-step:1062	 l-p:0.1329084187746048
epoch£º53	 i:3 	 global-step:1063	 l-p:-0.6202101111412048
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12235385924577713
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11967454850673676
epoch£º53	 i:6 	 global-step:1066	 l-p:0.1242675930261612
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12091454863548279
epoch£º53	 i:8 	 global-step:1068	 l-p:0.1344626247882843
epoch£º53	 i:9 	 global-step:1069	 l-p:0.1364230364561081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0138, 5.0138, 5.0138],
        [5.0138, 5.7298, 5.9357],
        [5.0138, 5.7818, 6.0333],
        [5.0138, 5.1210, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1283007562160492 
model_pd.l_d.mean(): -20.59379768371582 
model_pd.lagr.mean(): -20.465496063232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2171], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1283007562160492
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12305675446987152
epoch£º54	 i:2 	 global-step:1082	 l-p:0.12654590606689453
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11654891818761826
epoch£º54	 i:4 	 global-step:1084	 l-p:0.1251288652420044
epoch£º54	 i:5 	 global-step:1085	 l-p:0.1412523090839386
epoch£º54	 i:6 	 global-step:1086	 l-p:0.04973410442471504
epoch£º54	 i:7 	 global-step:1087	 l-p:0.15598691999912262
epoch£º54	 i:8 	 global-step:1088	 l-p:0.13251042366027832
epoch£º54	 i:9 	 global-step:1089	 l-p:0.14125001430511475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8611, 5.5963, 5.8362],
        [4.8611, 5.8973, 6.4395],
        [4.8611, 5.2947, 5.3095],
        [4.8611, 5.6053, 5.8533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.15564246475696564 
model_pd.l_d.mean(): -20.037296295166016 
model_pd.lagr.mean(): -19.881654739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-20.7506], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.15564246475696564
epoch£º55	 i:1 	 global-step:1101	 l-p:-0.7848246097564697
epoch£º55	 i:2 	 global-step:1102	 l-p:0.14931732416152954
epoch£º55	 i:3 	 global-step:1103	 l-p:0.15564197301864624
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10965918004512787
epoch£º55	 i:5 	 global-step:1105	 l-p:0.1147226095199585
epoch£º55	 i:6 	 global-step:1106	 l-p:0.11345387995243073
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12120094150304794
epoch£º55	 i:8 	 global-step:1108	 l-p:0.1133350133895874
epoch£º55	 i:9 	 global-step:1109	 l-p:0.14994138479232788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9238, 6.2318, 7.0814],
        [4.9238, 4.9238, 4.9238],
        [4.9238, 5.1621, 5.0997],
        [4.9238, 5.0004, 4.9521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.18312320113182068 
model_pd.l_d.mean(): -20.000104904174805 
model_pd.lagr.mean(): -19.81698226928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.7045], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.18312320113182068
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13572733104228973
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12788765132427216
epoch£º56	 i:3 	 global-step:1123	 l-p:0.12440679967403412
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12217751890420914
epoch£º56	 i:5 	 global-step:1125	 l-p:0.1296311318874359
epoch£º56	 i:6 	 global-step:1126	 l-p:0.1343836933374405
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14420045912265778
epoch£º56	 i:8 	 global-step:1128	 l-p:0.1660139113664627
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14165659248828888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8278, 5.8063, 6.2925],
        [4.8278, 5.9211, 6.5343],
        [4.8278, 4.8278, 4.8278],
        [4.8278, 4.8938, 4.8504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.16100238263607025 
model_pd.l_d.mean(): -20.344301223754883 
model_pd.lagr.mean(): -20.183298110961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.16100238263607025
epoch£º57	 i:1 	 global-step:1141	 l-p:0.1389024555683136
epoch£º57	 i:2 	 global-step:1142	 l-p:0.1630759835243225
epoch£º57	 i:3 	 global-step:1143	 l-p:0.059483010321855545
epoch£º57	 i:4 	 global-step:1144	 l-p:0.1249305009841919
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14687786996364594
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14737603068351746
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11315200477838516
epoch£º57	 i:8 	 global-step:1148	 l-p:0.12686316668987274
epoch£º57	 i:9 	 global-step:1149	 l-p:0.11401433497667313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9294, 5.2412, 5.1980],
        [4.9294, 5.1477, 5.0827],
        [4.9294, 6.1823, 6.9652],
        [4.9294, 4.9294, 4.9294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.13078193366527557 
model_pd.l_d.mean(): -20.6148681640625 
model_pd.lagr.mean(): -20.484086990356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.13078193366527557
epoch£º58	 i:1 	 global-step:1161	 l-p:0.13997012376785278
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12424476444721222
epoch£º58	 i:3 	 global-step:1163	 l-p:0.13025301694869995
epoch£º58	 i:4 	 global-step:1164	 l-p:0.1509477198123932
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13278722763061523
epoch£º58	 i:6 	 global-step:1166	 l-p:0.1346743404865265
epoch£º58	 i:7 	 global-step:1167	 l-p:0.14395639300346375
epoch£º58	 i:8 	 global-step:1168	 l-p:0.1547718048095703
epoch£º58	 i:9 	 global-step:1169	 l-p:0.08763575553894043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6160, 4.9343, 4.9073],
        [4.6160, 4.6180, 4.6160],
        [4.6160, 4.6160, 4.6160],
        [4.6160, 4.7686, 4.7075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.13095702230930328 
model_pd.l_d.mean(): -18.834362030029297 
model_pd.lagr.mean(): -18.703405380249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5796], device='cuda:0')), ('power', tensor([-19.6323], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.13095702230930328
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14608949422836304
epoch£º59	 i:2 	 global-step:1182	 l-p:0.16556991636753082
epoch£º59	 i:3 	 global-step:1183	 l-p:0.3569411635398865
epoch£º59	 i:4 	 global-step:1184	 l-p:0.15377411246299744
epoch£º59	 i:5 	 global-step:1185	 l-p:0.18062973022460938
epoch£º59	 i:6 	 global-step:1186	 l-p:0.1399383544921875
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12406108528375626
epoch£º59	 i:8 	 global-step:1188	 l-p:0.13060277700424194
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13422891497612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0032, 5.1117, 5.0526],
        [5.0032, 5.0032, 5.0032],
        [5.0032, 5.0052, 5.0033],
        [5.0032, 5.3502, 5.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15152116119861603 
model_pd.l_d.mean(): -18.204551696777344 
model_pd.lagr.mean(): -18.053030014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5590], device='cuda:0')), ('power', tensor([-18.9745], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.15152116119861603
epoch£º60	 i:1 	 global-step:1201	 l-p:0.13279062509536743
epoch£º60	 i:2 	 global-step:1202	 l-p:0.11343317478895187
epoch£º60	 i:3 	 global-step:1203	 l-p:0.11557814478874207
epoch£º60	 i:4 	 global-step:1204	 l-p:0.12959542870521545
epoch£º60	 i:5 	 global-step:1205	 l-p:0.12157317996025085
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12599709630012512
epoch£º60	 i:7 	 global-step:1207	 l-p:0.11329423636198044
epoch£º60	 i:8 	 global-step:1208	 l-p:0.1446334421634674
epoch£º60	 i:9 	 global-step:1209	 l-p:0.11628331989049911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 4.8474, 4.8474],
        [4.8474, 5.4709, 5.6208],
        [4.8474, 4.8539, 4.8480],
        [4.8474, 5.5586, 5.7822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15132199227809906 
model_pd.l_d.mean(): -19.231029510498047 
model_pd.lagr.mean(): -19.079708099365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-19.9577], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15132199227809906
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14555974304676056
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12608905136585236
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17638875544071198
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13829439878463745
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11722931265830994
epoch£º61	 i:6 	 global-step:1226	 l-p:0.1353198140859604
epoch£º61	 i:7 	 global-step:1227	 l-p:0.1299256682395935
epoch£º61	 i:8 	 global-step:1228	 l-p:0.1377505362033844
epoch£º61	 i:9 	 global-step:1229	 l-p:0.15248236060142517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8237, 4.8551, 4.8306],
        [4.8237, 5.7478, 6.1806],
        [4.8237, 4.9539, 4.8923],
        [4.8237, 5.0358, 4.9734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.1618840992450714 
model_pd.l_d.mean(): -19.955583572387695 
model_pd.lagr.mean(): -19.793699264526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.6929], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.1618840992450714
epoch£º62	 i:1 	 global-step:1241	 l-p:0.13106229901313782
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10828808695077896
epoch£º62	 i:3 	 global-step:1243	 l-p:0.1225816160440445
epoch£º62	 i:4 	 global-step:1244	 l-p:0.1788216382265091
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12624262273311615
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13355708122253418
epoch£º62	 i:7 	 global-step:1247	 l-p:0.1390044242143631
epoch£º62	 i:8 	 global-step:1248	 l-p:0.13778607547283173
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13403621315956116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9333, 4.9360, 4.9335],
        [4.9333, 6.2781, 7.1768],
        [4.9333, 6.2829, 7.1874],
        [4.9333, 4.9333, 4.9333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.1327338069677353 
model_pd.l_d.mean(): -20.04271697998047 
model_pd.lagr.mean(): -19.909982681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4595], device='cuda:0')), ('power', tensor([-20.7311], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.1327338069677353
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14918677508831024
epoch£º63	 i:2 	 global-step:1262	 l-p:0.03722923621535301
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15349513292312622
epoch£º63	 i:4 	 global-step:1264	 l-p:0.11600927263498306
epoch£º63	 i:5 	 global-step:1265	 l-p:0.13936549425125122
epoch£º63	 i:6 	 global-step:1266	 l-p:0.14348837733268738
epoch£º63	 i:7 	 global-step:1267	 l-p:0.182389497756958
epoch£º63	 i:8 	 global-step:1268	 l-p:0.1151387020945549
epoch£º63	 i:9 	 global-step:1269	 l-p:0.019878024235367775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6645, 4.6662, 4.6646],
        [4.6645, 4.6645, 4.6645],
        [4.6645, 4.6906, 4.6698],
        [4.6645, 4.6645, 4.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.13139380514621735 
model_pd.l_d.mean(): -19.788049697875977 
model_pd.lagr.mean(): -19.65665626525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.5651], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.13139380514621735
epoch£º64	 i:1 	 global-step:1281	 l-p:0.14740735292434692
epoch£º64	 i:2 	 global-step:1282	 l-p:0.250824898481369
epoch£º64	 i:3 	 global-step:1283	 l-p:0.22069045901298523
epoch£º64	 i:4 	 global-step:1284	 l-p:0.11126089841127396
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16892020404338837
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13798092305660248
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10177396237850189
epoch£º64	 i:8 	 global-step:1288	 l-p:0.14322254061698914
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11565262824296951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9468, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0190, 6.5946],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12497007101774216 
model_pd.l_d.mean(): -20.00855255126953 
model_pd.lagr.mean(): -19.883583068847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7012], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12497007101774216
epoch£º65	 i:1 	 global-step:1301	 l-p:0.13927972316741943
epoch£º65	 i:2 	 global-step:1302	 l-p:0.13500723242759705
epoch£º65	 i:3 	 global-step:1303	 l-p:0.13673143088817596
epoch£º65	 i:4 	 global-step:1304	 l-p:0.1211426630616188
epoch£º65	 i:5 	 global-step:1305	 l-p:0.16686402261257172
epoch£º65	 i:6 	 global-step:1306	 l-p:0.1353522539138794
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14535683393478394
epoch£º65	 i:8 	 global-step:1308	 l-p:0.148215189576149
epoch£º65	 i:9 	 global-step:1309	 l-p:0.2814064919948578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7085, 4.7085, 4.7085],
        [4.7085, 5.6967, 6.2181],
        [4.7085, 5.2368, 5.3287],
        [4.7085, 4.7322, 4.7130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14083653688430786 
model_pd.l_d.mean(): -20.467304229736328 
model_pd.lagr.mean(): -20.326467514038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4910], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14083653688430786
epoch£º66	 i:1 	 global-step:1321	 l-p:0.13944131135940552
epoch£º66	 i:2 	 global-step:1322	 l-p:0.1322012096643448
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12713506817817688
epoch£º66	 i:4 	 global-step:1324	 l-p:0.16593089699745178
epoch£º66	 i:5 	 global-step:1325	 l-p:0.13364900648593903
epoch£º66	 i:6 	 global-step:1326	 l-p:0.1365087479352951
epoch£º66	 i:7 	 global-step:1327	 l-p:0.18171954154968262
epoch£º66	 i:8 	 global-step:1328	 l-p:0.13006795942783356
epoch£º66	 i:9 	 global-step:1329	 l-p:0.1533748358488083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8989, 4.8989, 4.8989],
        [4.8989, 4.8989, 4.8989],
        [4.8989, 5.8760, 6.3564],
        [4.8989, 5.6693, 5.9438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.16615237295627594 
model_pd.l_d.mean(): -20.004287719726562 
model_pd.lagr.mean(): -19.838134765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4794], device='cuda:0')), ('power', tensor([-20.7127], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.16615237295627594
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11301324516534805
epoch£º67	 i:2 	 global-step:1342	 l-p:0.12897703051567078
epoch£º67	 i:3 	 global-step:1343	 l-p:0.129599466919899
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13698533177375793
epoch£º67	 i:5 	 global-step:1345	 l-p:0.21121777594089508
epoch£º67	 i:6 	 global-step:1346	 l-p:0.13393227756023407
epoch£º67	 i:7 	 global-step:1347	 l-p:0.1287926584482193
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14464175701141357
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10992909967899323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8884, 6.1548, 6.9688],
        [4.8884, 6.2118, 7.0960],
        [4.8884, 4.8982, 4.8895],
        [4.8884, 5.9382, 6.4999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.12088990956544876 
model_pd.l_d.mean(): -19.09015655517578 
model_pd.lagr.mean(): -18.969266891479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-19.8516], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.12088990956544876
epoch£º68	 i:1 	 global-step:1361	 l-p:0.36028507351875305
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15068283677101135
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11881648749113083
epoch£º68	 i:4 	 global-step:1364	 l-p:0.12765470147132874
epoch£º68	 i:5 	 global-step:1365	 l-p:0.1237451434135437
epoch£º68	 i:6 	 global-step:1366	 l-p:0.13096334040164948
epoch£º68	 i:7 	 global-step:1367	 l-p:0.15561655163764954
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11844922602176666
epoch£º68	 i:9 	 global-step:1369	 l-p:0.16050158441066742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8355, 5.0000, 4.9361],
        [4.8355, 5.9071, 6.5025],
        [4.8355, 5.0575, 4.9975],
        [4.8355, 4.8355, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.15230388939380646 
model_pd.l_d.mean(): -18.61500358581543 
model_pd.lagr.mean(): -18.46269989013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5706], device='cuda:0')), ('power', tensor([-19.4014], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.15230388939380646
epoch£º69	 i:1 	 global-step:1381	 l-p:0.13182908296585083
epoch£º69	 i:2 	 global-step:1382	 l-p:-0.20029036700725555
epoch£º69	 i:3 	 global-step:1383	 l-p:0.15426042675971985
epoch£º69	 i:4 	 global-step:1384	 l-p:0.1203722134232521
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12480197846889496
epoch£º69	 i:6 	 global-step:1386	 l-p:0.119340680539608
epoch£º69	 i:7 	 global-step:1387	 l-p:0.13281859457492828
epoch£º69	 i:8 	 global-step:1388	 l-p:0.1272861510515213
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12114964425563812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9306, 6.3162, 7.2715],
        [4.9306, 4.9583, 4.9362],
        [4.9306, 4.9306, 4.9306],
        [4.9306, 6.2266, 7.0700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10399814695119858 
model_pd.l_d.mean(): -17.98577880859375 
model_pd.lagr.mean(): -17.88178062438965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5762], device='cuda:0')), ('power', tensor([-18.7710], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10399814695119858
epoch£º70	 i:1 	 global-step:1401	 l-p:0.12655878067016602
epoch£º70	 i:2 	 global-step:1402	 l-p:0.12904244661331177
epoch£º70	 i:3 	 global-step:1403	 l-p:0.14046330749988556
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15240439772605896
epoch£º70	 i:5 	 global-step:1405	 l-p:0.18526582419872284
epoch£º70	 i:6 	 global-step:1406	 l-p:0.16521550714969635
epoch£º70	 i:7 	 global-step:1407	 l-p:0.10284333676099777
epoch£º70	 i:8 	 global-step:1408	 l-p:0.1266201138496399
epoch£º70	 i:9 	 global-step:1409	 l-p:0.15000656247138977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8210, 5.0123, 4.9496],
        [4.8210, 4.8241, 4.8212],
        [4.8210, 4.8210, 4.8210],
        [4.8210, 5.4243, 5.5657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.13794226944446564 
model_pd.l_d.mean(): -20.326967239379883 
model_pd.lagr.mean(): -20.18902587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-21.0375], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.13794226944446564
epoch£º71	 i:1 	 global-step:1421	 l-p:0.1292559653520584
epoch£º71	 i:2 	 global-step:1422	 l-p:0.09324925392866135
epoch£º71	 i:3 	 global-step:1423	 l-p:0.1216299757361412
epoch£º71	 i:4 	 global-step:1424	 l-p:0.1606615036725998
epoch£º71	 i:5 	 global-step:1425	 l-p:0.139139324426651
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14204815030097961
epoch£º71	 i:7 	 global-step:1427	 l-p:0.16101203858852386
epoch£º71	 i:8 	 global-step:1428	 l-p:0.14341707527637482
epoch£º71	 i:9 	 global-step:1429	 l-p:0.15501175820827484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8509, 4.8509, 4.8509],
        [4.8509, 4.8509, 4.8509],
        [4.8509, 4.8554, 4.8512],
        [4.8509, 4.8509, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12231538444757462 
model_pd.l_d.mean(): -19.1968936920166 
model_pd.lagr.mean(): -19.0745792388916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12231538444757462
epoch£º72	 i:1 	 global-step:1441	 l-p:0.1467249095439911
epoch£º72	 i:2 	 global-step:1442	 l-p:0.13569681346416473
epoch£º72	 i:3 	 global-step:1443	 l-p:0.1420106440782547
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14734195172786713
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11704409122467041
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15368449687957764
epoch£º72	 i:7 	 global-step:1447	 l-p:0.13301150500774384
epoch£º72	 i:8 	 global-step:1448	 l-p:0.1564180552959442
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.04174022749066353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8868, 4.8868, 4.8868],
        [4.8868, 5.0443, 4.9804],
        [4.8868, 5.8553, 6.3325],
        [4.8868, 5.1080, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1317933201789856 
model_pd.l_d.mean(): -19.311609268188477 
model_pd.lagr.mean(): -19.1798152923584 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-20.0034], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.1317933201789856
epoch£º73	 i:1 	 global-step:1461	 l-p:0.14397241175174713
epoch£º73	 i:2 	 global-step:1462	 l-p:0.22131803631782532
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11549545079469681
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12857051193714142
epoch£º73	 i:5 	 global-step:1465	 l-p:0.12093749642372131
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13365598022937775
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14521095156669617
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10627195239067078
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12822702527046204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9009, 4.9189, 4.9038],
        [4.9009, 5.2703, 5.2565],
        [4.9009, 5.6303, 5.8715],
        [4.9009, 5.1214, 5.0604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13520696759223938 
model_pd.l_d.mean(): -19.755630493164062 
model_pd.lagr.mean(): -19.620424270629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.4845], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13520696759223938
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11785116791725159
epoch£º74	 i:2 	 global-step:1482	 l-p:0.1447976976633072
epoch£º74	 i:3 	 global-step:1483	 l-p:0.1340371072292328
epoch£º74	 i:4 	 global-step:1484	 l-p:0.12525729835033417
epoch£º74	 i:5 	 global-step:1485	 l-p:0.18117594718933105
epoch£º74	 i:6 	 global-step:1486	 l-p:0.18466080725193024
epoch£º74	 i:7 	 global-step:1487	 l-p:0.10868524014949799
epoch£º74	 i:8 	 global-step:1488	 l-p:0.16222675144672394
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13272394239902496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8920, 5.2251, 5.1962],
        [4.8920, 5.1021, 5.0402],
        [4.8920, 4.8920, 4.8920],
        [4.8920, 5.6745, 5.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.1364288181066513 
model_pd.l_d.mean(): -20.096839904785156 
model_pd.lagr.mean(): -19.960411071777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.1364288181066513
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12808771431446075
epoch£º75	 i:2 	 global-step:1502	 l-p:0.14080145955085754
epoch£º75	 i:3 	 global-step:1503	 l-p:0.1422734260559082
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13767801225185394
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11957091838121414
epoch£º75	 i:6 	 global-step:1506	 l-p:0.1640528440475464
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13617005944252014
epoch£º75	 i:8 	 global-step:1508	 l-p:0.09892342984676361
epoch£º75	 i:9 	 global-step:1509	 l-p:0.14535850286483765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8378, 4.8383, 4.8378],
        [4.8378, 4.8378, 4.8378],
        [4.8378, 5.5728, 5.8274],
        [4.8378, 5.0654, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12408151477575302 
model_pd.l_d.mean(): -19.416542053222656 
model_pd.lagr.mean(): -19.292461395263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1678], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12408151477575302
epoch£º76	 i:1 	 global-step:1521	 l-p:0.13844993710517883
epoch£º76	 i:2 	 global-step:1522	 l-p:0.2667633295059204
epoch£º76	 i:3 	 global-step:1523	 l-p:0.1271219104528427
epoch£º76	 i:4 	 global-step:1524	 l-p:0.11962030082941055
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11829333752393723
epoch£º76	 i:6 	 global-step:1526	 l-p:0.1111256405711174
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11747094988822937
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12500178813934326
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16276469826698303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9340, 4.9511, 4.9366],
        [4.9340, 4.9343, 4.9340],
        [4.9340, 4.9758, 4.9450],
        [4.9340, 5.0350, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12634246051311493 
model_pd.l_d.mean(): -20.45311737060547 
model_pd.lagr.mean(): -20.32677459716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.1179], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12634246051311493
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11698099225759506
epoch£º77	 i:2 	 global-step:1542	 l-p:0.15160486102104187
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10697025805711746
epoch£º77	 i:4 	 global-step:1544	 l-p:0.13720113039016724
epoch£º77	 i:5 	 global-step:1545	 l-p:0.19165007770061493
epoch£º77	 i:6 	 global-step:1546	 l-p:0.12666630744934082
epoch£º77	 i:7 	 global-step:1547	 l-p:0.1754729449748993
epoch£º77	 i:8 	 global-step:1548	 l-p:0.20008189976215363
epoch£º77	 i:9 	 global-step:1549	 l-p:0.14020676910877228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8064, 5.3960, 5.5315],
        [4.8064, 5.6718, 6.0569],
        [4.8064, 4.9062, 4.8524],
        [4.8064, 4.8570, 4.8218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14392505586147308 
model_pd.l_d.mean(): -19.922985076904297 
model_pd.lagr.mean(): -19.77906036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.6659], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14392505586147308
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13526183366775513
epoch£º78	 i:2 	 global-step:1562	 l-p:0.1694711297750473
epoch£º78	 i:3 	 global-step:1563	 l-p:0.13437461853027344
epoch£º78	 i:4 	 global-step:1564	 l-p:0.1637706756591797
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12506109476089478
epoch£º78	 i:6 	 global-step:1566	 l-p:0.08517258614301682
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13314513862133026
epoch£º78	 i:8 	 global-step:1568	 l-p:0.1453908085823059
epoch£º78	 i:9 	 global-step:1569	 l-p:0.14050912857055664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8253, 5.4680, 5.6450],
        [4.8253, 4.8254, 4.8253],
        [4.8253, 4.8269, 4.8254],
        [4.8253, 4.8296, 4.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14757773280143738 
model_pd.l_d.mean(): -20.45136833190918 
model_pd.lagr.mean(): -20.303791046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1535], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14757773280143738
epoch£º79	 i:1 	 global-step:1581	 l-p:0.17190253734588623
epoch£º79	 i:2 	 global-step:1582	 l-p:0.13692979514598846
epoch£º79	 i:3 	 global-step:1583	 l-p:0.1371908187866211
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12769241631031036
epoch£º79	 i:5 	 global-step:1585	 l-p:0.19376669824123383
epoch£º79	 i:6 	 global-step:1586	 l-p:0.23010677099227905
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14395824074745178
epoch£º79	 i:8 	 global-step:1588	 l-p:0.1554265171289444
epoch£º79	 i:9 	 global-step:1589	 l-p:0.10195609927177429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8129, 6.1496, 7.0742],
        [4.8129, 4.8291, 4.8154],
        [4.8129, 4.8129, 4.8129],
        [4.8129, 5.7480, 6.2052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.17076778411865234 
model_pd.l_d.mean(): -19.077077865600586 
model_pd.lagr.mean(): -18.90631103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8170], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.17076778411865234
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12720772624015808
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13936421275138855
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12070972472429276
epoch£º80	 i:4 	 global-step:1604	 l-p:0.2459440678358078
epoch£º80	 i:5 	 global-step:1605	 l-p:0.13246314227581024
epoch£º80	 i:6 	 global-step:1606	 l-p:0.1286003440618515
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13222190737724304
epoch£º80	 i:8 	 global-step:1608	 l-p:0.12684902548789978
epoch£º80	 i:9 	 global-step:1609	 l-p:0.13116590678691864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 4.9184, 4.9130],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 5.8767, 6.3502],
        [4.9125, 4.9828, 4.9382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12235801666975021 
model_pd.l_d.mean(): -20.220449447631836 
model_pd.lagr.mean(): -20.09809112548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.12235801666975021
epoch£º81	 i:1 	 global-step:1621	 l-p:0.14380013942718506
epoch£º81	 i:2 	 global-step:1622	 l-p:0.13110731542110443
epoch£º81	 i:3 	 global-step:1623	 l-p:0.15584638714790344
epoch£º81	 i:4 	 global-step:1624	 l-p:0.16522791981697083
epoch£º81	 i:5 	 global-step:1625	 l-p:0.5319270491600037
epoch£º81	 i:6 	 global-step:1626	 l-p:0.1328507512807846
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.012425918132066727
epoch£º81	 i:8 	 global-step:1628	 l-p:-1.508587121963501
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13620220124721527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 4.7941, 4.7523],
        [4.7284, 5.3704, 5.5580],
        [4.7284, 4.9282, 4.8703],
        [4.7284, 5.7550, 6.3262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.25172722339630127 
model_pd.l_d.mean(): -20.295780181884766 
model_pd.lagr.mean(): -20.044052124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-21.0370], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.25172722339630127
epoch£º82	 i:1 	 global-step:1641	 l-p:0.13042320311069489
epoch£º82	 i:2 	 global-step:1642	 l-p:0.15342359244823456
epoch£º82	 i:3 	 global-step:1643	 l-p:0.1669180989265442
epoch£º82	 i:4 	 global-step:1644	 l-p:0.13605408370494843
epoch£º82	 i:5 	 global-step:1645	 l-p:0.13757391273975372
epoch£º82	 i:6 	 global-step:1646	 l-p:0.15046033263206482
epoch£º82	 i:7 	 global-step:1647	 l-p:0.13736288249492645
epoch£º82	 i:8 	 global-step:1648	 l-p:0.12850593030452728
epoch£º82	 i:9 	 global-step:1649	 l-p:0.138138085603714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9474, 5.8658, 6.2877],
        [4.9474, 5.1825, 5.1241],
        [4.9474, 4.9480, 4.9474],
        [4.9474, 5.4526, 5.5167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11888692528009415 
model_pd.l_d.mean(): -19.251197814941406 
model_pd.lagr.mean(): -19.13231086730957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9706], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11888692528009415
epoch£º83	 i:1 	 global-step:1661	 l-p:0.1065453514456749
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13716618716716766
epoch£º83	 i:3 	 global-step:1663	 l-p:0.131363183259964
epoch£º83	 i:4 	 global-step:1664	 l-p:0.1272411197423935
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12627093493938446
epoch£º83	 i:6 	 global-step:1666	 l-p:0.1322902888059616
epoch£º83	 i:7 	 global-step:1667	 l-p:0.15467432141304016
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.19617675244808197
epoch£º83	 i:9 	 global-step:1669	 l-p:0.13777057826519012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8804, 4.8806, 4.8804],
        [4.8804, 4.8888, 4.8813],
        [4.8804, 4.9535, 4.9081],
        [4.8804, 5.1099, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13600753247737885 
model_pd.l_d.mean(): -20.28609275817871 
model_pd.lagr.mean(): -20.15008544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-20.9916], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13600753247737885
epoch£º84	 i:1 	 global-step:1681	 l-p:0.12280063331127167
epoch£º84	 i:2 	 global-step:1682	 l-p:0.13972176611423492
epoch£º84	 i:3 	 global-step:1683	 l-p:0.148637592792511
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12543247640132904
epoch£º84	 i:5 	 global-step:1685	 l-p:0.14656837284564972
epoch£º84	 i:6 	 global-step:1686	 l-p:0.18465711176395416
epoch£º84	 i:7 	 global-step:1687	 l-p:0.06745820492506027
epoch£º84	 i:8 	 global-step:1688	 l-p:0.1364360898733139
epoch£º84	 i:9 	 global-step:1689	 l-p:0.1347389668226242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8291, 5.5559, 5.8086],
        [4.8291, 5.2917, 5.3377],
        [4.8291, 5.6603, 6.0119],
        [4.8291, 5.1527, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.16141696274280548 
model_pd.l_d.mean(): -20.015188217163086 
model_pd.lagr.mean(): -19.853771209716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.16141696274280548
epoch£º85	 i:1 	 global-step:1701	 l-p:0.13557317852973938
epoch£º85	 i:2 	 global-step:1702	 l-p:0.09140560030937195
epoch£º85	 i:3 	 global-step:1703	 l-p:0.1584359109401703
epoch£º85	 i:4 	 global-step:1704	 l-p:0.15323692560195923
epoch£º85	 i:5 	 global-step:1705	 l-p:0.16742178797721863
epoch£º85	 i:6 	 global-step:1706	 l-p:0.11873289197683334
epoch£º85	 i:7 	 global-step:1707	 l-p:0.1200297549366951
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13899464905261993
epoch£º85	 i:9 	 global-step:1709	 l-p:0.12461847066879272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9069, 5.7601, 6.1237],
        [4.9069, 4.9165, 4.9080],
        [4.9069, 4.9069, 4.9069],
        [4.9069, 5.2180, 5.1828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.5793523788452148 
model_pd.l_d.mean(): -17.353879928588867 
model_pd.lagr.mean(): -16.77452850341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6600], device='cuda:0')), ('power', tensor([-18.2178], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.5793523788452148
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11675821244716644
epoch£º86	 i:2 	 global-step:1722	 l-p:0.1312941014766693
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14097432792186737
epoch£º86	 i:4 	 global-step:1724	 l-p:0.12560616433620453
epoch£º86	 i:5 	 global-step:1725	 l-p:0.12544484436511993
epoch£º86	 i:6 	 global-step:1726	 l-p:0.13080504536628723
epoch£º86	 i:7 	 global-step:1727	 l-p:0.13611772656440735
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14058917760849
epoch£º86	 i:9 	 global-step:1729	 l-p:0.19268310070037842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6877, 4.7752, 4.7263],
        [4.6877, 4.6917, 4.6880],
        [4.6877, 4.6881, 4.6877],
        [4.6877, 5.1694, 5.2391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1529638022184372 
model_pd.l_d.mean(): -19.67074966430664 
model_pd.lagr.mean(): -19.517786026000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5793], device='cuda:0')), ('power', tensor([-20.4775], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.1529638022184372
epoch£º87	 i:1 	 global-step:1741	 l-p:0.13629215955734253
epoch£º87	 i:2 	 global-step:1742	 l-p:0.13600529730319977
epoch£º87	 i:3 	 global-step:1743	 l-p:0.1705881804227829
epoch£º87	 i:4 	 global-step:1744	 l-p:0.05621756613254547
epoch£º87	 i:5 	 global-step:1745	 l-p:0.00993831641972065
epoch£º87	 i:6 	 global-step:1746	 l-p:0.13078337907791138
epoch£º87	 i:7 	 global-step:1747	 l-p:0.22154182195663452
epoch£º87	 i:8 	 global-step:1748	 l-p:0.14909234642982483
epoch£º87	 i:9 	 global-step:1749	 l-p:0.14658981561660767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8849, 4.8850, 4.8849],
        [4.8849, 5.1778, 5.1376],
        [4.8849, 4.9856, 4.9315],
        [4.8849, 4.8902, 4.8854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.1538291871547699 
model_pd.l_d.mean(): -20.648605346679688 
model_pd.lagr.mean(): -20.494775772094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.3139], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.1538291871547699
epoch£º88	 i:1 	 global-step:1761	 l-p:0.12668465077877045
epoch£º88	 i:2 	 global-step:1762	 l-p:0.1292259842157364
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13920791447162628
epoch£º88	 i:4 	 global-step:1764	 l-p:0.13166578114032745
epoch£º88	 i:5 	 global-step:1765	 l-p:0.1482771784067154
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14860224723815918
epoch£º88	 i:7 	 global-step:1767	 l-p:0.148189514875412
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11828427761793137
epoch£º88	 i:9 	 global-step:1769	 l-p:0.1179070696234703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7805, 5.7454, 6.2442],
        [4.7805, 4.8046, 4.7853],
        [4.7805, 4.8130, 4.7883],
        [4.7805, 4.9300, 4.8700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11386750638484955 
model_pd.l_d.mean(): -19.359743118286133 
model_pd.lagr.mean(): -19.24587631225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5823], device='cuda:0')), ('power', tensor([-20.1662], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11386750638484955
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15337319672107697
epoch£º89	 i:2 	 global-step:1782	 l-p:0.13262100517749786
epoch£º89	 i:3 	 global-step:1783	 l-p:0.15016493201255798
epoch£º89	 i:4 	 global-step:1784	 l-p:0.21534377336502075
epoch£º89	 i:5 	 global-step:1785	 l-p:0.14111769199371338
epoch£º89	 i:6 	 global-step:1786	 l-p:0.1453944444656372
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.18000257015228271
epoch£º89	 i:8 	 global-step:1788	 l-p:0.11794289201498032
epoch£º89	 i:9 	 global-step:1789	 l-p:0.12753351032733917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9759, 6.0880, 6.7187],
        [4.9759, 4.9759, 4.9759],
        [4.9759, 4.9759, 4.9759],
        [4.9759, 5.0138, 4.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.16866859793663025 
model_pd.l_d.mean(): -19.37889862060547 
model_pd.lagr.mean(): -19.210229873657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-20.0688], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.16866859793663025
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11207706481218338
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12222683429718018
epoch£º90	 i:3 	 global-step:1803	 l-p:0.11915940791368484
epoch£º90	 i:4 	 global-step:1804	 l-p:0.1206604540348053
epoch£º90	 i:5 	 global-step:1805	 l-p:0.1365283578634262
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12311705201864243
epoch£º90	 i:7 	 global-step:1807	 l-p:0.15757767856121063
epoch£º90	 i:8 	 global-step:1808	 l-p:0.14702576398849487
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13849155604839325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7746, 4.7746, 4.7746],
        [4.7746, 4.7750, 4.7746],
        [4.7746, 4.7805, 4.7751],
        [4.7746, 5.5272, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13219675421714783 
model_pd.l_d.mean(): -19.322526931762695 
model_pd.lagr.mean(): -19.190330505371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.0467], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13219675421714783
epoch£º91	 i:1 	 global-step:1821	 l-p:0.18661008775234222
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2720436751842499
epoch£º91	 i:3 	 global-step:1823	 l-p:0.13594113290309906
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14104968309402466
epoch£º91	 i:5 	 global-step:1825	 l-p:0.16390343010425568
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10023580491542816
epoch£º91	 i:7 	 global-step:1827	 l-p:0.12935861945152283
epoch£º91	 i:8 	 global-step:1828	 l-p:0.13096703588962555
epoch£º91	 i:9 	 global-step:1829	 l-p:0.1526624858379364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8990, 4.8990, 4.8990],
        [4.8990, 5.9394, 6.5035],
        [4.8990, 5.0452, 4.9839],
        [4.8990, 4.9581, 4.9189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.13247665762901306 
model_pd.l_d.mean(): -19.61575698852539 
model_pd.lagr.mean(): -19.483280181884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.3764], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.13247665762901306
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13203616440296173
epoch£º92	 i:2 	 global-step:1842	 l-p:0.11994268000125885
epoch£º92	 i:3 	 global-step:1843	 l-p:-1.6425881385803223
epoch£º92	 i:4 	 global-step:1844	 l-p:0.1377537101507187
epoch£º92	 i:5 	 global-step:1845	 l-p:0.14413264393806458
epoch£º92	 i:6 	 global-step:1846	 l-p:0.15492980182170868
epoch£º92	 i:7 	 global-step:1847	 l-p:0.1430852860212326
epoch£º92	 i:8 	 global-step:1848	 l-p:0.10595134645700455
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12840472161769867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9038, 4.9038, 4.9038],
        [4.9038, 5.2691, 5.2589],
        [4.9038, 5.3030, 5.3098],
        [4.9038, 5.1776, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12621040642261505 
model_pd.l_d.mean(): -19.67438507080078 
model_pd.lagr.mean(): -19.548173904418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.3844], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12621040642261505
epoch£º93	 i:1 	 global-step:1861	 l-p:0.1303081065416336
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12462569773197174
epoch£º93	 i:3 	 global-step:1863	 l-p:0.14521996676921844
epoch£º93	 i:4 	 global-step:1864	 l-p:0.1549227237701416
epoch£º93	 i:5 	 global-step:1865	 l-p:0.18871308863162994
epoch£º93	 i:6 	 global-step:1866	 l-p:0.1426207572221756
epoch£º93	 i:7 	 global-step:1867	 l-p:0.14956055581569672
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.16267697513103485
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12662577629089355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9841, 5.5304, 5.6247],
        [4.9841, 4.9841, 4.9841],
        [4.9841, 5.0286, 4.9965],
        [4.9841, 4.9986, 4.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.14907805621623993 
model_pd.l_d.mean(): -20.695781707763672 
model_pd.lagr.mean(): -20.546703338623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3949], device='cuda:0')), ('power', tensor([-21.3254], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.14907805621623993
epoch£º94	 i:1 	 global-step:1881	 l-p:0.11324048787355423
epoch£º94	 i:2 	 global-step:1882	 l-p:0.17027953267097473
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12374492734670639
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12355294078588486
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11605754494667053
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11329713463783264
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12080418318510056
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13050521910190582
epoch£º94	 i:9 	 global-step:1889	 l-p:0.15640729665756226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8952, 4.8953, 4.8952],
        [4.8952, 4.8952, 4.8952],
        [4.8952, 4.8966, 4.8953],
        [4.8952, 4.9107, 4.8976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12278629094362259 
model_pd.l_d.mean(): -19.26405143737793 
model_pd.lagr.mean(): -19.141265869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-20.0075], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.12278629094362259
epoch£º95	 i:1 	 global-step:1901	 l-p:0.14851592481136322
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13646699488162994
epoch£º95	 i:3 	 global-step:1903	 l-p:0.1295158416032791
epoch£º95	 i:4 	 global-step:1904	 l-p:0.12773476541042328
epoch£º95	 i:5 	 global-step:1905	 l-p:0.23410335183143616
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13593199849128723
epoch£º95	 i:7 	 global-step:1907	 l-p:0.1373540610074997
epoch£º95	 i:8 	 global-step:1908	 l-p:0.1359022557735443
epoch£º95	 i:9 	 global-step:1909	 l-p:0.36929047107696533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7908, 5.9199, 6.6059],
        [4.7908, 4.7915, 4.7909],
        [4.7908, 5.5351, 5.8142],
        [4.7908, 4.7908, 4.7908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.1040368527173996 
model_pd.l_d.mean(): -18.36145782470703 
model_pd.lagr.mean(): -18.257421493530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6453], device='cuda:0')), ('power', tensor([-19.2214], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.1040368527173996
epoch£º96	 i:1 	 global-step:1921	 l-p:0.15209129452705383
epoch£º96	 i:2 	 global-step:1922	 l-p:0.1214018240571022
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13411934673786163
epoch£º96	 i:4 	 global-step:1924	 l-p:0.13842816650867462
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13623857498168945
epoch£º96	 i:6 	 global-step:1926	 l-p:0.1773129105567932
epoch£º96	 i:7 	 global-step:1927	 l-p:0.1200762689113617
epoch£º96	 i:8 	 global-step:1928	 l-p:0.13987483084201813
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12689977884292603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.5216, 5.5699],
        [5.0394, 5.9005, 6.2611],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 6.3565, 7.2214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.14717108011245728 
model_pd.l_d.mean(): -20.29973793029785 
model_pd.lagr.mean(): -20.15256690979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-20.9457], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.14717108011245728
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12230132520198822
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10776923596858978
epoch£º97	 i:3 	 global-step:1943	 l-p:0.14829887449741364
epoch£º97	 i:4 	 global-step:1944	 l-p:0.1855342984199524
epoch£º97	 i:5 	 global-step:1945	 l-p:0.13937626779079437
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11608053743839264
epoch£º97	 i:7 	 global-step:1947	 l-p:0.13259311020374298
epoch£º97	 i:8 	 global-step:1948	 l-p:0.14327219128608704
epoch£º97	 i:9 	 global-step:1949	 l-p:0.1273166835308075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 5.0911, 5.0908],
        [4.7284, 4.7284, 4.7284],
        [4.7284, 4.7677, 4.7392],
        [4.7284, 5.3218, 5.4769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13279232382774353 
model_pd.l_d.mean(): -20.320938110351562 
model_pd.lagr.mean(): -20.188146591186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-21.0655], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13279232382774353
epoch£º98	 i:1 	 global-step:1961	 l-p:0.1491614133119583
epoch£º98	 i:2 	 global-step:1962	 l-p:0.12943696975708008
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.030864018946886063
epoch£º98	 i:4 	 global-step:1964	 l-p:0.17162247002124786
epoch£º98	 i:5 	 global-step:1965	 l-p:0.18131910264492035
epoch£º98	 i:6 	 global-step:1966	 l-p:0.13635225594043732
epoch£º98	 i:7 	 global-step:1967	 l-p:0.14894075691699982
epoch£º98	 i:8 	 global-step:1968	 l-p:0.12327190488576889
epoch£º98	 i:9 	 global-step:1969	 l-p:0.13717469573020935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9241, 4.9241],
        [4.9241, 4.9242, 4.9241],
        [4.9241, 5.3863, 5.4309],
        [4.9241, 4.9242, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.1452375054359436 
model_pd.l_d.mean(): -19.399662017822266 
model_pd.lagr.mean(): -19.254425048828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4567], device='cuda:0')), ('power', tensor([-20.0783], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.1452375054359436
epoch£º99	 i:1 	 global-step:1981	 l-p:0.1409110277891159
epoch£º99	 i:2 	 global-step:1982	 l-p:0.11232716590166092
epoch£º99	 i:3 	 global-step:1983	 l-p:0.1440322995185852
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12520842254161835
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.0472448356449604
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12293483316898346
epoch£º99	 i:7 	 global-step:1987	 l-p:0.15971942245960236
epoch£º99	 i:8 	 global-step:1988	 l-p:0.1266006976366043
epoch£º99	 i:9 	 global-step:1989	 l-p:0.13950888812541962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8764, 4.8770, 4.8764],
        [4.8764, 5.3749, 5.4471],
        [4.8764, 4.9642, 4.9146],
        [4.8764, 4.8764, 4.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.16193677484989166 
model_pd.l_d.mean(): -20.743061065673828 
model_pd.lagr.mean(): -20.58112335205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.4021], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.16193677484989166
epoch£º100	 i:1 	 global-step:2001	 l-p:0.13343581557273865
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11211314797401428
epoch£º100	 i:3 	 global-step:2003	 l-p:0.11762817203998566
epoch£º100	 i:4 	 global-step:2004	 l-p:0.13966882228851318
epoch£º100	 i:5 	 global-step:2005	 l-p:0.14430435001850128
epoch£º100	 i:6 	 global-step:2006	 l-p:0.1381445974111557
epoch£º100	 i:7 	 global-step:2007	 l-p:2.3710219860076904
epoch£º100	 i:8 	 global-step:2008	 l-p:-0.6547011137008667
epoch£º100	 i:9 	 global-step:2009	 l-p:0.17071136832237244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8124, 4.8712, 4.8328],
        [4.8124, 4.8124, 4.8124],
        [4.8124, 4.8144, 4.8125],
        [4.8124, 4.8124, 4.8124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1360611766576767 
model_pd.l_d.mean(): -20.10513687133789 
model_pd.lagr.mean(): -19.96907615661621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.1360611766576767
epoch£º101	 i:1 	 global-step:2021	 l-p:0.1169670969247818
epoch£º101	 i:2 	 global-step:2022	 l-p:0.1056198850274086
epoch£º101	 i:3 	 global-step:2023	 l-p:0.04082449898123741
epoch£º101	 i:4 	 global-step:2024	 l-p:0.1370130032300949
epoch£º101	 i:5 	 global-step:2025	 l-p:0.13371418416500092
epoch£º101	 i:6 	 global-step:2026	 l-p:0.11803332716226578
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12559396028518677
epoch£º101	 i:8 	 global-step:2028	 l-p:0.1442125141620636
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1384107619524002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9761, 5.6923, 5.9284],
        [4.9761, 5.6972, 5.9375],
        [4.9761, 6.2050, 6.9788],
        [4.9761, 5.1470, 5.0850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11542238295078278 
model_pd.l_d.mean(): -20.0781307220459 
model_pd.lagr.mean(): -19.96270751953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.7715], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11542238295078278
epoch£º102	 i:1 	 global-step:2041	 l-p:0.16332575678825378
epoch£º102	 i:2 	 global-step:2042	 l-p:0.01521964743733406
epoch£º102	 i:3 	 global-step:2043	 l-p:0.1463569849729538
epoch£º102	 i:4 	 global-step:2044	 l-p:0.1400953084230423
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13479045033454895
epoch£º102	 i:6 	 global-step:2046	 l-p:0.14273536205291748
epoch£º102	 i:7 	 global-step:2047	 l-p:0.12749648094177246
epoch£º102	 i:8 	 global-step:2048	 l-p:0.14489060640335083
epoch£º102	 i:9 	 global-step:2049	 l-p:0.23216882348060608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6879, 4.7790, 4.7302],
        [4.6879, 5.0148, 5.0012],
        [4.6879, 4.7728, 4.7256],
        [4.6879, 4.6978, 4.6891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.12455138564109802 
model_pd.l_d.mean(): -19.899564743041992 
model_pd.lagr.mean(): -19.775012969970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5609], device='cuda:0')), ('power', tensor([-20.6900], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.12455138564109802
epoch£º103	 i:1 	 global-step:2061	 l-p:0.07467286288738251
epoch£º103	 i:2 	 global-step:2062	 l-p:0.1533609926700592
epoch£º103	 i:3 	 global-step:2063	 l-p:0.16106349229812622
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13235998153686523
epoch£º103	 i:5 	 global-step:2065	 l-p:0.13750968873500824
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13019460439682007
epoch£º103	 i:7 	 global-step:2067	 l-p:0.1522148847579956
epoch£º103	 i:8 	 global-step:2068	 l-p:0.1304803341627121
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12791785597801208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9658, 4.9676, 4.9659],
        [4.9658, 5.1130, 5.0521],
        [4.9658, 5.0240, 4.9854],
        [4.9658, 5.8965, 6.3411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12890952825546265 
model_pd.l_d.mean(): -20.7160701751709 
model_pd.lagr.mean(): -20.587160110473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3973], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12890952825546265
epoch£º104	 i:1 	 global-step:2081	 l-p:0.3805041015148163
epoch£º104	 i:2 	 global-step:2082	 l-p:0.15308386087417603
epoch£º104	 i:3 	 global-step:2083	 l-p:0.15864259004592896
epoch£º104	 i:4 	 global-step:2084	 l-p:0.129841610789299
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12669579684734344
epoch£º104	 i:6 	 global-step:2086	 l-p:0.1257573813199997
epoch£º104	 i:7 	 global-step:2087	 l-p:0.1456039845943451
epoch£º104	 i:8 	 global-step:2088	 l-p:0.1246655061841011
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13258510828018188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8395, 4.8395, 4.8395],
        [4.8395, 4.8395, 4.8395],
        [4.8395, 5.0425, 4.9862],
        [4.8395, 5.3608, 5.4540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.16076751053333282 
model_pd.l_d.mean(): -20.34807586669922 
model_pd.lagr.mean(): -20.18730926513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.0497], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.16076751053333282
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16111774742603302
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12355232983827591
epoch£º105	 i:3 	 global-step:2103	 l-p:0.1399645358324051
epoch£º105	 i:4 	 global-step:2104	 l-p:0.1428195685148239
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18189968168735504
epoch£º105	 i:6 	 global-step:2106	 l-p:0.16428571939468384
epoch£º105	 i:7 	 global-step:2107	 l-p:0.16650976240634918
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07363080978393555
epoch£º105	 i:9 	 global-step:2109	 l-p:0.1287359744310379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9102, 4.9131, 4.9103],
        [4.9102, 5.6171, 5.8541],
        [4.9102, 4.9287, 4.9134],
        [4.9102, 5.8248, 6.2619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13306349515914917 
model_pd.l_d.mean(): -18.52001190185547 
model_pd.lagr.mean(): -18.386947631835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-19.2983], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13306349515914917
epoch£º106	 i:1 	 global-step:2121	 l-p:0.1428324282169342
epoch£º106	 i:2 	 global-step:2122	 l-p:0.13301776349544525
epoch£º106	 i:3 	 global-step:2123	 l-p:0.13347315788269043
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12378179281949997
epoch£º106	 i:5 	 global-step:2125	 l-p:0.129225492477417
epoch£º106	 i:6 	 global-step:2126	 l-p:-0.8476802110671997
epoch£º106	 i:7 	 global-step:2127	 l-p:0.13474984467029572
epoch£º106	 i:8 	 global-step:2128	 l-p:0.1327628493309021
epoch£º106	 i:9 	 global-step:2129	 l-p:0.1154923066496849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9003, 4.9029, 4.9005],
        [4.9003, 4.9054, 4.9007],
        [4.9003, 5.9268, 6.4851],
        [4.9003, 5.7419, 6.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.1452285796403885 
model_pd.l_d.mean(): -19.37531089782715 
model_pd.lagr.mean(): -19.23008155822754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.1036], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.1452285796403885
epoch£º107	 i:1 	 global-step:2141	 l-p:0.12108003348112106
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.029316501691937447
epoch£º107	 i:3 	 global-step:2143	 l-p:0.1228298619389534
epoch£º107	 i:4 	 global-step:2144	 l-p:0.1285773068666458
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13160918653011322
epoch£º107	 i:6 	 global-step:2146	 l-p:0.1727307140827179
epoch£º107	 i:7 	 global-step:2147	 l-p:0.15093544125556946
epoch£º107	 i:8 	 global-step:2148	 l-p:0.12692809104919434
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1382356435060501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8342, 4.8253],
        [4.8240, 4.8242, 4.8240],
        [4.8240, 4.9191, 4.8684],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13067954778671265 
model_pd.l_d.mean(): -18.85418701171875 
model_pd.lagr.mean(): -18.723506927490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-19.6127], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13067954778671265
epoch£º108	 i:1 	 global-step:2161	 l-p:0.1637345850467682
epoch£º108	 i:2 	 global-step:2162	 l-p:0.10531725734472275
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13608960807323456
epoch£º108	 i:4 	 global-step:2164	 l-p:0.1389102041721344
epoch£º108	 i:5 	 global-step:2165	 l-p:0.1549195796251297
epoch£º108	 i:6 	 global-step:2166	 l-p:0.12788449227809906
epoch£º108	 i:7 	 global-step:2167	 l-p:0.12684425711631775
epoch£º108	 i:8 	 global-step:2168	 l-p:0.13363437354564667
epoch£º108	 i:9 	 global-step:2169	 l-p:0.16246196627616882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9291, 5.0128, 4.9646],
        [4.9291, 4.9306, 4.9292],
        [4.9291, 6.1867, 7.0080],
        [4.9291, 4.9291, 4.9291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11599590629339218 
model_pd.l_d.mean(): -20.014204025268555 
model_pd.lagr.mean(): -19.898208618164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.7176], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11599590629339218
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13467229902744293
epoch£º109	 i:2 	 global-step:2182	 l-p:0.1312195211648941
epoch£º109	 i:3 	 global-step:2183	 l-p:0.14611776173114777
epoch£º109	 i:4 	 global-step:2184	 l-p:0.3928971290588379
epoch£º109	 i:5 	 global-step:2185	 l-p:0.13976313173770905
epoch£º109	 i:6 	 global-step:2186	 l-p:0.1430922895669937
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12363315373659134
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13457274436950684
epoch£º109	 i:9 	 global-step:2189	 l-p:0.11706304550170898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9408, 4.9412, 4.9408],
        [4.9408, 4.9408, 4.9408],
        [4.9408, 4.9417, 4.9408],
        [4.9408, 4.9613, 4.9445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.6579426527023315 
model_pd.l_d.mean(): -20.80900001525879 
model_pd.lagr.mean(): -20.151058197021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.4399], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.6579426527023315
epoch£º110	 i:1 	 global-step:2201	 l-p:0.13156872987747192
epoch£º110	 i:2 	 global-step:2202	 l-p:0.09853299707174301
epoch£º110	 i:3 	 global-step:2203	 l-p:0.13467930257320404
epoch£º110	 i:4 	 global-step:2204	 l-p:0.14398528635501862
epoch£º110	 i:5 	 global-step:2205	 l-p:0.13493654131889343
epoch£º110	 i:6 	 global-step:2206	 l-p:0.13921095430850983
epoch£º110	 i:7 	 global-step:2207	 l-p:0.13349317014217377
epoch£º110	 i:8 	 global-step:2208	 l-p:0.23311986029148102
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21811625361442566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8588, 4.8632, 4.8592],
        [4.8588, 5.5366, 5.7565],
        [4.8588, 5.3722, 5.4610],
        [4.8588, 5.7227, 6.1183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.17196881771087646 
model_pd.l_d.mean(): -20.08875846862793 
model_pd.lagr.mean(): -19.916790008544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4937], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.17196881771087646
epoch£º111	 i:1 	 global-step:2221	 l-p:0.1351175457239151
epoch£º111	 i:2 	 global-step:2222	 l-p:0.43977591395378113
epoch£º111	 i:3 	 global-step:2223	 l-p:0.12737253308296204
epoch£º111	 i:4 	 global-step:2224	 l-p:0.11795701086521149
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12329532206058502
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11508527398109436
epoch£º111	 i:7 	 global-step:2227	 l-p:0.14042618870735168
epoch£º111	 i:8 	 global-step:2228	 l-p:0.13586445152759552
epoch£º111	 i:9 	 global-step:2229	 l-p:0.1105537936091423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9609, 5.0253, 4.9843],
        [4.9609, 5.0233, 4.9831],
        [4.9609, 4.9609, 4.9609],
        [4.9609, 4.9831, 4.9652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14736440777778625 
model_pd.l_d.mean(): -20.548276901245117 
model_pd.lagr.mean(): -20.40091323852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4242], device='cuda:0')), ('power', tensor([-21.2062], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14736440777778625
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.12824401259422302
epoch£º112	 i:2 	 global-step:2242	 l-p:0.15494081377983093
epoch£º112	 i:3 	 global-step:2243	 l-p:0.1348729282617569
epoch£º112	 i:4 	 global-step:2244	 l-p:0.1346529722213745
epoch£º112	 i:5 	 global-step:2245	 l-p:0.15540674328804016
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11932958662509918
epoch£º112	 i:7 	 global-step:2247	 l-p:0.1672913283109665
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13128432631492615
epoch£º112	 i:9 	 global-step:2249	 l-p:0.13383527100086212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7995, 4.7996, 4.7995],
        [4.7995, 5.4675, 5.6863],
        [4.7995, 4.8596, 4.8212],
        [4.7995, 4.7996, 4.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.09869039058685303 
model_pd.l_d.mean(): -19.49871063232422 
model_pd.lagr.mean(): -19.400020599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5252], device='cuda:0')), ('power', tensor([-20.2484], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.09869039058685303
epoch£º113	 i:1 	 global-step:2261	 l-p:0.1366434097290039
epoch£º113	 i:2 	 global-step:2262	 l-p:0.130105659365654
epoch£º113	 i:3 	 global-step:2263	 l-p:0.14399510622024536
epoch£º113	 i:4 	 global-step:2264	 l-p:0.13898897171020508
epoch£º113	 i:5 	 global-step:2265	 l-p:0.16412577033042908
epoch£º113	 i:6 	 global-step:2266	 l-p:0.5304247140884399
epoch£º113	 i:7 	 global-step:2267	 l-p:0.1376582235097885
epoch£º113	 i:8 	 global-step:2268	 l-p:0.1482561230659485
epoch£º113	 i:9 	 global-step:2269	 l-p:-1.4906888008117676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8029, 4.8029, 4.8029],
        [4.8029, 4.8044, 4.8029],
        [4.8029, 4.8037, 4.8029],
        [4.8029, 5.6696, 6.0778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1260043829679489 
model_pd.l_d.mean(): -19.596435546875 
model_pd.lagr.mean(): -19.470430374145508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-20.3751], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.1260043829679489
epoch£º114	 i:1 	 global-step:2281	 l-p:0.1414273977279663
epoch£º114	 i:2 	 global-step:2282	 l-p:0.14235760271549225
epoch£º114	 i:3 	 global-step:2283	 l-p:0.1575917899608612
epoch£º114	 i:4 	 global-step:2284	 l-p:0.13122151792049408
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13056117296218872
epoch£º114	 i:6 	 global-step:2286	 l-p:0.1276770383119583
epoch£º114	 i:7 	 global-step:2287	 l-p:0.13334117829799652
epoch£º114	 i:8 	 global-step:2288	 l-p:0.0884627103805542
epoch£º114	 i:9 	 global-step:2289	 l-p:0.13609051704406738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8590, 4.8590, 4.8590],
        [4.8590, 4.8590, 4.8590],
        [4.8590, 4.8590, 4.8590],
        [4.8590, 5.0529, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.12731562554836273 
model_pd.l_d.mean(): -20.264554977416992 
model_pd.lagr.mean(): -20.137239456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765], device='cuda:0')), ('power', tensor([-20.9728], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.12731562554836273
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11605090647935867
epoch£º115	 i:2 	 global-step:2302	 l-p:0.1211184486746788
epoch£º115	 i:3 	 global-step:2303	 l-p:0.6381645202636719
epoch£º115	 i:4 	 global-step:2304	 l-p:0.17217625677585602
epoch£º115	 i:5 	 global-step:2305	 l-p:0.1311633288860321
epoch£º115	 i:6 	 global-step:2306	 l-p:0.1366376429796219
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13647915422916412
epoch£º115	 i:8 	 global-step:2308	 l-p:0.44292914867401123
epoch£º115	 i:9 	 global-step:2309	 l-p:0.1925777792930603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7770, 4.8521, 4.8082],
        [4.7770, 4.8115, 4.7860],
        [4.7770, 4.7770, 4.7770],
        [4.7770, 4.9689, 4.9146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11802409589290619 
model_pd.l_d.mean(): -19.752134323120117 
model_pd.lagr.mean(): -19.634109497070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-20.5174], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11802409589290619
epoch£º116	 i:1 	 global-step:2321	 l-p:0.14100970327854156
epoch£º116	 i:2 	 global-step:2322	 l-p:0.365833580493927
epoch£º116	 i:3 	 global-step:2323	 l-p:0.1510840207338333
epoch£º116	 i:4 	 global-step:2324	 l-p:0.10309012979269028
epoch£º116	 i:5 	 global-step:2325	 l-p:0.1740351766347885
epoch£º116	 i:6 	 global-step:2326	 l-p:0.15331195294857025
epoch£º116	 i:7 	 global-step:2327	 l-p:0.13738469779491425
epoch£º116	 i:8 	 global-step:2328	 l-p:0.10913648456335068
epoch£º116	 i:9 	 global-step:2329	 l-p:0.1331186592578888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0272, 5.0282, 5.0272],
        [5.0272, 5.0892, 5.0491],
        [5.0272, 5.0272, 5.0272],
        [5.0272, 5.0272, 5.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12944462895393372 
model_pd.l_d.mean(): -20.764753341674805 
model_pd.lagr.mean(): -20.63530921936035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3770], device='cuda:0')), ('power', tensor([-21.3768], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12944462895393372
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13371950387954712
epoch£º117	 i:2 	 global-step:2342	 l-p:0.13443709909915924
epoch£º117	 i:3 	 global-step:2343	 l-p:0.11467330902814865
epoch£º117	 i:4 	 global-step:2344	 l-p:0.14235876500606537
epoch£º117	 i:5 	 global-step:2345	 l-p:0.12434375286102295
epoch£º117	 i:6 	 global-step:2346	 l-p:0.2799025774002075
epoch£º117	 i:7 	 global-step:2347	 l-p:0.14010679721832275
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.28066158294677734
epoch£º117	 i:9 	 global-step:2349	 l-p:0.15427878499031067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7121, 5.8615, 6.5997],
        [4.7121, 5.0061, 4.9815],
        [4.7121, 4.7312, 4.7157],
        [4.7121, 4.7121, 4.7121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12218093872070312 
model_pd.l_d.mean(): -19.742918014526367 
model_pd.lagr.mean(): -19.620737075805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5612], device='cuda:0')), ('power', tensor([-20.5320], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12218093872070312
epoch£º118	 i:1 	 global-step:2361	 l-p:0.14129671454429626
epoch£º118	 i:2 	 global-step:2362	 l-p:0.1414576917886734
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19654785096645355
epoch£º118	 i:4 	 global-step:2364	 l-p:0.14453373849391937
epoch£º118	 i:5 	 global-step:2365	 l-p:0.1523369401693344
epoch£º118	 i:6 	 global-step:2366	 l-p:0.1516539305448532
epoch£º118	 i:7 	 global-step:2367	 l-p:-3.8006374835968018
epoch£º118	 i:8 	 global-step:2368	 l-p:0.4901444911956787
epoch£º118	 i:9 	 global-step:2369	 l-p:0.1805499941110611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8881, 5.0810, 5.0246],
        [4.8881, 4.8960, 4.8890],
        [4.8881, 5.4221, 5.5269],
        [4.8881, 5.5574, 5.7707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16675563156604767 
model_pd.l_d.mean(): -20.72407341003418 
model_pd.lagr.mean(): -20.55731773376465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.3858], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16675563156604767
epoch£º119	 i:1 	 global-step:2381	 l-p:0.13331836462020874
epoch£º119	 i:2 	 global-step:2382	 l-p:0.1229272410273552
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11853929609060287
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11453106254339218
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11662652343511581
epoch£º119	 i:6 	 global-step:2386	 l-p:0.1251595914363861
epoch£º119	 i:7 	 global-step:2387	 l-p:0.14094853401184082
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.1242486909031868
epoch£º119	 i:9 	 global-step:2389	 l-p:0.14612333476543427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 4.9117, 4.9116],
        [4.9116, 4.9118, 4.9116],
        [4.9116, 4.9125, 4.9117],
        [4.9116, 5.3191, 5.3397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.13102778792381287 
model_pd.l_d.mean(): -20.30597686767578 
model_pd.lagr.mean(): -20.174949645996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0006], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.13102778792381287
epoch£º120	 i:1 	 global-step:2401	 l-p:0.12613515555858612
epoch£º120	 i:2 	 global-step:2402	 l-p:0.12777556478977203
epoch£º120	 i:3 	 global-step:2403	 l-p:0.15212298929691315
epoch£º120	 i:4 	 global-step:2404	 l-p:0.147414892911911
epoch£º120	 i:5 	 global-step:2405	 l-p:0.1271057277917862
epoch£º120	 i:6 	 global-step:2406	 l-p:0.14593547582626343
epoch£º120	 i:7 	 global-step:2407	 l-p:0.12614808976650238
epoch£º120	 i:8 	 global-step:2408	 l-p:0.0875774398446083
epoch£º120	 i:9 	 global-step:2409	 l-p:0.048997554928064346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 4.9189, 4.8650],
        [4.7389, 4.7647, 4.7447],
        [4.7389, 4.7389, 4.7389],
        [4.7389, 4.7395, 4.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.1472080498933792 
model_pd.l_d.mean(): -19.512258529663086 
model_pd.lagr.mean(): -19.36505126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.2340], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.1472080498933792
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10668924450874329
epoch£º121	 i:2 	 global-step:2422	 l-p:0.13416031002998352
epoch£º121	 i:3 	 global-step:2423	 l-p:0.13525031507015228
epoch£º121	 i:4 	 global-step:2424	 l-p:0.20093154907226562
epoch£º121	 i:5 	 global-step:2425	 l-p:0.12882663309574127
epoch£º121	 i:6 	 global-step:2426	 l-p:0.13877391815185547
epoch£º121	 i:7 	 global-step:2427	 l-p:0.16517512500286102
epoch£º121	 i:8 	 global-step:2428	 l-p:0.12525489926338196
epoch£º121	 i:9 	 global-step:2429	 l-p:0.13323910534381866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9519, 4.9785, 4.9578],
        [4.9519, 4.9616, 4.9531],
        [4.9519, 4.9519, 4.9519],
        [4.9519, 4.9587, 4.9526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13254086673259735 
model_pd.l_d.mean(): -18.616777420043945 
model_pd.lagr.mean(): -18.484235763549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.13254086673259735
epoch£º122	 i:1 	 global-step:2441	 l-p:0.11315515637397766
epoch£º122	 i:2 	 global-step:2442	 l-p:0.14787861704826355
epoch£º122	 i:3 	 global-step:2443	 l-p:0.1347585916519165
epoch£º122	 i:4 	 global-step:2444	 l-p:0.133661150932312
epoch£º122	 i:5 	 global-step:2445	 l-p:0.11103031039237976
epoch£º122	 i:6 	 global-step:2446	 l-p:0.14293260872364044
epoch£º122	 i:7 	 global-step:2447	 l-p:0.13065370917320251
epoch£º122	 i:8 	 global-step:2448	 l-p:0.11598426848649979
epoch£º122	 i:9 	 global-step:2449	 l-p:0.13312120735645294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7853, 4.7857, 4.7853],
        [4.7853, 4.8367, 4.8025],
        [4.7853, 4.7910, 4.7858],
        [4.7853, 4.7853, 4.7853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.13926833868026733 
model_pd.l_d.mean(): -20.52486228942871 
model_pd.lagr.mean(): -20.38559341430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.2382], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.13926833868026733
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13566476106643677
epoch£º123	 i:2 	 global-step:2462	 l-p:0.17600879073143005
epoch£º123	 i:3 	 global-step:2463	 l-p:0.1431974321603775
epoch£º123	 i:4 	 global-step:2464	 l-p:0.06847287714481354
epoch£º123	 i:5 	 global-step:2465	 l-p:0.14538849890232086
epoch£º123	 i:6 	 global-step:2466	 l-p:0.2102786898612976
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14651352167129517
epoch£º123	 i:8 	 global-step:2468	 l-p:5.526558876037598
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08913426101207733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6756, 4.6759, 4.6756],
        [4.6756, 5.3672, 5.6271],
        [4.6756, 4.7073, 4.6838],
        [4.6756, 4.7228, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.21987812221050262 
model_pd.l_d.mean(): -20.14448356628418 
model_pd.lagr.mean(): -19.924606323242188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.9062], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.21987812221050262
epoch£º124	 i:1 	 global-step:2481	 l-p:-0.18318544328212738
epoch£º124	 i:2 	 global-step:2482	 l-p:0.18096955120563507
epoch£º124	 i:3 	 global-step:2483	 l-p:0.13148345053195953
epoch£º124	 i:4 	 global-step:2484	 l-p:0.14064908027648926
epoch£º124	 i:5 	 global-step:2485	 l-p:0.10328042507171631
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11696549504995346
epoch£º124	 i:7 	 global-step:2487	 l-p:0.10889526456594467
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11589591950178146
epoch£º124	 i:9 	 global-step:2489	 l-p:0.11167103797197342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2933, 5.3084, 5.2955],
        [5.2933, 6.7117, 7.6683],
        [5.2933, 5.3124, 5.2965],
        [5.2933, 5.7491, 5.7754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.10582857578992844 
model_pd.l_d.mean(): -19.020652770996094 
model_pd.lagr.mean(): -18.914823532104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3972], device='cuda:0')), ('power', tensor([-19.6343], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.10582857578992844
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11200489103794098
epoch£º125	 i:2 	 global-step:2502	 l-p:0.11221736669540405
epoch£º125	 i:3 	 global-step:2503	 l-p:0.1295880675315857
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11510609090328217
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12889909744262695
epoch£º125	 i:6 	 global-step:2506	 l-p:0.12400545179843903
epoch£º125	 i:7 	 global-step:2507	 l-p:0.19418449699878693
epoch£º125	 i:8 	 global-step:2508	 l-p:0.14392708241939545
epoch£º125	 i:9 	 global-step:2509	 l-p:0.15763288736343384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6927, 5.3561, 5.5890],
        [4.6927, 5.1113, 5.1548],
        [4.6927, 4.6927, 4.6927],
        [4.6927, 4.6977, 4.6932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.1419575810432434 
model_pd.l_d.mean(): -20.695751190185547 
model_pd.lagr.mean(): -20.55379295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-21.4088], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.1419575810432434
epoch£º126	 i:1 	 global-step:2521	 l-p:0.16387221217155457
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16865785419940948
epoch£º126	 i:3 	 global-step:2523	 l-p:0.1756911426782608
epoch£º126	 i:4 	 global-step:2524	 l-p:0.14735254645347595
epoch£º126	 i:5 	 global-step:2525	 l-p:0.1555667519569397
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11348581314086914
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17040663957595825
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.046273503452539444
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.20557378232479095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6878, 4.7974, 4.7464],
        [4.6878, 4.7629, 4.7200],
        [4.6878, 4.8728, 4.8214],
        [4.6878, 5.4595, 5.7949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09847144782543182 
model_pd.l_d.mean(): -19.732093811035156 
model_pd.lagr.mean(): -19.633623123168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5375], device='cuda:0')), ('power', tensor([-20.4968], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09847144782543182
epoch£º127	 i:1 	 global-step:2541	 l-p:0.24362727999687195
epoch£º127	 i:2 	 global-step:2542	 l-p:0.13802187144756317
epoch£º127	 i:3 	 global-step:2543	 l-p:0.14091633260250092
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11357736587524414
epoch£º127	 i:5 	 global-step:2545	 l-p:0.14619816839694977
epoch£º127	 i:6 	 global-step:2546	 l-p:0.11907833069562912
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11689545214176178
epoch£º127	 i:8 	 global-step:2548	 l-p:0.11896898597478867
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11203302443027496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2054, 5.6830, 5.7288],
        [5.2054, 5.3938, 5.3313],
        [5.2054, 5.2084, 5.2056],
        [5.2054, 5.2062, 5.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09624394029378891 
model_pd.l_d.mean(): -19.65865135192871 
model_pd.lagr.mean(): -19.562406539916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-20.3178], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09624394029378891
epoch£º128	 i:1 	 global-step:2561	 l-p:0.1233205571770668
epoch£º128	 i:2 	 global-step:2562	 l-p:0.1217803955078125
epoch£º128	 i:3 	 global-step:2563	 l-p:0.19876985251903534
epoch£º128	 i:4 	 global-step:2564	 l-p:0.1420971304178238
epoch£º128	 i:5 	 global-step:2565	 l-p:0.1444830745458603
epoch£º128	 i:6 	 global-step:2566	 l-p:0.15670053660869598
epoch£º128	 i:7 	 global-step:2567	 l-p:0.12646150588989258
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14118240773677826
epoch£º128	 i:9 	 global-step:2569	 l-p:-0.05510632321238518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7559, 4.7567, 4.7559],
        [4.7559, 4.7726, 4.7588],
        [4.7559, 4.7559, 4.7559],
        [4.7559, 4.8818, 4.8280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.11935010552406311 
model_pd.l_d.mean(): -19.985822677612305 
model_pd.lagr.mean(): -19.866472244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-20.7640], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.11935010552406311
epoch£º129	 i:1 	 global-step:2581	 l-p:0.23817308247089386
epoch£º129	 i:2 	 global-step:2582	 l-p:0.14637209475040436
epoch£º129	 i:3 	 global-step:2583	 l-p:0.15547938644886017
epoch£º129	 i:4 	 global-step:2584	 l-p:0.13004420697689056
epoch£º129	 i:5 	 global-step:2585	 l-p:0.1259792447090149
epoch£º129	 i:6 	 global-step:2586	 l-p:0.1547817885875702
epoch£º129	 i:7 	 global-step:2587	 l-p:0.15062355995178223
epoch£º129	 i:8 	 global-step:2588	 l-p:0.14669159054756165
epoch£º129	 i:9 	 global-step:2589	 l-p:0.12790730595588684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8931, 5.0317, 4.9751],
        [4.8931, 4.8931, 4.8931],
        [4.8931, 4.8931, 4.8931],
        [4.8931, 4.8942, 4.8932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.05324016511440277 
model_pd.l_d.mean(): -18.751598358154297 
model_pd.lagr.mean(): -18.6983585357666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5354], device='cuda:0')), ('power', tensor([-19.5034], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.05324016511440277
epoch£º130	 i:1 	 global-step:2601	 l-p:0.16436412930488586
epoch£º130	 i:2 	 global-step:2602	 l-p:0.13259850442409515
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13835862278938293
epoch£º130	 i:4 	 global-step:2604	 l-p:0.1286666989326477
epoch£º130	 i:5 	 global-step:2605	 l-p:0.1563269942998886
epoch£º130	 i:6 	 global-step:2606	 l-p:0.12384694814682007
epoch£º130	 i:7 	 global-step:2607	 l-p:0.20654414594173431
epoch£º130	 i:8 	 global-step:2608	 l-p:0.12918531894683838
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13685673475265503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9149, 4.9149, 4.9149],
        [4.9149, 4.9902, 4.9460],
        [4.9149, 4.9150, 4.9149],
        [4.9149, 4.9149, 4.9149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.1394309252500534 
model_pd.l_d.mean(): -19.7170352935791 
model_pd.lagr.mean(): -19.577604293823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-20.4417], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.1394309252500534
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16040033102035522
epoch£º131	 i:2 	 global-step:2622	 l-p:0.15113255381584167
epoch£º131	 i:3 	 global-step:2623	 l-p:0.08015497028827667
epoch£º131	 i:4 	 global-step:2624	 l-p:0.12559685111045837
epoch£º131	 i:5 	 global-step:2625	 l-p:0.14877250790596008
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13280445337295532
epoch£º131	 i:7 	 global-step:2627	 l-p:0.1596919745206833
epoch£º131	 i:8 	 global-step:2628	 l-p:0.1252421885728836
epoch£º131	 i:9 	 global-step:2629	 l-p:0.15524537861347198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9342, 4.9343, 4.9342],
        [4.9342, 6.0200, 6.6522],
        [4.9342, 4.9342, 4.9342],
        [4.9342, 5.3588, 5.3916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.1171233281493187 
model_pd.l_d.mean(): -19.11957550048828 
model_pd.lagr.mean(): -19.002452850341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-19.8686], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.1171233281493187
epoch£º132	 i:1 	 global-step:2641	 l-p:0.11805964261293411
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12227489054203033
epoch£º132	 i:3 	 global-step:2643	 l-p:0.12845927476882935
epoch£º132	 i:4 	 global-step:2644	 l-p:0.13996733725070953
epoch£º132	 i:5 	 global-step:2645	 l-p:0.1298384815454483
epoch£º132	 i:6 	 global-step:2646	 l-p:0.13265615701675415
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.3241592347621918
epoch£º132	 i:8 	 global-step:2648	 l-p:0.1253243386745453
epoch£º132	 i:9 	 global-step:2649	 l-p:0.1693873107433319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9633, 4.9633, 4.9633],
        [4.9633, 4.9633, 4.9633],
        [4.9633, 5.2694, 5.2421],
        [4.9633, 5.0746, 5.0208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14211569726467133 
model_pd.l_d.mean(): -18.58525848388672 
model_pd.lagr.mean(): -18.44314193725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-19.3119], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14211569726467133
epoch£º133	 i:1 	 global-step:2661	 l-p:0.12338557839393616
epoch£º133	 i:2 	 global-step:2662	 l-p:0.135629341006279
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13458366692066193
epoch£º133	 i:4 	 global-step:2664	 l-p:0.13398435711860657
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12387621402740479
epoch£º133	 i:6 	 global-step:2666	 l-p:0.15726140141487122
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10740508884191513
epoch£º133	 i:8 	 global-step:2668	 l-p:0.159493088722229
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13608096539974213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9565, 5.2468, 5.2143],
        [4.9565, 5.0353, 4.9898],
        [4.9565, 4.9721, 4.9590],
        [4.9565, 4.9589, 4.9566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13903045654296875 
model_pd.l_d.mean(): -20.33591651916504 
model_pd.lagr.mean(): -20.19688606262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.0090], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13903045654296875
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12881343066692352
epoch£º134	 i:2 	 global-step:2682	 l-p:0.12976428866386414
epoch£º134	 i:3 	 global-step:2683	 l-p:0.193699449300766
epoch£º134	 i:4 	 global-step:2684	 l-p:0.12775452435016632
epoch£º134	 i:5 	 global-step:2685	 l-p:0.11525478214025497
epoch£º134	 i:6 	 global-step:2686	 l-p:0.1298200935125351
epoch£º134	 i:7 	 global-step:2687	 l-p:0.11708022654056549
epoch£º134	 i:8 	 global-step:2688	 l-p:0.11491650342941284
epoch£º134	 i:9 	 global-step:2689	 l-p:0.14984628558158875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9485, 4.9485],
        [4.9485, 6.0586, 6.7180],
        [4.9485, 5.0731, 5.0176],
        [4.9485, 5.8683, 6.3214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12527984380722046 
model_pd.l_d.mean(): -18.75283432006836 
model_pd.lagr.mean(): -18.627553939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.5188], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.12527984380722046
epoch£º135	 i:1 	 global-step:2701	 l-p:0.09575993567705154
epoch£º135	 i:2 	 global-step:2702	 l-p:0.1251632124185562
epoch£º135	 i:3 	 global-step:2703	 l-p:0.17985206842422485
epoch£º135	 i:4 	 global-step:2704	 l-p:0.1289377212524414
epoch£º135	 i:5 	 global-step:2705	 l-p:0.15725423395633698
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16906093060970306
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11557202786207199
epoch£º135	 i:8 	 global-step:2708	 l-p:0.12885865569114685
epoch£º135	 i:9 	 global-step:2709	 l-p:0.1410786658525467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 4.9327, 4.9326],
        [4.9326, 5.0753, 5.0186],
        [4.9326, 4.9333, 4.9326],
        [4.9326, 5.1044, 5.0476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.14105792343616486 
model_pd.l_d.mean(): -20.938060760498047 
model_pd.lagr.mean(): -20.7970027923584 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3873], device='cuda:0')), ('power', tensor([-21.5626], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.14105792343616486
epoch£º136	 i:1 	 global-step:2721	 l-p:0.14095085859298706
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15685412287712097
epoch£º136	 i:3 	 global-step:2723	 l-p:0.12046720087528229
epoch£º136	 i:4 	 global-step:2724	 l-p:0.14836303889751434
epoch£º136	 i:5 	 global-step:2725	 l-p:0.11636438220739365
epoch£º136	 i:6 	 global-step:2726	 l-p:0.18494892120361328
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1346040666103363
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17906366288661957
epoch£º136	 i:9 	 global-step:2729	 l-p:0.11643771082162857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9626, 4.9626, 4.9626],
        [4.9626, 4.9792, 4.9655],
        [4.9626, 4.9626, 4.9626],
        [4.9626, 5.9680, 6.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.2173025906085968 
model_pd.l_d.mean(): -20.639101028442383 
model_pd.lagr.mean(): -20.856403350830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.2908], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.2173025906085968
epoch£º137	 i:1 	 global-step:2741	 l-p:0.13477741181850433
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11629081517457962
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12511585652828217
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11655982583761215
epoch£º137	 i:5 	 global-step:2745	 l-p:0.130588561296463
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11374682188034058
epoch£º137	 i:7 	 global-step:2747	 l-p:0.10907074064016342
epoch£º137	 i:8 	 global-step:2748	 l-p:0.14096122980117798
epoch£º137	 i:9 	 global-step:2749	 l-p:0.14566965401172638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9450, 5.0152, 4.9730],
        [4.9450, 4.9450, 4.9450],
        [4.9450, 4.9450, 4.9450],
        [4.9450, 4.9450, 4.9450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13197942078113556 
model_pd.l_d.mean(): -20.644393920898438 
model_pd.lagr.mean(): -20.512414932250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153], device='cuda:0')), ('power', tensor([-21.2943], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13197942078113556
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12657366693019867
epoch£º138	 i:2 	 global-step:2762	 l-p:0.1394588053226471
epoch£º138	 i:3 	 global-step:2763	 l-p:0.26307961344718933
epoch£º138	 i:4 	 global-step:2764	 l-p:0.151445209980011
epoch£º138	 i:5 	 global-step:2765	 l-p:0.13410893082618713
epoch£º138	 i:6 	 global-step:2766	 l-p:0.06284206360578537
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.004461884498596191
epoch£º138	 i:8 	 global-step:2768	 l-p:0.1368093490600586
epoch£º138	 i:9 	 global-step:2769	 l-p:0.20628394186496735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8998, 5.8802, 6.4067],
        [4.8998, 6.0689, 6.8069],
        [4.8998, 4.8998, 4.8998],
        [4.8998, 4.9019, 4.8999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.08524244278669357 
model_pd.l_d.mean(): -18.715904235839844 
model_pd.lagr.mean(): -18.630661010742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-19.4725], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.08524244278669357
epoch£º139	 i:1 	 global-step:2781	 l-p:0.1494704931974411
epoch£º139	 i:2 	 global-step:2782	 l-p:0.1318521350622177
epoch£º139	 i:3 	 global-step:2783	 l-p:0.13532893359661102
epoch£º139	 i:4 	 global-step:2784	 l-p:0.12864047288894653
epoch£º139	 i:5 	 global-step:2785	 l-p:0.11740134656429291
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13436156511306763
epoch£º139	 i:7 	 global-step:2787	 l-p:0.12016294151544571
epoch£º139	 i:8 	 global-step:2788	 l-p:0.16960088908672333
epoch£º139	 i:9 	 global-step:2789	 l-p:0.16296210885047913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8772, 5.9064, 6.4901],
        [4.8772, 4.9108, 4.8860],
        [4.8772, 5.4738, 5.6372],
        [4.8772, 5.9258, 6.5308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.14227063953876495 
model_pd.l_d.mean(): -20.090377807617188 
model_pd.lagr.mean(): -19.94810676574707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.8152], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.14227063953876495
epoch£º140	 i:1 	 global-step:2801	 l-p:0.13655923306941986
epoch£º140	 i:2 	 global-step:2802	 l-p:0.13427752256393433
epoch£º140	 i:3 	 global-step:2803	 l-p:0.1721680760383606
epoch£º140	 i:4 	 global-step:2804	 l-p:0.10804197192192078
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13066668808460236
epoch£º140	 i:6 	 global-step:2806	 l-p:0.13725511729717255
epoch£º140	 i:7 	 global-step:2807	 l-p:0.13687990605831146
epoch£º140	 i:8 	 global-step:2808	 l-p:0.12624014914035797
epoch£º140	 i:9 	 global-step:2809	 l-p:0.12929393351078033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0309, 5.0310, 5.0309],
        [5.0309, 5.0310, 5.0309],
        [5.0309, 5.0309, 5.0309],
        [5.0309, 5.1604, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.1361769437789917 
model_pd.l_d.mean(): -20.141586303710938 
model_pd.lagr.mean(): -20.005409240722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.1361769437789917
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11883784830570221
epoch£º141	 i:2 	 global-step:2822	 l-p:0.41144606471061707
epoch£º141	 i:3 	 global-step:2823	 l-p:0.1284993588924408
epoch£º141	 i:4 	 global-step:2824	 l-p:0.13709448277950287
epoch£º141	 i:5 	 global-step:2825	 l-p:0.12967778742313385
epoch£º141	 i:6 	 global-step:2826	 l-p:0.13021895289421082
epoch£º141	 i:7 	 global-step:2827	 l-p:0.126362606883049
epoch£º141	 i:8 	 global-step:2828	 l-p:0.04819044843316078
epoch£º141	 i:9 	 global-step:2829	 l-p:0.19941365718841553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7062, 4.7986, 4.7517],
        [4.7062, 4.8646, 4.8123],
        [4.7062, 4.8480, 4.7954],
        [4.7062, 4.7066, 4.7062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.47356727719306946 
model_pd.l_d.mean(): -20.265649795532227 
model_pd.lagr.mean(): -19.792081832885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-21.0234], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.47356727719306946
epoch£º142	 i:1 	 global-step:2841	 l-p:0.16609664261341095
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14317356050014496
epoch£º142	 i:3 	 global-step:2843	 l-p:-0.3136278986930847
epoch£º142	 i:4 	 global-step:2844	 l-p:0.15265478193759918
epoch£º142	 i:5 	 global-step:2845	 l-p:0.15587402880191803
epoch£º142	 i:6 	 global-step:2846	 l-p:0.11304257065057755
epoch£º142	 i:7 	 global-step:2847	 l-p:0.11818141490221024
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12783658504486084
epoch£º142	 i:9 	 global-step:2849	 l-p:0.11206460744142532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.1700, 5.1699],
        [5.1699, 5.9434, 6.2308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11716683954000473 
model_pd.l_d.mean(): -18.95296859741211 
model_pd.lagr.mean(): -18.83580207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-19.6299], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11716683954000473
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11352942883968353
epoch£º143	 i:2 	 global-step:2862	 l-p:0.1167236715555191
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13291004300117493
epoch£º143	 i:4 	 global-step:2864	 l-p:0.127426877617836
epoch£º143	 i:5 	 global-step:2865	 l-p:0.1233748123049736
epoch£º143	 i:6 	 global-step:2866	 l-p:0.15727007389068604
epoch£º143	 i:7 	 global-step:2867	 l-p:0.14085866510868073
epoch£º143	 i:8 	 global-step:2868	 l-p:0.10172463953495026
epoch£º143	 i:9 	 global-step:2869	 l-p:0.17131377756595612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 5.8968, 6.6536],
        [4.7389, 5.2827, 5.4187],
        [4.7389, 4.8285, 4.7820],
        [4.7389, 4.7441, 4.7394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13802054524421692 
model_pd.l_d.mean(): -19.54542350769043 
model_pd.lagr.mean(): -19.40740203857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-20.2945], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13802054524421692
epoch£º144	 i:1 	 global-step:2881	 l-p:0.13731659948825836
epoch£º144	 i:2 	 global-step:2882	 l-p:0.8979681134223938
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11642645299434662
epoch£º144	 i:4 	 global-step:2884	 l-p:0.04442237690091133
epoch£º144	 i:5 	 global-step:2885	 l-p:0.17071032524108887
epoch£º144	 i:6 	 global-step:2886	 l-p:0.15110141038894653
epoch£º144	 i:7 	 global-step:2887	 l-p:0.09979791194200516
epoch£º144	 i:8 	 global-step:2888	 l-p:0.156777024269104
epoch£º144	 i:9 	 global-step:2889	 l-p:0.12231305986642838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.7362, 5.9854],
        [5.0241, 5.8313, 6.1681],
        [5.0241, 5.0330, 5.0252],
        [5.0241, 6.1066, 6.7282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12291354686021805 
model_pd.l_d.mean(): -20.473587036132812 
model_pd.lagr.mean(): -20.35067367553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.1254], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12291354686021805
epoch£º145	 i:1 	 global-step:2901	 l-p:0.19273829460144043
epoch£º145	 i:2 	 global-step:2902	 l-p:0.1185348853468895
epoch£º145	 i:3 	 global-step:2903	 l-p:0.122230663895607
epoch£º145	 i:4 	 global-step:2904	 l-p:0.13435475528240204
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11446266621351242
epoch£º145	 i:6 	 global-step:2906	 l-p:0.13535884022712708
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10579998046159744
epoch£º145	 i:8 	 global-step:2908	 l-p:0.14327369630336761
epoch£º145	 i:9 	 global-step:2909	 l-p:0.14317238330841064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8909, 4.8909, 4.8909],
        [4.8909, 4.9224, 4.8990],
        [4.8909, 5.1414, 5.1011],
        [4.8909, 4.9501, 4.9127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.15300224721431732 
model_pd.l_d.mean(): -19.637462615966797 
model_pd.lagr.mean(): -19.484460830688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.3595], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.15300224721431732
epoch£º146	 i:1 	 global-step:2921	 l-p:0.16736142337322235
epoch£º146	 i:2 	 global-step:2922	 l-p:0.1391025334596634
epoch£º146	 i:3 	 global-step:2923	 l-p:0.13617989420890808
epoch£º146	 i:4 	 global-step:2924	 l-p:-0.021553849801421165
epoch£º146	 i:5 	 global-step:2925	 l-p:0.14188067615032196
epoch£º146	 i:6 	 global-step:2926	 l-p:0.16152800619602203
epoch£º146	 i:7 	 global-step:2927	 l-p:0.14258943498134613
epoch£º146	 i:8 	 global-step:2928	 l-p:0.17253747582435608
epoch£º146	 i:9 	 global-step:2929	 l-p:0.1941457837820053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8221, 4.9906, 4.9371],
        [4.8221, 4.8222, 4.8221],
        [4.8221, 4.8660, 4.8359],
        [4.8221, 5.0702, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.1366291493177414 
model_pd.l_d.mean(): -20.78437042236328 
model_pd.lagr.mean(): -20.647741317749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-21.4616], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.1366291493177414
epoch£º147	 i:1 	 global-step:2941	 l-p:0.13038335740566254
epoch£º147	 i:2 	 global-step:2942	 l-p:0.13309496641159058
epoch£º147	 i:3 	 global-step:2943	 l-p:0.16095781326293945
epoch£º147	 i:4 	 global-step:2944	 l-p:0.1394355595111847
epoch£º147	 i:5 	 global-step:2945	 l-p:0.14718586206436157
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05776480957865715
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11968866735696793
epoch£º147	 i:8 	 global-step:2948	 l-p:0.14238840341567993
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12378852814435959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0169, 5.0381, 5.0211],
        [5.0169, 5.0267, 5.0182],
        [5.0169, 6.1192, 6.7654],
        [5.0169, 5.0169, 5.0169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.1231633797287941 
model_pd.l_d.mean(): -20.501428604125977 
model_pd.lagr.mean(): -20.378265380859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.1461], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.1231633797287941
epoch£º148	 i:1 	 global-step:2961	 l-p:0.15081147849559784
epoch£º148	 i:2 	 global-step:2962	 l-p:0.1283329874277115
epoch£º148	 i:3 	 global-step:2963	 l-p:0.16933390498161316
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1370043009519577
epoch£º148	 i:5 	 global-step:2965	 l-p:0.11860423535108566
epoch£º148	 i:6 	 global-step:2966	 l-p:0.1417679637670517
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10500077903270721
epoch£º148	 i:8 	 global-step:2968	 l-p:0.1205659806728363
epoch£º148	 i:9 	 global-step:2969	 l-p:0.13967080414295197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 4.9001, 4.8622],
        [4.8391, 4.8681, 4.8463],
        [4.8391, 5.8639, 6.4534],
        [4.8391, 5.2855, 5.3448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.19449478387832642 
model_pd.l_d.mean(): -18.755414962768555 
model_pd.lagr.mean(): -18.56092071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-19.5561], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.19449478387832642
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14283953607082367
epoch£º149	 i:2 	 global-step:2982	 l-p:0.23028479516506195
epoch£º149	 i:3 	 global-step:2983	 l-p:0.13946892321109772
epoch£º149	 i:4 	 global-step:2984	 l-p:0.10392560064792633
epoch£º149	 i:5 	 global-step:2985	 l-p:0.1425987333059311
epoch£º149	 i:6 	 global-step:2986	 l-p:0.13735266029834747
epoch£º149	 i:7 	 global-step:2987	 l-p:0.179461270570755
epoch£º149	 i:8 	 global-step:2988	 l-p:0.11623433977365494
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14683467149734497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8551, 5.1442, 5.1191],
        [4.8551, 5.2684, 5.3060],
        [4.8551, 5.1380, 5.1105],
        [4.8551, 4.8569, 4.8552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13420264422893524 
model_pd.l_d.mean(): -18.66222381591797 
model_pd.lagr.mean(): -18.52802085876465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-19.4304], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13420264422893524
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17180941998958588
epoch£º150	 i:2 	 global-step:3002	 l-p:0.06377609074115753
epoch£º150	 i:3 	 global-step:3003	 l-p:0.1216835156083107
epoch£º150	 i:4 	 global-step:3004	 l-p:0.12089872360229492
epoch£º150	 i:5 	 global-step:3005	 l-p:0.12665684521198273
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12319695949554443
epoch£º150	 i:7 	 global-step:3007	 l-p:0.12729038298130035
epoch£º150	 i:8 	 global-step:3008	 l-p:0.12842805683612823
epoch£º150	 i:9 	 global-step:3009	 l-p:0.16400031745433807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9225, 5.2713, 5.2696],
        [4.9225, 4.9225, 4.9225],
        [4.9225, 5.6900, 6.0043],
        [4.9225, 5.3899, 5.4578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13014449179172516 
model_pd.l_d.mean(): -20.736202239990234 
model_pd.lagr.mean(): -20.60605812072754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.3896], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13014449179172516
epoch£º151	 i:1 	 global-step:3021	 l-p:0.14095193147659302
epoch£º151	 i:2 	 global-step:3022	 l-p:0.2200830727815628
epoch£º151	 i:3 	 global-step:3023	 l-p:0.019736628979444504
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3467719256877899
epoch£º151	 i:5 	 global-step:3025	 l-p:0.15861941874027252
epoch£º151	 i:6 	 global-step:3026	 l-p:0.1477132886648178
epoch£º151	 i:7 	 global-step:3027	 l-p:0.06827415525913239
epoch£º151	 i:8 	 global-step:3028	 l-p:0.14478081464767456
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12968137860298157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9094, 5.0421, 4.9876],
        [4.9094, 5.8390, 6.3168],
        [4.9094, 5.0814, 5.0269],
        [4.9094, 5.0765, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.09888692945241928 
model_pd.l_d.mean(): -19.859479904174805 
model_pd.lagr.mean(): -19.76059341430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-20.6108], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.09888692945241928
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11933158338069916
epoch£º152	 i:2 	 global-step:3042	 l-p:0.1306268721818924
epoch£º152	 i:3 	 global-step:3043	 l-p:0.1260189265012741
epoch£º152	 i:4 	 global-step:3044	 l-p:0.13487140834331512
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12301895767450333
epoch£º152	 i:6 	 global-step:3046	 l-p:0.1286846101284027
epoch£º152	 i:7 	 global-step:3047	 l-p:0.14273983240127563
epoch£º152	 i:8 	 global-step:3048	 l-p:0.16294078528881073
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12502318620681763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9620, 4.9658, 4.9623],
        [4.9620, 4.9621, 4.9620],
        [4.9620, 4.9723, 4.9634],
        [4.9620, 4.9620, 4.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.1759086549282074 
model_pd.l_d.mean(): -19.3745174407959 
model_pd.lagr.mean(): -19.1986083984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.0581], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.1759086549282074
epoch£º153	 i:1 	 global-step:3061	 l-p:0.12960617244243622
epoch£º153	 i:2 	 global-step:3062	 l-p:0.06712537258863449
epoch£º153	 i:3 	 global-step:3063	 l-p:0.14171510934829712
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13708573579788208
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11808963865041733
epoch£º153	 i:6 	 global-step:3066	 l-p:0.1435404121875763
epoch£º153	 i:7 	 global-step:3067	 l-p:0.12174240499734879
epoch£º153	 i:8 	 global-step:3068	 l-p:0.1410488337278366
epoch£º153	 i:9 	 global-step:3069	 l-p:0.172219917178154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7397, 5.3093, 5.4701],
        [4.7397, 4.8731, 4.8210],
        [4.7397, 4.7610, 4.7442],
        [4.7397, 4.7436, 4.7400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.141423299908638 
model_pd.l_d.mean(): -18.744279861450195 
model_pd.lagr.mean(): -18.602855682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5893], device='cuda:0')), ('power', tensor([-19.5512], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.141423299908638
epoch£º154	 i:1 	 global-step:3081	 l-p:0.1306232064962387
epoch£º154	 i:2 	 global-step:3082	 l-p:0.20045089721679688
epoch£º154	 i:3 	 global-step:3083	 l-p:0.13997773826122284
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12102670222520828
epoch£º154	 i:5 	 global-step:3085	 l-p:0.15691368281841278
epoch£º154	 i:6 	 global-step:3086	 l-p:0.13411074876785278
epoch£º154	 i:7 	 global-step:3087	 l-p:0.14188170433044434
epoch£º154	 i:8 	 global-step:3088	 l-p:0.37196531891822815
epoch£º154	 i:9 	 global-step:3089	 l-p:0.13024674355983734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0544, 5.6952, 5.8845],
        [5.0544, 5.6066, 5.7260],
        [5.0544, 5.0969, 5.0671],
        [5.0544, 5.3767, 5.3575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.12937358021736145 
model_pd.l_d.mean(): -19.950172424316406 
model_pd.lagr.mean(): -19.820798873901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.6240], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.12937358021736145
epoch£º155	 i:1 	 global-step:3101	 l-p:0.24716369807720184
epoch£º155	 i:2 	 global-step:3102	 l-p:0.1307707577943802
epoch£º155	 i:3 	 global-step:3103	 l-p:0.12569637596607208
epoch£º155	 i:4 	 global-step:3104	 l-p:0.1177816167473793
epoch£º155	 i:5 	 global-step:3105	 l-p:0.1536325365304947
epoch£º155	 i:6 	 global-step:3106	 l-p:0.12744474411010742
epoch£º155	 i:7 	 global-step:3107	 l-p:0.16640976071357727
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12190267443656921
epoch£º155	 i:9 	 global-step:3109	 l-p:0.13682475686073303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8870, 5.9777, 6.6363],
        [4.8870, 6.0009, 6.6857],
        [4.8870, 5.0347, 4.9802],
        [4.8870, 5.9744, 6.6293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12619461119174957 
model_pd.l_d.mean(): -19.983003616333008 
model_pd.lagr.mean(): -19.856809616088867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.7141], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12619461119174957
epoch£º156	 i:1 	 global-step:3121	 l-p:0.1513543426990509
epoch£º156	 i:2 	 global-step:3122	 l-p:0.12533186376094818
epoch£º156	 i:3 	 global-step:3123	 l-p:0.07846016436815262
epoch£º156	 i:4 	 global-step:3124	 l-p:0.1466306447982788
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13939496874809265
epoch£º156	 i:6 	 global-step:3126	 l-p:0.1545574814081192
epoch£º156	 i:7 	 global-step:3127	 l-p:0.13000735640525818
epoch£º156	 i:8 	 global-step:3128	 l-p:0.12198881059885025
epoch£º156	 i:9 	 global-step:3129	 l-p:0.14602844417095184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9445, 5.0063, 4.9679],
        [4.9445, 5.4315, 5.5134],
        [4.9445, 5.8217, 6.2433],
        [4.9445, 5.1717, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.07945006340742111 
model_pd.l_d.mean(): -20.169660568237305 
model_pd.lagr.mean(): -20.0902099609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.8787], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.07945006340742111
epoch£º157	 i:1 	 global-step:3141	 l-p:0.11509035527706146
epoch£º157	 i:2 	 global-step:3142	 l-p:0.16390863060951233
epoch£º157	 i:3 	 global-step:3143	 l-p:0.12628746032714844
epoch£º157	 i:4 	 global-step:3144	 l-p:0.1450653374195099
epoch£º157	 i:5 	 global-step:3145	 l-p:0.18913738429546356
epoch£º157	 i:6 	 global-step:3146	 l-p:0.1329316645860672
epoch£º157	 i:7 	 global-step:3147	 l-p:0.153702512383461
epoch£º157	 i:8 	 global-step:3148	 l-p:0.140924870967865
epoch£º157	 i:9 	 global-step:3149	 l-p:0.2076382339000702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8970, 4.8969],
        [4.8969, 4.9048, 4.8979],
        [4.8969, 4.8970, 4.8969],
        [4.8969, 5.2925, 5.3194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.14895270764827728 
model_pd.l_d.mean(): -19.583742141723633 
model_pd.lagr.mean(): -19.434789657592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.14895270764827728
epoch£º158	 i:1 	 global-step:3161	 l-p:0.1346929371356964
epoch£º158	 i:2 	 global-step:3162	 l-p:0.12151198834180832
epoch£º158	 i:3 	 global-step:3163	 l-p:0.12073329836130142
epoch£º158	 i:4 	 global-step:3164	 l-p:0.1266309916973114
epoch£º158	 i:5 	 global-step:3165	 l-p:0.13046789169311523
epoch£º158	 i:6 	 global-step:3166	 l-p:0.15536054968833923
epoch£º158	 i:7 	 global-step:3167	 l-p:0.14182522892951965
epoch£º158	 i:8 	 global-step:3168	 l-p:0.1379641592502594
epoch£º158	 i:9 	 global-step:3169	 l-p:0.27693602442741394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8208, 4.9539, 4.9010],
        [4.8208, 4.8786, 4.8423],
        [4.8208, 4.9203, 4.8715],
        [4.8208, 4.8525, 4.8292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.41804030537605286 
model_pd.l_d.mean(): -20.487668991088867 
model_pd.lagr.mean(): -20.06962776184082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4828], device='cuda:0')), ('power', tensor([-21.2048], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.41804030537605286
epoch£º159	 i:1 	 global-step:3181	 l-p:0.13948509097099304
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13685698807239532
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.448038250207901
epoch£º159	 i:4 	 global-step:3184	 l-p:0.1544749140739441
epoch£º159	 i:5 	 global-step:3185	 l-p:0.13249513506889343
epoch£º159	 i:6 	 global-step:3186	 l-p:0.1213294193148613
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13583503663539886
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12447439879179001
epoch£º159	 i:9 	 global-step:3189	 l-p:0.1845245361328125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8677, 4.9223, 4.8872],
        [4.8677, 5.5553, 5.8066],
        [4.8677, 4.8677, 4.8677],
        [4.8677, 4.8682, 4.8677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.19721217453479767 
model_pd.l_d.mean(): -20.888843536376953 
model_pd.lagr.mean(): -20.691631317138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.5440], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.19721217453479767
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11666759848594666
epoch£º160	 i:2 	 global-step:3202	 l-p:0.12616102397441864
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13175545632839203
epoch£º160	 i:4 	 global-step:3204	 l-p:0.1416354924440384
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.07912132143974304
epoch£º160	 i:6 	 global-step:3206	 l-p:0.12825404107570648
epoch£º160	 i:7 	 global-step:3207	 l-p:0.13265779614448547
epoch£º160	 i:8 	 global-step:3208	 l-p:0.13389556109905243
epoch£º160	 i:9 	 global-step:3209	 l-p:0.1566387116909027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9397, 5.0731, 5.0187],
        [4.9397, 5.0032, 4.9642],
        [4.9397, 5.5149, 5.6643],
        [4.9397, 5.5890, 5.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.16019988059997559 
model_pd.l_d.mean(): -20.868896484375 
model_pd.lagr.mean(): -20.708696365356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4005], device='cuda:0')), ('power', tensor([-21.5061], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.16019988059997559
epoch£º161	 i:1 	 global-step:3221	 l-p:0.131147101521492
epoch£º161	 i:2 	 global-step:3222	 l-p:0.28330329060554504
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11708465218544006
epoch£º161	 i:4 	 global-step:3224	 l-p:0.036045435816049576
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19145455956459045
epoch£º161	 i:6 	 global-step:3226	 l-p:0.1307988464832306
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14571888744831085
epoch£º161	 i:8 	 global-step:3228	 l-p:0.1704995334148407
epoch£º161	 i:9 	 global-step:3229	 l-p:0.16710908710956573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6408, 5.1010, 5.1898],
        [4.6408, 4.6412, 4.6408],
        [4.6408, 4.7159, 4.6742],
        [4.6408, 5.2163, 5.3959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12154845148324966 
model_pd.l_d.mean(): -20.335466384887695 
model_pd.lagr.mean(): -20.213918685913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5525], device='cuda:0')), ('power', tensor([-21.1222], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12154845148324966
epoch£º162	 i:1 	 global-step:3241	 l-p:0.15463444590568542
epoch£º162	 i:2 	 global-step:3242	 l-p:0.16063058376312256
epoch£º162	 i:3 	 global-step:3243	 l-p:0.15913012623786926
epoch£º162	 i:4 	 global-step:3244	 l-p:0.07120504975318909
epoch£º162	 i:5 	 global-step:3245	 l-p:0.1285850554704666
epoch£º162	 i:6 	 global-step:3246	 l-p:0.19599856436252594
epoch£º162	 i:7 	 global-step:3247	 l-p:0.0859970673918724
epoch£º162	 i:8 	 global-step:3248	 l-p:0.5677502155303955
epoch£º162	 i:9 	 global-step:3249	 l-p:0.1252271682024002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9430, 5.8062, 6.2165],
        [4.9430, 4.9430, 4.9430],
        [4.9430, 5.2567, 5.2406],
        [4.9430, 4.9430, 4.9430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14483684301376343 
model_pd.l_d.mean(): -18.544214248657227 
model_pd.lagr.mean(): -18.399377822875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-19.3250], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14483684301376343
epoch£º163	 i:1 	 global-step:3261	 l-p:0.11734273284673691
epoch£º163	 i:2 	 global-step:3262	 l-p:0.1195468083024025
epoch£º163	 i:3 	 global-step:3263	 l-p:0.17110170423984528
epoch£º163	 i:4 	 global-step:3264	 l-p:0.13174912333488464
epoch£º163	 i:5 	 global-step:3265	 l-p:0.1382194459438324
epoch£º163	 i:6 	 global-step:3266	 l-p:0.116824671626091
epoch£º163	 i:7 	 global-step:3267	 l-p:0.1410735696554184
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12590643763542175
epoch£º163	 i:9 	 global-step:3269	 l-p:0.1319435089826584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9567, 5.1772, 5.1299],
        [4.9567, 4.9607, 4.9570],
        [4.9567, 4.9575, 4.9567],
        [4.9567, 4.9793, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1274646520614624 
model_pd.l_d.mean(): -20.163740158081055 
model_pd.lagr.mean(): -20.03627586364746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1274646520614624
epoch£º164	 i:1 	 global-step:3281	 l-p:0.1281343549489975
epoch£º164	 i:2 	 global-step:3282	 l-p:0.1466175615787506
epoch£º164	 i:3 	 global-step:3283	 l-p:0.1112433522939682
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14925318956375122
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12970541417598724
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12972228229045868
epoch£º164	 i:7 	 global-step:3287	 l-p:0.14510497450828552
epoch£º164	 i:8 	 global-step:3288	 l-p:0.1501993089914322
epoch£º164	 i:9 	 global-step:3289	 l-p:0.14788773655891418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8951, 4.8951, 4.8951],
        [4.8951, 5.6363, 5.9356],
        [4.8951, 5.4144, 5.5263],
        [4.8951, 5.0772, 5.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13100187480449677 
model_pd.l_d.mean(): -20.204376220703125 
model_pd.lagr.mean(): -20.073373794555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.9135], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13100187480449677
epoch£º165	 i:1 	 global-step:3301	 l-p:0.13213133811950684
epoch£º165	 i:2 	 global-step:3302	 l-p:0.16265231370925903
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14334668219089508
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.016629120334982872
epoch£º165	 i:5 	 global-step:3305	 l-p:0.40669554471969604
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12461749464273453
epoch£º165	 i:7 	 global-step:3307	 l-p:0.1427004188299179
epoch£º165	 i:8 	 global-step:3308	 l-p:0.014948777854442596
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13115327060222626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8703, 4.8719, 4.8704],
        [4.8703, 4.9373, 4.8972],
        [4.8703, 4.9465, 4.9033],
        [4.8703, 4.8703, 4.8703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.14294873178005219 
model_pd.l_d.mean(): -18.978899002075195 
model_pd.lagr.mean(): -18.83595085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-19.7437], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.14294873178005219
epoch£º166	 i:1 	 global-step:3321	 l-p:0.13213276863098145
epoch£º166	 i:2 	 global-step:3322	 l-p:0.14451293647289276
epoch£º166	 i:3 	 global-step:3323	 l-p:0.14301961660385132
epoch£º166	 i:4 	 global-step:3324	 l-p:0.12642785906791687
epoch£º166	 i:5 	 global-step:3325	 l-p:0.11149518936872482
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12366149574518204
epoch£º166	 i:7 	 global-step:3327	 l-p:0.13026462495326996
epoch£º166	 i:8 	 global-step:3328	 l-p:0.11892125755548477
epoch£º166	 i:9 	 global-step:3329	 l-p:0.837715208530426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0056, 5.0056, 5.0056],
        [5.0056, 5.1581, 5.1023],
        [5.0056, 6.2000, 6.9611],
        [5.0056, 5.0175, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13492543995380402 
model_pd.l_d.mean(): -19.4580135345459 
model_pd.lagr.mean(): -19.323087692260742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-20.1630], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13492543995380402
epoch£º167	 i:1 	 global-step:3341	 l-p:0.13003338873386383
epoch£º167	 i:2 	 global-step:3342	 l-p:0.1270916610956192
epoch£º167	 i:3 	 global-step:3343	 l-p:0.13898727297782898
epoch£º167	 i:4 	 global-step:3344	 l-p:0.13273748755455017
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12146515399217606
epoch£º167	 i:6 	 global-step:3346	 l-p:0.37040552496910095
epoch£º167	 i:7 	 global-step:3347	 l-p:0.07090027630329132
epoch£º167	 i:8 	 global-step:3348	 l-p:0.048048824071884155
epoch£º167	 i:9 	 global-step:3349	 l-p:0.12065830081701279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8246, 4.8241],
        [4.8240, 5.8396, 6.4302],
        [4.8240, 4.8241, 4.8240],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.12831571698188782 
model_pd.l_d.mean(): -18.658109664916992 
model_pd.lagr.mean(): -18.529794692993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.4495], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.12831571698188782
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12601737678050995
epoch£º168	 i:2 	 global-step:3362	 l-p:0.1464787870645523
epoch£º168	 i:3 	 global-step:3363	 l-p:0.14454145729541779
epoch£º168	 i:4 	 global-step:3364	 l-p:0.14144884049892426
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11996980756521225
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11652902513742447
epoch£º168	 i:7 	 global-step:3367	 l-p:0.1414274126291275
epoch£º168	 i:8 	 global-step:3368	 l-p:0.15155072510242462
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12819167971611023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8907, 4.8909, 4.8907],
        [4.8907, 4.9091, 4.8943],
        [4.8907, 5.0211, 4.9678],
        [4.8907, 4.9412, 4.9079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13750293850898743 
model_pd.l_d.mean(): -20.71607208251953 
model_pd.lagr.mean(): -20.578569412231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3809], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13750293850898743
epoch£º169	 i:1 	 global-step:3381	 l-p:0.14193782210350037
epoch£º169	 i:2 	 global-step:3382	 l-p:0.1336897611618042
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.1080133393406868
epoch£º169	 i:4 	 global-step:3384	 l-p:0.14050841331481934
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09745261073112488
epoch£º169	 i:6 	 global-step:3386	 l-p:0.051683928817510605
epoch£º169	 i:7 	 global-step:3387	 l-p:0.1524168699979782
epoch£º169	 i:8 	 global-step:3388	 l-p:0.1253892332315445
epoch£º169	 i:9 	 global-step:3389	 l-p:0.1486976444721222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9488, 5.1440, 5.0929],
        [4.9488, 4.9496, 4.9488],
        [4.9488, 4.9724, 4.9540],
        [4.9488, 4.9494, 4.9488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.15095172822475433 
model_pd.l_d.mean(): -18.792497634887695 
model_pd.lagr.mean(): -18.64154624938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5342], device='cuda:0')), ('power', tensor([-19.5436], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.15095172822475433
epoch£º170	 i:1 	 global-step:3401	 l-p:0.1409374475479126
epoch£º170	 i:2 	 global-step:3402	 l-p:0.13808462023735046
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12541280686855316
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12352127581834793
epoch£º170	 i:5 	 global-step:3405	 l-p:0.4302745759487152
epoch£º170	 i:6 	 global-step:3406	 l-p:0.12052362412214279
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12274765968322754
epoch£º170	 i:8 	 global-step:3408	 l-p:0.13009586930274963
epoch£º170	 i:9 	 global-step:3409	 l-p:0.15123482048511505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9569, 5.4455, 5.5327],
        [4.9569, 4.9583, 4.9570],
        [4.9569, 4.9820, 4.9626],
        [4.9569, 4.9570, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11723867058753967 
model_pd.l_d.mean(): -18.685169219970703 
model_pd.lagr.mean(): -18.567930221557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.4504], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11723867058753967
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483373284339905
epoch£º171	 i:2 	 global-step:3422	 l-p:0.16540051996707916
epoch£º171	 i:3 	 global-step:3423	 l-p:0.14427441358566284
epoch£º171	 i:4 	 global-step:3424	 l-p:0.07828871160745621
epoch£º171	 i:5 	 global-step:3425	 l-p:0.11680668592453003
epoch£º171	 i:6 	 global-step:3426	 l-p:0.13464073836803436
epoch£º171	 i:7 	 global-step:3427	 l-p:0.2529711425304413
epoch£º171	 i:8 	 global-step:3428	 l-p:0.13851581513881683
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15588697791099548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8075, 4.8080, 4.8075],
        [4.8075, 4.8129, 4.8080],
        [4.8075, 5.0546, 5.0202],
        [4.8075, 4.8075, 4.8075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.11587736755609512 
model_pd.l_d.mean(): -20.79187774658203 
model_pd.lagr.mean(): -20.676000595092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.4793], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.11587736755609512
epoch£º172	 i:1 	 global-step:3441	 l-p:0.19615161418914795
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09902282804250717
epoch£º172	 i:3 	 global-step:3443	 l-p:-0.16627608239650726
epoch£º172	 i:4 	 global-step:3444	 l-p:0.1268821805715561
epoch£º172	 i:5 	 global-step:3445	 l-p:0.13662709295749664
epoch£º172	 i:6 	 global-step:3446	 l-p:0.14091607928276062
epoch£º172	 i:7 	 global-step:3447	 l-p:0.1788821816444397
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12066814303398132
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12980076670646667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1352, 6.2636, 6.9315],
        [5.1352, 5.2356, 5.1843],
        [5.1352, 5.8314, 6.0679],
        [5.1352, 5.2477, 5.1940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11810436844825745 
model_pd.l_d.mean(): -19.46830940246582 
model_pd.lagr.mean(): -19.350204467773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4743], device='cuda:0')), ('power', tensor([-20.1656], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11810436844825745
epoch£º173	 i:1 	 global-step:3461	 l-p:0.0993179902434349
epoch£º173	 i:2 	 global-step:3462	 l-p:0.12843437492847443
epoch£º173	 i:3 	 global-step:3463	 l-p:0.143131822347641
epoch£º173	 i:4 	 global-step:3464	 l-p:0.1250164806842804
epoch£º173	 i:5 	 global-step:3465	 l-p:0.13911806046962738
epoch£º173	 i:6 	 global-step:3466	 l-p:0.13275183737277985
epoch£º173	 i:7 	 global-step:3467	 l-p:0.09165444225072861
epoch£º173	 i:8 	 global-step:3468	 l-p:0.09233657270669937
epoch£º173	 i:9 	 global-step:3469	 l-p:0.1417253315448761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6641, 5.4157, 5.7590],
        [4.6641, 4.6756, 4.6658],
        [4.6641, 4.7299, 4.6907],
        [4.6641, 4.7830, 4.7331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.15753211081027985 
model_pd.l_d.mean(): -20.258508682250977 
model_pd.lagr.mean(): -20.100976943969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-21.0342], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.15753211081027985
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13992007076740265
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10786330699920654
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15756964683532715
epoch£º174	 i:4 	 global-step:3484	 l-p:0.157753586769104
epoch£º174	 i:5 	 global-step:3485	 l-p:-0.521236777305603
epoch£º174	 i:6 	 global-step:3486	 l-p:0.18956823647022247
epoch£º174	 i:7 	 global-step:3487	 l-p:0.13602839410305023
epoch£º174	 i:8 	 global-step:3488	 l-p:0.1296098828315735
epoch£º174	 i:9 	 global-step:3489	 l-p:0.11855363845825195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.1446, 5.1421],
        [5.1419, 5.1535, 5.1436],
        [5.1419, 5.1960, 5.1603],
        [5.1419, 5.2255, 5.1786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.1214243471622467 
model_pd.l_d.mean(): -20.606670379638672 
model_pd.lagr.mean(): -20.485246658325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3579], device='cuda:0')), ('power', tensor([-21.1974], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.1214243471622467
epoch£º175	 i:1 	 global-step:3501	 l-p:0.12044992297887802
epoch£º175	 i:2 	 global-step:3502	 l-p:0.15717948973178864
epoch£º175	 i:3 	 global-step:3503	 l-p:0.19566194713115692
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13157682120800018
epoch£º175	 i:5 	 global-step:3505	 l-p:0.11829813569784164
epoch£º175	 i:6 	 global-step:3506	 l-p:0.115338034927845
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12438659369945526
epoch£º175	 i:8 	 global-step:3508	 l-p:0.12497895956039429
epoch£º175	 i:9 	 global-step:3509	 l-p:0.12435059994459152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9239, 4.9239, 4.9239],
        [4.9239, 4.9423, 4.9274],
        [4.9239, 5.4518, 5.5721],
        [4.9239, 5.1304, 5.0823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.09486062824726105 
model_pd.l_d.mean(): -19.31104850769043 
model_pd.lagr.mean(): -19.216188430786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.0413], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.09486062824726105
epoch£º176	 i:1 	 global-step:3521	 l-p:0.17336539924144745
epoch£º176	 i:2 	 global-step:3522	 l-p:0.13698534667491913
epoch£º176	 i:3 	 global-step:3523	 l-p:0.13735851645469666
epoch£º176	 i:4 	 global-step:3524	 l-p:0.10091224312782288
epoch£º176	 i:5 	 global-step:3525	 l-p:0.49917837977409363
epoch£º176	 i:6 	 global-step:3526	 l-p:0.13885299861431122
epoch£º176	 i:7 	 global-step:3527	 l-p:0.1350778341293335
epoch£º176	 i:8 	 global-step:3528	 l-p:-0.3156934678554535
epoch£º176	 i:9 	 global-step:3529	 l-p:0.15765121579170227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8008, 5.8643, 6.5157],
        [4.8008, 4.8014, 4.8009],
        [4.8008, 4.8965, 4.8485],
        [4.8008, 4.8011, 4.8009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11437848955392838 
model_pd.l_d.mean(): -19.82997703552246 
model_pd.lagr.mean(): -19.715599060058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.5858], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.11437848955392838
epoch£º177	 i:1 	 global-step:3541	 l-p:0.13294343650341034
epoch£º177	 i:2 	 global-step:3542	 l-p:0.17254363000392914
epoch£º177	 i:3 	 global-step:3543	 l-p:0.1105266660451889
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13880282640457153
epoch£º177	 i:5 	 global-step:3545	 l-p:0.10794991999864578
epoch£º177	 i:6 	 global-step:3546	 l-p:0.13082075119018555
epoch£º177	 i:7 	 global-step:3547	 l-p:0.12542890012264252
epoch£º177	 i:8 	 global-step:3548	 l-p:0.14899307489395142
epoch£º177	 i:9 	 global-step:3549	 l-p:0.1966727077960968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9387, 4.9389, 4.9387],
        [4.9387, 4.9387, 4.9387],
        [4.9387, 4.9387, 4.9387],
        [4.9387, 5.0075, 4.9663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.19904547929763794 
model_pd.l_d.mean(): -18.319555282592773 
model_pd.lagr.mean(): -18.12051010131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5403], device='cuda:0')), ('power', tensor([-19.0718], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.19904547929763794
epoch£º178	 i:1 	 global-step:3561	 l-p:0.14015689492225647
epoch£º178	 i:2 	 global-step:3562	 l-p:0.1376427859067917
epoch£º178	 i:3 	 global-step:3563	 l-p:0.11383479833602905
epoch£º178	 i:4 	 global-step:3564	 l-p:0.12950529158115387
epoch£º178	 i:5 	 global-step:3565	 l-p:0.15325281023979187
epoch£º178	 i:6 	 global-step:3566	 l-p:0.12534116208553314
epoch£º178	 i:7 	 global-step:3567	 l-p:0.2864953279495239
epoch£º178	 i:8 	 global-step:3568	 l-p:0.12287785112857819
epoch£º178	 i:9 	 global-step:3569	 l-p:0.07880379259586334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8146, 4.8155, 4.8146],
        [4.8146, 4.8349, 4.8186],
        [4.8146, 4.8337, 4.8182],
        [4.8146, 4.8150, 4.8146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.17456811666488647 
model_pd.l_d.mean(): -20.570735931396484 
model_pd.lagr.mean(): -20.396167755126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.2750], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.17456811666488647
epoch£º179	 i:1 	 global-step:3581	 l-p:0.5513726472854614
epoch£º179	 i:2 	 global-step:3582	 l-p:0.12614822387695312
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12481455504894257
epoch£º179	 i:4 	 global-step:3584	 l-p:0.14999006688594818
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1307586133480072
epoch£º179	 i:6 	 global-step:3586	 l-p:0.1602989137172699
epoch£º179	 i:7 	 global-step:3587	 l-p:0.12630049884319305
epoch£º179	 i:8 	 global-step:3588	 l-p:0.10530965775251389
epoch£º179	 i:9 	 global-step:3589	 l-p:0.2008938193321228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8972, 4.8980, 4.8972],
        [4.8972, 4.8977, 4.8972],
        [4.8972, 5.0858, 5.0354],
        [4.8972, 4.8972, 4.8972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12800374627113342 
model_pd.l_d.mean(): -20.887332916259766 
model_pd.lagr.mean(): -20.759328842163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4121], device='cuda:0')), ('power', tensor([-21.5366], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12800374627113342
epoch£º180	 i:1 	 global-step:3601	 l-p:0.14903013408184052
epoch£º180	 i:2 	 global-step:3602	 l-p:-2.8147847652435303
epoch£º180	 i:3 	 global-step:3603	 l-p:0.20638145506381989
epoch£º180	 i:4 	 global-step:3604	 l-p:-0.08428514003753662
epoch£º180	 i:5 	 global-step:3605	 l-p:0.15989793837070465
epoch£º180	 i:6 	 global-step:3606	 l-p:0.26351919770240784
epoch£º180	 i:7 	 global-step:3607	 l-p:0.12815918028354645
epoch£º180	 i:8 	 global-step:3608	 l-p:0.13514943420886993
epoch£º180	 i:9 	 global-step:3609	 l-p:0.1341996192932129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9307, 4.9372, 4.9314],
        [4.9307, 5.0031, 4.9605],
        [4.9307, 5.3228, 5.3524],
        [4.9307, 5.5853, 5.8100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1509445309638977 
model_pd.l_d.mean(): -20.478046417236328 
model_pd.lagr.mean(): -20.327102661132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.1509445309638977
epoch£º181	 i:1 	 global-step:3621	 l-p:0.1998813897371292
epoch£º181	 i:2 	 global-step:3622	 l-p:0.12429467588663101
epoch£º181	 i:3 	 global-step:3623	 l-p:0.12860558927059174
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13532760739326477
epoch£º181	 i:5 	 global-step:3625	 l-p:0.10633666813373566
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13399338722229004
epoch£º181	 i:7 	 global-step:3627	 l-p:0.1267014890909195
epoch£º181	 i:8 	 global-step:3628	 l-p:0.17259661853313446
epoch£º181	 i:9 	 global-step:3629	 l-p:0.19641320407390594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9394, 4.9475, 4.9404],
        [4.9394, 4.9891, 4.9557],
        [4.9394, 5.0613, 5.0081],
        [4.9394, 4.9394, 4.9394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12427760660648346 
model_pd.l_d.mean(): -20.25743865966797 
model_pd.lagr.mean(): -20.133161544799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.9272], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12427760660648346
epoch£º182	 i:1 	 global-step:3641	 l-p:0.14275209605693817
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1283995658159256
epoch£º182	 i:3 	 global-step:3643	 l-p:0.1214294508099556
epoch£º182	 i:4 	 global-step:3644	 l-p:0.2149846851825714
epoch£º182	 i:5 	 global-step:3645	 l-p:0.08480098843574524
epoch£º182	 i:6 	 global-step:3646	 l-p:0.18099847435951233
epoch£º182	 i:7 	 global-step:3647	 l-p:0.20534197986125946
epoch£º182	 i:8 	 global-step:3648	 l-p:0.14273594319820404
epoch£º182	 i:9 	 global-step:3649	 l-p:0.1136026605963707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0609, 5.3789, 5.3634],
        [5.0609, 5.2015, 5.1456],
        [5.0609, 5.0692, 5.0619],
        [5.0609, 5.7900, 6.0675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12625178694725037 
model_pd.l_d.mean(): -20.340782165527344 
model_pd.lagr.mean(): -20.21453094482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-20.9890], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12625178694725037
epoch£º183	 i:1 	 global-step:3661	 l-p:0.12626837193965912
epoch£º183	 i:2 	 global-step:3662	 l-p:0.1303309053182602
epoch£º183	 i:3 	 global-step:3663	 l-p:0.12541356682777405
epoch£º183	 i:4 	 global-step:3664	 l-p:0.1941647082567215
epoch£º183	 i:5 	 global-step:3665	 l-p:0.4117792546749115
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14067663252353668
epoch£º183	 i:7 	 global-step:3667	 l-p:-1.6110589504241943
epoch£º183	 i:8 	 global-step:3668	 l-p:0.12889441847801208
epoch£º183	 i:9 	 global-step:3669	 l-p:0.1287589967250824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9054, 4.9744, 4.9329],
        [4.9054, 5.6723, 6.0031],
        [4.9054, 4.9374, 4.9135],
        [4.9054, 4.9054, 4.9054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.1409325897693634 
model_pd.l_d.mean(): -19.021873474121094 
model_pd.lagr.mean(): -18.88094139099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5307], device='cuda:0')), ('power', tensor([-19.7719], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.1409325897693634
epoch£º184	 i:1 	 global-step:3681	 l-p:0.16305701434612274
epoch£º184	 i:2 	 global-step:3682	 l-p:0.129651740193367
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12197922170162201
epoch£º184	 i:4 	 global-step:3684	 l-p:0.13019461929798126
epoch£º184	 i:5 	 global-step:3685	 l-p:0.1005658507347107
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13618387281894684
epoch£º184	 i:7 	 global-step:3687	 l-p:0.12137677520513535
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11443664133548737
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09779229760169983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9300, 5.0471, 4.9943],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9445, 4.9323],
        [4.9300, 5.6584, 5.9506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12938331067562103 
model_pd.l_d.mean(): -19.371828079223633 
model_pd.lagr.mean(): -19.24244499206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.1168], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12938331067562103
epoch£º185	 i:1 	 global-step:3701	 l-p:0.24008584022521973
epoch£º185	 i:2 	 global-step:3702	 l-p:0.13699203729629517
epoch£º185	 i:3 	 global-step:3703	 l-p:0.1487206369638443
epoch£º185	 i:4 	 global-step:3704	 l-p:0.1451984941959381
epoch£º185	 i:5 	 global-step:3705	 l-p:0.014190797694027424
epoch£º185	 i:6 	 global-step:3706	 l-p:0.14426401257514954
epoch£º185	 i:7 	 global-step:3707	 l-p:0.16219401359558105
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11549000442028046
epoch£º185	 i:9 	 global-step:3709	 l-p:-2.6113104820251465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8944, 4.8944, 4.8944],
        [4.8944, 5.2419, 5.2495],
        [4.8944, 5.1998, 5.1867],
        [4.8944, 4.9971, 4.9466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.1323532611131668 
model_pd.l_d.mean(): -20.22100257873535 
model_pd.lagr.mean(): -20.08864974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.9387], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.1323532611131668
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13773375749588013
epoch£º186	 i:2 	 global-step:3722	 l-p:0.1975126415491104
epoch£º186	 i:3 	 global-step:3723	 l-p:0.1873374581336975
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10520841181278229
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11361978203058243
epoch£º186	 i:6 	 global-step:3726	 l-p:0.13410134613513947
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13955238461494446
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12080714851617813
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11620622128248215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1182, 5.1181],
        [5.1181, 5.2981, 5.2423],
        [5.1181, 5.4358, 5.4184],
        [5.1181, 5.1181, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.1926170289516449 
model_pd.l_d.mean(): -19.39527702331543 
model_pd.lagr.mean(): -19.202659606933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.0695], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.1926170289516449
epoch£º187	 i:1 	 global-step:3741	 l-p:0.14034801721572876
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11663993448019028
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13576315343379974
epoch£º187	 i:4 	 global-step:3744	 l-p:0.13871298730373383
epoch£º187	 i:5 	 global-step:3745	 l-p:0.12621676921844482
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05974186211824417
epoch£º187	 i:7 	 global-step:3747	 l-p:0.09836524724960327
epoch£º187	 i:8 	 global-step:3748	 l-p:0.10249051451683044
epoch£º187	 i:9 	 global-step:3749	 l-p:0.14389784634113312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6941, 5.3893, 5.6810],
        [4.6941, 4.7561, 4.7170],
        [4.6941, 4.6941, 4.6941],
        [4.6941, 5.3782, 5.6596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.1499219834804535 
model_pd.l_d.mean(): -20.689313888549805 
model_pd.lagr.mean(): -20.539392471313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-21.4258], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.1499219834804535
epoch£º188	 i:1 	 global-step:3761	 l-p:0.1325218379497528
epoch£º188	 i:2 	 global-step:3762	 l-p:0.12471292167901993
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14236043393611908
epoch£º188	 i:4 	 global-step:3764	 l-p:0.0890994444489479
epoch£º188	 i:5 	 global-step:3765	 l-p:0.18448896706104279
epoch£º188	 i:6 	 global-step:3766	 l-p:0.16655780375003815
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10969909280538559
epoch£º188	 i:8 	 global-step:3768	 l-p:0.1731719970703125
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11731035262346268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0382, 6.0354, 6.5816],
        [5.0382, 6.0515, 6.6147],
        [5.0382, 5.0382, 5.0382],
        [5.0382, 5.0382, 5.0382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10281315445899963 
model_pd.l_d.mean(): -19.67394256591797 
model_pd.lagr.mean(): -19.571128845214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.3667], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10281315445899963
epoch£º189	 i:1 	 global-step:3781	 l-p:0.21026931703090668
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11469436436891556
epoch£º189	 i:3 	 global-step:3783	 l-p:0.126626655459404
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12223204970359802
epoch£º189	 i:5 	 global-step:3785	 l-p:0.10520820319652557
epoch£º189	 i:6 	 global-step:3786	 l-p:0.12324205040931702
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11035657674074173
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11788221448659897
epoch£º189	 i:9 	 global-step:3789	 l-p:0.1342507004737854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0346, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.1334170550107956 
model_pd.l_d.mean(): -20.220399856567383 
model_pd.lagr.mean(): -20.08698272705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.1334170550107956
epoch£º190	 i:1 	 global-step:3801	 l-p:0.1392413079738617
epoch£º190	 i:2 	 global-step:3802	 l-p:0.13302503526210785
epoch£º190	 i:3 	 global-step:3803	 l-p:0.07738310098648071
epoch£º190	 i:4 	 global-step:3804	 l-p:0.24374911189079285
epoch£º190	 i:5 	 global-step:3805	 l-p:0.13900741934776306
epoch£º190	 i:6 	 global-step:3806	 l-p:0.20351473987102509
epoch£º190	 i:7 	 global-step:3807	 l-p:0.1521349847316742
epoch£º190	 i:8 	 global-step:3808	 l-p:0.15095266699790955
epoch£º190	 i:9 	 global-step:3809	 l-p:0.15822972357273102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6779, 5.5110, 5.9398],
        [4.6779, 4.7166, 4.6880],
        [4.6779, 4.6779, 4.6779],
        [4.6779, 4.6779, 4.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.16519862413406372 
model_pd.l_d.mean(): -18.84339714050293 
model_pd.lagr.mean(): -18.678197860717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6520], device='cuda:0')), ('power', tensor([-19.7154], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.16519862413406372
epoch£º191	 i:1 	 global-step:3821	 l-p:0.20700892806053162
epoch£º191	 i:2 	 global-step:3822	 l-p:0.021258024498820305
epoch£º191	 i:3 	 global-step:3823	 l-p:0.1356019377708435
epoch£º191	 i:4 	 global-step:3824	 l-p:0.16084228456020355
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10942995548248291
epoch£º191	 i:6 	 global-step:3826	 l-p:0.1219610944390297
epoch£º191	 i:7 	 global-step:3827	 l-p:0.1422567218542099
epoch£º191	 i:8 	 global-step:3828	 l-p:0.15518589317798615
epoch£º191	 i:9 	 global-step:3829	 l-p:0.1296727955341339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9334, 4.9493, 4.9359],
        [4.9334, 4.9430, 4.9346],
        [4.9334, 5.7662, 6.1599],
        [4.9334, 4.9561, 4.9378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19204279780387878 
model_pd.l_d.mean(): -20.20284080505371 
model_pd.lagr.mean(): -20.01079750061035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9085], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19204279780387878
epoch£º192	 i:1 	 global-step:3841	 l-p:0.14703191816806793
epoch£º192	 i:2 	 global-step:3842	 l-p:0.4419727325439453
epoch£º192	 i:3 	 global-step:3843	 l-p:0.1288510262966156
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5389028191566467
epoch£º192	 i:5 	 global-step:3845	 l-p:0.1306793987751007
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14513461291790009
epoch£º192	 i:7 	 global-step:3847	 l-p:0.15570606291294098
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12739631533622742
epoch£º192	 i:9 	 global-step:3849	 l-p:0.13173779845237732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9372, 5.7056, 6.0369],
        [4.9372, 4.9514, 4.9393],
        [4.9372, 4.9388, 4.9373],
        [4.9372, 4.9373, 4.9372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.13625167310237885 
model_pd.l_d.mean(): -20.545116424560547 
model_pd.lagr.mean(): -20.408864974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.13625167310237885
epoch£º193	 i:1 	 global-step:3861	 l-p:0.13793109357357025
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05910323187708855
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.08904958516359329
epoch£º193	 i:4 	 global-step:3864	 l-p:0.1329261213541031
epoch£º193	 i:5 	 global-step:3865	 l-p:0.1466551423072815
epoch£º193	 i:6 	 global-step:3866	 l-p:0.13397790491580963
epoch£º193	 i:7 	 global-step:3867	 l-p:0.10348702222108841
epoch£º193	 i:8 	 global-step:3868	 l-p:0.15018315613269806
epoch£º193	 i:9 	 global-step:3869	 l-p:0.112197145819664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7177, 4.7177, 4.7177],
        [4.7177, 5.1899, 5.2911],
        [4.7177, 4.8765, 4.8254],
        [4.7177, 4.7415, 4.7220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.09619449079036713 
model_pd.l_d.mean(): -20.92303466796875 
model_pd.lagr.mean(): -20.826839447021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.6311], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.09619449079036713
epoch£º194	 i:1 	 global-step:3881	 l-p:0.1433783620595932
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.017178067937493324
epoch£º194	 i:3 	 global-step:3883	 l-p:0.1334563046693802
epoch£º194	 i:4 	 global-step:3884	 l-p:0.13586221635341644
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11681213974952698
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12126421928405762
epoch£º194	 i:7 	 global-step:3887	 l-p:0.14190353453159332
epoch£º194	 i:8 	 global-step:3888	 l-p:0.13041485846042633
epoch£º194	 i:9 	 global-step:3889	 l-p:1.2963404655456543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1024, 5.1715, 5.1289],
        [5.1024, 5.6085, 5.7044],
        [5.1024, 5.1048, 5.1026],
        [5.1024, 5.5111, 5.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11473134905099869 
model_pd.l_d.mean(): -20.305063247680664 
model_pd.lagr.mean(): -20.190332412719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4129], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11473134905099869
epoch£º195	 i:1 	 global-step:3901	 l-p:0.11911246180534363
epoch£º195	 i:2 	 global-step:3902	 l-p:0.12006862461566925
epoch£º195	 i:3 	 global-step:3903	 l-p:0.12040431797504425
epoch£º195	 i:4 	 global-step:3904	 l-p:0.16077303886413574
epoch£º195	 i:5 	 global-step:3905	 l-p:0.1342746764421463
epoch£º195	 i:6 	 global-step:3906	 l-p:0.1420297920703888
epoch£º195	 i:7 	 global-step:3907	 l-p:0.22426585853099823
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11449350416660309
epoch£º195	 i:9 	 global-step:3909	 l-p:0.1071726381778717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7942, 4.7942, 4.7942],
        [4.7942, 4.7985, 4.7945],
        [4.7942, 4.7943, 4.7942],
        [4.7942, 4.7973, 4.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.13742241263389587 
model_pd.l_d.mean(): -19.624361038208008 
model_pd.lagr.mean(): -19.4869384765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5739], device='cuda:0')), ('power', tensor([-20.4252], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.13742241263389587
epoch£º196	 i:1 	 global-step:3921	 l-p:0.09884578734636307
epoch£º196	 i:2 	 global-step:3922	 l-p:0.08997783809900284
epoch£º196	 i:3 	 global-step:3923	 l-p:0.09522882103919983
epoch£º196	 i:4 	 global-step:3924	 l-p:0.13065598905086517
epoch£º196	 i:5 	 global-step:3925	 l-p:0.1557665467262268
epoch£º196	 i:6 	 global-step:3926	 l-p:0.12408986687660217
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12654796242713928
epoch£º196	 i:8 	 global-step:3928	 l-p:0.13584575057029724
epoch£º196	 i:9 	 global-step:3929	 l-p:0.12398765236139297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9352, 4.9355, 4.9352],
        [4.9352, 5.3408, 5.3815],
        [4.9352, 4.9352, 4.9352],
        [4.9352, 4.9481, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.18536078929901123 
model_pd.l_d.mean(): -19.068532943725586 
model_pd.lagr.mean(): -18.8831729888916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-19.8258], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.18536078929901123
epoch£º197	 i:1 	 global-step:3941	 l-p:0.19378644227981567
epoch£º197	 i:2 	 global-step:3942	 l-p:0.15182681381702423
epoch£º197	 i:3 	 global-step:3943	 l-p:0.1136619821190834
epoch£º197	 i:4 	 global-step:3944	 l-p:0.0007798051810823381
epoch£º197	 i:5 	 global-step:3945	 l-p:0.1342521607875824
epoch£º197	 i:6 	 global-step:3946	 l-p:0.12888161838054657
epoch£º197	 i:7 	 global-step:3947	 l-p:0.1332276463508606
epoch£º197	 i:8 	 global-step:3948	 l-p:0.13209551572799683
epoch£º197	 i:9 	 global-step:3949	 l-p:0.12072528898715973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9714, 4.9909, 4.9746],
        [4.9714, 4.9715, 4.9714],
        [4.9714, 4.9714, 4.9714],
        [4.9714, 4.9721, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1278945654630661 
model_pd.l_d.mean(): -20.525814056396484 
model_pd.lagr.mean(): -20.397918701171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.1875], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1278945654630661
epoch£º198	 i:1 	 global-step:3961	 l-p:0.13780787587165833
epoch£º198	 i:2 	 global-step:3962	 l-p:0.1496916562318802
epoch£º198	 i:3 	 global-step:3963	 l-p:0.1229056790471077
epoch£º198	 i:4 	 global-step:3964	 l-p:0.10888571292161942
epoch£º198	 i:5 	 global-step:3965	 l-p:0.12790855765342712
epoch£º198	 i:6 	 global-step:3966	 l-p:0.14037789404392242
epoch£º198	 i:7 	 global-step:3967	 l-p:0.2756514549255371
epoch£º198	 i:8 	 global-step:3968	 l-p:0.1413409560918808
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12410161644220352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9916, 4.9923, 4.9916],
        [4.9916, 5.0016, 4.9927],
        [4.9916, 4.9916, 4.9916],
        [4.9916, 5.1046, 5.0508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.1656627207994461 
model_pd.l_d.mean(): -19.046628952026367 
model_pd.lagr.mean(): -18.880966186523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-19.7598], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.1656627207994461
epoch£º199	 i:1 	 global-step:3981	 l-p:0.11751680821180344
epoch£º199	 i:2 	 global-step:3982	 l-p:0.12080390006303787
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12708234786987305
epoch£º199	 i:4 	 global-step:3984	 l-p:0.14025898277759552
epoch£º199	 i:5 	 global-step:3985	 l-p:0.17372258007526398
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11406750231981277
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11667338758707047
epoch£º199	 i:8 	 global-step:3988	 l-p:0.11265379935503006
epoch£º199	 i:9 	 global-step:3989	 l-p:0.15627790987491608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0232, 5.0232, 5.0232],
        [5.0232, 5.0235, 5.0232],
        [5.0232, 5.0349, 5.0246],
        [5.0232, 5.3584, 5.3552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.14448122680187225 
model_pd.l_d.mean(): -20.526086807250977 
model_pd.lagr.mean(): -20.38160514831543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4162], device='cuda:0')), ('power', tensor([-21.1756], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.14448122680187225
epoch£º200	 i:1 	 global-step:4001	 l-p:0.13946770131587982
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12589047849178314
epoch£º200	 i:3 	 global-step:4003	 l-p:0.19323569536209106
epoch£º200	 i:4 	 global-step:4004	 l-p:0.13263148069381714
epoch£º200	 i:5 	 global-step:4005	 l-p:0.2451111227273941
epoch£º200	 i:6 	 global-step:4006	 l-p:0.1581098735332489
epoch£º200	 i:7 	 global-step:4007	 l-p:0.15053437650203705
epoch£º200	 i:8 	 global-step:4008	 l-p:0.11471311002969742
epoch£º200	 i:9 	 global-step:4009	 l-p:0.717486560344696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9198, 4.9199, 4.9198],
        [4.9198, 5.3880, 5.4722],
        [4.9198, 4.9198, 4.9198],
        [4.9198, 5.0090, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.361032098531723 
model_pd.l_d.mean(): -18.556886672973633 
model_pd.lagr.mean(): -18.19585418701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5605], device='cuda:0')), ('power', tensor([-19.3323], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.361032098531723
epoch£º201	 i:1 	 global-step:4021	 l-p:0.10966193675994873
epoch£º201	 i:2 	 global-step:4022	 l-p:0.08151102066040039
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13063311576843262
epoch£º201	 i:4 	 global-step:4024	 l-p:0.1491030901670456
epoch£º201	 i:5 	 global-step:4025	 l-p:0.13450603187084198
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12256721407175064
epoch£º201	 i:7 	 global-step:4027	 l-p:0.1351860910654068
epoch£º201	 i:8 	 global-step:4028	 l-p:0.12193650752305984
epoch£º201	 i:9 	 global-step:4029	 l-p:0.14950476586818695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0117, 5.0119, 5.0117],
        [5.0117, 5.0118, 5.0117],
        [5.0117, 5.1000, 5.0507],
        [5.0117, 5.0242, 5.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.13338623940944672 
model_pd.l_d.mean(): -20.060258865356445 
model_pd.lagr.mean(): -19.92687225341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.7404], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.13338623940944672
epoch£º202	 i:1 	 global-step:4041	 l-p:0.14931628108024597
epoch£º202	 i:2 	 global-step:4042	 l-p:0.13127833604812622
epoch£º202	 i:3 	 global-step:4043	 l-p:0.15026234090328217
epoch£º202	 i:4 	 global-step:4044	 l-p:0.19718913733959198
epoch£º202	 i:5 	 global-step:4045	 l-p:0.14595429599285126
epoch£º202	 i:6 	 global-step:4046	 l-p:0.13242989778518677
epoch£º202	 i:7 	 global-step:4047	 l-p:0.13370274007320404
epoch£º202	 i:8 	 global-step:4048	 l-p:0.03856945037841797
epoch£º202	 i:9 	 global-step:4049	 l-p:0.210481196641922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.3476, 5.4806],
        [4.8243, 4.9077, 4.8599],
        [4.8243, 4.8753, 4.8393],
        [4.8243, 4.8244, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.06953843683004379 
model_pd.l_d.mean(): -18.92906951904297 
model_pd.lagr.mean(): -18.85953140258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-19.6923], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.06953843683004379
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10468467324972153
epoch£º203	 i:2 	 global-step:4062	 l-p:0.1091863289475441
epoch£º203	 i:3 	 global-step:4063	 l-p:0.13938434422016144
epoch£º203	 i:4 	 global-step:4064	 l-p:-0.014000091701745987
epoch£º203	 i:5 	 global-step:4065	 l-p:0.18800343573093414
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13480132818222046
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13844068348407745
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12696468830108643
epoch£º203	 i:9 	 global-step:4069	 l-p:0.1347767412662506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9546, 5.4648, 5.5770],
        [4.9546, 4.9546, 4.9546],
        [4.9546, 5.4553, 5.5605],
        [4.9546, 5.0616, 5.0085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.1228165403008461 
model_pd.l_d.mean(): -20.3723201751709 
model_pd.lagr.mean(): -20.24950408935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0676], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.1228165403008461
epoch£º204	 i:1 	 global-step:4081	 l-p:0.15242204070091248
epoch£º204	 i:2 	 global-step:4082	 l-p:0.30758342146873474
epoch£º204	 i:3 	 global-step:4083	 l-p:0.31289711594581604
epoch£º204	 i:4 	 global-step:4084	 l-p:0.14396800100803375
epoch£º204	 i:5 	 global-step:4085	 l-p:0.10360700637102127
epoch£º204	 i:6 	 global-step:4086	 l-p:0.1427801251411438
epoch£º204	 i:7 	 global-step:4087	 l-p:0.1495489776134491
epoch£º204	 i:8 	 global-step:4088	 l-p:0.1427629590034485
epoch£º204	 i:9 	 global-step:4089	 l-p:0.1335955709218979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9503, 5.0170, 4.9744],
        [4.9503, 4.9503, 4.9503],
        [4.9503, 4.9504, 4.9503],
        [4.9503, 4.9503, 4.9503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.18840983510017395 
model_pd.l_d.mean(): -19.691621780395508 
model_pd.lagr.mean(): -19.503211975097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-20.4236], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.18840983510017395
epoch£º205	 i:1 	 global-step:4101	 l-p:0.1366007924079895
epoch£º205	 i:2 	 global-step:4102	 l-p:0.1250511109828949
epoch£º205	 i:3 	 global-step:4103	 l-p:0.1580962836742401
epoch£º205	 i:4 	 global-step:4104	 l-p:0.1534835398197174
epoch£º205	 i:5 	 global-step:4105	 l-p:0.0731092020869255
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14254049956798553
epoch£º205	 i:7 	 global-step:4107	 l-p:0.17050346732139587
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11225216835737228
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12313748151063919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 5.0670, 5.0377],
        [5.0280, 5.9752, 6.4747],
        [5.0280, 5.5954, 5.7456],
        [5.0280, 5.0280, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13938215374946594 
model_pd.l_d.mean(): -20.223487854003906 
model_pd.lagr.mean(): -20.0841064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-20.9178], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13938215374946594
epoch£º206	 i:1 	 global-step:4121	 l-p:0.10292953252792358
epoch£º206	 i:2 	 global-step:4122	 l-p:0.12199046462774277
epoch£º206	 i:3 	 global-step:4123	 l-p:0.15110360085964203
epoch£º206	 i:4 	 global-step:4124	 l-p:0.1149538904428482
epoch£º206	 i:5 	 global-step:4125	 l-p:0.16779910027980804
epoch£º206	 i:6 	 global-step:4126	 l-p:0.1360919326543808
epoch£º206	 i:7 	 global-step:4127	 l-p:0.11340564489364624
epoch£º206	 i:8 	 global-step:4128	 l-p:0.17041032016277313
epoch£º206	 i:9 	 global-step:4129	 l-p:0.12894868850708008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9938, 4.9758],
        [4.9725, 4.9725, 4.9725],
        [4.9725, 5.2571, 5.2334],
        [4.9725, 5.1429, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.13198596239089966 
model_pd.l_d.mean(): -19.841346740722656 
model_pd.lagr.mean(): -19.709360122680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5270], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.13198596239089966
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12597036361694336
epoch£º207	 i:2 	 global-step:4142	 l-p:0.20229415595531464
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16058671474456787
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16470488905906677
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1359064280986786
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12252607196569443
epoch£º207	 i:7 	 global-step:4147	 l-p:0.57097989320755
epoch£º207	 i:8 	 global-step:4148	 l-p:0.12738516926765442
epoch£º207	 i:9 	 global-step:4149	 l-p:0.1348654329776764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.8915, 4.8914],
        [4.8914, 5.3771, 5.4766],
        [4.8914, 5.0205, 4.9652],
        [4.8914, 4.8914, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.13262830674648285 
model_pd.l_d.mean(): -20.86733055114746 
model_pd.lagr.mean(): -20.734703063964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.5234], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.13262830674648285
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.15350981056690216
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12531021237373352
epoch£º208	 i:3 	 global-step:4163	 l-p:0.12909331917762756
epoch£º208	 i:4 	 global-step:4164	 l-p:0.13654330372810364
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14291957020759583
epoch£º208	 i:6 	 global-step:4166	 l-p:0.18740428984165192
epoch£º208	 i:7 	 global-step:4167	 l-p:0.09754987806081772
epoch£º208	 i:8 	 global-step:4168	 l-p:0.21020270884037018
epoch£º208	 i:9 	 global-step:4169	 l-p:0.15456394851207733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8773, 5.2160, 5.2225],
        [4.8773, 5.8038, 6.3035],
        [4.8773, 4.9276, 4.8916],
        [4.8773, 5.1284, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.08327023684978485 
model_pd.l_d.mean(): -20.242319107055664 
model_pd.lagr.mean(): -20.325590133666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.9817], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.08327023684978485
epoch£º209	 i:1 	 global-step:4181	 l-p:0.1365913301706314
epoch£º209	 i:2 	 global-step:4182	 l-p:0.12858904898166656
epoch£º209	 i:3 	 global-step:4183	 l-p:0.14129063487052917
epoch£º209	 i:4 	 global-step:4184	 l-p:0.142800971865654
epoch£º209	 i:5 	 global-step:4185	 l-p:0.10170011222362518
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13482585549354553
epoch£º209	 i:7 	 global-step:4187	 l-p:0.20046183466911316
epoch£º209	 i:8 	 global-step:4188	 l-p:0.15680158138275146
epoch£º209	 i:9 	 global-step:4189	 l-p:0.12428604811429977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0026, 5.0026, 5.0026],
        [5.0026, 5.0026, 5.0026],
        [5.0026, 5.1574, 5.1009],
        [5.0026, 5.0026, 5.0026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.17077529430389404 
model_pd.l_d.mean(): -20.62519073486328 
model_pd.lagr.mean(): -20.454416275024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.2784], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.17077529430389404
epoch£º210	 i:1 	 global-step:4201	 l-p:0.1386323869228363
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14797717332839966
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12521763145923615
epoch£º210	 i:4 	 global-step:4204	 l-p:0.13508330285549164
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12279415875673294
epoch£º210	 i:6 	 global-step:4206	 l-p:0.11828546226024628
epoch£º210	 i:7 	 global-step:4207	 l-p:0.12348449230194092
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.3307308852672577
epoch£º210	 i:9 	 global-step:4209	 l-p:0.12624770402908325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0662, 5.4720, 5.5063],
        [5.0662, 5.0663, 5.0662],
        [5.0662, 5.2085, 5.1511],
        [5.0662, 5.0663, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.13839147984981537 
model_pd.l_d.mean(): -19.563905715942383 
model_pd.lagr.mean(): -19.425514221191406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4469], device='cuda:0')), ('power', tensor([-20.2343], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.13839147984981537
epoch£º211	 i:1 	 global-step:4221	 l-p:0.1270947903394699
epoch£º211	 i:2 	 global-step:4222	 l-p:0.1505223959684372
epoch£º211	 i:3 	 global-step:4223	 l-p:0.17180880904197693
epoch£º211	 i:4 	 global-step:4224	 l-p:0.1250763088464737
epoch£º211	 i:5 	 global-step:4225	 l-p:0.15359468758106232
epoch£º211	 i:6 	 global-step:4226	 l-p:0.10608983784914017
epoch£º211	 i:7 	 global-step:4227	 l-p:0.11786418408155441
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12420106679201126
epoch£º211	 i:9 	 global-step:4229	 l-p:0.13257552683353424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9751, 5.4480, 5.5324],
        [4.9751, 4.9752, 4.9751],
        [4.9751, 4.9751, 4.9751],
        [4.9751, 5.4175, 5.4810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1375962197780609 
model_pd.l_d.mean(): -20.943103790283203 
model_pd.lagr.mean(): -20.80550765991211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3865], device='cuda:0')), ('power', tensor([-21.5668], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1375962197780609
epoch£º212	 i:1 	 global-step:4241	 l-p:0.11819587647914886
epoch£º212	 i:2 	 global-step:4242	 l-p:0.1440625935792923
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12057095021009445
epoch£º212	 i:4 	 global-step:4244	 l-p:0.24620936810970306
epoch£º212	 i:5 	 global-step:4245	 l-p:0.15063680708408356
epoch£º212	 i:6 	 global-step:4246	 l-p:0.11993889510631561
epoch£º212	 i:7 	 global-step:4247	 l-p:0.1851438730955124
epoch£º212	 i:8 	 global-step:4248	 l-p:-0.9997193813323975
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10819881409406662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7771, 4.7776, 4.7771],
        [4.7771, 4.7771, 4.7771],
        [4.7771, 4.9229, 4.8684],
        [4.7771, 4.7839, 4.7772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12231418490409851 
model_pd.l_d.mean(): -19.526752471923828 
model_pd.lagr.mean(): -19.404438018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5788], device='cuda:0')), ('power', tensor([-20.3314], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12231418490409851
epoch£º213	 i:1 	 global-step:4261	 l-p:0.13258600234985352
epoch£º213	 i:2 	 global-step:4262	 l-p:0.1391778141260147
epoch£º213	 i:3 	 global-step:4263	 l-p:0.13414977490901947
epoch£º213	 i:4 	 global-step:4264	 l-p:0.020560383796691895
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14378011226654053
epoch£º213	 i:6 	 global-step:4266	 l-p:0.13463257253170013
epoch£º213	 i:7 	 global-step:4267	 l-p:0.11960111558437347
epoch£º213	 i:8 	 global-step:4268	 l-p:0.44868898391723633
epoch£º213	 i:9 	 global-step:4269	 l-p:0.09651436656713486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8272, 4.8272, 4.8272],
        [4.8272, 4.8527, 4.8307],
        [4.8272, 4.9692, 4.9139],
        [4.8272, 4.8272, 4.8272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.1402536928653717 
model_pd.l_d.mean(): -20.610637664794922 
model_pd.lagr.mean(): -20.47038459777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-21.3137], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.1402536928653717
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12439719587564468
epoch£º214	 i:2 	 global-step:4282	 l-p:0.09173997491598129
epoch£º214	 i:3 	 global-step:4283	 l-p:0.18617068231105804
epoch£º214	 i:4 	 global-step:4284	 l-p:0.13782456517219543
epoch£º214	 i:5 	 global-step:4285	 l-p:0.3247986137866974
epoch£º214	 i:6 	 global-step:4286	 l-p:0.17262086272239685
epoch£º214	 i:7 	 global-step:4287	 l-p:0.4606030285358429
epoch£º214	 i:8 	 global-step:4288	 l-p:0.14248992502689362
epoch£º214	 i:9 	 global-step:4289	 l-p:0.18539302051067352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9984, 5.1568, 5.1003],
        [4.9984, 4.9987, 4.9984],
        [4.9984, 5.0104, 4.9995],
        [4.9984, 4.9985, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.15978896617889404 
model_pd.l_d.mean(): -20.523405075073242 
model_pd.lagr.mean(): -20.363616943359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.15978896617889404
epoch£º215	 i:1 	 global-step:4301	 l-p:0.12997782230377197
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14428241550922394
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11818300187587738
epoch£º215	 i:4 	 global-step:4304	 l-p:4.032661437988281
epoch£º215	 i:5 	 global-step:4305	 l-p:0.12549911439418793
epoch£º215	 i:6 	 global-step:4306	 l-p:0.13928058743476868
epoch£º215	 i:7 	 global-step:4307	 l-p:0.11865135282278061
epoch£º215	 i:8 	 global-step:4308	 l-p:0.12832660973072052
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11150582879781723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0823, 5.0830, 5.0823],
        [5.0823, 5.4354, 5.4393],
        [5.0823, 5.0988, 5.0844],
        [5.0823, 5.0823, 5.0823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11852333694696426 
model_pd.l_d.mean(): -19.117694854736328 
model_pd.lagr.mean(): -18.99917221069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-19.7967], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11852333694696426
epoch£º216	 i:1 	 global-step:4321	 l-p:0.1184157282114029
epoch£º216	 i:2 	 global-step:4322	 l-p:0.1572607308626175
epoch£º216	 i:3 	 global-step:4323	 l-p:0.15006950497627258
epoch£º216	 i:4 	 global-step:4324	 l-p:0.15143996477127075
epoch£º216	 i:5 	 global-step:4325	 l-p:0.18878339231014252
epoch£º216	 i:6 	 global-step:4326	 l-p:0.08566860109567642
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12580442428588867
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12295976281166077
epoch£º216	 i:9 	 global-step:4329	 l-p:0.1565510779619217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9541, 4.9626, 4.9545],
        [4.9541, 5.3954, 5.4601],
        [4.9541, 5.1409, 5.0883],
        [4.9541, 4.9541, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.13429395854473114 
model_pd.l_d.mean(): -20.628311157226562 
model_pd.lagr.mean(): -20.494016647338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.2842], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.13429395854473114
epoch£º217	 i:1 	 global-step:4341	 l-p:0.10390480607748032
epoch£º217	 i:2 	 global-step:4342	 l-p:0.2481996864080429
epoch£º217	 i:3 	 global-step:4343	 l-p:0.1598528027534485
epoch£º217	 i:4 	 global-step:4344	 l-p:0.1726737767457962
epoch£º217	 i:5 	 global-step:4345	 l-p:0.1297895610332489
epoch£º217	 i:6 	 global-step:4346	 l-p:0.13334986567497253
epoch£º217	 i:7 	 global-step:4347	 l-p:0.287411630153656
epoch£º217	 i:8 	 global-step:4348	 l-p:0.2138504534959793
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13903675973415375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9595, 4.9619, 4.9596],
        [4.9595, 4.9595, 4.9595],
        [4.9595, 4.9596, 4.9595],
        [4.9595, 4.9595, 4.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12264802306890488 
model_pd.l_d.mean(): -20.660396575927734 
model_pd.lagr.mean(): -20.537748336791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.3169], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12264802306890488
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13353753089904785
epoch£º218	 i:2 	 global-step:4362	 l-p:0.13659343123435974
epoch£º218	 i:3 	 global-step:4363	 l-p:0.17739234864711761
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12672853469848633
epoch£º218	 i:5 	 global-step:4365	 l-p:0.25687503814697266
epoch£º218	 i:6 	 global-step:4366	 l-p:0.6217791438102722
epoch£º218	 i:7 	 global-step:4367	 l-p:0.1259741485118866
epoch£º218	 i:8 	 global-step:4368	 l-p:0.19697383046150208
epoch£º218	 i:9 	 global-step:4369	 l-p:0.08987144380807877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0179, 5.0223, 5.0181],
        [5.0179, 5.0281, 5.0187],
        [5.0179, 5.1762, 5.1192],
        [5.0179, 6.0427, 6.6260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11964491754770279 
model_pd.l_d.mean(): -18.856182098388672 
model_pd.lagr.mean(): -18.73653793334961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-19.5765], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.11964491754770279
epoch£º219	 i:1 	 global-step:4381	 l-p:0.04286105930805206
epoch£º219	 i:2 	 global-step:4382	 l-p:0.13764968514442444
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11595001816749573
epoch£º219	 i:4 	 global-step:4384	 l-p:0.15602543950080872
epoch£º219	 i:5 	 global-step:4385	 l-p:0.11611171811819077
epoch£º219	 i:6 	 global-step:4386	 l-p:0.14327993988990784
epoch£º219	 i:7 	 global-step:4387	 l-p:0.12767747044563293
epoch£º219	 i:8 	 global-step:4388	 l-p:0.11719177663326263
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13013799488544464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0530, 5.0532, 5.0530],
        [5.0530, 5.1161, 5.0739],
        [5.0530, 5.0530, 5.0530],
        [5.0530, 5.0650, 5.0540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.14565938711166382 
model_pd.l_d.mean(): -20.34610939025879 
model_pd.lagr.mean(): -20.200450897216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.0001], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.14565938711166382
epoch£º220	 i:1 	 global-step:4401	 l-p:0.12677909433841705
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10777465254068375
epoch£º220	 i:3 	 global-step:4403	 l-p:0.13951627910137177
epoch£º220	 i:4 	 global-step:4404	 l-p:0.1490018367767334
epoch£º220	 i:5 	 global-step:4405	 l-p:0.15217426419258118
epoch£º220	 i:6 	 global-step:4406	 l-p:0.055697545409202576
epoch£º220	 i:7 	 global-step:4407	 l-p:0.14174890518188477
epoch£º220	 i:8 	 global-step:4408	 l-p:0.15822972357273102
epoch£º220	 i:9 	 global-step:4409	 l-p:0.1670147329568863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7709, 5.1912, 5.2546],
        [4.7709, 5.0291, 5.0021],
        [4.7709, 4.8660, 4.8135],
        [4.7709, 4.7710, 4.7709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.16124214231967926 
model_pd.l_d.mean(): -20.192171096801758 
model_pd.lagr.mean(): -20.030929565429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5210], device='cuda:0')), ('power', tensor([-20.9451], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.16124214231967926
epoch£º221	 i:1 	 global-step:4421	 l-p:0.11815331876277924
epoch£º221	 i:2 	 global-step:4422	 l-p:0.14224332571029663
epoch£º221	 i:3 	 global-step:4423	 l-p:0.1549811065196991
epoch£º221	 i:4 	 global-step:4424	 l-p:0.07716153562068939
epoch£º221	 i:5 	 global-step:4425	 l-p:0.06170930713415146
epoch£º221	 i:6 	 global-step:4426	 l-p:0.12762334942817688
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13835453987121582
epoch£º221	 i:8 	 global-step:4428	 l-p:0.17248278856277466
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13362887501716614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8986, 5.8886, 6.4529],
        [4.8986, 5.4195, 5.5461],
        [4.8986, 5.8448, 6.3632],
        [4.8986, 5.3769, 5.4718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.11896146833896637 
model_pd.l_d.mean(): -19.598194122314453 
model_pd.lagr.mean(): -19.479232788085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5572], device='cuda:0')), ('power', tensor([-20.3816], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.11896146833896637
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11686696857213974
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14638416469097137
epoch£º222	 i:3 	 global-step:4443	 l-p:0.14744828641414642
epoch£º222	 i:4 	 global-step:4444	 l-p:0.12878502905368805
epoch£º222	 i:5 	 global-step:4445	 l-p:0.13542957603931427
epoch£º222	 i:6 	 global-step:4446	 l-p:0.20720264315605164
epoch£º222	 i:7 	 global-step:4447	 l-p:0.09533126652240753
epoch£º222	 i:8 	 global-step:4448	 l-p:0.058538034558296204
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.1065630316734314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 4.8278, 4.8277],
        [4.8277, 4.9321, 4.8779],
        [4.8277, 5.0143, 4.9636],
        [4.8277, 4.8359, 4.8275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.25726696848869324 
model_pd.l_d.mean(): -20.74207878112793 
model_pd.lagr.mean(): -20.484811782836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.4427], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.25726696848869324
epoch£º223	 i:1 	 global-step:4461	 l-p:0.13924528658390045
epoch£º223	 i:2 	 global-step:4462	 l-p:0.13327983021736145
epoch£º223	 i:3 	 global-step:4463	 l-p:0.11807190626859665
epoch£º223	 i:4 	 global-step:4464	 l-p:0.12109046429395676
epoch£º223	 i:5 	 global-step:4465	 l-p:0.1960722953081131
epoch£º223	 i:6 	 global-step:4466	 l-p:0.2830362617969513
epoch£º223	 i:7 	 global-step:4467	 l-p:0.23461386561393738
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13591642677783966
epoch£º223	 i:9 	 global-step:4469	 l-p:0.16689623892307281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0357, 5.0357],
        [5.0357, 5.0357, 5.0357],
        [5.0357, 5.0660, 5.0410],
        [5.0357, 5.0364, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.1221175342798233 
model_pd.l_d.mean(): -20.5737361907959 
model_pd.lagr.mean(): -20.451618194580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.2065], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.1221175342798233
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12359865754842758
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13439127802848816
epoch£º224	 i:3 	 global-step:4483	 l-p:0.14683590829372406
epoch£º224	 i:4 	 global-step:4484	 l-p:0.14201276004314423
epoch£º224	 i:5 	 global-step:4485	 l-p:0.07547363638877869
epoch£º224	 i:6 	 global-step:4486	 l-p:0.15510499477386475
epoch£º224	 i:7 	 global-step:4487	 l-p:0.15427950024604797
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10856914520263672
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12164623290300369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.2218, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.1165756955742836 
model_pd.l_d.mean(): -19.738494873046875 
model_pd.lagr.mean(): -19.621919631958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400], device='cuda:0')), ('power', tensor([-20.4037], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.1165756955742836
epoch£º225	 i:1 	 global-step:4501	 l-p:0.013085088692605495
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12361612170934677
epoch£º225	 i:3 	 global-step:4503	 l-p:0.17280294001102448
epoch£º225	 i:4 	 global-step:4504	 l-p:0.1201712042093277
epoch£º225	 i:5 	 global-step:4505	 l-p:0.1726236343383789
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11893828958272934
epoch£º225	 i:7 	 global-step:4507	 l-p:0.12538428604602814
epoch£º225	 i:8 	 global-step:4508	 l-p:0.16521748900413513
epoch£º225	 i:9 	 global-step:4509	 l-p:0.12730863690376282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9764, 5.1360, 5.0790],
        [4.9764, 5.5084, 5.6376],
        [4.9764, 5.0413, 4.9975],
        [4.9764, 4.9764, 4.9764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12490057945251465 
model_pd.l_d.mean(): -20.594226837158203 
model_pd.lagr.mean(): -20.46932601928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12490057945251465
epoch£º226	 i:1 	 global-step:4521	 l-p:0.21627521514892578
epoch£º226	 i:2 	 global-step:4522	 l-p:0.093134306371212
epoch£º226	 i:3 	 global-step:4523	 l-p:0.23866818845272064
epoch£º226	 i:4 	 global-step:4524	 l-p:0.14849108457565308
epoch£º226	 i:5 	 global-step:4525	 l-p:0.1783486306667328
epoch£º226	 i:6 	 global-step:4526	 l-p:0.124575175344944
epoch£º226	 i:7 	 global-step:4527	 l-p:0.1136430874466896
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14238908886909485
epoch£º226	 i:9 	 global-step:4529	 l-p:0.19364972412586212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9906, 5.1062, 5.0497],
        [4.9906, 4.9916, 4.9906],
        [4.9906, 5.0125, 4.9931],
        [4.9906, 4.9907, 4.9906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1474979817867279 
model_pd.l_d.mean(): -19.997352600097656 
model_pd.lagr.mean(): -19.849855422973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-20.6807], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1474979817867279
epoch£º227	 i:1 	 global-step:4541	 l-p:0.11990810930728912
epoch£º227	 i:2 	 global-step:4542	 l-p:0.12921234965324402
epoch£º227	 i:3 	 global-step:4543	 l-p:0.16550907492637634
epoch£º227	 i:4 	 global-step:4544	 l-p:0.2112017720937729
epoch£º227	 i:5 	 global-step:4545	 l-p:0.20448756217956543
epoch£º227	 i:6 	 global-step:4546	 l-p:0.12588031589984894
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12425119429826736
epoch£º227	 i:8 	 global-step:4548	 l-p:0.12691959738731384
epoch£º227	 i:9 	 global-step:4549	 l-p:0.17028018832206726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0182, 5.0182, 5.0182],
        [5.0182, 5.0182, 5.0182],
        [5.0182, 5.0183, 5.0182],
        [5.0182, 5.0481, 5.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1316758096218109 
model_pd.l_d.mean(): -20.522876739501953 
model_pd.lagr.mean(): -20.39120101928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.1845], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1316758096218109
epoch£º228	 i:1 	 global-step:4561	 l-p:0.1289360225200653
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13833087682724
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1295636147260666
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09396020323038101
epoch£º228	 i:5 	 global-step:4565	 l-p:0.19356711208820343
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13425210118293762
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14209218323230743
epoch£º228	 i:8 	 global-step:4568	 l-p:0.17280524969100952
epoch£º228	 i:9 	 global-step:4569	 l-p:0.17881515622138977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9766, 4.9767, 4.9766],
        [4.9766, 5.5341, 5.6834],
        [4.9766, 4.9894, 4.9771],
        [4.9766, 4.9847, 4.9767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.13712596893310547 
model_pd.l_d.mean(): -18.6424503326416 
model_pd.lagr.mean(): -18.505325317382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5444], device='cuda:0')), ('power', tensor([-19.4024], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.13712596893310547
epoch£º229	 i:1 	 global-step:4581	 l-p:0.14046403765678406
epoch£º229	 i:2 	 global-step:4582	 l-p:0.15950174629688263
epoch£º229	 i:3 	 global-step:4583	 l-p:0.171462282538414
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11762484908103943
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13368239998817444
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.060093238949775696
epoch£º229	 i:7 	 global-step:4587	 l-p:0.121785469353199
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11237636208534241
epoch£º229	 i:9 	 global-step:4589	 l-p:0.12861546874046326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0868, 5.1082, 5.0893],
        [5.0868, 6.0698, 6.6006],
        [5.0868, 5.1167, 5.0917],
        [5.0868, 5.2927, 5.2404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.10758921504020691 
model_pd.l_d.mean(): -19.03952980041504 
model_pd.lagr.mean(): -18.93194007873535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-19.7501], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.10758921504020691
epoch£º230	 i:1 	 global-step:4601	 l-p:0.1321972906589508
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12211841344833374
epoch£º230	 i:3 	 global-step:4603	 l-p:0.15398000180721283
epoch£º230	 i:4 	 global-step:4604	 l-p:0.1698865443468094
epoch£º230	 i:5 	 global-step:4605	 l-p:0.13059619069099426
epoch£º230	 i:6 	 global-step:4606	 l-p:0.2363005131483078
epoch£º230	 i:7 	 global-step:4607	 l-p:0.15237024426460266
epoch£º230	 i:8 	 global-step:4608	 l-p:0.1411048024892807
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.10142365843057632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8814, 4.8814, 4.8814],
        [4.8814, 4.8814, 4.8814],
        [4.8814, 5.7612, 6.2150],
        [4.8814, 4.9288, 4.8918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12914973497390747 
model_pd.l_d.mean(): -20.139930725097656 
model_pd.lagr.mean(): -20.010780334472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.8803], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12914973497390747
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14236949384212494
epoch£º231	 i:2 	 global-step:4622	 l-p:0.12565454840660095
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13442744314670563
epoch£º231	 i:4 	 global-step:4624	 l-p:0.13840991258621216
epoch£º231	 i:5 	 global-step:4625	 l-p:0.2723212242126465
epoch£º231	 i:6 	 global-step:4626	 l-p:0.13639788329601288
epoch£º231	 i:7 	 global-step:4627	 l-p:0.0616842545568943
epoch£º231	 i:8 	 global-step:4628	 l-p:0.04674858599901199
epoch£º231	 i:9 	 global-step:4629	 l-p:0.16914039850234985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7763, 4.7763, 4.7763],
        [4.7763, 4.7983, 4.7768],
        [4.7763, 5.1426, 5.1713],
        [4.7763, 4.7769, 4.7761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12486302107572556 
model_pd.l_d.mean(): -20.680496215820312 
model_pd.lagr.mean(): -20.555633544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4879], device='cuda:0')), ('power', tensor([-21.4049], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12486302107572556
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18944701552391052
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05647556111216545
epoch£º232	 i:3 	 global-step:4643	 l-p:0.14071209728717804
epoch£º232	 i:4 	 global-step:4644	 l-p:0.14080996811389923
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13375315070152283
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14836184680461884
epoch£º232	 i:7 	 global-step:4647	 l-p:0.12391427904367447
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11937003582715988
epoch£º232	 i:9 	 global-step:4649	 l-p:0.12137877196073532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0947, 5.0614],
        [5.0523, 5.0705, 5.0538],
        [5.0523, 5.3567, 5.3387],
        [5.0523, 5.8851, 6.2702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.08481453359127045 
model_pd.l_d.mean(): -19.999101638793945 
model_pd.lagr.mean(): -19.914287567138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.7057], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.08481453359127045
epoch£º233	 i:1 	 global-step:4661	 l-p:0.13709695637226105
epoch£º233	 i:2 	 global-step:4662	 l-p:0.11049526184797287
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13289405405521393
epoch£º233	 i:4 	 global-step:4664	 l-p:0.13811808824539185
epoch£º233	 i:5 	 global-step:4665	 l-p:0.15475016832351685
epoch£º233	 i:6 	 global-step:4666	 l-p:0.14767825603485107
epoch£º233	 i:7 	 global-step:4667	 l-p:0.1353389322757721
epoch£º233	 i:8 	 global-step:4668	 l-p:0.12057846039533615
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12988708913326263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 4.9703, 4.9683],
        [4.9684, 4.9763, 4.9682],
        [4.9684, 4.9815, 4.9686],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12316152453422546 
model_pd.l_d.mean(): -20.02397918701172 
model_pd.lagr.mean(): -19.90081787109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.7472], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.12316152453422546
epoch£º234	 i:1 	 global-step:4681	 l-p:0.5408617854118347
epoch£º234	 i:2 	 global-step:4682	 l-p:0.1247706413269043
epoch£º234	 i:3 	 global-step:4683	 l-p:0.12900348007678986
epoch£º234	 i:4 	 global-step:4684	 l-p:0.19842368364334106
epoch£º234	 i:5 	 global-step:4685	 l-p:0.03698182478547096
epoch£º234	 i:6 	 global-step:4686	 l-p:0.13044427335262299
epoch£º234	 i:7 	 global-step:4687	 l-p:0.14490242302417755
epoch£º234	 i:8 	 global-step:4688	 l-p:0.13985241949558258
epoch£º234	 i:9 	 global-step:4689	 l-p:0.07263896614313126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7858, 5.4234, 5.6597],
        [4.7858, 4.7858, 4.7858],
        [4.7858, 4.7870, 4.7854],
        [4.7858, 4.7858, 4.7858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13790367543697357 
model_pd.l_d.mean(): -20.549694061279297 
model_pd.lagr.mean(): -20.41179084777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.2661], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13790367543697357
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.32121723890304565
epoch£º235	 i:2 	 global-step:4702	 l-p:0.134795144200325
epoch£º235	 i:3 	 global-step:4703	 l-p:0.0491146594285965
epoch£º235	 i:4 	 global-step:4704	 l-p:0.16355426609516144
epoch£º235	 i:5 	 global-step:4705	 l-p:0.17330214381217957
epoch£º235	 i:6 	 global-step:4706	 l-p:0.13064594566822052
epoch£º235	 i:7 	 global-step:4707	 l-p:0.14047281444072723
epoch£º235	 i:8 	 global-step:4708	 l-p:0.1235634982585907
epoch£º235	 i:9 	 global-step:4709	 l-p:0.12258073687553406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 5.1054, 5.0573],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 5.0316, 4.9734],
        [4.8974, 4.9192, 4.8983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13090677559375763 
model_pd.l_d.mean(): -20.494335174560547 
model_pd.lagr.mean(): -20.363428115844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13090677559375763
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13499030470848083
epoch£º236	 i:2 	 global-step:4722	 l-p:0.13498437404632568
epoch£º236	 i:3 	 global-step:4723	 l-p:0.1384630799293518
epoch£º236	 i:4 	 global-step:4724	 l-p:0.3356892764568329
epoch£º236	 i:5 	 global-step:4725	 l-p:0.11200132220983505
epoch£º236	 i:6 	 global-step:4726	 l-p:0.14923706650733948
epoch£º236	 i:7 	 global-step:4727	 l-p:0.114837147295475
epoch£º236	 i:8 	 global-step:4728	 l-p:0.1521124243736267
epoch£º236	 i:9 	 global-step:4729	 l-p:0.1276526004076004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8244, 5.5559, 5.8734],
        [4.8244, 4.9017, 4.8511],
        [4.8244, 4.8244, 4.8244],
        [4.8244, 4.9506, 4.8927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.05066237226128578 
model_pd.l_d.mean(): -19.581409454345703 
model_pd.lagr.mean(): -19.530746459960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5423], device='cuda:0')), ('power', tensor([-20.3494], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.05066237226128578
epoch£º237	 i:1 	 global-step:4741	 l-p:0.132891446352005
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.037890948355197906
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14687718451023102
epoch£º237	 i:4 	 global-step:4744	 l-p:0.12754470109939575
epoch£º237	 i:5 	 global-step:4745	 l-p:0.1874948889017105
epoch£º237	 i:6 	 global-step:4746	 l-p:0.12392433732748032
epoch£º237	 i:7 	 global-step:4747	 l-p:0.1450672447681427
epoch£º237	 i:8 	 global-step:4748	 l-p:0.13124237954616547
epoch£º237	 i:9 	 global-step:4749	 l-p:0.12021871656179428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9808, 5.8701, 6.3206],
        [4.9808, 5.0298, 4.9917],
        [4.9808, 5.4909, 5.6044],
        [4.9808, 4.9808, 4.9808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.13201995193958282 
model_pd.l_d.mean(): -17.288978576660156 
model_pd.lagr.mean(): -17.156959533691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6337], device='cuda:0')), ('power', tensor([-18.1253], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.13201995193958282
epoch£º238	 i:1 	 global-step:4761	 l-p:0.1971026510000229
epoch£º238	 i:2 	 global-step:4762	 l-p:0.12694190442562103
epoch£º238	 i:3 	 global-step:4763	 l-p:0.13314001262187958
epoch£º238	 i:4 	 global-step:4764	 l-p:0.1577100306749344
epoch£º238	 i:5 	 global-step:4765	 l-p:0.1993744969367981
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09317109733819962
epoch£º238	 i:7 	 global-step:4767	 l-p:0.18404920399188995
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12464641034603119
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12719707190990448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9628, 5.4570, 5.5601],
        [4.9628, 5.7587, 6.1201],
        [4.9628, 5.0487, 4.9961],
        [4.9628, 5.3190, 5.3318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.1583285629749298 
model_pd.l_d.mean(): -20.348209381103516 
model_pd.lagr.mean(): -20.18988037109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.0516], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.1583285629749298
epoch£º239	 i:1 	 global-step:4781	 l-p:0.1093016043305397
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12825092673301697
epoch£º239	 i:3 	 global-step:4783	 l-p:0.1361585259437561
epoch£º239	 i:4 	 global-step:4784	 l-p:-0.10023868083953857
epoch£º239	 i:5 	 global-step:4785	 l-p:0.13205258548259735
epoch£º239	 i:6 	 global-step:4786	 l-p:0.1219778060913086
epoch£º239	 i:7 	 global-step:4787	 l-p:0.15792521834373474
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11743409931659698
epoch£º239	 i:9 	 global-step:4789	 l-p:0.1035648062825203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7702, 5.0885, 5.0901],
        [4.7702, 4.8019, 4.7718],
        [4.7702, 4.8309, 4.7852],
        [4.7702, 4.7701, 4.7702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.043871019035577774 
model_pd.l_d.mean(): -20.714401245117188 
model_pd.lagr.mean(): -20.758272171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4213], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.043871019035577774
epoch£º240	 i:1 	 global-step:4801	 l-p:0.16233187913894653
epoch£º240	 i:2 	 global-step:4802	 l-p:0.14044147729873657
epoch£º240	 i:3 	 global-step:4803	 l-p:0.06483335047960281
epoch£º240	 i:4 	 global-step:4804	 l-p:0.13032954931259155
epoch£º240	 i:5 	 global-step:4805	 l-p:0.1270952969789505
epoch£º240	 i:6 	 global-step:4806	 l-p:0.145225390791893
epoch£º240	 i:7 	 global-step:4807	 l-p:0.15288949012756348
epoch£º240	 i:8 	 global-step:4808	 l-p:0.20118090510368347
epoch£º240	 i:9 	 global-step:4809	 l-p:0.008056554943323135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9236, 4.9732, 4.9339],
        [4.9236, 4.9236, 4.9236],
        [4.9236, 5.4266, 5.5395],
        [4.9236, 5.6785, 6.0061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.34427520632743835 
model_pd.l_d.mean(): -20.44830322265625 
model_pd.lagr.mean(): -20.104028701782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.1421], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.34427520632743835
epoch£º241	 i:1 	 global-step:4821	 l-p:0.3575403392314911
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11997587978839874
epoch£º241	 i:3 	 global-step:4823	 l-p:0.15540997684001923
epoch£º241	 i:4 	 global-step:4824	 l-p:0.134065642952919
epoch£º241	 i:5 	 global-step:4825	 l-p:0.12761811912059784
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11834121495485306
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11930766701698303
epoch£º241	 i:8 	 global-step:4828	 l-p:0.1336769461631775
epoch£º241	 i:9 	 global-step:4829	 l-p:0.07092534750699997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0557, 5.3535, 5.3325],
        [5.0557, 5.0731, 5.0564],
        [5.0557, 5.0557, 5.0557],
        [5.0557, 5.7604, 6.0273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.14042988419532776 
model_pd.l_d.mean(): -20.276094436645508 
model_pd.lagr.mean(): -20.135663986206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-20.9462], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.14042988419532776
epoch£º242	 i:1 	 global-step:4841	 l-p:0.1247742548584938
epoch£º242	 i:2 	 global-step:4842	 l-p:0.15747113525867462
epoch£º242	 i:3 	 global-step:4843	 l-p:0.1265910118818283
epoch£º242	 i:4 	 global-step:4844	 l-p:0.14922742545604706
epoch£º242	 i:5 	 global-step:4845	 l-p:0.1324802041053772
epoch£º242	 i:6 	 global-step:4846	 l-p:0.09520920366048813
epoch£º242	 i:7 	 global-step:4847	 l-p:0.13411468267440796
epoch£º242	 i:8 	 global-step:4848	 l-p:0.1246306374669075
epoch£º242	 i:9 	 global-step:4849	 l-p:0.26742956042289734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9689, 4.9941, 4.9703],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 5.5872, 5.7882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.1544085443019867 
model_pd.l_d.mean(): -20.061735153198242 
model_pd.lagr.mean(): -19.907325744628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.1544085443019867
epoch£º243	 i:1 	 global-step:4861	 l-p:0.34977760910987854
epoch£º243	 i:2 	 global-step:4862	 l-p:0.1294461339712143
epoch£º243	 i:3 	 global-step:4863	 l-p:0.13985824584960938
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12119196355342865
epoch£º243	 i:5 	 global-step:4865	 l-p:0.1184418722987175
epoch£º243	 i:6 	 global-step:4866	 l-p:0.195839062333107
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.09641890972852707
epoch£º243	 i:8 	 global-step:4868	 l-p:0.13140837848186493
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13165786862373352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8766, 4.8888, 4.8751],
        [4.8766, 5.1567, 5.1348],
        [4.8766, 4.9078, 4.8785],
        [4.8766, 5.4180, 5.5648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.05515187233686447 
model_pd.l_d.mean(): -20.49510383605957 
model_pd.lagr.mean(): -20.439952850341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.05515187233686447
epoch£º244	 i:1 	 global-step:4881	 l-p:0.13947953283786774
epoch£º244	 i:2 	 global-step:4882	 l-p:0.12288806587457657
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12926802039146423
epoch£º244	 i:4 	 global-step:4884	 l-p:0.1255134493112564
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.06132398173213005
epoch£º244	 i:6 	 global-step:4886	 l-p:0.13509579002857208
epoch£º244	 i:7 	 global-step:4887	 l-p:0.2219039797782898
epoch£º244	 i:8 	 global-step:4888	 l-p:0.08168118447065353
epoch£º244	 i:9 	 global-step:4889	 l-p:0.15735895931720734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.0081, 4.9492],
        [4.8648, 5.1495, 5.1301],
        [4.8648, 4.8648, 4.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.08363498747348785 
model_pd.l_d.mean(): -20.004287719726562 
model_pd.lagr.mean(): -19.920652389526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-20.7625], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.08363498747348785
epoch£º245	 i:1 	 global-step:4901	 l-p:0.21294616162776947
epoch£º245	 i:2 	 global-step:4902	 l-p:-0.3056994080543518
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14886541664600372
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11590764671564102
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13196063041687012
epoch£º245	 i:6 	 global-step:4906	 l-p:0.08010227233171463
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1683882474899292
epoch£º245	 i:8 	 global-step:4908	 l-p:0.1242818683385849
epoch£º245	 i:9 	 global-step:4909	 l-p:0.1234099492430687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 5.6221, 5.7100],
        [5.1325, 5.1326, 5.1325],
        [5.1325, 5.1848, 5.1450],
        [5.1325, 5.1325, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12045295536518097 
model_pd.l_d.mean(): -19.456945419311523 
model_pd.lagr.mean(): -19.33649253845215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-20.1510], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12045295536518097
epoch£º246	 i:1 	 global-step:4921	 l-p:0.12988348305225372
epoch£º246	 i:2 	 global-step:4922	 l-p:0.12733417749404907
epoch£º246	 i:3 	 global-step:4923	 l-p:0.1367647349834442
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13089288771152496
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12898260354995728
epoch£º246	 i:6 	 global-step:4926	 l-p:0.027831973508000374
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13892525434494019
epoch£º246	 i:8 	 global-step:4928	 l-p:0.15404349565505981
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12092164903879166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 5.0493, 5.0493],
        [5.0493, 5.0572, 5.0487],
        [5.0493, 5.1392, 5.0847],
        [5.0493, 5.0493, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.07592250406742096 
model_pd.l_d.mean(): -19.953645706176758 
model_pd.lagr.mean(): -19.877723693847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-20.6225], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.07592250406742096
epoch£º247	 i:1 	 global-step:4941	 l-p:0.13495208323001862
epoch£º247	 i:2 	 global-step:4942	 l-p:0.16997939348220825
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13954225182533264
epoch£º247	 i:4 	 global-step:4944	 l-p:0.1420285403728485
epoch£º247	 i:5 	 global-step:4945	 l-p:0.19282537698745728
epoch£º247	 i:6 	 global-step:4946	 l-p:0.1290719360113144
epoch£º247	 i:7 	 global-step:4947	 l-p:0.13003811240196228
epoch£º247	 i:8 	 global-step:4948	 l-p:0.11992993950843811
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11528994888067245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0460, 5.0460, 5.0460],
        [5.0460, 5.0460, 5.0460],
        [5.0460, 5.0530, 5.0452],
        [5.0460, 5.0460, 5.0460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.10971271246671677 
model_pd.l_d.mean(): -20.61448097229004 
model_pd.lagr.mean(): -20.50476837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.2619], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.10971271246671677
epoch£º248	 i:1 	 global-step:4961	 l-p:0.14007501304149628
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12368734180927277
epoch£º248	 i:3 	 global-step:4963	 l-p:0.17741844058036804
epoch£º248	 i:4 	 global-step:4964	 l-p:0.12761636078357697
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13680192828178406
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13287797570228577
epoch£º248	 i:7 	 global-step:4967	 l-p:0.14309774339199066
epoch£º248	 i:8 	 global-step:4968	 l-p:0.1222970187664032
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.15958324074745178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9006, 5.2959, 5.3360],
        [4.9006, 5.3443, 5.4166],
        [4.9006, 4.9669, 4.9184],
        [4.9006, 5.6388, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.006004981696605682 
model_pd.l_d.mean(): -20.6778507232666 
model_pd.lagr.mean(): -20.671846389770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.3645], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.006004981696605682
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15245768427848816
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12303941696882248
epoch£º249	 i:3 	 global-step:4983	 l-p:0.18176127970218658
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21216191351413727
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13354407250881195
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10747087001800537
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12473492324352264
epoch£º249	 i:8 	 global-step:4988	 l-p:0.15053898096084595
epoch£º249	 i:9 	 global-step:4989	 l-p:0.49437415599823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 5.1167, 5.0600],
        [4.9395, 4.9395, 4.9395],
        [4.9395, 4.9498, 4.9378],
        [4.9395, 4.9395, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16068343818187714 
model_pd.l_d.mean(): -19.521953582763672 
model_pd.lagr.mean(): -19.361270904541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.2635], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16068343818187714
epoch£º250	 i:1 	 global-step:5001	 l-p:0.16915486752986908
epoch£º250	 i:2 	 global-step:5002	 l-p:0.12794454395771027
epoch£º250	 i:3 	 global-step:5003	 l-p:0.14026600122451782
epoch£º250	 i:4 	 global-step:5004	 l-p:0.12827971577644348
epoch£º250	 i:5 	 global-step:5005	 l-p:0.12652768194675446
epoch£º250	 i:6 	 global-step:5006	 l-p:0.14376966655254364
epoch£º250	 i:7 	 global-step:5007	 l-p:0.12437160313129425
epoch£º250	 i:8 	 global-step:5008	 l-p:0.16293473541736603
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08772523701190948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.2101, 5.1496],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 5.1302, 5.0783],
        [5.0521, 5.0521, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18398120999336243 
model_pd.l_d.mean(): -20.15690040588379 
model_pd.lagr.mean(): -19.972919464111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.8441], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18398120999336243
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13463667035102844
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12298353016376495
epoch£º251	 i:3 	 global-step:5023	 l-p:0.119593046605587
epoch£º251	 i:4 	 global-step:5024	 l-p:0.14219477772712708
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11592628061771393
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07897401601076126
epoch£º251	 i:7 	 global-step:5027	 l-p:0.1258781999349594
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12721997499465942
epoch£º251	 i:9 	 global-step:5029	 l-p:0.1305529922246933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0204, 5.0204, 5.0204],
        [5.0204, 5.9338, 6.4041],
        [5.0204, 5.0947, 5.0437],
        [5.0204, 5.1837, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12824472784996033 
model_pd.l_d.mean(): -20.094636917114258 
model_pd.lagr.mean(): -19.966392517089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12824472784996033
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21430827677249908
epoch£º252	 i:2 	 global-step:5042	 l-p:0.1272055059671402
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10369459539651871
epoch£º252	 i:4 	 global-step:5044	 l-p:0.17751727998256683
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12636028230190277
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5342342853546143
epoch£º252	 i:7 	 global-step:5047	 l-p:0.13442137837409973
epoch£º252	 i:8 	 global-step:5048	 l-p:0.1303853839635849
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07687947899103165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8350, 5.6683, 6.0837],
        [4.8350, 4.9044, 4.8533],
        [4.8350, 4.8962, 4.8482],
        [4.8350, 5.2284, 5.2715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.13382339477539062 
model_pd.l_d.mean(): -20.601703643798828 
model_pd.lagr.mean(): -20.467880249023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.3105], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.13382339477539062
epoch£º253	 i:1 	 global-step:5061	 l-p:0.22801223397254944
epoch£º253	 i:2 	 global-step:5062	 l-p:0.11031002551317215
epoch£º253	 i:3 	 global-step:5063	 l-p:0.13835200667381287
epoch£º253	 i:4 	 global-step:5064	 l-p:0.07846666872501373
epoch£º253	 i:5 	 global-step:5065	 l-p:0.141651451587677
epoch£º253	 i:6 	 global-step:5066	 l-p:0.1315012127161026
epoch£º253	 i:7 	 global-step:5067	 l-p:0.23726558685302734
epoch£º253	 i:8 	 global-step:5068	 l-p:0.14332683384418488
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12494517862796783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0129, 5.0221, 5.0114],
        [5.0129, 5.0534, 5.0178],
        [5.0129, 5.0129, 5.0129],
        [5.0129, 5.0129, 5.0129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.1571776121854782 
model_pd.l_d.mean(): -20.20060157775879 
model_pd.lagr.mean(): -20.043424606323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4580], device='cuda:0')), ('power', tensor([-20.8892], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.1571776121854782
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09933776408433914
epoch£º254	 i:2 	 global-step:5082	 l-p:0.1426936537027359
epoch£º254	 i:3 	 global-step:5083	 l-p:0.1317450851202011
epoch£º254	 i:4 	 global-step:5084	 l-p:0.104396291077137
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11702980846166611
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11476317793130875
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14846359193325043
epoch£º254	 i:8 	 global-step:5088	 l-p:0.15059316158294678
epoch£º254	 i:9 	 global-step:5089	 l-p:0.1536436229944229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0551, 5.9604, 6.4184],
        [5.0551, 5.1890, 5.1274],
        [5.0551, 5.4995, 5.5621],
        [5.0551, 6.1158, 6.7342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.15712396800518036 
model_pd.l_d.mean(): -18.833473205566406 
model_pd.lagr.mean(): -18.676349639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-19.5653], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.15712396800518036
epoch£º255	 i:1 	 global-step:5101	 l-p:0.1156584620475769
epoch£º255	 i:2 	 global-step:5102	 l-p:0.0903138816356659
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12296794354915619
epoch£º255	 i:4 	 global-step:5104	 l-p:0.17294993996620178
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12644228339195251
epoch£º255	 i:6 	 global-step:5106	 l-p:0.11781316250562668
epoch£º255	 i:7 	 global-step:5107	 l-p:0.22616271674633026
epoch£º255	 i:8 	 global-step:5108	 l-p:0.1901894509792328
epoch£º255	 i:9 	 global-step:5109	 l-p:0.14323332905769348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9521, 4.9799, 4.9518],
        [4.9521, 4.9527, 4.9514],
        [4.9521, 4.9521, 4.9521],
        [4.9521, 5.3122, 5.3282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.14431211352348328 
model_pd.l_d.mean(): -20.630752563476562 
model_pd.lagr.mean(): -20.486440658569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4360], device='cuda:0')), ('power', tensor([-21.3016], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.14431211352348328
epoch£º256	 i:1 	 global-step:5121	 l-p:-0.9440106153488159
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13405422866344452
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13181281089782715
epoch£º256	 i:4 	 global-step:5124	 l-p:0.08653992414474487
epoch£º256	 i:5 	 global-step:5125	 l-p:0.13316769897937775
epoch£º256	 i:6 	 global-step:5126	 l-p:0.13760115206241608
epoch£º256	 i:7 	 global-step:5127	 l-p:0.2613994777202606
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12210580706596375
epoch£º256	 i:9 	 global-step:5129	 l-p:0.4280294179916382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8819, 4.8819, 4.8819],
        [4.8819, 4.8819, 4.8819],
        [4.8819, 5.0529, 4.9952],
        [4.8819, 4.8844, 4.8799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12437333166599274 
model_pd.l_d.mean(): -19.610490798950195 
model_pd.lagr.mean(): -19.48611831665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-20.3305], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12437333166599274
epoch£º257	 i:1 	 global-step:5141	 l-p:0.1405945122241974
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12482600659132004
epoch£º257	 i:3 	 global-step:5143	 l-p:0.07231539487838745
epoch£º257	 i:4 	 global-step:5144	 l-p:0.1736535280942917
epoch£º257	 i:5 	 global-step:5145	 l-p:0.1284247189760208
epoch£º257	 i:6 	 global-step:5146	 l-p:0.13022802770137787
epoch£º257	 i:7 	 global-step:5147	 l-p:0.17061324417591095
epoch£º257	 i:8 	 global-step:5148	 l-p:0.2837476432323456
epoch£º257	 i:9 	 global-step:5149	 l-p:0.13180243968963623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0316, 5.0314, 5.0315],
        [5.0316, 5.0618, 5.0325],
        [5.0316, 5.0316, 5.0316],
        [5.0316, 5.4108, 5.4337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12388347089290619 
model_pd.l_d.mean(): -20.08821678161621 
model_pd.lagr.mean(): -19.964332580566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-20.7780], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12388347089290619
epoch£º258	 i:1 	 global-step:5161	 l-p:0.12639537453651428
epoch£º258	 i:2 	 global-step:5162	 l-p:0.1824924200773239
epoch£º258	 i:3 	 global-step:5163	 l-p:0.10642774403095245
epoch£º258	 i:4 	 global-step:5164	 l-p:0.11888303607702255
epoch£º258	 i:5 	 global-step:5165	 l-p:0.16040372848510742
epoch£º258	 i:6 	 global-step:5166	 l-p:0.16648364067077637
epoch£º258	 i:7 	 global-step:5167	 l-p:0.12168427556753159
epoch£º258	 i:8 	 global-step:5168	 l-p:0.11846570670604706
epoch£º258	 i:9 	 global-step:5169	 l-p:0.1397102177143097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0139, 5.3883, 5.4093],
        [5.0139, 5.0139, 5.0139],
        [5.0139, 5.0139, 5.0139],
        [5.0139, 5.0818, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11530520766973495 
model_pd.l_d.mean(): -18.976093292236328 
model_pd.lagr.mean(): -18.860788345336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-19.6916], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11530520766973495
epoch£º259	 i:1 	 global-step:5181	 l-p:0.10178615152835846
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15412183105945587
epoch£º259	 i:3 	 global-step:5183	 l-p:0.1473277509212494
epoch£º259	 i:4 	 global-step:5184	 l-p:-1.2486571073532104
epoch£º259	 i:5 	 global-step:5185	 l-p:-0.004324054811149836
epoch£º259	 i:6 	 global-step:5186	 l-p:0.12288278341293335
epoch£º259	 i:7 	 global-step:5187	 l-p:0.1274573802947998
epoch£º259	 i:8 	 global-step:5188	 l-p:0.06421788781881332
epoch£º259	 i:9 	 global-step:5189	 l-p:0.15141315758228302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8631, 5.3761, 5.5031],
        [4.8631, 5.1153, 5.0813],
        [4.8631, 4.8631, 4.8631],
        [4.8631, 4.9315, 4.8794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.19934040307998657 
model_pd.l_d.mean(): -20.1546630859375 
model_pd.lagr.mean(): -19.955322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-20.9076], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.19934040307998657
epoch£º260	 i:1 	 global-step:5201	 l-p:0.07531403005123138
epoch£º260	 i:2 	 global-step:5202	 l-p:0.19311264157295227
epoch£º260	 i:3 	 global-step:5203	 l-p:0.11096982657909393
epoch£º260	 i:4 	 global-step:5204	 l-p:0.02752089872956276
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13702617585659027
epoch£º260	 i:6 	 global-step:5206	 l-p:0.13387218117713928
epoch£º260	 i:7 	 global-step:5207	 l-p:0.15562398731708527
epoch£º260	 i:8 	 global-step:5208	 l-p:0.3776289224624634
epoch£º260	 i:9 	 global-step:5209	 l-p:0.13026803731918335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9957, 4.9957, 4.9957],
        [4.9957, 5.3220, 5.3172],
        [4.9957, 5.3641, 5.3826],
        [4.9957, 5.0006, 4.9936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.18235209584236145 
model_pd.l_d.mean(): -20.067371368408203 
model_pd.lagr.mean(): -19.885019302368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-20.7446], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.18235209584236145
epoch£º261	 i:1 	 global-step:5221	 l-p:0.09613239020109177
epoch£º261	 i:2 	 global-step:5222	 l-p:0.15673241019248962
epoch£º261	 i:3 	 global-step:5223	 l-p:0.11706487089395523
epoch£º261	 i:4 	 global-step:5224	 l-p:0.15279310941696167
epoch£º261	 i:5 	 global-step:5225	 l-p:0.13665775954723358
epoch£º261	 i:6 	 global-step:5226	 l-p:0.12256024777889252
epoch£º261	 i:7 	 global-step:5227	 l-p:0.13429300487041473
epoch£º261	 i:8 	 global-step:5228	 l-p:0.1427994817495346
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13735903799533844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0402, 5.0404, 5.0397],
        [5.0402, 5.0503, 5.0380],
        [5.0402, 5.0399, 5.0401],
        [5.0402, 5.0400, 5.0401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.12423240393400192 
model_pd.l_d.mean(): -19.54498291015625 
model_pd.lagr.mean(): -19.42074966430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.2475], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.12423240393400192
epoch£º262	 i:1 	 global-step:5241	 l-p:0.13048619031906128
epoch£º262	 i:2 	 global-step:5242	 l-p:0.14180046319961548
epoch£º262	 i:3 	 global-step:5243	 l-p:-0.08904195576906204
epoch£º262	 i:4 	 global-step:5244	 l-p:0.07225224375724792
epoch£º262	 i:5 	 global-step:5245	 l-p:0.2847175598144531
epoch£º262	 i:6 	 global-step:5246	 l-p:0.18674641847610474
epoch£º262	 i:7 	 global-step:5247	 l-p:0.14943592250347137
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.015172996558248997
epoch£º262	 i:9 	 global-step:5249	 l-p:0.13478122651576996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9487, 5.4104, 5.4925],
        [4.9487, 5.2276, 5.2019],
        [4.9487, 4.9502, 4.9471],
        [4.9487, 5.3545, 5.3987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.13242202997207642 
model_pd.l_d.mean(): -19.34600067138672 
model_pd.lagr.mean(): -19.213579177856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5044], device='cuda:0')), ('power', tensor([-20.0727], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.13242202997207642
epoch£º263	 i:1 	 global-step:5261	 l-p:0.3320161700248718
epoch£º263	 i:2 	 global-step:5262	 l-p:0.1355542093515396
epoch£º263	 i:3 	 global-step:5263	 l-p:0.13558383285999298
epoch£º263	 i:4 	 global-step:5264	 l-p:0.18520744144916534
epoch£º263	 i:5 	 global-step:5265	 l-p:0.13995428383350372
epoch£º263	 i:6 	 global-step:5266	 l-p:0.16329391300678253
epoch£º263	 i:7 	 global-step:5267	 l-p:0.13208170235157013
epoch£º263	 i:8 	 global-step:5268	 l-p:0.08908090740442276
epoch£º263	 i:9 	 global-step:5269	 l-p:0.1250767707824707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0682, 5.0681],
        [5.0683, 5.5738, 5.6790],
        [5.0683, 5.8369, 6.1623],
        [5.0683, 5.4151, 5.4178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.1518317461013794 
model_pd.l_d.mean(): -20.49730682373047 
model_pd.lagr.mean(): -20.345474243164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.1517], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.1518317461013794
epoch£º264	 i:1 	 global-step:5281	 l-p:0.12981654703617096
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10287380963563919
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12148565798997879
epoch£º264	 i:4 	 global-step:5284	 l-p:0.14277924597263336
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14114148914813995
epoch£º264	 i:6 	 global-step:5286	 l-p:0.1721300482749939
epoch£º264	 i:7 	 global-step:5287	 l-p:0.21145617961883545
epoch£º264	 i:8 	 global-step:5288	 l-p:0.1457187831401825
epoch£º264	 i:9 	 global-step:5289	 l-p:0.11968310922384262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.8771, 6.2759],
        [5.0351, 5.1887, 5.1263],
        [5.0351, 5.0351, 5.0351],
        [5.0351, 5.0351, 5.0351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11227694898843765 
model_pd.l_d.mean(): -19.797895431518555 
model_pd.lagr.mean(): -19.685619354248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.5226], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11227694898843765
epoch£º265	 i:1 	 global-step:5301	 l-p:0.1432933211326599
epoch£º265	 i:2 	 global-step:5302	 l-p:0.13342775404453278
epoch£º265	 i:3 	 global-step:5303	 l-p:0.2061239629983902
epoch£º265	 i:4 	 global-step:5304	 l-p:0.0875302329659462
epoch£º265	 i:5 	 global-step:5305	 l-p:0.15422800183296204
epoch£º265	 i:6 	 global-step:5306	 l-p:0.12968938052654266
epoch£º265	 i:7 	 global-step:5307	 l-p:0.13243460655212402
epoch£º265	 i:8 	 global-step:5308	 l-p:0.14633958041667938
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12136546522378922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0448, 5.0445, 5.0447],
        [5.0448, 5.0447, 5.0448],
        [5.0448, 5.0462, 5.0436],
        [5.0448, 5.0448, 5.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.12448950856924057 
model_pd.l_d.mean(): -20.352739334106445 
model_pd.lagr.mean(): -20.22825050354004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.0258], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.12448950856924057
epoch£º266	 i:1 	 global-step:5321	 l-p:0.1314244568347931
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12624192237854004
epoch£º266	 i:3 	 global-step:5323	 l-p:0.547210693359375
epoch£º266	 i:4 	 global-step:5324	 l-p:0.15925054252147675
epoch£º266	 i:5 	 global-step:5325	 l-p:5.742260456085205
epoch£º266	 i:6 	 global-step:5326	 l-p:0.14155495166778564
epoch£º266	 i:7 	 global-step:5327	 l-p:0.21228739619255066
epoch£º266	 i:8 	 global-step:5328	 l-p:0.1308305561542511
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13451088964939117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0247, 5.0247, 5.0247],
        [5.0247, 5.1139, 5.0557],
        [5.0247, 5.0247, 5.0247],
        [5.0247, 5.0247, 5.0239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.13210830092430115 
model_pd.l_d.mean(): -19.96307945251465 
model_pd.lagr.mean(): -19.830970764160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4890], device='cuda:0')), ('power', tensor([-20.6808], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.13210830092430115
epoch£º267	 i:1 	 global-step:5341	 l-p:0.14680756628513336
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11760326474905014
epoch£º267	 i:3 	 global-step:5343	 l-p:0.13548703491687775
epoch£º267	 i:4 	 global-step:5344	 l-p:0.1209055483341217
epoch£º267	 i:5 	 global-step:5345	 l-p:0.21841947734355927
epoch£º267	 i:6 	 global-step:5346	 l-p:0.5475211143493652
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12544266879558563
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13590379059314728
epoch£º267	 i:9 	 global-step:5349	 l-p:0.18185564875602722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9703, 4.9702, 4.9703],
        [4.9703, 5.1782, 5.1258],
        [4.9703, 4.9699, 4.9694],
        [4.9703, 4.9703, 4.9703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.15084338188171387 
model_pd.l_d.mean(): -20.239665985107422 
model_pd.lagr.mean(): -20.088823318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-20.9493], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.15084338188171387
epoch£º268	 i:1 	 global-step:5361	 l-p:0.13836629688739777
epoch£º268	 i:2 	 global-step:5362	 l-p:0.1518353670835495
epoch£º268	 i:3 	 global-step:5363	 l-p:0.12223075330257416
epoch£º268	 i:4 	 global-step:5364	 l-p:0.24588797986507416
epoch£º268	 i:5 	 global-step:5365	 l-p:0.2712036371231079
epoch£º268	 i:6 	 global-step:5366	 l-p:0.1788278967142105
epoch£º268	 i:7 	 global-step:5367	 l-p:0.1318897306919098
epoch£º268	 i:8 	 global-step:5368	 l-p:0.14737969636917114
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12738752365112305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0322, 5.0321, 5.0322],
        [5.0322, 5.0317, 5.0320],
        [5.0322, 5.0435, 5.0287],
        [5.0322, 5.0361, 5.0298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.18218953907489777 
model_pd.l_d.mean(): -20.420331954956055 
model_pd.lagr.mean(): -20.238142013549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.1040], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.18218953907489777
epoch£º269	 i:1 	 global-step:5381	 l-p:0.21507245302200317
epoch£º269	 i:2 	 global-step:5382	 l-p:0.13991402089595795
epoch£º269	 i:3 	 global-step:5383	 l-p:0.11546654254198074
epoch£º269	 i:4 	 global-step:5384	 l-p:0.12400003522634506
epoch£º269	 i:5 	 global-step:5385	 l-p:0.12340707331895828
epoch£º269	 i:6 	 global-step:5386	 l-p:0.10815727710723877
epoch£º269	 i:7 	 global-step:5387	 l-p:0.18609577417373657
epoch£º269	 i:8 	 global-step:5388	 l-p:0.14221207797527313
epoch£º269	 i:9 	 global-step:5389	 l-p:0.10534099489450455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9350, 4.9342, 4.9348],
        [4.9350, 5.1878, 5.1511],
        [4.9350, 4.9348, 4.9350],
        [4.9350, 5.0433, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.1359865516424179 
model_pd.l_d.mean(): -20.645238876342773 
model_pd.lagr.mean(): -20.509252548217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4417], device='cuda:0')), ('power', tensor([-21.3221], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.1359865516424179
epoch£º270	 i:1 	 global-step:5401	 l-p:0.018260855227708817
epoch£º270	 i:2 	 global-step:5402	 l-p:0.0503498911857605
epoch£º270	 i:3 	 global-step:5403	 l-p:0.1585080623626709
epoch£º270	 i:4 	 global-step:5404	 l-p:0.14538727700710297
epoch£º270	 i:5 	 global-step:5405	 l-p:0.022441769018769264
epoch£º270	 i:6 	 global-step:5406	 l-p:0.2552873194217682
epoch£º270	 i:7 	 global-step:5407	 l-p:0.13487406075000763
epoch£º270	 i:8 	 global-step:5408	 l-p:0.13462482392787933
epoch£º270	 i:9 	 global-step:5409	 l-p:0.18835332989692688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8828, 4.8828, 4.8828],
        [4.8828, 4.9694, 4.9097],
        [4.8828, 5.2631, 5.2951],
        [4.8828, 5.5868, 5.8749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.09878475219011307 
model_pd.l_d.mean(): -20.48699951171875 
model_pd.lagr.mean(): -20.388214111328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4833], device='cuda:0')), ('power', tensor([-21.2046], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.09878475219011307
epoch£º271	 i:1 	 global-step:5421	 l-p:0.1265777051448822
epoch£º271	 i:2 	 global-step:5422	 l-p:0.16625092923641205
epoch£º271	 i:3 	 global-step:5423	 l-p:0.1686468869447708
epoch£º271	 i:4 	 global-step:5424	 l-p:0.13835451006889343
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12120411545038223
epoch£º271	 i:6 	 global-step:5426	 l-p:-0.12899674475193024
epoch£º271	 i:7 	 global-step:5427	 l-p:0.1288447231054306
epoch£º271	 i:8 	 global-step:5428	 l-p:0.23302245140075684
epoch£º271	 i:9 	 global-step:5429	 l-p:0.1373152881860733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9954, 4.9947, 4.9946],
        [4.9954, 5.4824, 5.5798],
        [4.9954, 4.9966, 4.9932],
        [4.9954, 4.9946, 4.9950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.2928902804851532 
model_pd.l_d.mean(): -19.212366104125977 
model_pd.lagr.mean(): -18.919475555419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-19.9313], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.2928902804851532
epoch£º272	 i:1 	 global-step:5441	 l-p:0.13656340539455414
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11328864842653275
epoch£º272	 i:3 	 global-step:5443	 l-p:0.08174498379230499
epoch£º272	 i:4 	 global-step:5444	 l-p:0.13547474145889282
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12131930887699127
epoch£º272	 i:6 	 global-step:5446	 l-p:0.1665719896554947
epoch£º272	 i:7 	 global-step:5447	 l-p:0.12424816191196442
epoch£º272	 i:8 	 global-step:5448	 l-p:0.15059520304203033
epoch£º272	 i:9 	 global-step:5449	 l-p:0.1322856843471527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0222, 5.0894, 5.0366],
        [5.0222, 5.0218, 5.0222],
        [5.0222, 5.0252, 5.0194],
        [5.0222, 5.0215, 5.0215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.14188528060913086 
model_pd.l_d.mean(): -18.519203186035156 
model_pd.lagr.mean(): -18.377317428588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5395], device='cuda:0')), ('power', tensor([-19.2727], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.14188528060913086
epoch£º273	 i:1 	 global-step:5461	 l-p:0.1320382058620453
epoch£º273	 i:2 	 global-step:5462	 l-p:0.13625268638134003
epoch£º273	 i:3 	 global-step:5463	 l-p:0.3454591631889343
epoch£º273	 i:4 	 global-step:5464	 l-p:0.1195325255393982
epoch£º273	 i:5 	 global-step:5465	 l-p:0.08072875440120697
epoch£º273	 i:6 	 global-step:5466	 l-p:0.14399462938308716
epoch£º273	 i:7 	 global-step:5467	 l-p:0.13528092205524445
epoch£º273	 i:8 	 global-step:5468	 l-p:0.135726198554039
epoch£º273	 i:9 	 global-step:5469	 l-p:0.2949412763118744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8272, 5.0225, 4.9687],
        [4.8272, 5.5093, 5.7841],
        [4.8272, 4.8264, 4.8271],
        [4.8272, 5.5075, 5.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.05491219460964203 
model_pd.l_d.mean(): -20.694082260131836 
model_pd.lagr.mean(): -20.639169692993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-21.4009], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.05491219460964203
epoch£º274	 i:1 	 global-step:5481	 l-p:0.138872891664505
epoch£º274	 i:2 	 global-step:5482	 l-p:0.2523856461048126
epoch£º274	 i:3 	 global-step:5483	 l-p:-0.23750516772270203
epoch£º274	 i:4 	 global-step:5484	 l-p:0.15415078401565552
epoch£º274	 i:5 	 global-step:5485	 l-p:0.14367154240608215
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12708936631679535
epoch£º274	 i:7 	 global-step:5487	 l-p:0.12615837156772614
epoch£º274	 i:8 	 global-step:5488	 l-p:-0.5941125154495239
epoch£º274	 i:9 	 global-step:5489	 l-p:0.10906139761209488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 5.1848, 5.1568],
        [4.9125, 5.2079, 5.1907],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 4.9109, 4.9115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.1620868593454361 
model_pd.l_d.mean(): -20.7784366607666 
model_pd.lagr.mean(): -20.616350173950195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.1620868593454361
epoch£º275	 i:1 	 global-step:5501	 l-p:0.134193554520607
epoch£º275	 i:2 	 global-step:5502	 l-p:0.12437634915113449
epoch£º275	 i:3 	 global-step:5503	 l-p:0.21656785905361176
epoch£º275	 i:4 	 global-step:5504	 l-p:0.18655212223529816
epoch£º275	 i:5 	 global-step:5505	 l-p:0.1738775223493576
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11600061506032944
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10470367223024368
epoch£º275	 i:8 	 global-step:5508	 l-p:0.15009790658950806
epoch£º275	 i:9 	 global-step:5509	 l-p:0.12749892473220825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1274, 5.8179, 6.0679],
        [5.1274, 5.1274, 5.1265],
        [5.1274, 5.1274, 5.1274],
        [5.1274, 5.1271, 5.1273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12379708141088486 
model_pd.l_d.mean(): -20.774078369140625 
model_pd.lagr.mean(): -20.65028190612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3680], device='cuda:0')), ('power', tensor([-21.3770], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12379708141088486
epoch£º276	 i:1 	 global-step:5521	 l-p:0.1449994295835495
epoch£º276	 i:2 	 global-step:5522	 l-p:0.14397716522216797
epoch£º276	 i:3 	 global-step:5523	 l-p:0.13328363001346588
epoch£º276	 i:4 	 global-step:5524	 l-p:0.1557636559009552
epoch£º276	 i:5 	 global-step:5525	 l-p:0.17234039306640625
epoch£º276	 i:6 	 global-step:5526	 l-p:0.13783608376979828
epoch£º276	 i:7 	 global-step:5527	 l-p:0.1164577379822731
epoch£º276	 i:8 	 global-step:5528	 l-p:0.11092782765626907
epoch£º276	 i:9 	 global-step:5529	 l-p:0.15898005664348602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9987, 5.4811, 5.5750],
        [4.9987, 5.5679, 5.7282],
        [4.9987, 4.9986, 4.9987],
        [4.9987, 5.3715, 5.3925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.14512960612773895 
model_pd.l_d.mean(): -20.20147132873535 
model_pd.lagr.mean(): -20.05634117126465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.9099], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.14512960612773895
epoch£º277	 i:1 	 global-step:5541	 l-p:0.12422830611467361
epoch£º277	 i:2 	 global-step:5542	 l-p:0.1255747526884079
epoch£º277	 i:3 	 global-step:5543	 l-p:0.1456325352191925
epoch£º277	 i:4 	 global-step:5544	 l-p:-0.21171490848064423
epoch£º277	 i:5 	 global-step:5545	 l-p:0.10342435538768768
epoch£º277	 i:6 	 global-step:5546	 l-p:0.1457870453596115
epoch£º277	 i:7 	 global-step:5547	 l-p:0.03264894336462021
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13849703967571259
epoch£º277	 i:9 	 global-step:5549	 l-p:0.14382566511631012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8266, 4.8250, 4.8262],
        [4.8266, 4.8243, 4.8238],
        [4.8266, 4.8370, 4.8174],
        [4.8266, 5.3123, 5.4225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.13573093712329865 
model_pd.l_d.mean(): -19.174718856811523 
model_pd.lagr.mean(): -19.03898811340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.9367], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.13573093712329865
epoch£º278	 i:1 	 global-step:5561	 l-p:0.17827224731445312
epoch£º278	 i:2 	 global-step:5562	 l-p:0.07299236953258514
epoch£º278	 i:3 	 global-step:5563	 l-p:0.12317802011966705
epoch£º278	 i:4 	 global-step:5564	 l-p:0.17791850864887238
epoch£º278	 i:5 	 global-step:5565	 l-p:0.12192834913730621
epoch£º278	 i:6 	 global-step:5566	 l-p:0.15089616179466248
epoch£º278	 i:7 	 global-step:5567	 l-p:0.12226511538028717
epoch£º278	 i:8 	 global-step:5568	 l-p:0.18439792096614838
epoch£º278	 i:9 	 global-step:5569	 l-p:0.12893331050872803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0364, 5.0797, 5.0373],
        [5.0364, 5.0377, 5.0335],
        [5.0364, 5.5246, 5.6202],
        [5.0364, 5.0704, 5.0340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.13453055918216705 
model_pd.l_d.mean(): -20.484041213989258 
model_pd.lagr.mean(): -20.349510192871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.1463], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.13453055918216705
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21261556446552277
epoch£º279	 i:2 	 global-step:5582	 l-p:0.10726813226938248
epoch£º279	 i:3 	 global-step:5583	 l-p:0.09276005625724792
epoch£º279	 i:4 	 global-step:5584	 l-p:0.1364225596189499
epoch£º279	 i:5 	 global-step:5585	 l-p:0.15302105247974396
epoch£º279	 i:6 	 global-step:5586	 l-p:0.15080365538597107
epoch£º279	 i:7 	 global-step:5587	 l-p:0.12356486916542053
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13840807974338531
epoch£º279	 i:9 	 global-step:5589	 l-p:0.1360611766576767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0599, 5.0597, 5.0599],
        [5.0599, 5.0613, 5.0571],
        [5.0599, 5.0715, 5.0547],
        [5.0599, 5.0969, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12501953542232513 
model_pd.l_d.mean(): -20.041706085205078 
model_pd.lagr.mean(): -19.91668701171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-20.7016], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12501953542232513
epoch£º280	 i:1 	 global-step:5601	 l-p:0.14879210293293
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14773871004581451
epoch£º280	 i:3 	 global-step:5603	 l-p:0.1308743953704834
epoch£º280	 i:4 	 global-step:5604	 l-p:0.13279828429222107
epoch£º280	 i:5 	 global-step:5605	 l-p:0.10534894466400146
epoch£º280	 i:6 	 global-step:5606	 l-p:0.0964636579155922
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12394800037145615
epoch£º280	 i:8 	 global-step:5608	 l-p:0.13272692263126373
epoch£º280	 i:9 	 global-step:5609	 l-p:0.09741075336933136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8523, 4.9436, 4.8801],
        [4.8523, 4.8522, 4.8523],
        [4.8523, 4.8500, 4.8515],
        [4.8523, 4.8559, 4.8443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.0964631661772728 
model_pd.l_d.mean(): -20.619125366210938 
model_pd.lagr.mean(): -20.522663116455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.3305], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.0964631661772728
epoch£º281	 i:1 	 global-step:5621	 l-p:0.0991906151175499
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15710976719856262
epoch£º281	 i:3 	 global-step:5623	 l-p:0.3307560980319977
epoch£º281	 i:4 	 global-step:5624	 l-p:0.10351137071847916
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11883185803890228
epoch£º281	 i:6 	 global-step:5626	 l-p:0.1637471616268158
epoch£º281	 i:7 	 global-step:5627	 l-p:0.13793617486953735
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13264918327331543
epoch£º281	 i:9 	 global-step:5629	 l-p:0.12105222791433334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9772, 4.9854, 4.9703],
        [4.9772, 5.1078, 5.0416],
        [4.9772, 4.9895, 4.9698],
        [4.9772, 5.6459, 5.8924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.09970241039991379 
model_pd.l_d.mean(): -18.578081130981445 
model_pd.lagr.mean(): -18.478378295898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5939], device='cuda:0')), ('power', tensor([-19.3879], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.09970241039991379
epoch£º282	 i:1 	 global-step:5641	 l-p:0.135576069355011
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18359296023845673
epoch£º282	 i:3 	 global-step:5643	 l-p:0.21981599926948547
epoch£º282	 i:4 	 global-step:5644	 l-p:0.17692451179027557
epoch£º282	 i:5 	 global-step:5645	 l-p:0.14837519824504852
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11335373669862747
epoch£º282	 i:7 	 global-step:5647	 l-p:0.1494494378566742
epoch£º282	 i:8 	 global-step:5648	 l-p:0.13097557425498962
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12771688401699066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0745, 5.3298, 5.2889],
        [5.0745, 5.7703, 6.0305],
        [5.0745, 5.3298, 5.2889],
        [5.0745, 5.0738, 5.0744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.08421730995178223 
model_pd.l_d.mean(): -20.070201873779297 
model_pd.lagr.mean(): -19.985984802246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-20.7642], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.08421730995178223
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12170162051916122
epoch£º283	 i:2 	 global-step:5662	 l-p:0.1263650804758072
epoch£º283	 i:3 	 global-step:5663	 l-p:0.14268596470355988
epoch£º283	 i:4 	 global-step:5664	 l-p:0.1966903954744339
epoch£º283	 i:5 	 global-step:5665	 l-p:0.13364550471305847
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12398885190486908
epoch£º283	 i:7 	 global-step:5667	 l-p:0.3594982326030731
epoch£º283	 i:8 	 global-step:5668	 l-p:0.178097665309906
epoch£º283	 i:9 	 global-step:5669	 l-p:0.3403567373752594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0049, 5.0032, 5.0041],
        [5.0049, 5.0036, 5.0046],
        [5.0049, 5.0048, 5.0049],
        [5.0049, 5.1479, 5.0817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.18242576718330383 
model_pd.l_d.mean(): -20.75779151916504 
model_pd.lagr.mean(): -20.57536506652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.4067], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.18242576718330383
epoch£º284	 i:1 	 global-step:5681	 l-p:0.17487548291683197
epoch£º284	 i:2 	 global-step:5682	 l-p:0.13661164045333862
epoch£º284	 i:3 	 global-step:5683	 l-p:0.15297728776931763
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12273509800434113
epoch£º284	 i:5 	 global-step:5685	 l-p:0.030583566054701805
epoch£º284	 i:6 	 global-step:5686	 l-p:0.1278805285692215
epoch£º284	 i:7 	 global-step:5687	 l-p:0.13863535225391388
epoch£º284	 i:8 	 global-step:5688	 l-p:0.1170714795589447
epoch£º284	 i:9 	 global-step:5689	 l-p:0.10154163092374802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1174, 5.1174, 5.1174],
        [5.1174, 5.1163, 5.1169],
        [5.1174, 5.1293, 5.1120],
        [5.1174, 5.3604, 5.3137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11176639050245285 
model_pd.l_d.mean(): -20.45631217956543 
model_pd.lagr.mean(): -20.344545364379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4089], device='cuda:0')), ('power', tensor([-21.0976], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11176639050245285
epoch£º285	 i:1 	 global-step:5701	 l-p:0.12622904777526855
epoch£º285	 i:2 	 global-step:5702	 l-p:0.13509535789489746
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12202569097280502
epoch£º285	 i:4 	 global-step:5704	 l-p:0.12809619307518005
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.3971257209777832
epoch£º285	 i:6 	 global-step:5706	 l-p:0.03746151179075241
epoch£º285	 i:7 	 global-step:5707	 l-p:-0.04001804441213608
epoch£º285	 i:8 	 global-step:5708	 l-p:0.19221407175064087
epoch£º285	 i:9 	 global-step:5709	 l-p:0.1269540786743164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9447, 4.9423, 4.9431],
        [4.9447, 4.9447, 4.9447],
        [4.9447, 4.9447, 4.9447],
        [4.9447, 4.9441, 4.9447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.1307731419801712 
model_pd.l_d.mean(): -20.625228881835938 
model_pd.lagr.mean(): -20.494455337524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3009], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.1307731419801712
epoch£º286	 i:1 	 global-step:5721	 l-p:0.13179531693458557
epoch£º286	 i:2 	 global-step:5722	 l-p:0.25840234756469727
epoch£º286	 i:3 	 global-step:5723	 l-p:0.10358484089374542
epoch£º286	 i:4 	 global-step:5724	 l-p:0.07628040015697479
epoch£º286	 i:5 	 global-step:5725	 l-p:0.007644114550203085
epoch£º286	 i:6 	 global-step:5726	 l-p:0.13085760176181793
epoch£º286	 i:7 	 global-step:5727	 l-p:0.13494104146957397
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14066433906555176
epoch£º286	 i:9 	 global-step:5729	 l-p:0.13849058747291565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0043, 5.2003, 5.1419],
        [5.0043, 5.2786, 5.2474],
        [5.0043, 5.0298, 4.9970],
        [5.0043, 5.0031, 5.0013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.13988307118415833 
model_pd.l_d.mean(): -20.427549362182617 
model_pd.lagr.mean(): -20.28766632080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.1088], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.13988307118415833
epoch£º287	 i:1 	 global-step:5741	 l-p:0.12816974520683289
epoch£º287	 i:2 	 global-step:5742	 l-p:0.18272948265075684
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.025688419118523598
epoch£º287	 i:4 	 global-step:5744	 l-p:0.13318240642547607
epoch£º287	 i:5 	 global-step:5745	 l-p:0.14533323049545288
epoch£º287	 i:6 	 global-step:5746	 l-p:0.13906802237033844
epoch£º287	 i:7 	 global-step:5747	 l-p:0.142639622092247
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14320459961891174
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.03948482871055603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9344, 4.9317, 4.9329],
        [4.9344, 4.9344, 4.9344],
        [4.9344, 4.9339, 4.9343],
        [4.9344, 5.0732, 5.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.19521667063236237 
model_pd.l_d.mean(): -20.571823120117188 
model_pd.lagr.mean(): -20.376605987548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.19521667063236237
epoch£º288	 i:1 	 global-step:5761	 l-p:0.1452319622039795
epoch£º288	 i:2 	 global-step:5762	 l-p:0.14764873683452606
epoch£º288	 i:3 	 global-step:5763	 l-p:0.5844350457191467
epoch£º288	 i:4 	 global-step:5764	 l-p:0.1408253163099289
epoch£º288	 i:5 	 global-step:5765	 l-p:0.1325252652168274
epoch£º288	 i:6 	 global-step:5766	 l-p:0.14808699488639832
epoch£º288	 i:7 	 global-step:5767	 l-p:0.12430020421743393
epoch£º288	 i:8 	 global-step:5768	 l-p:0.09988975524902344
epoch£º288	 i:9 	 global-step:5769	 l-p:0.40133288502693176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9869, 5.3254, 5.3269],
        [4.9869, 4.9866, 4.9869],
        [4.9869, 4.9867, 4.9869],
        [4.9869, 4.9846, 4.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11736898869276047 
model_pd.l_d.mean(): -19.02410316467285 
model_pd.lagr.mean(): -18.906734466552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5397], device='cuda:0')), ('power', tensor([-19.7833], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11736898869276047
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09225724637508392
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8312264084815979
epoch£º289	 i:3 	 global-step:5783	 l-p:0.1366124451160431
epoch£º289	 i:4 	 global-step:5784	 l-p:0.15135003626346588
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7873255610466003
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17850640416145325
epoch£º289	 i:7 	 global-step:5787	 l-p:0.1437157541513443
epoch£º289	 i:8 	 global-step:5788	 l-p:0.12316697835922241
epoch£º289	 i:9 	 global-step:5789	 l-p:0.14203134179115295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 4.9905, 4.9905],
        [4.9905, 4.9895, 4.9904],
        [4.9905, 4.9905, 4.9905],
        [4.9905, 5.0120, 4.9812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.15302197635173798 
model_pd.l_d.mean(): -20.612533569335938 
model_pd.lagr.mean(): -20.459510803222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.2681], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.15302197635173798
epoch£º290	 i:1 	 global-step:5801	 l-p:0.1286085695028305
epoch£º290	 i:2 	 global-step:5802	 l-p:0.12778301537036896
epoch£º290	 i:3 	 global-step:5803	 l-p:0.9729481339454651
epoch£º290	 i:4 	 global-step:5804	 l-p:0.1294356733560562
epoch£º290	 i:5 	 global-step:5805	 l-p:0.12770359218120575
epoch£º290	 i:6 	 global-step:5806	 l-p:0.14099763333797455
epoch£º290	 i:7 	 global-step:5807	 l-p:0.21735329926013947
epoch£º290	 i:8 	 global-step:5808	 l-p:0.15107855200767517
epoch£º290	 i:9 	 global-step:5809	 l-p:0.14818847179412842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8545, 4.8819, 4.8420],
        [4.8545, 5.0851, 5.0402],
        [4.8545, 4.8507, 4.8528],
        [4.8545, 4.8544, 4.8545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.0972166433930397 
model_pd.l_d.mean(): -20.91400909423828 
model_pd.lagr.mean(): -20.816791534423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.5846], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.0972166433930397
epoch£º291	 i:1 	 global-step:5821	 l-p:0.11709477007389069
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12096314877271652
epoch£º291	 i:3 	 global-step:5823	 l-p:0.16674058139324188
epoch£º291	 i:4 	 global-step:5824	 l-p:0.1298387497663498
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.12808454036712646
epoch£º291	 i:6 	 global-step:5826	 l-p:0.178401917219162
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16360744833946228
epoch£º291	 i:8 	 global-step:5828	 l-p:0.09900661557912827
epoch£º291	 i:9 	 global-step:5829	 l-p:0.13257311284542084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1119, 5.1119, 5.1119],
        [5.1119, 5.1107, 5.1116],
        [5.1119, 5.1118, 5.1119],
        [5.1119, 5.1119, 5.1119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.12876304984092712 
model_pd.l_d.mean(): -20.044754028320312 
model_pd.lagr.mean(): -19.915990829467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4336], device='cuda:0')), ('power', tensor([-20.7068], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.12876304984092712
epoch£º292	 i:1 	 global-step:5841	 l-p:0.1240568533539772
epoch£º292	 i:2 	 global-step:5842	 l-p:0.1181347444653511
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11646289378404617
epoch£º292	 i:4 	 global-step:5844	 l-p:0.1013808399438858
epoch£º292	 i:5 	 global-step:5845	 l-p:0.1559305191040039
epoch£º292	 i:6 	 global-step:5846	 l-p:0.11528575420379639
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18492522835731506
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12894155085086823
epoch£º292	 i:9 	 global-step:5849	 l-p:0.1269129067659378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0562, 5.1144, 5.0602],
        [5.0562, 5.1116, 5.0587],
        [5.0562, 5.0631, 5.0481],
        [5.0562, 5.0588, 5.0497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12845197319984436 
model_pd.l_d.mean(): -20.610973358154297 
model_pd.lagr.mean(): -20.482521057128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-21.2472], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12845197319984436
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11638956516981125
epoch£º293	 i:2 	 global-step:5862	 l-p:0.28263434767723083
epoch£º293	 i:3 	 global-step:5863	 l-p:5.8136887550354
epoch£º293	 i:4 	 global-step:5864	 l-p:0.1326090693473816
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12374419718980789
epoch£º293	 i:6 	 global-step:5866	 l-p:-0.17965291440486908
epoch£º293	 i:7 	 global-step:5867	 l-p:0.1281924843788147
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14880533516407013
epoch£º293	 i:9 	 global-step:5869	 l-p:0.26015716791152954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9393, 4.9468, 4.9277],
        [4.9393, 5.2162, 5.1878],
        [4.9393, 5.2875, 5.2967],
        [4.9393, 5.0071, 4.9467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.1624838411808014 
model_pd.l_d.mean(): -19.485065460205078 
model_pd.lagr.mean(): -19.322582244873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-20.2069], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.1624838411808014
epoch£º294	 i:1 	 global-step:5881	 l-p:0.14372460544109344
epoch£º294	 i:2 	 global-step:5882	 l-p:4.296499729156494
epoch£º294	 i:3 	 global-step:5883	 l-p:0.12885083258152008
epoch£º294	 i:4 	 global-step:5884	 l-p:0.11596755683422089
epoch£º294	 i:5 	 global-step:5885	 l-p:0.46784597635269165
epoch£º294	 i:6 	 global-step:5886	 l-p:0.1555798053741455
epoch£º294	 i:7 	 global-step:5887	 l-p:0.1136137917637825
epoch£º294	 i:8 	 global-step:5888	 l-p:0.1294509619474411
epoch£º294	 i:9 	 global-step:5889	 l-p:-0.31267112493515015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9935, 4.9565],
        [4.9677, 4.9676, 4.9677],
        [4.9677, 4.9671, 4.9677],
        [4.9677, 4.9677, 4.9677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.1407000720500946 
model_pd.l_d.mean(): -20.866357803344727 
model_pd.lagr.mean(): -20.725658416748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.5145], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.1407000720500946
epoch£º295	 i:1 	 global-step:5901	 l-p:0.12558701634407043
epoch£º295	 i:2 	 global-step:5902	 l-p:0.024393577128648758
epoch£º295	 i:3 	 global-step:5903	 l-p:-0.03765945881605148
epoch£º295	 i:4 	 global-step:5904	 l-p:0.04156266897916794
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13862085342407227
epoch£º295	 i:6 	 global-step:5906	 l-p:0.15410026907920837
epoch£º295	 i:7 	 global-step:5907	 l-p:0.1251816302537918
epoch£º295	 i:8 	 global-step:5908	 l-p:0.13854080438613892
epoch£º295	 i:9 	 global-step:5909	 l-p:0.14027363061904907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0528, 5.0964, 5.0487],
        [5.0528, 5.0510, 5.0523],
        [5.0528, 5.0525, 5.0527],
        [5.0528, 5.0528, 5.0528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.16243930160999298 
model_pd.l_d.mean(): -20.472505569458008 
model_pd.lagr.mean(): -20.31006622314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4271], device='cuda:0')), ('power', tensor([-21.1326], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.16243930160999298
epoch£º296	 i:1 	 global-step:5921	 l-p:0.1331833451986313
epoch£º296	 i:2 	 global-step:5922	 l-p:0.13673250377178192
epoch£º296	 i:3 	 global-step:5923	 l-p:0.19888316094875336
epoch£º296	 i:4 	 global-step:5924	 l-p:0.17647548019886017
epoch£º296	 i:5 	 global-step:5925	 l-p:0.10647764056921005
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12537707388401031
epoch£º296	 i:7 	 global-step:5927	 l-p:0.1370193511247635
epoch£º296	 i:8 	 global-step:5928	 l-p:0.13355396687984467
epoch£º296	 i:9 	 global-step:5929	 l-p:0.10453906655311584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9705, 5.6619, 5.9299],
        [4.9705, 4.9882, 4.9576],
        [4.9705, 4.9671, 4.9689],
        [4.9705, 4.9702, 4.9705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.5148037075996399 
model_pd.l_d.mean(): -19.514951705932617 
model_pd.lagr.mean(): -19.00014877319336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5218], device='cuda:0')), ('power', tensor([-20.2613], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.5148037075996399
epoch£º297	 i:1 	 global-step:5941	 l-p:0.12499146163463593
epoch£º297	 i:2 	 global-step:5942	 l-p:0.13429759442806244
epoch£º297	 i:3 	 global-step:5943	 l-p:0.11078767478466034
epoch£º297	 i:4 	 global-step:5944	 l-p:0.13555723428726196
epoch£º297	 i:5 	 global-step:5945	 l-p:0.14313605427742004
epoch£º297	 i:6 	 global-step:5946	 l-p:-0.9634293913841248
epoch£º297	 i:7 	 global-step:5947	 l-p:0.09980237483978271
epoch£º297	 i:8 	 global-step:5948	 l-p:0.12279239296913147
epoch£º297	 i:9 	 global-step:5949	 l-p:0.1581265777349472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7992, 4.7992, 4.7992],
        [4.7992, 4.7934, 4.7900],
        [4.7992, 4.7991, 4.7992],
        [4.7992, 5.6078, 6.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.14042535424232483 
model_pd.l_d.mean(): -19.839027404785156 
model_pd.lagr.mean(): -19.6986026763916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5305], device='cuda:0')), ('power', tensor([-20.5978], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.14042535424232483
epoch£º298	 i:1 	 global-step:5961	 l-p:0.6387872695922852
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15389437973499298
epoch£º298	 i:3 	 global-step:5963	 l-p:0.12986931204795837
epoch£º298	 i:4 	 global-step:5964	 l-p:0.19730857014656067
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12740226089954376
epoch£º298	 i:6 	 global-step:5966	 l-p:-0.2423003762960434
epoch£º298	 i:7 	 global-step:5967	 l-p:0.1520477533340454
epoch£º298	 i:8 	 global-step:5968	 l-p:0.13000403344631195
epoch£º298	 i:9 	 global-step:5969	 l-p:0.3000779151916504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0137, 5.0108, 5.0127],
        [5.0137, 5.0218, 5.0020],
        [5.0137, 5.0640, 5.0106],
        [5.0137, 5.1470, 5.0769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12208189070224762 
model_pd.l_d.mean(): -20.472871780395508 
model_pd.lagr.mean(): -20.35079002380371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-21.1512], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12208189070224762
epoch£º299	 i:1 	 global-step:5981	 l-p:0.17438754439353943
epoch£º299	 i:2 	 global-step:5982	 l-p:0.2485494315624237
epoch£º299	 i:3 	 global-step:5983	 l-p:0.12857474386692047
epoch£º299	 i:4 	 global-step:5984	 l-p:0.13283640146255493
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14666634798049927
epoch£º299	 i:6 	 global-step:5986	 l-p:0.13292764127254486
epoch£º299	 i:7 	 global-step:5987	 l-p:0.289349228143692
epoch£º299	 i:8 	 global-step:5988	 l-p:0.09502830356359482
epoch£º299	 i:9 	 global-step:5989	 l-p:0.20656442642211914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0231, 5.7164, 5.9807],
        [5.0231, 5.0200, 5.0183],
        [5.0231, 5.0231, 5.0231],
        [5.0231, 5.0198, 5.0186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.16593217849731445 
model_pd.l_d.mean(): -20.656204223632812 
model_pd.lagr.mean(): -20.490272521972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.3101], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.16593217849731445
epoch£º300	 i:1 	 global-step:6001	 l-p:0.21628612279891968
epoch£º300	 i:2 	 global-step:6002	 l-p:0.13340643048286438
epoch£º300	 i:3 	 global-step:6003	 l-p:0.13363902270793915
epoch£º300	 i:4 	 global-step:6004	 l-p:0.13412216305732727
epoch£º300	 i:5 	 global-step:6005	 l-p:0.1281251758337021
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11990382522344589
epoch£º300	 i:7 	 global-step:6007	 l-p:0.31253552436828613
epoch£º300	 i:8 	 global-step:6008	 l-p:0.1293940544128418
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14316044747829437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9374, 4.9403, 4.9243],
        [4.9374, 4.9374, 4.9374],
        [4.9374, 5.0848, 5.0154],
        [4.9374, 4.9374, 4.9374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.05916588008403778 
model_pd.l_d.mean(): -19.23186683654785 
model_pd.lagr.mean(): -19.172700881958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5224], device='cuda:0')), ('power', tensor([-19.9757], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.05916588008403778
epoch£º301	 i:1 	 global-step:6021	 l-p:0.12730388343334198
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11925937235355377
epoch£º301	 i:3 	 global-step:6023	 l-p:0.1493823081254959
epoch£º301	 i:4 	 global-step:6024	 l-p:0.10263318568468094
epoch£º301	 i:5 	 global-step:6025	 l-p:0.14147844910621643
epoch£º301	 i:6 	 global-step:6026	 l-p:0.1448216438293457
epoch£º301	 i:7 	 global-step:6027	 l-p:0.018305689096450806
epoch£º301	 i:8 	 global-step:6028	 l-p:0.718816339969635
epoch£º301	 i:9 	 global-step:6029	 l-p:0.09107010811567307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8459, 4.8511, 4.8278],
        [4.8459, 4.8453, 4.8459],
        [4.8459, 5.4193, 5.5988],
        [4.8459, 4.8454, 4.8459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.14681749045848846 
model_pd.l_d.mean(): -20.742244720458984 
model_pd.lagr.mean(): -20.595426559448242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-21.4386], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.14681749045848846
epoch£º302	 i:1 	 global-step:6041	 l-p:0.1871064007282257
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12256243079900742
epoch£º302	 i:3 	 global-step:6043	 l-p:0.07788518071174622
epoch£º302	 i:4 	 global-step:6044	 l-p:0.14935961365699768
epoch£º302	 i:5 	 global-step:6045	 l-p:0.1353004276752472
epoch£º302	 i:6 	 global-step:6046	 l-p:0.24671290814876556
epoch£º302	 i:7 	 global-step:6047	 l-p:0.14638276398181915
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14347296953201294
epoch£º302	 i:9 	 global-step:6049	 l-p:-0.12547557055950165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9058, 4.9058, 4.9058],
        [4.9058, 4.9096, 4.8901],
        [4.9058, 5.7126, 6.0952],
        [4.9058, 4.9387, 4.8911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.13934874534606934 
model_pd.l_d.mean(): -19.162450790405273 
model_pd.lagr.mean(): -19.023101806640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-19.9079], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.13934874534606934
epoch£º303	 i:1 	 global-step:6061	 l-p:0.16710734367370605
epoch£º303	 i:2 	 global-step:6062	 l-p:0.13812097907066345
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.6339794993400574
epoch£º303	 i:4 	 global-step:6064	 l-p:0.20270054042339325
epoch£º303	 i:5 	 global-step:6065	 l-p:0.09155911207199097
epoch£º303	 i:6 	 global-step:6066	 l-p:0.13710980117321014
epoch£º303	 i:7 	 global-step:6067	 l-p:0.13163256645202637
epoch£º303	 i:8 	 global-step:6068	 l-p:0.13914570212364197
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09804080426692963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1347, 5.1347],
        [5.1347, 5.1347, 5.1347],
        [5.1347, 5.1343, 5.1347],
        [5.1347, 5.1346, 5.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12621347606182098 
model_pd.l_d.mean(): -19.06873321533203 
model_pd.lagr.mean(): -18.942520141601562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4560], device='cuda:0')), ('power', tensor([-19.7430], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12621347606182098
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11143157631158829
epoch£º304	 i:2 	 global-step:6082	 l-p:0.1276419460773468
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14281392097473145
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15883786976337433
epoch£º304	 i:5 	 global-step:6085	 l-p:0.16872073709964752
epoch£º304	 i:6 	 global-step:6086	 l-p:0.075159952044487
epoch£º304	 i:7 	 global-step:6087	 l-p:0.1492331475019455
epoch£º304	 i:8 	 global-step:6088	 l-p:0.1258009970188141
epoch£º304	 i:9 	 global-step:6089	 l-p:0.1314142644405365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1251, 5.1762, 5.1223],
        [5.1251, 5.1250, 5.1251],
        [5.1251, 5.8274, 6.0892],
        [5.1251, 5.1289, 5.1152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.1335870772600174 
model_pd.l_d.mean(): -20.151010513305664 
model_pd.lagr.mean(): -20.017423629760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4453], device='cuda:0')), ('power', tensor([-20.8262], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.1335870772600174
epoch£º305	 i:1 	 global-step:6101	 l-p:0.1280720829963684
epoch£º305	 i:2 	 global-step:6102	 l-p:0.09273026138544083
epoch£º305	 i:3 	 global-step:6103	 l-p:0.1827997863292694
epoch£º305	 i:4 	 global-step:6104	 l-p:0.18345916271209717
epoch£º305	 i:5 	 global-step:6105	 l-p:0.12667924165725708
epoch£º305	 i:6 	 global-step:6106	 l-p:0.13173972070217133
epoch£º305	 i:7 	 global-step:6107	 l-p:0.17682483792304993
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11265908926725388
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11273548752069473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1148, 5.1148, 5.1148],
        [5.1148, 5.7869, 6.0236],
        [5.1148, 5.1148, 5.1148],
        [5.1148, 5.2208, 5.1499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.15669672191143036 
model_pd.l_d.mean(): -19.03229331970215 
model_pd.lagr.mean(): -18.87559700012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-19.7522], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.15669672191143036
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11829953640699387
epoch£º306	 i:2 	 global-step:6122	 l-p:0.12540659308433533
epoch£º306	 i:3 	 global-step:6123	 l-p:0.09307198226451874
epoch£º306	 i:4 	 global-step:6124	 l-p:0.13829652965068817
epoch£º306	 i:5 	 global-step:6125	 l-p:0.13375476002693176
epoch£º306	 i:6 	 global-step:6126	 l-p:0.14422650635242462
epoch£º306	 i:7 	 global-step:6127	 l-p:0.11799968779087067
epoch£º306	 i:8 	 global-step:6128	 l-p:-0.12808528542518616
epoch£º306	 i:9 	 global-step:6129	 l-p:0.03003557212650776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9558, 4.9558, 4.9558],
        [4.9558, 4.9501, 4.9480],
        [4.9558, 5.3739, 5.4259],
        [4.9558, 4.9498, 4.9492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14324010908603668 
model_pd.l_d.mean(): -20.59807777404785 
model_pd.lagr.mean(): -20.454837799072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-21.2826], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14324010908603668
epoch£º307	 i:1 	 global-step:6141	 l-p:0.13444243371486664
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12117607146501541
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14135971665382385
epoch£º307	 i:4 	 global-step:6144	 l-p:0.14088162779808044
epoch£º307	 i:5 	 global-step:6145	 l-p:0.11190003156661987
epoch£º307	 i:6 	 global-step:6146	 l-p:-0.05370316281914711
epoch£º307	 i:7 	 global-step:6147	 l-p:0.27059486508369446
epoch£º307	 i:8 	 global-step:6148	 l-p:0.15608689188957214
epoch£º307	 i:9 	 global-step:6149	 l-p:0.12963058054447174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8446, 4.8477],
        [4.8488, 4.8433, 4.8469],
        [4.8488, 4.8410, 4.8376],
        [4.8488, 4.9192, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.13226318359375 
model_pd.l_d.mean(): -20.992700576782227 
model_pd.lagr.mean(): -20.860437393188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.6594], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.13226318359375
epoch£º308	 i:1 	 global-step:6161	 l-p:0.09654144197702408
epoch£º308	 i:2 	 global-step:6162	 l-p:0.14251214265823364
epoch£º308	 i:3 	 global-step:6163	 l-p:0.15831035375595093
epoch£º308	 i:4 	 global-step:6164	 l-p:0.15521004796028137
epoch£º308	 i:5 	 global-step:6165	 l-p:0.8825446367263794
epoch£º308	 i:6 	 global-step:6166	 l-p:0.062019675970077515
epoch£º308	 i:7 	 global-step:6167	 l-p:0.2964106798171997
epoch£º308	 i:8 	 global-step:6168	 l-p:0.12920062243938446
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13183721899986267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9672, 4.9773, 4.9482],
        [4.9672, 5.2048, 5.1569],
        [4.9672, 4.9671, 4.9672],
        [4.9672, 5.3399, 5.3616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.17626677453517914 
model_pd.l_d.mean(): -19.05769920349121 
model_pd.lagr.mean(): -18.881431579589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5269], device='cuda:0')), ('power', tensor([-19.8042], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.17626677453517914
epoch£º309	 i:1 	 global-step:6181	 l-p:0.3721666932106018
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14018850028514862
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12198854237794876
epoch£º309	 i:4 	 global-step:6184	 l-p:0.12019946426153183
epoch£º309	 i:5 	 global-step:6185	 l-p:0.13157619535923004
epoch£º309	 i:6 	 global-step:6186	 l-p:0.1349775493144989
epoch£º309	 i:7 	 global-step:6187	 l-p:0.16616080701351166
epoch£º309	 i:8 	 global-step:6188	 l-p:0.1540142446756363
epoch£º309	 i:9 	 global-step:6189	 l-p:0.12944333255290985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1578, 5.1596],
        [5.1603, 5.1602, 5.1603],
        [5.1603, 5.4167, 5.3707],
        [5.1603, 5.7032, 5.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.15394710004329681 
model_pd.l_d.mean(): -20.709148406982422 
model_pd.lagr.mean(): -20.555200576782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3760], device='cuda:0')), ('power', tensor([-21.3196], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.15394710004329681
epoch£º310	 i:1 	 global-step:6201	 l-p:-0.003353128442540765
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12057896703481674
epoch£º310	 i:3 	 global-step:6203	 l-p:0.11361701786518097
epoch£º310	 i:4 	 global-step:6204	 l-p:0.14480523765087128
epoch£º310	 i:5 	 global-step:6205	 l-p:0.13645538687705994
epoch£º310	 i:6 	 global-step:6206	 l-p:0.15254724025726318
epoch£º310	 i:7 	 global-step:6207	 l-p:0.13306957483291626
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10513997822999954
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13700160384178162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0432, 5.0869, 5.0324],
        [5.0432, 5.0833, 5.0309],
        [5.0432, 5.0519, 5.0264],
        [5.0432, 5.1111, 5.0458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11719346046447754 
model_pd.l_d.mean(): -19.068397521972656 
model_pd.lagr.mean(): -18.951204299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-19.7832], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11719346046447754
epoch£º311	 i:1 	 global-step:6221	 l-p:0.2873900830745697
epoch£º311	 i:2 	 global-step:6222	 l-p:0.2258129119873047
epoch£º311	 i:3 	 global-step:6223	 l-p:0.13377521932125092
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12392320483922958
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11511204391717911
epoch£º311	 i:6 	 global-step:6226	 l-p:0.18393811583518982
epoch£º311	 i:7 	 global-step:6227	 l-p:-0.017740778625011444
epoch£º311	 i:8 	 global-step:6228	 l-p:0.14732380211353302
epoch£º311	 i:9 	 global-step:6229	 l-p:0.1376454383134842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9728, 5.0134, 4.9581],
        [4.9728, 4.9699, 4.9722],
        [4.9728, 4.9728, 4.9728],
        [4.9728, 5.4435, 5.5313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): -0.016524208709597588 
model_pd.l_d.mean(): -20.7661190032959 
model_pd.lagr.mean(): -20.782642364501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.4282], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:-0.016524208709597588
epoch£º312	 i:1 	 global-step:6241	 l-p:0.11119043827056885
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10847515612840652
epoch£º312	 i:3 	 global-step:6243	 l-p:0.2746911644935608
epoch£º312	 i:4 	 global-step:6244	 l-p:0.14768746495246887
epoch£º312	 i:5 	 global-step:6245	 l-p:0.0841844379901886
epoch£º312	 i:6 	 global-step:6246	 l-p:0.16266094148159027
epoch£º312	 i:7 	 global-step:6247	 l-p:0.13305725157260895
epoch£º312	 i:8 	 global-step:6248	 l-p:0.13754230737686157
epoch£º312	 i:9 	 global-step:6249	 l-p:-0.04044654592871666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9668, 4.9595, 4.9591],
        [4.9668, 5.5550, 5.7351],
        [4.9668, 5.0062, 4.9509],
        [4.9668, 4.9622, 4.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.15061037242412567 
model_pd.l_d.mean(): -20.67889976501465 
model_pd.lagr.mean(): -20.528289794921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4373], device='cuda:0')), ('power', tensor([-21.3516], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.15061037242412567
epoch£º313	 i:1 	 global-step:6261	 l-p:0.13285209238529205
epoch£º313	 i:2 	 global-step:6262	 l-p:0.25404152274131775
epoch£º313	 i:3 	 global-step:6263	 l-p:0.1900966912508011
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14564882218837738
epoch£º313	 i:5 	 global-step:6265	 l-p:0.11839797347784042
epoch£º313	 i:6 	 global-step:6266	 l-p:0.16970527172088623
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13221430778503418
epoch£º313	 i:8 	 global-step:6268	 l-p:0.07377008348703384
epoch£º313	 i:9 	 global-step:6269	 l-p:0.01743990182876587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9396, 4.9396, 4.9396],
        [4.9396, 4.9340, 4.9376],
        [4.9396, 4.9339, 4.9254],
        [4.9396, 4.9402, 4.9198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.036695823073387146 
model_pd.l_d.mean(): -19.99505043029785 
model_pd.lagr.mean(): -19.958354949951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5267], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.036695823073387146
epoch£º314	 i:1 	 global-step:6281	 l-p:0.1401965171098709
epoch£º314	 i:2 	 global-step:6282	 l-p:0.15112614631652832
epoch£º314	 i:3 	 global-step:6283	 l-p:0.1257622092962265
epoch£º314	 i:4 	 global-step:6284	 l-p:1.01230788230896
epoch£º314	 i:5 	 global-step:6285	 l-p:0.2441025823354721
epoch£º314	 i:6 	 global-step:6286	 l-p:0.13784974813461304
epoch£º314	 i:7 	 global-step:6287	 l-p:0.12834975123405457
epoch£º314	 i:8 	 global-step:6288	 l-p:0.09806590527296066
epoch£º314	 i:9 	 global-step:6289	 l-p:0.1388522982597351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1187, 5.3412, 5.2836],
        [5.1187, 5.1565, 5.1061],
        [5.1187, 5.1187, 5.1187],
        [5.1187, 5.5537, 5.6067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.12839670479297638 
model_pd.l_d.mean(): -20.20176887512207 
model_pd.lagr.mean(): -20.07337188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-20.8585], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.12839670479297638
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11593125015497208
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11048362404108047
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12832506000995636
epoch£º315	 i:4 	 global-step:6304	 l-p:0.13187465071678162
epoch£º315	 i:5 	 global-step:6305	 l-p:0.18960599601268768
epoch£º315	 i:6 	 global-step:6306	 l-p:0.2025524526834488
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15263420343399048
epoch£º315	 i:8 	 global-step:6308	 l-p:0.11980611830949783
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13605497777462006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9857, 4.9816, 4.9846],
        [4.9857, 4.9849, 4.9856],
        [4.9857, 5.0404, 4.9769],
        [4.9857, 4.9783, 4.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.13898739218711853 
model_pd.l_d.mean(): -20.96759796142578 
model_pd.lagr.mean(): -20.828611373901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3908], device='cuda:0')), ('power', tensor([-21.5959], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.13898739218711853
epoch£º316	 i:1 	 global-step:6321	 l-p:0.07730136066675186
epoch£º316	 i:2 	 global-step:6322	 l-p:0.4894406795501709
epoch£º316	 i:3 	 global-step:6323	 l-p:0.14790086448192596
epoch£º316	 i:4 	 global-step:6324	 l-p:0.12108614295721054
epoch£º316	 i:5 	 global-step:6325	 l-p:0.14794254302978516
epoch£º316	 i:6 	 global-step:6326	 l-p:0.105893075466156
epoch£º316	 i:7 	 global-step:6327	 l-p:0.041627321392297745
epoch£º316	 i:8 	 global-step:6328	 l-p:0.1335209459066391
epoch£º316	 i:9 	 global-step:6329	 l-p:0.12000908702611923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7124, 4.6988, 4.6997],
        [4.7124, 4.7117, 4.7124],
        [4.7124, 4.7056, 4.6826],
        [4.7124, 4.7019, 4.6867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.1289687156677246 
model_pd.l_d.mean(): -20.399330139160156 
model_pd.lagr.mean(): -20.270360946655273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-21.1806], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.1289687156677246
epoch£º317	 i:1 	 global-step:6341	 l-p:0.18049688637256622
epoch£º317	 i:2 	 global-step:6342	 l-p:0.1962439864873886
epoch£º317	 i:3 	 global-step:6343	 l-p:0.13801544904708862
epoch£º317	 i:4 	 global-step:6344	 l-p:0.2467591017484665
epoch£º317	 i:5 	 global-step:6345	 l-p:0.13648700714111328
epoch£º317	 i:6 	 global-step:6346	 l-p:0.1326957792043686
epoch£º317	 i:7 	 global-step:6347	 l-p:0.13077691197395325
epoch£º317	 i:8 	 global-step:6348	 l-p:0.1544899344444275
epoch£º317	 i:9 	 global-step:6349	 l-p:0.14097650349140167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8647, 4.8546, 4.8525],
        [4.8647, 4.8646, 4.8647],
        [4.8647, 4.8598, 4.8635],
        [4.8647, 4.8643, 4.8647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.069346122443676 
model_pd.l_d.mean(): -20.253643035888672 
model_pd.lagr.mean(): -20.184297561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5078], device='cuda:0')), ('power', tensor([-20.9937], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.069346122443676
epoch£º318	 i:1 	 global-step:6361	 l-p:0.14387458562850952
epoch£º318	 i:2 	 global-step:6362	 l-p:0.1297561377286911
epoch£º318	 i:3 	 global-step:6363	 l-p:0.2014923393726349
epoch£º318	 i:4 	 global-step:6364	 l-p:0.1322670727968216
epoch£º318	 i:5 	 global-step:6365	 l-p:0.020304717123508453
epoch£º318	 i:6 	 global-step:6366	 l-p:-0.2239951640367508
epoch£º318	 i:7 	 global-step:6367	 l-p:0.13421620428562164
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12128118425607681
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12358409911394119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 5.1149, 5.0919],
        [5.1089, 5.1399, 5.0926],
        [5.1089, 5.1089, 5.1089],
        [5.1089, 5.1089, 5.1089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.16232633590698242 
model_pd.l_d.mean(): -20.502660751342773 
model_pd.lagr.mean(): -20.340333938598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.1525], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.16232633590698242
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11460833251476288
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12512966990470886
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13221563398838043
epoch£º319	 i:4 	 global-step:6384	 l-p:0.13693703711032867
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12364909797906876
epoch£º319	 i:6 	 global-step:6386	 l-p:0.4547111392021179
epoch£º319	 i:7 	 global-step:6387	 l-p:-0.05500257387757301
epoch£º319	 i:8 	 global-step:6388	 l-p:0.16579337418079376
epoch£º319	 i:9 	 global-step:6389	 l-p:0.18179385364055634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9983, 5.0188, 4.9754],
        [4.9983, 5.0673, 4.9975],
        [4.9983, 5.0538, 4.9888],
        [4.9983, 4.9962, 4.9980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.16052356362342834 
model_pd.l_d.mean(): -20.163503646850586 
model_pd.lagr.mean(): -20.002979278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-20.8351], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.16052356362342834
epoch£º320	 i:1 	 global-step:6401	 l-p:0.14406432211399078
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10232651233673096
epoch£º320	 i:3 	 global-step:6403	 l-p:0.18493042886257172
epoch£º320	 i:4 	 global-step:6404	 l-p:0.11689257621765137
epoch£º320	 i:5 	 global-step:6405	 l-p:0.14420898258686066
epoch£º320	 i:6 	 global-step:6406	 l-p:0.12807510793209076
epoch£º320	 i:7 	 global-step:6407	 l-p:0.20938631892204285
epoch£º320	 i:8 	 global-step:6408	 l-p:0.2293739765882492
epoch£º320	 i:9 	 global-step:6409	 l-p:0.14335858821868896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0536, 5.0467, 5.0496],
        [5.0536, 5.3877, 5.3818],
        [5.0536, 5.0508, 5.0531],
        [5.0536, 5.0534, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.1176031306385994 
model_pd.l_d.mean(): -19.575952529907227 
model_pd.lagr.mean(): -19.458349227905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-20.2821], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.1176031306385994
epoch£º321	 i:1 	 global-step:6421	 l-p:0.33689063787460327
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11421235650777817
epoch£º321	 i:3 	 global-step:6423	 l-p:0.13277003169059753
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12510228157043457
epoch£º321	 i:5 	 global-step:6425	 l-p:-0.2550677955150604
epoch£º321	 i:6 	 global-step:6426	 l-p:0.12643489241600037
epoch£º321	 i:7 	 global-step:6427	 l-p:0.23947246372699738
epoch£º321	 i:8 	 global-step:6428	 l-p:0.1420079618692398
epoch£º321	 i:9 	 global-step:6429	 l-p:0.15254531800746918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9791, 5.8111, 6.2082],
        [4.9791, 4.9791, 4.9791],
        [4.9791, 4.9788, 4.9791],
        [4.9791, 4.9791, 4.9791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.17749829590320587 
model_pd.l_d.mean(): -20.643508911132812 
model_pd.lagr.mean(): -20.46601104736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4343], device='cuda:0')), ('power', tensor([-21.3128], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.17749829590320587
epoch£º322	 i:1 	 global-step:6441	 l-p:0.14365069568157196
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11666233092546463
epoch£º322	 i:3 	 global-step:6443	 l-p:0.18691998720169067
epoch£º322	 i:4 	 global-step:6444	 l-p:0.1032448559999466
epoch£º322	 i:5 	 global-step:6445	 l-p:0.13483315706253052
epoch£º322	 i:6 	 global-step:6446	 l-p:0.12717148661613464
epoch£º322	 i:7 	 global-step:6447	 l-p:-0.007301230449229479
epoch£º322	 i:8 	 global-step:6448	 l-p:1.4971996545791626
epoch£º322	 i:9 	 global-step:6449	 l-p:1.4420194625854492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0341, 5.0341, 5.0341],
        [5.0341, 5.0312, 5.0336],
        [5.0341, 5.1932, 5.1193],
        [5.0341, 5.0262, 5.0288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.30112701654434204 
model_pd.l_d.mean(): -20.371355056762695 
model_pd.lagr.mean(): -20.070228576660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-21.0580], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.30112701654434204
epoch£º323	 i:1 	 global-step:6461	 l-p:0.14114134013652802
epoch£º323	 i:2 	 global-step:6462	 l-p:0.09993009269237518
epoch£º323	 i:3 	 global-step:6463	 l-p:0.1612224578857422
epoch£º323	 i:4 	 global-step:6464	 l-p:0.12793971598148346
epoch£º323	 i:5 	 global-step:6465	 l-p:0.11988513916730881
epoch£º323	 i:6 	 global-step:6466	 l-p:0.27693212032318115
epoch£º323	 i:7 	 global-step:6467	 l-p:0.11608142405748367
epoch£º323	 i:8 	 global-step:6468	 l-p:0.11296534538269043
epoch£º323	 i:9 	 global-step:6469	 l-p:0.12549810111522675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2451, 5.2451, 5.2451],
        [5.2451, 5.2451, 5.2451],
        [5.2451, 5.2411, 5.2437],
        [5.2451, 5.2553, 5.2290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.13144080340862274 
model_pd.l_d.mean(): -19.664966583251953 
model_pd.lagr.mean(): -19.533525466918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.3648], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.13144080340862274
epoch£º324	 i:1 	 global-step:6481	 l-p:0.1189509779214859
epoch£º324	 i:2 	 global-step:6482	 l-p:0.14767533540725708
epoch£º324	 i:3 	 global-step:6483	 l-p:0.13009309768676758
epoch£º324	 i:4 	 global-step:6484	 l-p:0.09494674205780029
epoch£º324	 i:5 	 global-step:6485	 l-p:0.16109555959701538
epoch£º324	 i:6 	 global-step:6486	 l-p:0.13115805387496948
epoch£º324	 i:7 	 global-step:6487	 l-p:0.13201162219047546
epoch£º324	 i:8 	 global-step:6488	 l-p:0.09641485661268234
epoch£º324	 i:9 	 global-step:6489	 l-p:-0.3085952699184418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8624, 4.8788, 4.8296],
        [4.8624, 4.8557, 4.8605],
        [4.8624, 4.8622, 4.8624],
        [4.8624, 4.9477, 4.8692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.13490188121795654 
model_pd.l_d.mean(): -19.285648345947266 
model_pd.lagr.mean(): -19.150747299194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5302], device='cuda:0')), ('power', tensor([-20.0380], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.13490188121795654
epoch£º325	 i:1 	 global-step:6501	 l-p:0.11535696685314178
epoch£º325	 i:2 	 global-step:6502	 l-p:-0.44892624020576477
epoch£º325	 i:3 	 global-step:6503	 l-p:0.1399739384651184
epoch£º325	 i:4 	 global-step:6504	 l-p:0.12927031517028809
epoch£º325	 i:5 	 global-step:6505	 l-p:0.1407887488603592
epoch£º325	 i:6 	 global-step:6506	 l-p:0.12501613795757294
epoch£º325	 i:7 	 global-step:6507	 l-p:0.12257921695709229
epoch£º325	 i:8 	 global-step:6508	 l-p:0.10892330855131149
epoch£º325	 i:9 	 global-step:6509	 l-p:0.1290932297706604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8966, 4.9731, 4.8961],
        [4.8966, 4.8958, 4.8966],
        [4.8966, 5.6565, 5.9954],
        [4.8966, 4.9600, 4.8862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.13275714218616486 
model_pd.l_d.mean(): -20.319887161254883 
model_pd.lagr.mean(): -20.187129974365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4914], device='cuda:0')), ('power', tensor([-21.0440], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.13275714218616486
epoch£º326	 i:1 	 global-step:6521	 l-p:0.13125571608543396
epoch£º326	 i:2 	 global-step:6522	 l-p:0.16119109094142914
epoch£º326	 i:3 	 global-step:6523	 l-p:0.10132695734500885
epoch£º326	 i:4 	 global-step:6524	 l-p:0.16457971930503845
epoch£º326	 i:5 	 global-step:6525	 l-p:0.07993488758802414
epoch£º326	 i:6 	 global-step:6526	 l-p:0.12568143010139465
epoch£º326	 i:7 	 global-step:6527	 l-p:0.13740791380405426
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11466735601425171
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13398417830467224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8874, 5.6593, 6.0103],
        [4.8874, 4.8872, 4.8874],
        [4.8874, 4.8750, 4.8691],
        [4.8874, 4.8865, 4.8873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.13372081518173218 
model_pd.l_d.mean(): -20.16073989868164 
model_pd.lagr.mean(): -20.027019500732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5001], device='cuda:0')), ('power', tensor([-20.8919], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.13372081518173218
epoch£º327	 i:1 	 global-step:6541	 l-p:-0.0945749580860138
epoch£º327	 i:2 	 global-step:6542	 l-p:0.11820539832115173
epoch£º327	 i:3 	 global-step:6543	 l-p:0.1154351606965065
epoch£º327	 i:4 	 global-step:6544	 l-p:0.04358948767185211
epoch£º327	 i:5 	 global-step:6545	 l-p:0.15910513699054718
epoch£º327	 i:6 	 global-step:6546	 l-p:0.15652087330818176
epoch£º327	 i:7 	 global-step:6547	 l-p:0.0949469581246376
epoch£º327	 i:8 	 global-step:6548	 l-p:0.13004274666309357
epoch£º327	 i:9 	 global-step:6549	 l-p:0.1275096982717514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0198, 5.0155, 5.0189],
        [5.0198, 5.0180, 5.0196],
        [5.0198, 5.0989, 5.0230],
        [5.0198, 5.5176, 5.6216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.28352823853492737 
model_pd.l_d.mean(): -20.09899139404297 
model_pd.lagr.mean(): -19.81546401977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4836], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.28352823853492737
epoch£º328	 i:1 	 global-step:6561	 l-p:0.1244242861866951
epoch£º328	 i:2 	 global-step:6562	 l-p:0.1032605841755867
epoch£º328	 i:3 	 global-step:6563	 l-p:0.4829502999782562
epoch£º328	 i:4 	 global-step:6564	 l-p:0.16659972071647644
epoch£º328	 i:5 	 global-step:6565	 l-p:0.11175210028886795
epoch£º328	 i:6 	 global-step:6566	 l-p:0.20522181689739227
epoch£º328	 i:7 	 global-step:6567	 l-p:0.16606780886650085
epoch£º328	 i:8 	 global-step:6568	 l-p:0.12846873700618744
epoch£º328	 i:9 	 global-step:6569	 l-p:0.10527648776769638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1107, 5.1107, 5.1107],
        [5.1107, 5.3712, 5.3258],
        [5.1107, 5.1101, 5.1107],
        [5.1107, 5.1028, 5.0990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.12889088690280914 
model_pd.l_d.mean(): -19.032791137695312 
model_pd.lagr.mean(): -18.903900146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5140], device='cuda:0')), ('power', tensor([-19.7659], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.12889088690280914
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11194528639316559
epoch£º329	 i:2 	 global-step:6582	 l-p:0.11589226126670837
epoch£º329	 i:3 	 global-step:6583	 l-p:0.14228317141532898
epoch£º329	 i:4 	 global-step:6584	 l-p:0.11706976592540741
epoch£º329	 i:5 	 global-step:6585	 l-p:0.1611117571592331
epoch£º329	 i:6 	 global-step:6586	 l-p:0.12608295679092407
epoch£º329	 i:7 	 global-step:6587	 l-p:0.8342116475105286
epoch£º329	 i:8 	 global-step:6588	 l-p:0.11356236785650253
epoch£º329	 i:9 	 global-step:6589	 l-p:0.009591464884579182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9343, 4.9337, 4.9343],
        [4.9343, 4.9334, 4.9343],
        [4.9343, 4.9338, 4.9343],
        [4.9343, 4.9343, 4.9343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.13086023926734924 
model_pd.l_d.mean(): -20.542739868164062 
model_pd.lagr.mean(): -20.411880493164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4556], device='cuda:0')), ('power', tensor([-21.2327], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.13086023926734924
epoch£º330	 i:1 	 global-step:6601	 l-p:0.1633511483669281
epoch£º330	 i:2 	 global-step:6602	 l-p:0.15308748185634613
epoch£º330	 i:3 	 global-step:6603	 l-p:0.04704713821411133
epoch£º330	 i:4 	 global-step:6604	 l-p:0.16035038232803345
epoch£º330	 i:5 	 global-step:6605	 l-p:0.05743417516350746
epoch£º330	 i:6 	 global-step:6606	 l-p:0.1383640021085739
epoch£º330	 i:7 	 global-step:6607	 l-p:0.12645196914672852
epoch£º330	 i:8 	 global-step:6608	 l-p:0.13621081411838531
epoch£º330	 i:9 	 global-step:6609	 l-p:0.1512802243232727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7919, 4.8285, 4.7582],
        [4.7919, 5.4682, 5.7424],
        [4.7919, 4.7890, 4.7915],
        [4.7919, 5.3656, 5.5509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.09781501442193985 
model_pd.l_d.mean(): -20.29283332824707 
model_pd.lagr.mean(): -20.195018768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5348], device='cuda:0')), ('power', tensor([-21.0609], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.09781501442193985
epoch£º331	 i:1 	 global-step:6621	 l-p:0.13517755270004272
epoch£º331	 i:2 	 global-step:6622	 l-p:0.15501900017261505
epoch£º331	 i:3 	 global-step:6623	 l-p:0.12155462056398392
epoch£º331	 i:4 	 global-step:6624	 l-p:0.15632759034633636
epoch£º331	 i:5 	 global-step:6625	 l-p:0.1689777970314026
epoch£º331	 i:6 	 global-step:6626	 l-p:0.1321839839220047
epoch£º331	 i:7 	 global-step:6627	 l-p:0.7674784660339355
epoch£º331	 i:8 	 global-step:6628	 l-p:0.1259712427854538
epoch£º331	 i:9 	 global-step:6629	 l-p:0.11803397536277771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1058, 5.1245, 5.0791],
        [5.1058, 5.2826, 5.2089],
        [5.1058, 5.1728, 5.1000],
        [5.1058, 5.1038, 5.1056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.12562574446201324 
model_pd.l_d.mean(): -20.2833194732666 
model_pd.lagr.mean(): -20.15769386291504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4493], device='cuda:0')), ('power', tensor([-20.9640], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.12562574446201324
epoch£º332	 i:1 	 global-step:6641	 l-p:0.1063312217593193
epoch£º332	 i:2 	 global-step:6642	 l-p:0.18156234920024872
epoch£º332	 i:3 	 global-step:6643	 l-p:0.10562276840209961
epoch£º332	 i:4 	 global-step:6644	 l-p:0.16178840398788452
epoch£º332	 i:5 	 global-step:6645	 l-p:0.1349909007549286
epoch£º332	 i:6 	 global-step:6646	 l-p:0.12883979082107544
epoch£º332	 i:7 	 global-step:6647	 l-p:0.1411573737859726
epoch£º332	 i:8 	 global-step:6648	 l-p:0.16236892342567444
epoch£º332	 i:9 	 global-step:6649	 l-p:0.11923796683549881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0614, 5.0614, 5.0614],
        [5.0614, 5.3024, 5.2495],
        [5.0614, 5.0614, 5.0614],
        [5.0614, 5.0614, 5.0614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.15436723828315735 
model_pd.l_d.mean(): -20.373567581176758 
model_pd.lagr.mean(): -20.219200134277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-21.0426], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.15436723828315735
epoch£º333	 i:1 	 global-step:6661	 l-p:0.12020234763622284
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12258642166852951
epoch£º333	 i:3 	 global-step:6663	 l-p:-0.4279850721359253
epoch£º333	 i:4 	 global-step:6664	 l-p:0.20061780512332916
epoch£º333	 i:5 	 global-step:6665	 l-p:0.1461116522550583
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12536846101284027
epoch£º333	 i:7 	 global-step:6667	 l-p:0.07372014224529266
epoch£º333	 i:8 	 global-step:6668	 l-p:0.1437951922416687
epoch£º333	 i:9 	 global-step:6669	 l-p:0.06624478846788406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9646, 4.9642, 4.9646],
        [4.9646, 4.9625, 4.9644],
        [4.9646, 4.9931, 4.9325],
        [4.9646, 4.9595, 4.9339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.15273796021938324 
model_pd.l_d.mean(): -20.90011215209961 
model_pd.lagr.mean(): -20.747373580932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.5492], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.15273796021938324
epoch£º334	 i:1 	 global-step:6681	 l-p:0.27435293793678284
epoch£º334	 i:2 	 global-step:6682	 l-p:0.146755650639534
epoch£º334	 i:3 	 global-step:6683	 l-p:0.07322804629802704
epoch£º334	 i:4 	 global-step:6684	 l-p:0.15639102458953857
epoch£º334	 i:5 	 global-step:6685	 l-p:-2.9503815174102783
epoch£º334	 i:6 	 global-step:6686	 l-p:0.1346503049135208
epoch£º334	 i:7 	 global-step:6687	 l-p:0.21857374906539917
epoch£º334	 i:8 	 global-step:6688	 l-p:0.1260853111743927
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11112251877784729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1406, 5.1324, 5.1243],
        [5.1406, 5.9094, 6.2285],
        [5.1406, 5.1406, 5.1406],
        [5.1406, 5.9907, 6.3853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10066656768321991 
model_pd.l_d.mean(): -19.243894577026367 
model_pd.lagr.mean(): -19.14322853088379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4991], device='cuda:0')), ('power', tensor([-19.9640], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10066656768321991
epoch£º335	 i:1 	 global-step:6701	 l-p:0.12930887937545776
epoch£º335	 i:2 	 global-step:6702	 l-p:0.155168354511261
epoch£º335	 i:3 	 global-step:6703	 l-p:0.15396524965763092
epoch£º335	 i:4 	 global-step:6704	 l-p:0.1320175975561142
epoch£º335	 i:5 	 global-step:6705	 l-p:0.1349782794713974
epoch£º335	 i:6 	 global-step:6706	 l-p:0.25957489013671875
epoch£º335	 i:7 	 global-step:6707	 l-p:0.11950307339429855
epoch£º335	 i:8 	 global-step:6708	 l-p:0.13733111321926117
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12451171875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0070, 4.9939, 4.9897],
        [5.0070, 5.0051, 4.9752],
        [5.0070, 5.2399, 5.1843],
        [5.0070, 4.9949, 4.9862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11944213509559631 
model_pd.l_d.mean(): -19.383407592773438 
model_pd.lagr.mean(): -19.263965606689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-20.0744], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11944213509559631
epoch£º336	 i:1 	 global-step:6721	 l-p:0.08146662265062332
epoch£º336	 i:2 	 global-step:6722	 l-p:0.13402120769023895
epoch£º336	 i:3 	 global-step:6723	 l-p:0.12818530201911926
epoch£º336	 i:4 	 global-step:6724	 l-p:0.14495673775672913
epoch£º336	 i:5 	 global-step:6725	 l-p:0.13546541333198547
epoch£º336	 i:6 	 global-step:6726	 l-p:-0.02529684081673622
epoch£º336	 i:7 	 global-step:6727	 l-p:0.12408208847045898
epoch£º336	 i:8 	 global-step:6728	 l-p:0.1455596685409546
epoch£º336	 i:9 	 global-step:6729	 l-p:0.03524290397763252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8418, 5.6968, 6.1313],
        [4.8418, 4.8243, 4.8279],
        [4.8418, 4.9129, 4.8294],
        [4.8418, 5.2602, 5.3193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13984762132167816 
model_pd.l_d.mean(): -18.45923614501953 
model_pd.lagr.mean(): -18.31938934326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5783], device='cuda:0')), ('power', tensor([-19.2518], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13984762132167816
epoch£º337	 i:1 	 global-step:6741	 l-p:-0.23788419365882874
epoch£º337	 i:2 	 global-step:6742	 l-p:0.07138003408908844
epoch£º337	 i:3 	 global-step:6743	 l-p:0.13358977437019348
epoch£º337	 i:4 	 global-step:6744	 l-p:0.11600188910961151
epoch£º337	 i:5 	 global-step:6745	 l-p:0.134108766913414
epoch£º337	 i:6 	 global-step:6746	 l-p:0.13296009600162506
epoch£º337	 i:7 	 global-step:6747	 l-p:0.1529456079006195
epoch£º337	 i:8 	 global-step:6748	 l-p:0.1495903581380844
epoch£º337	 i:9 	 global-step:6749	 l-p:0.3468262851238251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9087, 4.8928, 4.8820],
        [4.9087, 4.8946, 4.9006],
        [4.9087, 4.9049, 4.9082],
        [4.9087, 4.9087, 4.9087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.15321652591228485 
model_pd.l_d.mean(): -20.285367965698242 
model_pd.lagr.mean(): -20.132150650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-21.0158], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.15321652591228485
epoch£º338	 i:1 	 global-step:6761	 l-p:0.12361005693674088
epoch£º338	 i:2 	 global-step:6762	 l-p:0.10164455324411392
epoch£º338	 i:3 	 global-step:6763	 l-p:0.13191178441047668
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.10148981958627701
epoch£º338	 i:5 	 global-step:6765	 l-p:0.15562935173511505
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10292213410139084
epoch£º338	 i:7 	 global-step:6767	 l-p:0.17099285125732422
epoch£º338	 i:8 	 global-step:6768	 l-p:0.10874129831790924
epoch£º338	 i:9 	 global-step:6769	 l-p:0.1489754319190979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0448, 5.6131, 5.7694],
        [5.0448, 5.0310, 5.0297],
        [5.0448, 5.0324, 5.0368],
        [5.0448, 5.0448, 5.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.12505879998207092 
model_pd.l_d.mean(): -20.411645889282227 
model_pd.lagr.mean(): -20.28658676147461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-21.0935], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.12505879998207092
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13664892315864563
epoch£º339	 i:2 	 global-step:6782	 l-p:0.12937676906585693
epoch£º339	 i:3 	 global-step:6783	 l-p:0.5014874339103699
epoch£º339	 i:4 	 global-step:6784	 l-p:0.15482401847839355
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.04241376742720604
epoch£º339	 i:6 	 global-step:6786	 l-p:0.1464678794145584
epoch£º339	 i:7 	 global-step:6787	 l-p:0.12704752385616302
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15741337835788727
epoch£º339	 i:9 	 global-step:6789	 l-p:0.011546969413757324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0390, 5.4177, 5.4368],
        [5.0390, 5.0373, 5.0389],
        [5.0390, 5.0389, 5.0390],
        [5.0390, 5.0263, 5.0157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.12355577200651169 
model_pd.l_d.mean(): -19.943376541137695 
model_pd.lagr.mean(): -19.819820404052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4875], device='cuda:0')), ('power', tensor([-20.6594], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.12355577200651169
epoch£º340	 i:1 	 global-step:6801	 l-p:0.13622605800628662
epoch£º340	 i:2 	 global-step:6802	 l-p:0.606356680393219
epoch£º340	 i:3 	 global-step:6803	 l-p:0.14259058237075806
epoch£º340	 i:4 	 global-step:6804	 l-p:0.12827561795711517
epoch£º340	 i:5 	 global-step:6805	 l-p:0.11848262697458267
epoch£º340	 i:6 	 global-step:6806	 l-p:0.2842918634414673
epoch£º340	 i:7 	 global-step:6807	 l-p:0.12449152022600174
epoch£º340	 i:8 	 global-step:6808	 l-p:0.18949024379253387
epoch£º340	 i:9 	 global-step:6809	 l-p:0.1180134043097496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1095, 5.1089, 5.1095],
        [5.1095, 5.1080, 5.0783],
        [5.1095, 5.5889, 5.6719],
        [5.1095, 5.6065, 5.7024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.13001039624214172 
model_pd.l_d.mean(): -20.103282928466797 
model_pd.lagr.mean(): -19.9732723236084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-20.7979], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.13001039624214172
epoch£º341	 i:1 	 global-step:6821	 l-p:0.14633460342884064
epoch£º341	 i:2 	 global-step:6822	 l-p:0.13894455134868622
epoch£º341	 i:3 	 global-step:6823	 l-p:0.11925217509269714
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12932394444942474
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12054353207349777
epoch£º341	 i:6 	 global-step:6826	 l-p:-0.15450654923915863
epoch£º341	 i:7 	 global-step:6827	 l-p:0.14535251259803772
epoch£º341	 i:8 	 global-step:6828	 l-p:0.08066212385892868
epoch£º341	 i:9 	 global-step:6829	 l-p:0.24152013659477234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9566, 4.9566, 4.9566],
        [4.9566, 5.4931, 5.6315],
        [4.9566, 5.6897, 5.9959],
        [4.9566, 5.4940, 5.6332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.14566974341869354 
model_pd.l_d.mean(): -20.715728759765625 
model_pd.lagr.mean(): -20.570058822631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415], device='cuda:0')), ('power', tensor([-21.3932], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.14566974341869354
epoch£º342	 i:1 	 global-step:6841	 l-p:0.1747952103614807
epoch£º342	 i:2 	 global-step:6842	 l-p:0.13994167745113373
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12397528439760208
epoch£º342	 i:4 	 global-step:6844	 l-p:0.12924312055110931
epoch£º342	 i:5 	 global-step:6845	 l-p:0.22988571226596832
epoch£º342	 i:6 	 global-step:6846	 l-p:-0.143207848072052
epoch£º342	 i:7 	 global-step:6847	 l-p:0.06456547975540161
epoch£º342	 i:8 	 global-step:6848	 l-p:0.03514095023274422
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13600972294807434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0719, 5.6406, 5.7947],
        [5.0719, 5.0719, 5.0719],
        [5.0719, 5.8318, 6.1498],
        [5.0719, 5.0604, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13466675579547882 
model_pd.l_d.mean(): -20.74421501159668 
model_pd.lagr.mean(): -20.609548568725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3989], device='cuda:0')), ('power', tensor([-21.3784], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13466675579547882
epoch£º343	 i:1 	 global-step:6861	 l-p:0.1308739185333252
epoch£º343	 i:2 	 global-step:6862	 l-p:0.1318962275981903
epoch£º343	 i:3 	 global-step:6863	 l-p:0.11501286178827286
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13135327398777008
epoch£º343	 i:5 	 global-step:6865	 l-p:0.5687323212623596
epoch£º343	 i:6 	 global-step:6866	 l-p:0.1292121410369873
epoch£º343	 i:7 	 global-step:6867	 l-p:1.0115288496017456
epoch£º343	 i:8 	 global-step:6868	 l-p:0.1261114925146103
epoch£º343	 i:9 	 global-step:6869	 l-p:0.12872101366519928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[5.0087, 5.7464, 6.0511],
        [5.0087, 5.5775, 5.7371],
        [5.0087, 4.9981, 4.9748],
        [5.0087, 4.9966, 4.9767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14328955113887787 
model_pd.l_d.mean(): -20.773391723632812 
model_pd.lagr.mean(): -20.630102157592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4144], device='cuda:0')), ('power', tensor([-21.4237], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14328955113887787
epoch£º344	 i:1 	 global-step:6881	 l-p:0.08774149417877197
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13140080869197845
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12679839134216309
epoch£º344	 i:4 	 global-step:6884	 l-p:0.09787014126777649
epoch£º344	 i:5 	 global-step:6885	 l-p:0.1501086950302124
epoch£º344	 i:6 	 global-step:6886	 l-p:0.13309699296951294
epoch£º344	 i:7 	 global-step:6887	 l-p:0.08962885290384293
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14857977628707886
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.10376083105802536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8824, 4.8719, 4.8790],
        [4.8824, 5.5870, 5.8753],
        [4.8824, 4.8696, 4.8773],
        [4.8824, 4.9445, 4.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.1249641552567482 
model_pd.l_d.mean(): -19.504913330078125 
model_pd.lagr.mean(): -19.37994956970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5012], device='cuda:0')), ('power', tensor([-20.2301], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.1249641552567482
epoch£º345	 i:1 	 global-step:6901	 l-p:0.14091037213802338
epoch£º345	 i:2 	 global-step:6902	 l-p:0.12344732135534286
epoch£º345	 i:3 	 global-step:6903	 l-p:0.146709606051445
epoch£º345	 i:4 	 global-step:6904	 l-p:0.12236668169498444
epoch£º345	 i:5 	 global-step:6905	 l-p:-0.21272586286067963
epoch£º345	 i:6 	 global-step:6906	 l-p:0.08840342611074448
epoch£º345	 i:7 	 global-step:6907	 l-p:0.1900152564048767
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13359294831752777
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12486526370048523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0299, 5.0238, 5.0286],
        [5.0299, 5.0298, 5.0299],
        [5.0299, 5.0159, 5.0220],
        [5.0299, 5.1499, 5.0627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.14913715422153473 
model_pd.l_d.mean(): -19.89942169189453 
model_pd.lagr.mean(): -19.75028419494629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-20.6405], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.14913715422153473
epoch£º346	 i:1 	 global-step:6921	 l-p:0.13188378512859344
epoch£º346	 i:2 	 global-step:6922	 l-p:0.14173150062561035
epoch£º346	 i:3 	 global-step:6923	 l-p:0.23814570903778076
epoch£º346	 i:4 	 global-step:6924	 l-p:0.13113747537136078
epoch£º346	 i:5 	 global-step:6925	 l-p:0.10614477097988129
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12197761982679367
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12908168137073517
epoch£º346	 i:8 	 global-step:6928	 l-p:0.13564234972000122
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.12665322422981262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0010, 5.0010, 5.0010],
        [5.0010, 5.0784, 4.9919],
        [5.0010, 5.7935, 6.1487],
        [5.0010, 4.9981, 5.0007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.12110254913568497 
model_pd.l_d.mean(): -19.967391967773438 
model_pd.lagr.mean(): -19.846288681030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5077], device='cuda:0')), ('power', tensor([-20.7043], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.12110254913568497
epoch£º347	 i:1 	 global-step:6941	 l-p:0.08079671859741211
epoch£º347	 i:2 	 global-step:6942	 l-p:0.08629680424928665
epoch£º347	 i:3 	 global-step:6943	 l-p:0.08020827919244766
epoch£º347	 i:4 	 global-step:6944	 l-p:0.12731017172336578
epoch£º347	 i:5 	 global-step:6945	 l-p:0.17012520134449005
epoch£º347	 i:6 	 global-step:6946	 l-p:0.2577609717845917
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12553684413433075
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11920823156833649
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13630500435829163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0391, 5.0214, 5.0205],
        [5.0391, 5.0253, 5.0318],
        [5.0391, 5.0217, 5.0229],
        [5.0391, 5.0391, 5.0391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.13625968992710114 
model_pd.l_d.mean(): -20.422618865966797 
model_pd.lagr.mean(): -20.286359786987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.0907], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.13625968992710114
epoch£º348	 i:1 	 global-step:6961	 l-p:0.0010992336319759488
epoch£º348	 i:2 	 global-step:6962	 l-p:0.1612963080406189
epoch£º348	 i:3 	 global-step:6963	 l-p:0.19354839622974396
epoch£º348	 i:4 	 global-step:6964	 l-p:0.12867818772792816
epoch£º348	 i:5 	 global-step:6965	 l-p:0.11988405883312225
epoch£º348	 i:6 	 global-step:6966	 l-p:0.1486201286315918
epoch£º348	 i:7 	 global-step:6967	 l-p:-0.027147388085722923
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13950859010219574
epoch£º348	 i:9 	 global-step:6969	 l-p:0.05676603317260742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9724, 4.9724, 4.9724],
        [4.9724, 4.9714, 4.9723],
        [4.9724, 4.9712, 4.9723],
        [4.9724, 4.9576, 4.9649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.1393326371908188 
model_pd.l_d.mean(): -18.900432586669922 
model_pd.lagr.mean(): -18.76110076904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-19.6653], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.1393326371908188
epoch£º349	 i:1 	 global-step:6981	 l-p:0.1277686059474945
epoch£º349	 i:2 	 global-step:6982	 l-p:0.13451269268989563
epoch£º349	 i:3 	 global-step:6983	 l-p:0.15739217400550842
epoch£º349	 i:4 	 global-step:6984	 l-p:0.1243346631526947
epoch£º349	 i:5 	 global-step:6985	 l-p:0.14525672793388367
epoch£º349	 i:6 	 global-step:6986	 l-p:0.1581445336341858
epoch£º349	 i:7 	 global-step:6987	 l-p:-0.007285256404429674
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13837040960788727
epoch£º349	 i:9 	 global-step:6989	 l-p:0.08275993913412094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8130, 4.8117, 4.8130],
        [4.8130, 5.2571, 5.3363],
        [4.8130, 5.1181, 5.1032],
        [4.8130, 5.2658, 5.3515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.21819369494915009 
model_pd.l_d.mean(): -20.494728088378906 
model_pd.lagr.mean(): -20.276535034179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5139], device='cuda:0')), ('power', tensor([-21.2437], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.21819369494915009
epoch£º350	 i:1 	 global-step:7001	 l-p:0.13086001574993134
epoch£º350	 i:2 	 global-step:7002	 l-p:0.06754781305789948
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11914942413568497
epoch£º350	 i:4 	 global-step:7004	 l-p:0.2847798764705658
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13381510972976685
epoch£º350	 i:6 	 global-step:7006	 l-p:0.12189457565546036
epoch£º350	 i:7 	 global-step:7007	 l-p:0.05588046833872795
epoch£º350	 i:8 	 global-step:7008	 l-p:0.1364208310842514
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12343598902225494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0244, 5.0060, 5.0089],
        [5.0244, 5.0053, 4.9995],
        [5.0244, 5.1813, 5.0975],
        [5.0244, 5.2002, 5.1209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): -0.022016244009137154 
model_pd.l_d.mean(): -20.173744201660156 
model_pd.lagr.mean(): -20.19576072692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867], device='cuda:0')), ('power', tensor([-20.8914], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:-0.022016244009137154
epoch£º351	 i:1 	 global-step:7021	 l-p:1.0959968566894531
epoch£º351	 i:2 	 global-step:7022	 l-p:0.10967250913381577
epoch£º351	 i:3 	 global-step:7023	 l-p:0.13941724598407745
epoch£º351	 i:4 	 global-step:7024	 l-p:0.1221267357468605
epoch£º351	 i:5 	 global-step:7025	 l-p:0.1325664520263672
epoch£º351	 i:6 	 global-step:7026	 l-p:0.15299996733665466
epoch£º351	 i:7 	 global-step:7027	 l-p:0.11998166888952255
epoch£º351	 i:8 	 global-step:7028	 l-p:0.06796884536743164
epoch£º351	 i:9 	 global-step:7029	 l-p:0.1339094638824463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9627, 4.9434, 4.9489],
        [4.9627, 5.0418, 4.9517],
        [4.9627, 4.9421, 4.9290],
        [4.9627, 5.1673, 5.0978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.09226727485656738 
model_pd.l_d.mean(): -20.984256744384766 
model_pd.lagr.mean(): -20.89198875427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.6245], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.09226727485656738
epoch£º352	 i:1 	 global-step:7041	 l-p:0.1885046362876892
epoch£º352	 i:2 	 global-step:7042	 l-p:0.14180688560009003
epoch£º352	 i:3 	 global-step:7043	 l-p:0.19230280816555023
epoch£º352	 i:4 	 global-step:7044	 l-p:0.1455877125263214
epoch£º352	 i:5 	 global-step:7045	 l-p:0.7804364562034607
epoch£º352	 i:6 	 global-step:7046	 l-p:0.5074583888053894
epoch£º352	 i:7 	 global-step:7047	 l-p:0.1134423017501831
epoch£º352	 i:8 	 global-step:7048	 l-p:0.1372462958097458
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11856634169816971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1263, 5.1214, 5.1255],
        [5.1263, 5.1245, 5.1262],
        [5.1263, 5.1246, 5.1262],
        [5.1263, 5.1263, 5.1263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12594980001449585 
model_pd.l_d.mean(): -20.453176498413086 
model_pd.lagr.mean(): -20.327226638793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12594980001449585
epoch£º353	 i:1 	 global-step:7061	 l-p:0.13182926177978516
epoch£º353	 i:2 	 global-step:7062	 l-p:0.1747206449508667
epoch£º353	 i:3 	 global-step:7063	 l-p:0.1392705887556076
epoch£º353	 i:4 	 global-step:7064	 l-p:0.11259473860263824
epoch£º353	 i:5 	 global-step:7065	 l-p:0.16605223715305328
epoch£º353	 i:6 	 global-step:7066	 l-p:0.14798001945018768
epoch£º353	 i:7 	 global-step:7067	 l-p:0.14121420681476593
epoch£º353	 i:8 	 global-step:7068	 l-p:-0.21100929379463196
epoch£º353	 i:9 	 global-step:7069	 l-p:0.11858867108821869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0093, 4.9920, 4.9989],
        [5.0093, 5.2764, 5.2320],
        [5.0093, 5.8309, 6.2115],
        [5.0093, 5.0041, 5.0085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12723970413208008 
model_pd.l_d.mean(): -20.74972915649414 
model_pd.lagr.mean(): -20.62248992919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4036], device='cuda:0')), ('power', tensor([-21.3888], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12723970413208008
epoch£º354	 i:1 	 global-step:7081	 l-p:0.27795472741127014
epoch£º354	 i:2 	 global-step:7082	 l-p:0.08057349920272827
epoch£º354	 i:3 	 global-step:7083	 l-p:0.1827748864889145
epoch£º354	 i:4 	 global-step:7084	 l-p:0.14130550622940063
epoch£º354	 i:5 	 global-step:7085	 l-p:0.1353711038827896
epoch£º354	 i:6 	 global-step:7086	 l-p:0.14353862404823303
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13868889212608337
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12463178485631943
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.036170437932014465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9859, 5.8060, 6.1878],
        [4.9859, 4.9731, 4.9811],
        [4.9859, 4.9829, 4.9856],
        [4.9859, 4.9848, 4.9859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13099290430545807 
model_pd.l_d.mean(): -20.144201278686523 
model_pd.lagr.mean(): -20.013208389282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.8491], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13099290430545807
epoch£º355	 i:1 	 global-step:7101	 l-p:0.12827958166599274
epoch£º355	 i:2 	 global-step:7102	 l-p:0.16846084594726562
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12438195198774338
epoch£º355	 i:4 	 global-step:7104	 l-p:0.14772632718086243
epoch£º355	 i:5 	 global-step:7105	 l-p:0.1678324043750763
epoch£º355	 i:6 	 global-step:7106	 l-p:0.15002413094043732
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08295758813619614
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14035764336585999
epoch£º355	 i:9 	 global-step:7109	 l-p:0.0837264433503151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8216, 4.7931, 4.7915],
        [4.8216, 5.2304, 5.2825],
        [4.8216, 4.8216, 4.8216],
        [4.8216, 4.7928, 4.7892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16300402581691742 
model_pd.l_d.mean(): -19.89017105102539 
model_pd.lagr.mean(): -19.7271671295166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-20.6371], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16300402581691742
epoch£º356	 i:1 	 global-step:7121	 l-p:0.15075188875198364
epoch£º356	 i:2 	 global-step:7122	 l-p:0.1478700041770935
epoch£º356	 i:3 	 global-step:7123	 l-p:0.1978478580713272
epoch£º356	 i:4 	 global-step:7124	 l-p:0.07954149693250656
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10912293195724487
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18225151300430298
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06923304498195648
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08828768879175186
epoch£º356	 i:9 	 global-step:7129	 l-p:0.1481824815273285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9218, 5.6695, 5.9911],
        [4.9218, 4.9200, 4.9217],
        [4.9218, 4.9139, 4.9201],
        [4.9218, 4.9002, 4.8743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 4.066180229187012 
model_pd.l_d.mean(): -20.009998321533203 
model_pd.lagr.mean(): -15.943818092346191 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5413], device='cuda:0')), ('power', tensor([-20.7817], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:4.066180229187012
epoch£º357	 i:1 	 global-step:7141	 l-p:0.15444914996623993
epoch£º357	 i:2 	 global-step:7142	 l-p:0.1371787041425705
epoch£º357	 i:3 	 global-step:7143	 l-p:1.030571460723877
epoch£º357	 i:4 	 global-step:7144	 l-p:0.06678884476423264
epoch£º357	 i:5 	 global-step:7145	 l-p:0.13779518008232117
epoch£º357	 i:6 	 global-step:7146	 l-p:0.1320101022720337
epoch£º357	 i:7 	 global-step:7147	 l-p:0.12690843641757965
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11659953743219376
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12826259434223175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9398, 4.9390, 4.9398],
        [4.9398, 4.9398, 4.9398],
        [4.9398, 4.9421, 4.8805],
        [4.9398, 5.5220, 5.6978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.07941676676273346 
model_pd.l_d.mean(): -20.620887756347656 
model_pd.lagr.mean(): -20.541471481323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.3119], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.07941676676273346
epoch£º358	 i:1 	 global-step:7161	 l-p:0.03267204388976097
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15526063740253448
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13317351043224335
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09354577213525772
epoch£º358	 i:5 	 global-step:7165	 l-p:0.12959693372249603
epoch£º358	 i:6 	 global-step:7166	 l-p:0.1474684476852417
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13389281928539276
epoch£º358	 i:8 	 global-step:7168	 l-p:0.2732033431529999
epoch£º358	 i:9 	 global-step:7169	 l-p:0.19050592184066772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9711, 4.9539, 4.9628],
        [4.9711, 4.9711, 4.9711],
        [4.9711, 4.9601, 4.9155],
        [4.9711, 4.9711, 4.9711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1410062313079834 
model_pd.l_d.mean(): -20.287548065185547 
model_pd.lagr.mean(): -20.146541595458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-20.9962], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1410062313079834
epoch£º359	 i:1 	 global-step:7181	 l-p:0.1273518055677414
epoch£º359	 i:2 	 global-step:7182	 l-p:0.13194212317466736
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12425035983324051
epoch£º359	 i:4 	 global-step:7184	 l-p:0.5071401596069336
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14466410875320435
epoch£º359	 i:6 	 global-step:7186	 l-p:0.24117831885814667
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09542594850063324
epoch£º359	 i:8 	 global-step:7188	 l-p:0.17055045068264008
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14186888933181763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9016, 4.8757, 4.8527],
        [4.9016, 4.8764, 4.8511],
        [4.9016, 4.8732, 4.8685],
        [4.9016, 4.9015, 4.9016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.13121764361858368 
model_pd.l_d.mean(): -20.131134033203125 
model_pd.lagr.mean(): -19.999916076660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5239], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.13121764361858368
epoch£º360	 i:1 	 global-step:7201	 l-p:0.11326003074645996
epoch£º360	 i:2 	 global-step:7202	 l-p:0.18710994720458984
epoch£º360	 i:3 	 global-step:7203	 l-p:0.018154077231884003
epoch£º360	 i:4 	 global-step:7204	 l-p:0.16244594752788544
epoch£º360	 i:5 	 global-step:7205	 l-p:0.15008144080638885
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14667479693889618
epoch£º360	 i:7 	 global-step:7207	 l-p:0.08177915215492249
epoch£º360	 i:8 	 global-step:7208	 l-p:0.13107994198799133
epoch£º360	 i:9 	 global-step:7209	 l-p:0.14104948937892914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8488, 4.8488],
        [4.8488, 4.8488, 4.8488],
        [4.8488, 4.8487, 4.8488],
        [4.8488, 4.8473, 4.8487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.1771710366010666 
model_pd.l_d.mean(): -20.642528533935547 
model_pd.lagr.mean(): -20.465356826782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.3706], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.1771710366010666
epoch£º361	 i:1 	 global-step:7221	 l-p:0.19019266963005066
epoch£º361	 i:2 	 global-step:7222	 l-p:-0.1465255469083786
epoch£º361	 i:3 	 global-step:7223	 l-p:0.09291650354862213
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11245519667863846
epoch£º361	 i:5 	 global-step:7225	 l-p:0.11554767191410065
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12883585691452026
epoch£º361	 i:7 	 global-step:7227	 l-p:0.13299861550331116
epoch£º361	 i:8 	 global-step:7228	 l-p:-0.22262130677700043
epoch£º361	 i:9 	 global-step:7229	 l-p:0.3465895652770996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0869, 5.0732, 5.0386],
        [5.0869, 5.0869, 5.0869],
        [5.0869, 5.0869, 5.0869],
        [5.0869, 5.5296, 5.5860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.13101647794246674 
model_pd.l_d.mean(): -20.359037399291992 
model_pd.lagr.mean(): -20.2280216217041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4432], device='cuda:0')), ('power', tensor([-21.0343], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.13101647794246674
epoch£º362	 i:1 	 global-step:7241	 l-p:0.1957702785730362
epoch£º362	 i:2 	 global-step:7242	 l-p:0.13589827716350555
epoch£º362	 i:3 	 global-step:7243	 l-p:0.13790003955364227
epoch£º362	 i:4 	 global-step:7244	 l-p:0.17960181832313538
epoch£º362	 i:5 	 global-step:7245	 l-p:0.13189053535461426
epoch£º362	 i:6 	 global-step:7246	 l-p:0.14045415818691254
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1336086094379425
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12310832738876343
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13762076199054718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1223, 5.1002, 5.0977],
        [5.1223, 5.1218, 5.1223],
        [5.1223, 5.1210, 5.1222],
        [5.1223, 5.1224, 5.0698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.19508399069309235 
model_pd.l_d.mean(): -20.694990158081055 
model_pd.lagr.mean(): -20.499906539916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3926], device='cuda:0')), ('power', tensor([-21.3223], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.19508399069309235
epoch£º363	 i:1 	 global-step:7261	 l-p:0.15031477808952332
epoch£º363	 i:2 	 global-step:7262	 l-p:0.08310651779174805
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11465700715780258
epoch£º363	 i:4 	 global-step:7264	 l-p:0.12735390663146973
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14190679788589478
epoch£º363	 i:6 	 global-step:7266	 l-p:0.11656860262155533
epoch£º363	 i:7 	 global-step:7267	 l-p:0.15545214712619781
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12031102925539017
epoch£º363	 i:9 	 global-step:7269	 l-p:0.1234833300113678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1269, 6.0416, 6.4958],
        [5.1269, 5.1045, 5.1020],
        [5.1269, 5.1268, 5.1269],
        [5.1269, 5.1375, 5.0737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14770060777664185 
model_pd.l_d.mean(): -19.3327579498291 
model_pd.lagr.mean(): -19.185056686401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.0205], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14770060777664185
epoch£º364	 i:1 	 global-step:7281	 l-p:0.12148506939411163
epoch£º364	 i:2 	 global-step:7282	 l-p:-1.025638461112976
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11937153339385986
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13848146796226501
epoch£º364	 i:5 	 global-step:7285	 l-p:0.08951183408498764
epoch£º364	 i:6 	 global-step:7286	 l-p:0.054919615387916565
epoch£º364	 i:7 	 global-step:7287	 l-p:0.17637236416339874
epoch£º364	 i:8 	 global-step:7288	 l-p:0.13296735286712646
epoch£º364	 i:9 	 global-step:7289	 l-p:0.04477634280920029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8608, 4.8608, 4.8608],
        [4.8608, 4.8396, 4.7913],
        [4.8608, 4.8294, 4.8059],
        [4.8608, 4.8546, 4.8598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.0982687771320343 
model_pd.l_d.mean(): -20.775602340698242 
model_pd.lagr.mean(): -20.67733383178711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.4775], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.0982687771320343
epoch£º365	 i:1 	 global-step:7301	 l-p:0.17632953822612762
epoch£º365	 i:2 	 global-step:7302	 l-p:0.1501726508140564
epoch£º365	 i:3 	 global-step:7303	 l-p:0.11085746437311172
epoch£º365	 i:4 	 global-step:7304	 l-p:0.14578427374362946
epoch£º365	 i:5 	 global-step:7305	 l-p:0.137347012758255
epoch£º365	 i:6 	 global-step:7306	 l-p:-0.40883758664131165
epoch£º365	 i:7 	 global-step:7307	 l-p:0.11340917646884918
epoch£º365	 i:8 	 global-step:7308	 l-p:0.1261637806892395
epoch£º365	 i:9 	 global-step:7309	 l-p:0.13455787301063538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9393, 4.9356, 4.9389],
        [4.9393, 5.6248, 5.8873],
        [4.9393, 5.7128, 6.0545],
        [4.9393, 4.9363, 4.9390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11647862941026688 
model_pd.l_d.mean(): -18.692195892333984 
model_pd.lagr.mean(): -18.57571792602539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6125], device='cuda:0')), ('power', tensor([-19.5222], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11647862941026688
epoch£º366	 i:1 	 global-step:7321	 l-p:0.1369880884885788
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13700376451015472
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13768358528614044
epoch£º366	 i:4 	 global-step:7324	 l-p:-0.7141960859298706
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13268911838531494
epoch£º366	 i:6 	 global-step:7326	 l-p:0.09393776953220367
epoch£º366	 i:7 	 global-step:7327	 l-p:0.14087341725826263
epoch£º366	 i:8 	 global-step:7328	 l-p:0.06747175008058548
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12537847459316254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.0808, 5.8285, 6.1315],
        [5.0808, 5.0679, 5.0242],
        [5.0808, 5.0563, 5.0396],
        [5.0808, 5.1692, 5.0718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.2287772297859192 
model_pd.l_d.mean(): -19.335233688354492 
model_pd.lagr.mean(): -19.106456756591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.0592], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.2287772297859192
epoch£º367	 i:1 	 global-step:7341	 l-p:0.14307457208633423
epoch£º367	 i:2 	 global-step:7342	 l-p:0.14245551824569702
epoch£º367	 i:3 	 global-step:7343	 l-p:0.14381390810012817
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13072240352630615
epoch£º367	 i:5 	 global-step:7345	 l-p:0.13003003597259521
epoch£º367	 i:6 	 global-step:7346	 l-p:0.1430276334285736
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12052983790636063
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10060413181781769
epoch£º367	 i:9 	 global-step:7349	 l-p:0.12950341403484344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2099, 5.2099, 5.2099],
        [5.2099, 5.2099, 5.2099],
        [5.2099, 5.2077, 5.2097],
        [5.2099, 5.2613, 5.1737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15620562434196472 
model_pd.l_d.mean(): -20.297588348388672 
model_pd.lagr.mean(): -20.141382217407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-20.9502], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15620562434196472
epoch£º368	 i:1 	 global-step:7361	 l-p:0.0847935825586319
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12413319945335388
epoch£º368	 i:3 	 global-step:7363	 l-p:0.20281711220741272
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11636008322238922
epoch£º368	 i:5 	 global-step:7365	 l-p:0.13566887378692627
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15761211514472961
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11755713075399399
epoch£º368	 i:8 	 global-step:7368	 l-p:0.10902902483940125
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18273429572582245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0189, 5.1308, 5.0314],
        [5.0189, 5.0171, 5.0188],
        [5.0189, 5.0189, 5.0189],
        [5.0189, 5.7472, 6.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.1458086520433426 
model_pd.l_d.mean(): -19.733911514282227 
model_pd.lagr.mean(): -19.588102340698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5402], device='cuda:0')), ('power', tensor([-20.5015], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.1458086520433426
epoch£º369	 i:1 	 global-step:7381	 l-p:0.12360349297523499
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08312013745307922
epoch£º369	 i:3 	 global-step:7383	 l-p:0.17470945417881012
epoch£º369	 i:4 	 global-step:7384	 l-p:0.1276462972164154
epoch£º369	 i:5 	 global-step:7385	 l-p:0.1234918013215065
epoch£º369	 i:6 	 global-step:7386	 l-p:0.16877885162830353
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09310897439718246
epoch£º369	 i:8 	 global-step:7388	 l-p:-3.3466169834136963
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12787692248821259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0186, 5.0605, 4.9667],
        [5.0186, 5.0162, 5.0184],
        [5.0186, 5.5357, 5.6510],
        [5.0186, 4.9886, 4.9871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.13191384077072144 
model_pd.l_d.mean(): -17.956552505493164 
model_pd.lagr.mean(): -17.82463836669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-18.7272], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.13191384077072144
epoch£º370	 i:1 	 global-step:7401	 l-p:0.1366550475358963
epoch£º370	 i:2 	 global-step:7402	 l-p:0.07103574275970459
epoch£º370	 i:3 	 global-step:7403	 l-p:0.13786888122558594
epoch£º370	 i:4 	 global-step:7404	 l-p:0.1335706114768982
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13489778339862823
epoch£º370	 i:6 	 global-step:7406	 l-p:0.15223033726215363
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11683999747037888
epoch£º370	 i:8 	 global-step:7408	 l-p:-5.424009799957275
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12220153212547302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.0346, 5.0346],
        [5.0346, 5.0334, 5.0345],
        [5.0346, 5.7565, 6.0405],
        [5.0346, 5.0236, 5.0318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.09757431596517563 
model_pd.l_d.mean(): -20.284154891967773 
model_pd.lagr.mean(): -20.381729125976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.9851], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.09757431596517563
epoch£º371	 i:1 	 global-step:7421	 l-p:0.12920980155467987
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12136875838041306
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14552542567253113
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11169005185365677
epoch£º371	 i:5 	 global-step:7425	 l-p:-0.0902799665927887
epoch£º371	 i:6 	 global-step:7426	 l-p:0.11764492839574814
epoch£º371	 i:7 	 global-step:7427	 l-p:0.19884681701660156
epoch£º371	 i:8 	 global-step:7428	 l-p:0.12985067069530487
epoch£º371	 i:9 	 global-step:7429	 l-p:0.1219184622168541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 5.6345, 5.9968],
        [4.8474, 4.8474, 4.8474],
        [4.8474, 4.8460, 4.8474],
        [4.8474, 4.8539, 4.7658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.13017085194587708 
model_pd.l_d.mean(): -19.425941467285156 
model_pd.lagr.mean(): -19.2957706451416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.2043], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.13017085194587708
epoch£º372	 i:1 	 global-step:7441	 l-p:0.06895139813423157
epoch£º372	 i:2 	 global-step:7442	 l-p:0.13454869389533997
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15797917544841766
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18111644685268402
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13206341862678528
epoch£º372	 i:6 	 global-step:7446	 l-p:0.20495803654193878
epoch£º372	 i:7 	 global-step:7447	 l-p:0.16460563242435455
epoch£º372	 i:8 	 global-step:7448	 l-p:0.14185014367103577
epoch£º372	 i:9 	 global-step:7449	 l-p:0.08552916347980499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8499, 5.5048, 5.7481],
        [4.8499, 4.8312, 4.8432],
        [4.8499, 4.8499, 4.8499],
        [4.8499, 4.8896, 4.7866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.1439729928970337 
model_pd.l_d.mean(): -19.023685455322266 
model_pd.lagr.mean(): -18.87971305847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.8191], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.1439729928970337
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.04315079748630524
epoch£º373	 i:2 	 global-step:7462	 l-p:0.08074887841939926
epoch£º373	 i:3 	 global-step:7463	 l-p:0.15702128410339355
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11836830526590347
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13325974345207214
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14462953805923462
epoch£º373	 i:7 	 global-step:7467	 l-p:0.15948359668254852
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14678449928760529
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10242810100317001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9575, 4.9544, 4.8782],
        [4.9575, 4.9327, 4.9442],
        [4.9575, 4.9382, 4.9498],
        [4.9575, 5.7145, 6.0372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09903743863105774 
model_pd.l_d.mean(): -20.609943389892578 
model_pd.lagr.mean(): -20.510906219482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.2972], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09903743863105774
epoch£º374	 i:1 	 global-step:7481	 l-p:0.2338661104440689
epoch£º374	 i:2 	 global-step:7482	 l-p:0.06189139187335968
epoch£º374	 i:3 	 global-step:7483	 l-p:0.1259448528289795
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13861246407032013
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1352589726448059
epoch£º374	 i:6 	 global-step:7486	 l-p:0.09786521643400192
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10802532732486725
epoch£º374	 i:8 	 global-step:7488	 l-p:0.14086280763149261
epoch£º374	 i:9 	 global-step:7489	 l-p:0.06722865253686905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9056, 4.9017, 4.9052],
        [4.9056, 4.9056, 4.9056],
        [4.9056, 4.9010, 4.9051],
        [4.9056, 4.9055, 4.9056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.0700792595744133 
model_pd.l_d.mean(): -20.469921112060547 
model_pd.lagr.mean(): -20.39984130859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.1667], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.0700792595744133
epoch£º375	 i:1 	 global-step:7501	 l-p:0.3718821108341217
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14868131279945374
epoch£º375	 i:3 	 global-step:7503	 l-p:0.12175612151622772
epoch£º375	 i:4 	 global-step:7504	 l-p:0.1272706687450409
epoch£º375	 i:5 	 global-step:7505	 l-p:0.13327373564243317
epoch£º375	 i:6 	 global-step:7506	 l-p:0.18723218142986298
epoch£º375	 i:7 	 global-step:7507	 l-p:0.13228310644626617
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14611594378948212
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11690003424882889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0617, 5.0331, 5.0400],
        [5.0617, 5.1065, 5.0086],
        [5.0617, 5.0616, 5.0617],
        [5.0617, 5.0403, 4.9940]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11039397865533829 
model_pd.l_d.mean(): -19.676008224487305 
model_pd.lagr.mean(): -19.565614700317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.3997], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11039397865533829
epoch£º376	 i:1 	 global-step:7521	 l-p:0.16784150898456573
epoch£º376	 i:2 	 global-step:7522	 l-p:0.1671009361743927
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13314901292324066
epoch£º376	 i:4 	 global-step:7524	 l-p:0.32815513014793396
epoch£º376	 i:5 	 global-step:7525	 l-p:0.13620413839817047
epoch£º376	 i:6 	 global-step:7526	 l-p:0.13502103090286255
epoch£º376	 i:7 	 global-step:7527	 l-p:0.11021900177001953
epoch£º376	 i:8 	 global-step:7528	 l-p:0.1325603723526001
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.08899468928575516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0542, 5.0370, 4.9813],
        [5.0542, 5.0254, 5.0332],
        [5.0542, 5.0286, 5.0389],
        [5.0542, 5.7428, 5.9946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.015546073205769062 
model_pd.l_d.mean(): -20.280113220214844 
model_pd.lagr.mean(): -20.2956600189209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.9576], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.015546073205769062
epoch£º377	 i:1 	 global-step:7541	 l-p:0.026049919426441193
epoch£º377	 i:2 	 global-step:7542	 l-p:0.16625037789344788
epoch£º377	 i:3 	 global-step:7543	 l-p:0.23592357337474823
epoch£º377	 i:4 	 global-step:7544	 l-p:0.11496727913618088
epoch£º377	 i:5 	 global-step:7545	 l-p:0.1291700005531311
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14150367677211761
epoch£º377	 i:7 	 global-step:7547	 l-p:0.13149601221084595
epoch£º377	 i:8 	 global-step:7548	 l-p:0.07352393865585327
epoch£º377	 i:9 	 global-step:7549	 l-p:0.12135293334722519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2201, 5.2531, 5.1653],
        [5.2201, 5.2201, 5.2201],
        [5.2201, 5.1926, 5.1841],
        [5.2201, 5.1981, 5.2068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.13634513318538666 
model_pd.l_d.mean(): -19.89695930480957 
model_pd.lagr.mean(): -19.7606143951416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-20.5833], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.13634513318538666
epoch£º378	 i:1 	 global-step:7561	 l-p:0.11195755004882812
epoch£º378	 i:2 	 global-step:7562	 l-p:0.13185591995716095
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13738347589969635
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10580221563577652
epoch£º378	 i:5 	 global-step:7565	 l-p:0.15550263226032257
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14388862252235413
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12418639659881592
epoch£º378	 i:8 	 global-step:7568	 l-p:0.12448129057884216
epoch£º378	 i:9 	 global-step:7569	 l-p:0.1348574459552765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 4.9663, 4.9322],
        [4.9992, 4.9992, 4.9992],
        [4.9992, 4.9899, 4.9974],
        [4.9992, 4.9761, 4.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.14658702909946442 
model_pd.l_d.mean(): -19.108741760253906 
model_pd.lagr.mean(): -18.962154388427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.8638], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.14658702909946442
epoch£º379	 i:1 	 global-step:7581	 l-p:0.09327200055122375
epoch£º379	 i:2 	 global-step:7582	 l-p:0.12454518675804138
epoch£º379	 i:3 	 global-step:7583	 l-p:0.091812863945961
epoch£º379	 i:4 	 global-step:7584	 l-p:0.14170654118061066
epoch£º379	 i:5 	 global-step:7585	 l-p:0.14040593802928925
epoch£º379	 i:6 	 global-step:7586	 l-p:0.07691572606563568
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13409243524074554
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14800234138965607
epoch£º379	 i:9 	 global-step:7589	 l-p:0.1238124892115593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7716, 4.9189, 4.8207],
        [4.7716, 4.7576, 4.7682],
        [4.7716, 4.7713, 4.7716],
        [4.7716, 4.7687, 4.7714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.15757951140403748 
model_pd.l_d.mean(): -20.415029525756836 
model_pd.lagr.mean(): -20.257450103759766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-21.1699], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.15757951140403748
epoch£º380	 i:1 	 global-step:7601	 l-p:0.1858406960964203
epoch£º380	 i:2 	 global-step:7602	 l-p:0.12980394065380096
epoch£º380	 i:3 	 global-step:7603	 l-p:0.12948957085609436
epoch£º380	 i:4 	 global-step:7604	 l-p:0.12222832441329956
epoch£º380	 i:5 	 global-step:7605	 l-p:0.16146858036518097
epoch£º380	 i:6 	 global-step:7606	 l-p:0.10794664174318314
epoch£º380	 i:7 	 global-step:7607	 l-p:0.028514375910162926
epoch£º380	 i:8 	 global-step:7608	 l-p:-0.06044605001807213
epoch£º380	 i:9 	 global-step:7609	 l-p:0.7392132878303528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7285, 4.7284, 4.7285],
        [4.7285, 5.1251, 5.1710],
        [4.7285, 4.6916, 4.7060],
        [4.7285, 4.6757, 4.6573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.3565443456172943 
model_pd.l_d.mean(): -20.108736038208008 
model_pd.lagr.mean(): -19.7521915435791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5972], device='cuda:0')), ('power', tensor([-20.9386], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.3565443456172943
epoch£º381	 i:1 	 global-step:7621	 l-p:0.21449746191501617
epoch£º381	 i:2 	 global-step:7622	 l-p:0.10456620901823044
epoch£º381	 i:3 	 global-step:7623	 l-p:0.08720169216394424
epoch£º381	 i:4 	 global-step:7624	 l-p:0.32213687896728516
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12679463624954224
epoch£º381	 i:6 	 global-step:7626	 l-p:0.12904943525791168
epoch£º381	 i:7 	 global-step:7627	 l-p:0.18024644255638123
epoch£º381	 i:8 	 global-step:7628	 l-p:0.11095970124006271
epoch£º381	 i:9 	 global-step:7629	 l-p:0.1389382779598236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1337, 5.0999, 5.0961],
        [5.1337, 5.1334, 5.1337],
        [5.1337, 5.1330, 5.1336],
        [5.1337, 5.1014, 5.0780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.12885835766792297 
model_pd.l_d.mean(): -20.783855438232422 
model_pd.lagr.mean(): -20.654996871948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3702], device='cuda:0')), ('power', tensor([-21.3892], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.12885835766792297
epoch£º382	 i:1 	 global-step:7641	 l-p:0.14480730891227722
epoch£º382	 i:2 	 global-step:7642	 l-p:0.15217213332653046
epoch£º382	 i:3 	 global-step:7643	 l-p:0.13492175936698914
epoch£º382	 i:4 	 global-step:7644	 l-p:-0.09989998489618301
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11531414091587067
epoch£º382	 i:6 	 global-step:7646	 l-p:0.05814516544342041
epoch£º382	 i:7 	 global-step:7647	 l-p:0.17893022298812866
epoch£º382	 i:8 	 global-step:7648	 l-p:-0.013169498182833195
epoch£º382	 i:9 	 global-step:7649	 l-p:0.10869010537862778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1078, 5.1051, 5.1076],
        [5.1078, 5.1078, 5.0288],
        [5.1078, 5.1078, 5.1079],
        [5.1078, 5.0787, 5.0882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.1271708905696869 
model_pd.l_d.mean(): -20.190073013305664 
model_pd.lagr.mean(): -20.062902450561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4492], device='cuda:0')), ('power', tensor([-20.8696], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.1271708905696869
epoch£º383	 i:1 	 global-step:7661	 l-p:0.293599933385849
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12944607436656952
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12141871452331543
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08350121229887009
epoch£º383	 i:5 	 global-step:7665	 l-p:0.1313285082578659
epoch£º383	 i:6 	 global-step:7666	 l-p:0.14089417457580566
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13424445688724518
epoch£º383	 i:8 	 global-step:7668	 l-p:0.15812751650810242
epoch£º383	 i:9 	 global-step:7669	 l-p:0.3768916130065918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0080, 5.5387, 5.6626],
        [5.0080, 4.9731, 4.9318],
        [5.0080, 5.2617, 5.2021],
        [5.0080, 4.9845, 4.9976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.23350484669208527 
model_pd.l_d.mean(): -20.758935928344727 
model_pd.lagr.mean(): -20.52543067932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328], device='cuda:0')), ('power', tensor([-21.4279], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.23350484669208527
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15316936373710632
epoch£º384	 i:2 	 global-step:7682	 l-p:0.13198745250701904
epoch£º384	 i:3 	 global-step:7683	 l-p:0.1376032680273056
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13780473172664642
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10583502799272537
epoch£º384	 i:6 	 global-step:7686	 l-p:0.10604604333639145
epoch£º384	 i:7 	 global-step:7687	 l-p:-0.10182599723339081
epoch£º384	 i:8 	 global-step:7688	 l-p:0.10874053090810776
epoch£º384	 i:9 	 global-step:7689	 l-p:0.13067851960659027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0347, 5.0306, 4.9473],
        [5.0347, 5.0327, 5.0346],
        [5.0347, 5.0347, 5.0347],
        [5.0347, 5.5329, 5.6297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.134794220328331 
model_pd.l_d.mean(): -20.22393798828125 
model_pd.lagr.mean(): -20.089143753051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4764], device='cuda:0')), ('power', tensor([-20.9317], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.134794220328331
epoch£º385	 i:1 	 global-step:7701	 l-p:0.12142345309257507
epoch£º385	 i:2 	 global-step:7702	 l-p:0.14116103947162628
epoch£º385	 i:3 	 global-step:7703	 l-p:0.10826653242111206
epoch£º385	 i:4 	 global-step:7704	 l-p:0.11562106758356094
epoch£º385	 i:5 	 global-step:7705	 l-p:0.14274482429027557
epoch£º385	 i:6 	 global-step:7706	 l-p:0.08351032435894012
epoch£º385	 i:7 	 global-step:7707	 l-p:0.11915908753871918
epoch£º385	 i:8 	 global-step:7708	 l-p:0.18770551681518555
epoch£º385	 i:9 	 global-step:7709	 l-p:0.23813964426517487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8356, 4.7967, 4.8093],
        [4.8356, 5.0237, 4.9373],
        [4.8356, 4.8356, 4.8356],
        [4.8356, 4.8046, 4.8200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.16295310854911804 
model_pd.l_d.mean(): -20.17437744140625 
model_pd.lagr.mean(): -20.011425018310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5548], device='cuda:0')), ('power', tensor([-20.9617], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.16295310854911804
epoch£º386	 i:1 	 global-step:7721	 l-p:0.1424504518508911
epoch£º386	 i:2 	 global-step:7722	 l-p:0.15682457387447357
epoch£º386	 i:3 	 global-step:7723	 l-p:0.31989020109176636
epoch£º386	 i:4 	 global-step:7724	 l-p:0.01515432819724083
epoch£º386	 i:5 	 global-step:7725	 l-p:0.08054300397634506
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11185014247894287
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12175314873456955
epoch£º386	 i:8 	 global-step:7728	 l-p:0.16333377361297607
epoch£º386	 i:9 	 global-step:7729	 l-p:0.12733405828475952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2620, 5.2425, 5.2535],
        [5.2620, 5.7615, 5.8432],
        [5.2620, 5.2616, 5.2620],
        [5.2620, 5.2328, 5.2386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.12553682923316956 
model_pd.l_d.mean(): -20.596538543701172 
model_pd.lagr.mean(): -20.47100257873535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3677], device='cuda:0')), ('power', tensor([-21.1972], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.12553682923316956
epoch£º387	 i:1 	 global-step:7741	 l-p:0.12672100961208344
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12471683323383331
epoch£º387	 i:3 	 global-step:7743	 l-p:-0.007361454889178276
epoch£º387	 i:4 	 global-step:7744	 l-p:0.11787649244070053
epoch£º387	 i:5 	 global-step:7745	 l-p:0.13393370807170868
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10902436077594757
epoch£º387	 i:7 	 global-step:7747	 l-p:0.147442027926445
epoch£º387	 i:8 	 global-step:7748	 l-p:0.17132242023944855
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11790826171636581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1165, 5.1136, 5.1163],
        [5.1165, 5.0943, 5.1071],
        [5.1165, 5.1130, 5.1162],
        [5.1165, 5.1045, 5.0302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.11434423178434372 
model_pd.l_d.mean(): -19.43657112121582 
model_pd.lagr.mean(): -19.322227478027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5493], device='cuda:0')), ('power', tensor([-20.2102], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.11434423178434372
epoch£º388	 i:1 	 global-step:7761	 l-p:0.005261363927274942
epoch£º388	 i:2 	 global-step:7762	 l-p:0.041366662830114365
epoch£º388	 i:3 	 global-step:7763	 l-p:0.19542178511619568
epoch£º388	 i:4 	 global-step:7764	 l-p:0.13442140817642212
epoch£º388	 i:5 	 global-step:7765	 l-p:0.16046811640262604
epoch£º388	 i:6 	 global-step:7766	 l-p:0.2912546396255493
epoch£º388	 i:7 	 global-step:7767	 l-p:0.14853045344352722
epoch£º388	 i:8 	 global-step:7768	 l-p:0.12478284537792206
epoch£º388	 i:9 	 global-step:7769	 l-p:0.12644582986831665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0028, 5.0679, 4.9530],
        [5.0028, 4.9725, 4.9085],
        [5.0028, 5.0026, 5.0028],
        [5.0028, 4.9670, 4.9785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.09508568793535233 
model_pd.l_d.mean(): -19.393089294433594 
model_pd.lagr.mean(): -19.298004150390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.1424], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.09508568793535233
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11113971471786499
epoch£º389	 i:2 	 global-step:7782	 l-p:0.14495116472244263
epoch£º389	 i:3 	 global-step:7783	 l-p:0.1336410641670227
epoch£º389	 i:4 	 global-step:7784	 l-p:0.17870523035526276
epoch£º389	 i:5 	 global-step:7785	 l-p:0.13463594019412994
epoch£º389	 i:6 	 global-step:7786	 l-p:0.12707284092903137
epoch£º389	 i:7 	 global-step:7787	 l-p:0.03063896670937538
epoch£º389	 i:8 	 global-step:7788	 l-p:0.3620518147945404
epoch£º389	 i:9 	 global-step:7789	 l-p:0.05632892623543739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.0344, 5.0346],
        [5.0346, 4.9899, 4.9696],
        [5.0346, 5.5760, 5.7048],
        [5.0346, 5.0344, 5.0346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.03538838401436806 
model_pd.l_d.mean(): -20.66387367248535 
model_pd.lagr.mean(): -20.62848472595215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4322], device='cuda:0')), ('power', tensor([-21.3312], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.03538838401436806
epoch£º390	 i:1 	 global-step:7801	 l-p:0.11682070791721344
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12482231855392456
epoch£º390	 i:3 	 global-step:7803	 l-p:0.1572829931974411
epoch£º390	 i:4 	 global-step:7804	 l-p:0.12518738210201263
epoch£º390	 i:5 	 global-step:7805	 l-p:0.20379380881786346
epoch£º390	 i:6 	 global-step:7806	 l-p:0.12102774530649185
epoch£º390	 i:7 	 global-step:7807	 l-p:0.1331319361925125
epoch£º390	 i:8 	 global-step:7808	 l-p:0.15166418254375458
epoch£º390	 i:9 	 global-step:7809	 l-p:0.14948083460330963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0938, 5.0860, 5.0926],
        [5.0938, 5.0864, 5.0012],
        [5.0938, 5.0685, 5.0034],
        [5.0938, 5.0902, 5.0935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.1649072766304016 
model_pd.l_d.mean(): -20.685861587524414 
model_pd.lagr.mean(): -20.520954132080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4078], device='cuda:0')), ('power', tensor([-21.3285], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.1649072766304016
epoch£º391	 i:1 	 global-step:7821	 l-p:-0.25094160437583923
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13137561082839966
epoch£º391	 i:3 	 global-step:7823	 l-p:0.11946024000644684
epoch£º391	 i:4 	 global-step:7824	 l-p:0.10735876858234406
epoch£º391	 i:5 	 global-step:7825	 l-p:0.10254410654306412
epoch£º391	 i:6 	 global-step:7826	 l-p:0.21211402118206024
epoch£º391	 i:7 	 global-step:7827	 l-p:0.11334968358278275
epoch£º391	 i:8 	 global-step:7828	 l-p:0.11934836208820343
epoch£º391	 i:9 	 global-step:7829	 l-p:0.13173003494739532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9986, 4.9639, 4.9775],
        [4.9986, 4.9983, 4.9986],
        [4.9986, 5.7935, 6.1412],
        [4.9986, 4.9986, 4.9986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.09108453243970871 
model_pd.l_d.mean(): -19.894498825073242 
model_pd.lagr.mean(): -19.80341339111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-20.5921], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.09108453243970871
epoch£º392	 i:1 	 global-step:7841	 l-p:0.9882414937019348
epoch£º392	 i:2 	 global-step:7842	 l-p:0.13700255751609802
epoch£º392	 i:3 	 global-step:7843	 l-p:0.035380247980356216
epoch£º392	 i:4 	 global-step:7844	 l-p:0.1456565111875534
epoch£º392	 i:5 	 global-step:7845	 l-p:0.12538407742977142
epoch£º392	 i:6 	 global-step:7846	 l-p:0.1331360638141632
epoch£º392	 i:7 	 global-step:7847	 l-p:6.309617042541504
epoch£º392	 i:8 	 global-step:7848	 l-p:0.11218640208244324
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11194615811109543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0611, 5.7044, 5.9131],
        [5.0611, 5.0611, 5.0611],
        [5.0611, 5.2841, 5.2056],
        [5.0611, 5.0611, 5.0611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.15378527343273163 
model_pd.l_d.mean(): -20.309547424316406 
model_pd.lagr.mean(): -20.15576171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4738], device='cuda:0')), ('power', tensor([-21.0155], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.15378527343273163
epoch£º393	 i:1 	 global-step:7861	 l-p:0.09059518575668335
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15424665808677673
epoch£º393	 i:3 	 global-step:7863	 l-p:0.11817564070224762
epoch£º393	 i:4 	 global-step:7864	 l-p:0.21724973618984222
epoch£º393	 i:5 	 global-step:7865	 l-p:0.12619133293628693
epoch£º393	 i:6 	 global-step:7866	 l-p:-0.3092583417892456
epoch£º393	 i:7 	 global-step:7867	 l-p:0.14242257177829742
epoch£º393	 i:8 	 global-step:7868	 l-p:0.1351517289876938
epoch£º393	 i:9 	 global-step:7869	 l-p:0.1269213706254959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0668, 5.0311, 5.0430],
        [5.0668, 5.0664, 5.0668],
        [5.0668, 5.0323, 4.9744],
        [5.0668, 5.7897, 6.0659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.19917073845863342 
model_pd.l_d.mean(): -20.56163215637207 
model_pd.lagr.mean(): -20.36246109008789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.2244], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.19917073845863342
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11893514543771744
epoch£º394	 i:2 	 global-step:7882	 l-p:0.11375568062067032
epoch£º394	 i:3 	 global-step:7883	 l-p:0.11335917562246323
epoch£º394	 i:4 	 global-step:7884	 l-p:0.10943170636892319
epoch£º394	 i:5 	 global-step:7885	 l-p:0.1793777048587799
epoch£º394	 i:6 	 global-step:7886	 l-p:0.1582907736301422
epoch£º394	 i:7 	 global-step:7887	 l-p:0.1405992954969406
epoch£º394	 i:8 	 global-step:7888	 l-p:-0.10628204792737961
epoch£º394	 i:9 	 global-step:7889	 l-p:0.15654249489307404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9809, 4.9793, 4.9808],
        [4.9809, 4.9347, 4.9395],
        [4.9809, 4.9809, 4.9809],
        [4.9809, 4.9432, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.12162530422210693 
model_pd.l_d.mean(): -20.117002487182617 
model_pd.lagr.mean(): -19.995376586914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5086], device='cuda:0')), ('power', tensor([-20.8564], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.12162530422210693
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12387658655643463
epoch£º395	 i:2 	 global-step:7902	 l-p:0.14002370834350586
epoch£º395	 i:3 	 global-step:7903	 l-p:0.1572917401790619
epoch£º395	 i:4 	 global-step:7904	 l-p:0.08677993714809418
epoch£º395	 i:5 	 global-step:7905	 l-p:0.08122720569372177
epoch£º395	 i:6 	 global-step:7906	 l-p:0.14851947128772736
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11806844174861908
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1710004210472107
epoch£º395	 i:9 	 global-step:7909	 l-p:0.16462451219558716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9092, 5.0938, 5.0004],
        [4.9092, 5.1532, 5.0876],
        [4.9092, 4.8907, 4.9040],
        [4.9092, 4.8977, 4.9070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.13914500176906586 
model_pd.l_d.mean(): -21.061256408691406 
model_pd.lagr.mean(): -20.92211151123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.7160], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.13914500176906586
epoch£º396	 i:1 	 global-step:7921	 l-p:-0.012749719433486462
epoch£º396	 i:2 	 global-step:7922	 l-p:0.12491289526224136
epoch£º396	 i:3 	 global-step:7923	 l-p:0.13939279317855835
epoch£º396	 i:4 	 global-step:7924	 l-p:0.09063412249088287
epoch£º396	 i:5 	 global-step:7925	 l-p:0.1612078994512558
epoch£º396	 i:6 	 global-step:7926	 l-p:0.11798372119665146
epoch£º396	 i:7 	 global-step:7927	 l-p:-0.0012078904546797276
epoch£º396	 i:8 	 global-step:7928	 l-p:0.15177485346794128
epoch£º396	 i:9 	 global-step:7929	 l-p:0.10200227797031403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9751, 5.6598, 5.9105],
        [4.9751, 4.9697, 4.9745],
        [4.9751, 4.9294, 4.9375],
        [4.9751, 4.9322, 4.9434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11379478126764297 
model_pd.l_d.mean(): -19.298234939575195 
model_pd.lagr.mean(): -19.18444061279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5888], device='cuda:0')), ('power', tensor([-20.1106], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11379478126764297
epoch£º397	 i:1 	 global-step:7941	 l-p:-0.2981945276260376
epoch£º397	 i:2 	 global-step:7942	 l-p:0.16424250602722168
epoch£º397	 i:3 	 global-step:7943	 l-p:0.12316669523715973
epoch£º397	 i:4 	 global-step:7944	 l-p:0.0041625308804214
epoch£º397	 i:5 	 global-step:7945	 l-p:0.18459643423557281
epoch£º397	 i:6 	 global-step:7946	 l-p:-0.0030861019622534513
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13501757383346558
epoch£º397	 i:8 	 global-step:7948	 l-p:0.1274588704109192
epoch£º397	 i:9 	 global-step:7949	 l-p:0.22489887475967407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1817, 5.4508, 5.3894],
        [5.1817, 5.1817, 5.1817],
        [5.1817, 5.1803, 5.1817],
        [5.1817, 5.3791, 5.2862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.17338640987873077 
model_pd.l_d.mean(): -19.30175018310547 
model_pd.lagr.mean(): -19.12836456298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.0119], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.17338640987873077
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10485414415597916
epoch£º398	 i:2 	 global-step:7962	 l-p:0.1615070253610611
epoch£º398	 i:3 	 global-step:7963	 l-p:0.13750827312469482
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14050906896591187
epoch£º398	 i:5 	 global-step:7965	 l-p:0.1177486777305603
epoch£º398	 i:6 	 global-step:7966	 l-p:0.14369796216487885
epoch£º398	 i:7 	 global-step:7967	 l-p:0.126790851354599
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12521198391914368
epoch£º398	 i:9 	 global-step:7969	 l-p:0.11509575694799423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0881, 5.0877, 5.0881],
        [5.0881, 5.0466, 5.0557],
        [5.0881, 5.1826, 5.0636],
        [5.0881, 5.0612, 4.9846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11478761583566666 
model_pd.l_d.mean(): -20.033893585205078 
model_pd.lagr.mean(): -19.919105529785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4720], device='cuda:0')), ('power', tensor([-20.7350], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11478761583566666
epoch£º399	 i:1 	 global-step:7981	 l-p:0.0785210132598877
epoch£º399	 i:2 	 global-step:7982	 l-p:0.12083501368761063
epoch£º399	 i:3 	 global-step:7983	 l-p:0.3186863362789154
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13008874654769897
epoch£º399	 i:5 	 global-step:7985	 l-p:0.1413060873746872
epoch£º399	 i:6 	 global-step:7986	 l-p:0.13322636485099792
epoch£º399	 i:7 	 global-step:7987	 l-p:0.1423921436071396
epoch£º399	 i:8 	 global-step:7988	 l-p:0.1585860252380371
epoch£º399	 i:9 	 global-step:7989	 l-p:-1.2492361068725586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9573, 4.9476, 4.8415],
        [4.9573, 5.0738, 4.9558],
        [4.9573, 4.9411, 4.9533],
        [4.9573, 4.9380, 4.8389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.143005833029747 
model_pd.l_d.mean(): -19.65260887145996 
model_pd.lagr.mean(): -19.50960350036621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.3800], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.143005833029747
epoch£º400	 i:1 	 global-step:8001	 l-p:0.10377441346645355
epoch£º400	 i:2 	 global-step:8002	 l-p:0.10393451154232025
epoch£º400	 i:3 	 global-step:8003	 l-p:0.14033889770507812
epoch£º400	 i:4 	 global-step:8004	 l-p:0.1422591507434845
epoch£º400	 i:5 	 global-step:8005	 l-p:0.38916388154029846
epoch£º400	 i:6 	 global-step:8006	 l-p:0.14294511079788208
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.05193029344081879
epoch£º400	 i:8 	 global-step:8008	 l-p:0.10996246337890625
epoch£º400	 i:9 	 global-step:8009	 l-p:0.131905660033226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9796, 5.1449, 5.0410],
        [4.9796, 4.9794, 4.9796],
        [4.9796, 5.0921, 4.9731],
        [4.9796, 5.2857, 5.2505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14878079295158386 
model_pd.l_d.mean(): -19.490816116333008 
model_pd.lagr.mean(): -19.3420352935791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5841], device='cuda:0')), ('power', tensor([-20.3006], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14878079295158386
epoch£º401	 i:1 	 global-step:8021	 l-p:0.1523471176624298
epoch£º401	 i:2 	 global-step:8022	 l-p:0.14865219593048096
epoch£º401	 i:3 	 global-step:8023	 l-p:0.7201500535011292
epoch£º401	 i:4 	 global-step:8024	 l-p:0.12438815832138062
epoch£º401	 i:5 	 global-step:8025	 l-p:0.03719940409064293
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14906957745552063
epoch£º401	 i:7 	 global-step:8027	 l-p:0.1166723221540451
epoch£º401	 i:8 	 global-step:8028	 l-p:0.09764153510332108
epoch£º401	 i:9 	 global-step:8029	 l-p:0.15114319324493408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9687, 4.9687, 4.9687],
        [4.9687, 4.9682, 4.9687],
        [4.9687, 4.9221, 4.9334],
        [4.9687, 4.9687, 4.9687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13849316537380219 
model_pd.l_d.mean(): -20.75702667236328 
model_pd.lagr.mean(): -20.618534088134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.4222], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13849316537380219
epoch£º402	 i:1 	 global-step:8041	 l-p:0.1407688707113266
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13794782757759094
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09940764307975769
epoch£º402	 i:4 	 global-step:8044	 l-p:0.12168247252702713
epoch£º402	 i:5 	 global-step:8045	 l-p:0.08579162508249283
epoch£º402	 i:6 	 global-step:8046	 l-p:0.14437498152256012
epoch£º402	 i:7 	 global-step:8047	 l-p:0.21359412372112274
epoch£º402	 i:8 	 global-step:8048	 l-p:-1.6404398679733276
epoch£º402	 i:9 	 global-step:8049	 l-p:0.1631094515323639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0021, 4.9620, 4.9769],
        [5.0021, 5.0021, 5.0021],
        [5.0021, 4.9648, 4.9808],
        [5.0021, 4.9866, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.18749888241291046 
model_pd.l_d.mean(): -20.52623176574707 
model_pd.lagr.mean(): -20.338733673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4429], device='cuda:0')), ('power', tensor([-21.2030], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.18749888241291046
epoch£º403	 i:1 	 global-step:8061	 l-p:0.13045191764831543
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13861936330795288
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12949638068675995
epoch£º403	 i:4 	 global-step:8064	 l-p:0.08613725751638412
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13984577357769012
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12520554661750793
epoch£º403	 i:7 	 global-step:8067	 l-p:0.3852476477622986
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12484071403741837
epoch£º403	 i:9 	 global-step:8069	 l-p:0.08463532477617264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0648, 5.1897, 5.0731],
        [5.0648, 5.5385, 5.6106],
        [5.0648, 5.0370, 5.0529],
        [5.0648, 5.0601, 5.0644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.15552346408367157 
model_pd.l_d.mean(): -19.17416763305664 
model_pd.lagr.mean(): -19.018644332885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5377], device='cuda:0')), ('power', tensor([-19.9330], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.15552346408367157
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11443629115819931
epoch£º404	 i:2 	 global-step:8082	 l-p:0.13172003626823425
epoch£º404	 i:3 	 global-step:8083	 l-p:0.1292758584022522
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12106120586395264
epoch£º404	 i:5 	 global-step:8085	 l-p:0.035732220858335495
epoch£º404	 i:6 	 global-step:8086	 l-p:0.16029119491577148
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12107587605714798
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.2822442054748535
epoch£º404	 i:9 	 global-step:8089	 l-p:0.14752233028411865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1055, 5.0953, 4.9973],
        [5.1055, 5.1055, 5.1055],
        [5.1055, 5.1055, 5.1055],
        [5.1055, 5.0569, 5.0175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.09982713311910629 
model_pd.l_d.mean(): -20.364160537719727 
model_pd.lagr.mean(): -20.264333724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498], device='cuda:0')), ('power', tensor([-21.0462], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.09982713311910629
epoch£º405	 i:1 	 global-step:8101	 l-p:0.16661742329597473
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10490024089813232
epoch£º405	 i:3 	 global-step:8103	 l-p:-0.023482684046030045
epoch£º405	 i:4 	 global-step:8104	 l-p:-0.2598422169685364
epoch£º405	 i:5 	 global-step:8105	 l-p:-0.004703841172158718
epoch£º405	 i:6 	 global-step:8106	 l-p:0.15608876943588257
epoch£º405	 i:7 	 global-step:8107	 l-p:0.14931824803352356
epoch£º405	 i:8 	 global-step:8108	 l-p:0.12525831162929535
epoch£º405	 i:9 	 global-step:8109	 l-p:0.13220922648906708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1053, 5.0614, 5.0707],
        [5.1053, 5.0872, 5.1002],
        [5.1053, 5.0561, 5.0179],
        [5.1053, 5.2775, 5.1738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): -0.033647824078798294 
model_pd.l_d.mean(): -19.68152618408203 
model_pd.lagr.mean(): -19.715173721313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4875], device='cuda:0')), ('power', tensor([-20.3946], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:-0.033647824078798294
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09771620482206345
epoch£º406	 i:2 	 global-step:8122	 l-p:0.12954288721084595
epoch£º406	 i:3 	 global-step:8123	 l-p:0.16207724809646606
epoch£º406	 i:4 	 global-step:8124	 l-p:0.14942798018455505
epoch£º406	 i:5 	 global-step:8125	 l-p:0.3296261131763458
epoch£º406	 i:6 	 global-step:8126	 l-p:0.09335263073444366
epoch£º406	 i:7 	 global-step:8127	 l-p:0.06803522258996964
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12656241655349731
epoch£º406	 i:9 	 global-step:8129	 l-p:0.1282697468996048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0669, 5.0609, 5.0662],
        [5.0669, 5.0616, 5.0663],
        [5.0669, 5.1584, 5.0353],
        [5.0669, 5.0578, 4.9549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.15841397643089294 
model_pd.l_d.mean(): -20.641618728637695 
model_pd.lagr.mean(): -20.483203887939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.2977], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.15841397643089294
epoch£º407	 i:1 	 global-step:8141	 l-p:0.12581081688404083
epoch£º407	 i:2 	 global-step:8142	 l-p:0.15051695704460144
epoch£º407	 i:3 	 global-step:8143	 l-p:0.10123026371002197
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.8436936140060425
epoch£º407	 i:5 	 global-step:8145	 l-p:0.14602313935756683
epoch£º407	 i:6 	 global-step:8146	 l-p:0.14031313359737396
epoch£º407	 i:7 	 global-step:8147	 l-p:0.3317064642906189
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11562232673168182
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12217246741056442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.1193, 5.1348],
        [5.1480, 5.1226, 5.1377],
        [5.1480, 5.1009, 5.0609],
        [5.1480, 5.2313, 5.1091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.34962671995162964 
model_pd.l_d.mean(): -20.580181121826172 
model_pd.lagr.mean(): -20.230554580688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.2321], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.34962671995162964
epoch£º408	 i:1 	 global-step:8161	 l-p:0.1395449936389923
epoch£º408	 i:2 	 global-step:8162	 l-p:0.14576853811740875
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13907575607299805
epoch£º408	 i:4 	 global-step:8164	 l-p:0.2661353349685669
epoch£º408	 i:5 	 global-step:8165	 l-p:0.2055278867483139
epoch£º408	 i:6 	 global-step:8166	 l-p:0.12021129578351974
epoch£º408	 i:7 	 global-step:8167	 l-p:0.121835857629776
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12916623055934906
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09405817836523056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1537, 5.1344, 5.1478],
        [5.1537, 5.1325, 5.1466],
        [5.1537, 5.1521, 5.1536],
        [5.1537, 5.1525, 5.1536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.1284109652042389 
model_pd.l_d.mean(): -20.602094650268555 
model_pd.lagr.mean(): -20.473684310913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3874], device='cuda:0')), ('power', tensor([-21.2230], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.1284109652042389
epoch£º409	 i:1 	 global-step:8181	 l-p:0.16569499671459198
epoch£º409	 i:2 	 global-step:8182	 l-p:0.32473045587539673
epoch£º409	 i:3 	 global-step:8183	 l-p:0.1450275033712387
epoch£º409	 i:4 	 global-step:8184	 l-p:0.14073650538921356
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12400136888027191
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.014337139204144478
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.12509098649024963
epoch£º409	 i:8 	 global-step:8188	 l-p:0.1432839035987854
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11682344228029251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1107, 5.0809, 5.0970],
        [5.1107, 5.0626, 5.0671],
        [5.1107, 5.1051, 5.1101],
        [5.1107, 5.1107, 5.1107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15432728826999664 
model_pd.l_d.mean(): -20.465856552124023 
model_pd.lagr.mean(): -20.3115291595459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-21.1359], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15432728826999664
epoch£º410	 i:1 	 global-step:8201	 l-p:0.1331859678030014
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09678724408149719
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12892621755599976
epoch£º410	 i:4 	 global-step:8204	 l-p:0.3394738733768463
epoch£º410	 i:5 	 global-step:8205	 l-p:0.13176000118255615
epoch£º410	 i:6 	 global-step:8206	 l-p:0.08941096812486649
epoch£º410	 i:7 	 global-step:8207	 l-p:0.1299409419298172
epoch£º410	 i:8 	 global-step:8208	 l-p:0.1458740532398224
epoch£º410	 i:9 	 global-step:8209	 l-p:0.08362098783254623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0034, 5.0034, 5.0034],
        [5.0034, 5.0027, 5.0034],
        [5.0034, 5.0032, 5.0034],
        [5.0034, 5.0022, 5.0033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.5394930243492126 
model_pd.l_d.mean(): -20.367538452148438 
model_pd.lagr.mean(): -19.828044891357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4792], device='cuda:0')), ('power', tensor([-21.0797], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.5394930243492126
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13559186458587646
epoch£º411	 i:2 	 global-step:8222	 l-p:0.15145599842071533
epoch£º411	 i:3 	 global-step:8223	 l-p:0.1733778715133667
epoch£º411	 i:4 	 global-step:8224	 l-p:0.0884036123752594
epoch£º411	 i:5 	 global-step:8225	 l-p:0.36630287766456604
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07587652653455734
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12246552109718323
epoch£º411	 i:8 	 global-step:8228	 l-p:0.13531461358070374
epoch£º411	 i:9 	 global-step:8229	 l-p:0.1421155333518982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9784, 4.9726, 4.9777],
        [4.9784, 5.7494, 6.0727],
        [4.9784, 4.9784, 4.9784],
        [4.9784, 4.9258, 4.9335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.27935102581977844 
model_pd.l_d.mean(): -19.18282699584961 
model_pd.lagr.mean(): -18.90347671508789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5363], device='cuda:0')), ('power', tensor([-19.9403], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.27935102581977844
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10813654959201813
epoch£º412	 i:2 	 global-step:8242	 l-p:0.1367841511964798
epoch£º412	 i:3 	 global-step:8243	 l-p:0.14613084495067596
epoch£º412	 i:4 	 global-step:8244	 l-p:0.12231246381998062
epoch£º412	 i:5 	 global-step:8245	 l-p:0.15669454634189606
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09071840345859528
epoch£º412	 i:7 	 global-step:8247	 l-p:0.07721614837646484
epoch£º412	 i:8 	 global-step:8248	 l-p:0.1636132448911667
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11862582713365555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9226, 4.8775, 4.8940],
        [4.9226, 4.9226, 4.9226],
        [4.9226, 5.6136, 5.8716],
        [4.9226, 4.9226, 4.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.1495668888092041 
model_pd.l_d.mean(): -19.940235137939453 
model_pd.lagr.mean(): -19.790668487548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-20.6944], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.1495668888092041
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14736202359199524
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13528986275196075
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07460252940654755
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11958722025156021
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18155746161937714
epoch£º413	 i:6 	 global-step:8266	 l-p:0.16959288716316223
epoch£º413	 i:7 	 global-step:8267	 l-p:0.1548624187707901
epoch£º413	 i:8 	 global-step:8268	 l-p:0.024517718702554703
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13063178956508636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8867, 4.8830, 4.8864],
        [4.8867, 5.0006, 4.8774],
        [4.8867, 4.8867, 4.8867],
        [4.8867, 4.8624, 4.8789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08878865092992783 
model_pd.l_d.mean(): -19.560335159301758 
model_pd.lagr.mean(): -19.471546173095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.3349], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.08878865092992783
epoch£º414	 i:1 	 global-step:8281	 l-p:0.1445823758840561
epoch£º414	 i:2 	 global-step:8282	 l-p:0.1730516403913498
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12574148178100586
epoch£º414	 i:4 	 global-step:8284	 l-p:0.1511903703212738
epoch£º414	 i:5 	 global-step:8285	 l-p:0.17450158298015594
epoch£º414	 i:6 	 global-step:8286	 l-p:0.0912744477391243
epoch£º414	 i:7 	 global-step:8287	 l-p:0.10498817265033722
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13439732789993286
epoch£º414	 i:9 	 global-step:8289	 l-p:0.16058288514614105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9310, 5.0196, 4.8912],
        [4.9310, 5.3957, 5.4695],
        [4.9310, 4.9684, 4.8377],
        [4.9310, 4.9172, 4.9282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10735050588846207 
model_pd.l_d.mean(): -19.90116310119629 
model_pd.lagr.mean(): -19.793811798095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5680], device='cuda:0')), ('power', tensor([-20.6989], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.10735050588846207
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12476944923400879
epoch£º415	 i:2 	 global-step:8302	 l-p:0.15872900187969208
epoch£º415	 i:3 	 global-step:8303	 l-p:0.12982669472694397
epoch£º415	 i:4 	 global-step:8304	 l-p:0.1416369080543518
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.15497000515460968
epoch£º415	 i:6 	 global-step:8306	 l-p:0.13404065370559692
epoch£º415	 i:7 	 global-step:8307	 l-p:0.13312414288520813
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05063747614622116
epoch£º415	 i:9 	 global-step:8309	 l-p:0.1280757635831833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9589, 4.9283, 4.9462],
        [4.9589, 4.9589, 4.9589],
        [4.9589, 5.0356, 4.9060],
        [4.9589, 4.9590, 4.9590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.16942517459392548 
model_pd.l_d.mean(): -20.659523010253906 
model_pd.lagr.mean(): -20.490097045898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-21.3608], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.16942517459392548
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.9641056060791016
epoch£º416	 i:2 	 global-step:8322	 l-p:0.07035663723945618
epoch£º416	 i:3 	 global-step:8323	 l-p:0.11382672935724258
epoch£º416	 i:4 	 global-step:8324	 l-p:0.124070905148983
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08164104074239731
epoch£º416	 i:6 	 global-step:8326	 l-p:0.10986894369125366
epoch£º416	 i:7 	 global-step:8327	 l-p:0.12438306212425232
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11926563084125519
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12748046219348907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0263, 5.0182, 5.0252],
        [5.0263, 5.0095, 5.0222],
        [5.0263, 4.9828, 4.9983],
        [5.0263, 5.0263, 5.0263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.31663572788238525 
model_pd.l_d.mean(): -20.427595138549805 
model_pd.lagr.mean(): -20.110960006713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.1221], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.31663572788238525
epoch£º417	 i:1 	 global-step:8341	 l-p:0.1403692066669464
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12885552644729614
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12019427865743637
epoch£º417	 i:4 	 global-step:8344	 l-p:0.1243271604180336
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14408260583877563
epoch£º417	 i:6 	 global-step:8346	 l-p:0.03962898254394531
epoch£º417	 i:7 	 global-step:8347	 l-p:0.1043003499507904
epoch£º417	 i:8 	 global-step:8348	 l-p:0.13935574889183044
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14437906444072723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9459, 4.9457, 4.9459],
        [4.9459, 4.9458, 4.9459],
        [4.9459, 5.1379, 5.0412],
        [4.9459, 4.8834, 4.8774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.25672152638435364 
model_pd.l_d.mean(): -19.64055824279785 
model_pd.lagr.mean(): -19.897279739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5188], device='cuda:0')), ('power', tensor([-20.3852], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.25672152638435364
epoch£º418	 i:1 	 global-step:8361	 l-p:0.15035711228847504
epoch£º418	 i:2 	 global-step:8362	 l-p:0.10763999074697495
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13655903935432434
epoch£º418	 i:4 	 global-step:8364	 l-p:0.14740154147148132
epoch£º418	 i:5 	 global-step:8365	 l-p:0.11842308938503265
epoch£º418	 i:6 	 global-step:8366	 l-p:0.1204814538359642
epoch£º418	 i:7 	 global-step:8367	 l-p:0.17991627752780914
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11558710038661957
epoch£º418	 i:9 	 global-step:8369	 l-p:0.12985166907310486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9499, 4.8873, 4.8817],
        [4.9499, 4.9832, 4.8520],
        [4.9499, 4.9198, 4.8151],
        [4.9499, 4.9409, 4.9486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.15421313047409058 
model_pd.l_d.mean(): -20.318025588989258 
model_pd.lagr.mean(): -20.1638126373291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5070], device='cuda:0')), ('power', tensor([-21.0580], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.15421313047409058
epoch£º419	 i:1 	 global-step:8381	 l-p:0.1315861940383911
epoch£º419	 i:2 	 global-step:8382	 l-p:0.11610999703407288
epoch£º419	 i:3 	 global-step:8383	 l-p:0.13318771123886108
epoch£º419	 i:4 	 global-step:8384	 l-p:0.14071844518184662
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08520325273275375
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.01169393490999937
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12820710241794586
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13999894261360168
epoch£º419	 i:9 	 global-step:8389	 l-p:0.33492469787597656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9758, 5.1233, 5.0087],
        [4.9758, 4.9175, 4.9209],
        [4.9758, 4.9121, 4.8756],
        [4.9758, 4.9753, 4.9758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.07470738887786865 
model_pd.l_d.mean(): -19.638578414916992 
model_pd.lagr.mean(): -19.563871383666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5065], device='cuda:0')), ('power', tensor([-20.3706], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.07470738887786865
epoch£º420	 i:1 	 global-step:8401	 l-p:0.12970200181007385
epoch£º420	 i:2 	 global-step:8402	 l-p:0.13534225523471832
epoch£º420	 i:3 	 global-step:8403	 l-p:0.1345418393611908
epoch£º420	 i:4 	 global-step:8404	 l-p:0.1308916062116623
epoch£º420	 i:5 	 global-step:8405	 l-p:0.25650766491889954
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07810134440660477
epoch£º420	 i:7 	 global-step:8407	 l-p:0.1254308968782425
epoch£º420	 i:8 	 global-step:8408	 l-p:0.11040454357862473
epoch£º420	 i:9 	 global-step:8409	 l-p:0.24017180502414703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0522, 4.9964, 4.9434],
        [5.0522, 5.6707, 5.8549],
        [5.0522, 5.0180, 5.0358],
        [5.0522, 5.0462, 5.0515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09921803325414658 
model_pd.l_d.mean(): -20.314422607421875 
model_pd.lagr.mean(): -20.2152042388916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4641], device='cuda:0')), ('power', tensor([-21.0105], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09921803325414658
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12594570219516754
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11580833047628403
epoch£º421	 i:3 	 global-step:8423	 l-p:0.1373353749513626
epoch£º421	 i:4 	 global-step:8424	 l-p:0.1766713708639145
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06729215383529663
epoch£º421	 i:6 	 global-step:8426	 l-p:1.0908071994781494
epoch£º421	 i:7 	 global-step:8427	 l-p:0.14219383895397186
epoch£º421	 i:8 	 global-step:8428	 l-p:0.134962797164917
epoch£º421	 i:9 	 global-step:8429	 l-p:0.12628911435604095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0729, 5.0585, 5.0698],
        [5.0729, 5.0729, 5.0729],
        [5.0729, 5.0722, 5.0729],
        [5.0729, 5.0984, 4.9726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.2608768343925476 
model_pd.l_d.mean(): -20.275617599487305 
model_pd.lagr.mean(): -20.014739990234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.9769], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.2608768343925476
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12947730720043182
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11794381588697433
epoch£º422	 i:3 	 global-step:8443	 l-p:0.12294197827577591
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16105273365974426
epoch£º422	 i:5 	 global-step:8445	 l-p:0.08968427777290344
epoch£º422	 i:6 	 global-step:8446	 l-p:0.08356732130050659
epoch£º422	 i:7 	 global-step:8447	 l-p:0.0019667099695652723
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15789861977100372
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12403036653995514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1379, 5.1378, 5.1379],
        [5.1379, 5.2212, 5.0936],
        [5.1379, 5.1213, 5.1337],
        [5.1379, 5.1203, 5.0192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.1078399047255516 
model_pd.l_d.mean(): -20.10331153869629 
model_pd.lagr.mean(): -19.995471954345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-20.8002], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.1078399047255516
epoch£º423	 i:1 	 global-step:8461	 l-p:4.174671173095703
epoch£º423	 i:2 	 global-step:8462	 l-p:0.2954864203929901
epoch£º423	 i:3 	 global-step:8463	 l-p:0.14111346006393433
epoch£º423	 i:4 	 global-step:8464	 l-p:0.19025824964046478
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11395557224750519
epoch£º423	 i:6 	 global-step:8466	 l-p:0.1343913972377777
epoch£º423	 i:7 	 global-step:8467	 l-p:0.13187356293201447
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12386615574359894
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12097809463739395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1758, 5.1223, 5.1167],
        [5.1758, 5.1736, 5.1757],
        [5.1758, 5.4592, 5.4006],
        [5.1758, 5.1758, 5.1758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.12883612513542175 
model_pd.l_d.mean(): -20.148807525634766 
model_pd.lagr.mean(): -20.01997184753418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-20.8417], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.12883612513542175
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13357335329055786
epoch£º424	 i:2 	 global-step:8482	 l-p:-1.6238312721252441
epoch£º424	 i:3 	 global-step:8483	 l-p:0.1029793992638588
epoch£º424	 i:4 	 global-step:8484	 l-p:0.0444336012005806
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14385440945625305
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13341543078422546
epoch£º424	 i:7 	 global-step:8487	 l-p:0.08835020661354065
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.05426616594195366
epoch£º424	 i:9 	 global-step:8489	 l-p:0.2644262909889221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0302, 4.9739, 4.9796],
        [5.0302, 4.9724, 4.9145],
        [5.0302, 5.1841, 5.0703],
        [5.0302, 5.0302, 5.0302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1686738133430481 
model_pd.l_d.mean(): -19.515687942504883 
model_pd.lagr.mean(): -19.347013473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.2245], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1686738133430481
epoch£º425	 i:1 	 global-step:8501	 l-p:0.142479807138443
epoch£º425	 i:2 	 global-step:8502	 l-p:0.24548548460006714
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.42889854311943054
epoch£º425	 i:4 	 global-step:8504	 l-p:0.08991047739982605
epoch£º425	 i:5 	 global-step:8505	 l-p:0.11851932108402252
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11910729110240936
epoch£º425	 i:7 	 global-step:8507	 l-p:0.013096918351948261
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07508666813373566
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12308268994092941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1129, 5.1049, 5.1118],
        [5.1129, 5.1122, 5.1129],
        [5.1129, 5.1105, 5.1128],
        [5.1129, 5.4755, 5.4653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.6440253853797913 
model_pd.l_d.mean(): -19.04414176940918 
model_pd.lagr.mean(): -19.688167572021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5303], device='cuda:0')), ('power', tensor([-19.7941], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.6440253853797913
epoch£º426	 i:1 	 global-step:8521	 l-p:0.18701712787151337
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13619203865528107
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14575085043907166
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12310811877250671
epoch£º426	 i:5 	 global-step:8525	 l-p:0.1267307847738266
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12472201883792877
epoch£º426	 i:7 	 global-step:8527	 l-p:0.0995911955833435
epoch£º426	 i:8 	 global-step:8528	 l-p:0.15294016897678375
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11404895782470703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0572, 5.0515, 5.0566],
        [5.0572, 5.0547, 5.0571],
        [5.0572, 5.0124, 5.0285],
        [5.0572, 5.0618, 4.9396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.16203807294368744 
model_pd.l_d.mean(): -19.812862396240234 
model_pd.lagr.mean(): -19.65082359313965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-20.4881], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.16203807294368744
epoch£º427	 i:1 	 global-step:8541	 l-p:0.1034521833062172
epoch£º427	 i:2 	 global-step:8542	 l-p:0.26532095670700073
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11591155081987381
epoch£º427	 i:4 	 global-step:8544	 l-p:0.12027986347675323
epoch£º427	 i:5 	 global-step:8545	 l-p:0.004510259721428156
epoch£º427	 i:6 	 global-step:8546	 l-p:0.1739199459552765
epoch£º427	 i:7 	 global-step:8547	 l-p:0.07536949217319489
epoch£º427	 i:8 	 global-step:8548	 l-p:0.15448737144470215
epoch£º427	 i:9 	 global-step:8549	 l-p:0.1411714106798172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[4.9930, 5.4516, 5.5146],
        [4.9930, 4.9308, 4.8736],
        [4.9930, 5.3179, 5.2893],
        [4.9930, 4.9392, 4.8615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.2555826008319855 
model_pd.l_d.mean(): -20.60609245300293 
model_pd.lagr.mean(): -20.350509643554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-21.2961], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.2555826008319855
epoch£º428	 i:1 	 global-step:8561	 l-p:0.11943920701742172
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10654865205287933
epoch£º428	 i:3 	 global-step:8563	 l-p:0.13408420979976654
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12281646579504013
epoch£º428	 i:5 	 global-step:8565	 l-p:0.004459790885448456
epoch£º428	 i:6 	 global-step:8566	 l-p:0.16022926568984985
epoch£º428	 i:7 	 global-step:8567	 l-p:0.13188977539539337
epoch£º428	 i:8 	 global-step:8568	 l-p:0.17194880545139313
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11783044785261154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9483, 4.9053, 4.8047],
        [4.9483, 4.9482, 4.9483],
        [4.9483, 5.1266, 5.0215],
        [4.9483, 5.6247, 5.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11052101105451584 
model_pd.l_d.mean(): -19.40524673461914 
model_pd.lagr.mean(): -19.29472541809082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5920], device='cuda:0')), ('power', tensor([-20.2221], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11052101105451584
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10570991039276123
epoch£º429	 i:2 	 global-step:8582	 l-p:0.15044590830802917
epoch£º429	 i:3 	 global-step:8583	 l-p:0.15696674585342407
epoch£º429	 i:4 	 global-step:8584	 l-p:0.168426051735878
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.005819692276418209
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11075189709663391
epoch£º429	 i:7 	 global-step:8587	 l-p:0.11253122985363007
epoch£º429	 i:8 	 global-step:8588	 l-p:0.07969768345355988
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11286420375108719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9791, 4.9489, 4.9675],
        [4.9791, 5.6607, 5.9023],
        [4.9791, 4.9211, 4.9305],
        [4.9791, 4.9778, 4.9791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.32224592566490173 
model_pd.l_d.mean(): -19.348501205444336 
model_pd.lagr.mean(): -19.026254653930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5956], device='cuda:0')), ('power', tensor([-20.1684], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.32224592566490173
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14908920228481293
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13913731276988983
epoch£º430	 i:3 	 global-step:8603	 l-p:0.14970390498638153
epoch£º430	 i:4 	 global-step:8604	 l-p:0.1318921595811844
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12209920585155487
epoch£º430	 i:6 	 global-step:8606	 l-p:0.12034273147583008
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12394604086875916
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11315527558326721
epoch£º430	 i:9 	 global-step:8609	 l-p:0.14918066561222076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9209, 5.0303, 4.9011],
        [4.9209, 5.5923, 5.8303],
        [4.9209, 4.9089, 4.9188],
        [4.9209, 4.8649, 4.7754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.04128991812467575 
model_pd.l_d.mean(): -20.11847496032715 
model_pd.lagr.mean(): -20.077184677124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5269], device='cuda:0')), ('power', tensor([-20.8766], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.04128991812467575
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10859724134206772
epoch£º431	 i:2 	 global-step:8622	 l-p:0.13784238696098328
epoch£º431	 i:3 	 global-step:8623	 l-p:0.13295288383960724
epoch£º431	 i:4 	 global-step:8624	 l-p:0.16556796431541443
epoch£º431	 i:5 	 global-step:8625	 l-p:0.08532395213842392
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12775373458862305
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13934050500392914
epoch£º431	 i:8 	 global-step:8628	 l-p:0.14155079424381256
epoch£º431	 i:9 	 global-step:8629	 l-p:0.17464162409305573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9270, 5.5274, 5.7053],
        [4.9270, 4.9270, 4.9270],
        [4.9270, 4.9091, 4.9228],
        [4.9270, 4.8945, 4.9141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1453600972890854 
model_pd.l_d.mean(): -20.183330535888672 
model_pd.lagr.mean(): -20.0379695892334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-20.9369], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1453600972890854
epoch£º432	 i:1 	 global-step:8641	 l-p:0.1542900800704956
epoch£º432	 i:2 	 global-step:8642	 l-p:0.0982215479016304
epoch£º432	 i:3 	 global-step:8643	 l-p:0.12294823676347733
epoch£º432	 i:4 	 global-step:8644	 l-p:0.1126963198184967
epoch£º432	 i:5 	 global-step:8645	 l-p:0.13249310851097107
epoch£º432	 i:6 	 global-step:8646	 l-p:0.1374017298221588
epoch£º432	 i:7 	 global-step:8647	 l-p:0.04164215177297592
epoch£º432	 i:8 	 global-step:8648	 l-p:0.2734060287475586
epoch£º432	 i:9 	 global-step:8649	 l-p:0.14590364694595337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0201, 4.9555, 4.9029],
        [5.0201, 5.2129, 5.1119],
        [5.0201, 5.0201, 5.0201],
        [5.0201, 4.9953, 5.0121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13993440568447113 
model_pd.l_d.mean(): -20.10019874572754 
model_pd.lagr.mean(): -19.960264205932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5006], device='cuda:0')), ('power', tensor([-20.8313], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13993440568447113
epoch£º433	 i:1 	 global-step:8661	 l-p:0.14504081010818481
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13188207149505615
epoch£º433	 i:3 	 global-step:8663	 l-p:0.35118457674980164
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11364160478115082
epoch£º433	 i:5 	 global-step:8665	 l-p:0.04911200329661369
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.03742998093366623
epoch£º433	 i:7 	 global-step:8667	 l-p:0.18660520017147064
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14597730338573456
epoch£º433	 i:9 	 global-step:8669	 l-p:0.13905876874923706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0600, 5.0356, 4.9248],
        [5.0600, 5.3129, 5.2395],
        [5.0600, 4.9953, 4.9827],
        [5.0600, 4.9970, 4.9467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14542537927627563 
model_pd.l_d.mean(): -20.88943862915039 
model_pd.lagr.mean(): -20.7440128326416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4035], device='cuda:0')), ('power', tensor([-21.5299], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14542537927627563
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14649063348770142
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11807872354984283
epoch£º434	 i:3 	 global-step:8683	 l-p:0.4261917471885681
epoch£º434	 i:4 	 global-step:8684	 l-p:0.04165208712220192
epoch£º434	 i:5 	 global-step:8685	 l-p:0.1915004551410675
epoch£º434	 i:6 	 global-step:8686	 l-p:0.12198279052972794
epoch£º434	 i:7 	 global-step:8687	 l-p:0.15110869705677032
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14021095633506775
epoch£º434	 i:9 	 global-step:8689	 l-p:0.09817878156900406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0000, 4.9998, 5.0000],
        [5.0000, 5.0000, 5.0000],
        [5.0000, 4.9999, 5.0000],
        [5.0000, 5.5986, 5.7684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.15344414114952087 
model_pd.l_d.mean(): -20.821840286254883 
model_pd.lagr.mean(): -20.66839599609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4210], device='cuda:0')), ('power', tensor([-21.4795], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.15344414114952087
epoch£º435	 i:1 	 global-step:8701	 l-p:0.09009216725826263
epoch£º435	 i:2 	 global-step:8702	 l-p:0.10852399468421936
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12584500014781952
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13590092957019806
epoch£º435	 i:5 	 global-step:8705	 l-p:0.08538362383842468
epoch£º435	 i:6 	 global-step:8706	 l-p:0.07768984884023666
epoch£º435	 i:7 	 global-step:8707	 l-p:0.17610454559326172
epoch£º435	 i:8 	 global-step:8708	 l-p:0.16499850153923035
epoch£º435	 i:9 	 global-step:8709	 l-p:0.1291913390159607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9266, 5.4782, 5.6160],
        [4.9266, 5.2298, 5.1897],
        [4.9266, 4.9186, 4.9256],
        [4.9266, 5.0052, 4.8679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10351923108100891 
model_pd.l_d.mean(): -20.48444175720215 
model_pd.lagr.mean(): -20.380922317504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-21.2293], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10351923108100891
epoch£º436	 i:1 	 global-step:8721	 l-p:0.09563463926315308
epoch£º436	 i:2 	 global-step:8722	 l-p:0.09047717601060867
epoch£º436	 i:3 	 global-step:8723	 l-p:0.8818453550338745
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12098685652017593
epoch£º436	 i:5 	 global-step:8725	 l-p:0.1377364695072174
epoch£º436	 i:6 	 global-step:8726	 l-p:0.14674513041973114
epoch£º436	 i:7 	 global-step:8727	 l-p:0.15917803347110748
epoch£º436	 i:8 	 global-step:8728	 l-p:0.14062447845935822
epoch£º436	 i:9 	 global-step:8729	 l-p:0.11014945805072784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0286, 5.0248, 5.0283],
        [5.0286, 5.7939, 6.1027],
        [5.0286, 5.2240, 5.1231],
        [5.0286, 5.2556, 5.1694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.018319906666874886 
model_pd.l_d.mean(): -19.470489501953125 
model_pd.lagr.mean(): -19.45216941833496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-20.1775], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.018319906666874886
epoch£º437	 i:1 	 global-step:8741	 l-p:0.26321855187416077
epoch£º437	 i:2 	 global-step:8742	 l-p:0.16783854365348816
epoch£º437	 i:3 	 global-step:8743	 l-p:0.07321396470069885
epoch£º437	 i:4 	 global-step:8744	 l-p:0.03427184373140335
epoch£º437	 i:5 	 global-step:8745	 l-p:0.10543994605541229
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13776221871376038
epoch£º437	 i:7 	 global-step:8747	 l-p:0.125961571931839
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1642853170633316
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11033342778682709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2466, 5.1954, 5.1422],
        [5.2466, 5.3211, 5.1904],
        [5.2466, 5.2236, 5.2390],
        [5.2466, 5.2466, 5.2466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12329316139221191 
model_pd.l_d.mean(): -19.376482009887695 
model_pd.lagr.mean(): -19.253189086914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-20.0176], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12329316139221191
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13087047636508942
epoch£º438	 i:2 	 global-step:8762	 l-p:0.17894290387630463
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10529658943414688
epoch£º438	 i:4 	 global-step:8764	 l-p:0.07891392707824707
epoch£º438	 i:5 	 global-step:8765	 l-p:0.13208428025245667
epoch£º438	 i:6 	 global-step:8766	 l-p:0.1750936508178711
epoch£º438	 i:7 	 global-step:8767	 l-p:0.1176673099398613
epoch£º438	 i:8 	 global-step:8768	 l-p:0.13976973295211792
epoch£º438	 i:9 	 global-step:8769	 l-p:0.11799964308738708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2343, 5.2118, 5.1122],
        [5.2343, 5.2329, 5.1181],
        [5.2343, 5.2368, 5.1199],
        [5.2343, 5.2341, 5.2343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.08683586865663528 
model_pd.l_d.mean(): -20.754554748535156 
model_pd.lagr.mean(): -20.6677188873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3585], device='cuda:0')), ('power', tensor([-21.3476], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.08683586865663528
epoch£º439	 i:1 	 global-step:8781	 l-p:0.11789874732494354
epoch£º439	 i:2 	 global-step:8782	 l-p:0.20469073951244354
epoch£º439	 i:3 	 global-step:8783	 l-p:0.11678305268287659
epoch£º439	 i:4 	 global-step:8784	 l-p:0.12865090370178223
epoch£º439	 i:5 	 global-step:8785	 l-p:0.530548095703125
epoch£º439	 i:6 	 global-step:8786	 l-p:0.13246864080429077
epoch£º439	 i:7 	 global-step:8787	 l-p:0.18227294087409973
epoch£º439	 i:8 	 global-step:8788	 l-p:0.1421821564435959
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12448549270629883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.0899, 5.1064],
        [5.1144, 5.0947, 5.1091],
        [5.1144, 5.0498, 5.0330],
        [5.1144, 5.2974, 5.1899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.05190658941864967 
model_pd.l_d.mean(): -19.37396812438965 
model_pd.lagr.mean(): -19.32206153869629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.0701], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.05190658941864967
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12634415924549103
epoch£º440	 i:2 	 global-step:8802	 l-p:0.10930147022008896
epoch£º440	 i:3 	 global-step:8803	 l-p:0.15277013182640076
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14196886122226715
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13315176963806152
epoch£º440	 i:6 	 global-step:8806	 l-p:1.0798579454421997
epoch£º440	 i:7 	 global-step:8807	 l-p:0.08636414259672165
epoch£º440	 i:8 	 global-step:8808	 l-p:0.14680977165699005
epoch£º440	 i:9 	 global-step:8809	 l-p:0.13474978506565094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0495, 5.0418, 5.0485],
        [5.0495, 5.2174, 5.1045],
        [5.0495, 5.0487, 5.0495],
        [5.0495, 4.9948, 4.9123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.04476677626371384 
model_pd.l_d.mean(): -19.159929275512695 
model_pd.lagr.mean(): -19.115161895751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.9217], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.04476677626371384
epoch£º441	 i:1 	 global-step:8821	 l-p:0.13734959065914154
epoch£º441	 i:2 	 global-step:8822	 l-p:0.5784788727760315
epoch£º441	 i:3 	 global-step:8823	 l-p:0.14644251763820648
epoch£º441	 i:4 	 global-step:8824	 l-p:-0.0317709818482399
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13266749680042267
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12573598325252533
epoch£º441	 i:7 	 global-step:8827	 l-p:0.1608043611049652
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11417233198881149
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12185541540384293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1721, 5.1735],
        [5.1736, 5.6056, 5.6361],
        [5.1736, 5.2267, 5.0928],
        [5.1736, 5.1550, 5.1688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12334782630205154 
model_pd.l_d.mean(): -20.27713394165039 
model_pd.lagr.mean(): -20.153785705566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136], device='cuda:0')), ('power', tensor([-20.9212], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12334782630205154
epoch£º442	 i:1 	 global-step:8841	 l-p:0.13027864694595337
epoch£º442	 i:2 	 global-step:8842	 l-p:0.38616523146629333
epoch£º442	 i:3 	 global-step:8843	 l-p:0.1493457555770874
epoch£º442	 i:4 	 global-step:8844	 l-p:0.14906921982765198
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.14808008074760437
epoch£º442	 i:6 	 global-step:8846	 l-p:0.1363803595304489
epoch£º442	 i:7 	 global-step:8847	 l-p:0.1381630301475525
epoch£º442	 i:8 	 global-step:8848	 l-p:0.11808828264474869
epoch£º442	 i:9 	 global-step:8849	 l-p:-0.5351051092147827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 5.1251, 5.1437],
        [5.1595, 5.1588, 5.1595],
        [5.1595, 5.1585, 5.1595],
        [5.1595, 5.1150, 5.1320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.1131356954574585 
model_pd.l_d.mean(): -20.04645347595215 
model_pd.lagr.mean(): -19.933317184448242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-20.6994], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.1131356954574585
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13338971138000488
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12842173874378204
epoch£º443	 i:3 	 global-step:8863	 l-p:-1.047051191329956
epoch£º443	 i:4 	 global-step:8864	 l-p:0.15831515192985535
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13740988075733185
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11659596860408783
epoch£º443	 i:7 	 global-step:8867	 l-p:0.025689486414194107
epoch£º443	 i:8 	 global-step:8868	 l-p:0.12776055932044983
epoch£º443	 i:9 	 global-step:8869	 l-p:0.4568816125392914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0647, 5.2194, 5.1009],
        [5.0647, 5.0187, 5.0375],
        [5.0647, 4.9960, 4.9536],
        [5.0647, 5.1623, 5.0282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 2.820662498474121 
model_pd.l_d.mean(): -18.971294403076172 
model_pd.lagr.mean(): -16.150630950927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5352], device='cuda:0')), ('power', tensor([-19.7253], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:2.820662498474121
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14177434146404266
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13390760123729706
epoch£º444	 i:3 	 global-step:8883	 l-p:0.1272730976343155
epoch£º444	 i:4 	 global-step:8884	 l-p:0.6642704010009766
epoch£º444	 i:5 	 global-step:8885	 l-p:0.12845386564731598
epoch£º444	 i:6 	 global-step:8886	 l-p:0.14742498099803925
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15713609755039215
epoch£º444	 i:8 	 global-step:8888	 l-p:0.16587474942207336
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.21004925668239594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9681, 4.9677, 4.9681],
        [4.9681, 4.8995, 4.8266],
        [4.9681, 4.9578, 4.9666],
        [4.9681, 4.8993, 4.9011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.30170387029647827 
model_pd.l_d.mean(): -20.639528274536133 
model_pd.lagr.mean(): -20.941232681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4646], device='cuda:0')), ('power', tensor([-21.3397], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.30170387029647827
epoch£º445	 i:1 	 global-step:8901	 l-p:0.1407039910554886
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10259377956390381
epoch£º445	 i:3 	 global-step:8903	 l-p:0.003594031324610114
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13627459108829498
epoch£º445	 i:5 	 global-step:8905	 l-p:0.16690564155578613
epoch£º445	 i:6 	 global-step:8906	 l-p:0.10867471247911453
epoch£º445	 i:7 	 global-step:8907	 l-p:0.1162351742386818
epoch£º445	 i:8 	 global-step:8908	 l-p:0.1329922080039978
epoch£º445	 i:9 	 global-step:8909	 l-p:0.12874414026737213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.9947, 4.9277, 4.8551],
        [4.9947, 5.4580, 5.5216],
        [4.9947, 4.9600, 4.8429],
        [4.9947, 5.4839, 5.5669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.11151283979415894 
model_pd.l_d.mean(): -20.202312469482422 
model_pd.lagr.mean(): -20.09079933166504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5003], device='cuda:0')), ('power', tensor([-20.9342], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.11151283979415894
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14069561660289764
epoch£º446	 i:2 	 global-step:8922	 l-p:0.13127130270004272
epoch£º446	 i:3 	 global-step:8923	 l-p:0.09913656860589981
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14122174680233002
epoch£º446	 i:5 	 global-step:8925	 l-p:0.1184064969420433
epoch£º446	 i:6 	 global-step:8926	 l-p:0.14227426052093506
epoch£º446	 i:7 	 global-step:8927	 l-p:0.14885708689689636
epoch£º446	 i:8 	 global-step:8928	 l-p:0.04890274256467819
epoch£º446	 i:9 	 global-step:8929	 l-p:0.07802749425172806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9594, 4.9545, 4.9590],
        [4.9594, 4.9215, 4.9427],
        [4.9594, 4.8975, 4.8061],
        [4.9594, 4.8998, 4.8046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10410358011722565 
model_pd.l_d.mean(): -20.392295837402344 
model_pd.lagr.mean(): -20.288192749023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-21.1192], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10410358011722565
epoch£º447	 i:1 	 global-step:8941	 l-p:0.144950270652771
epoch£º447	 i:2 	 global-step:8942	 l-p:0.1505337357521057
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13723674416542053
epoch£º447	 i:4 	 global-step:8944	 l-p:0.13507741689682007
epoch£º447	 i:5 	 global-step:8945	 l-p:0.0021012781653553247
epoch£º447	 i:6 	 global-step:8946	 l-p:0.1483599990606308
epoch£º447	 i:7 	 global-step:8947	 l-p:0.10235432535409927
epoch£º447	 i:8 	 global-step:8948	 l-p:0.09949595481157303
epoch£º447	 i:9 	 global-step:8949	 l-p:0.11957421153783798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9681, 5.5437, 5.6955],
        [4.9681, 5.3614, 5.3762],
        [4.9681, 4.9398, 4.9586],
        [4.9681, 4.9680, 4.9681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.1372726857662201 
model_pd.l_d.mean(): -19.82123374938965 
model_pd.lagr.mean(): -19.683961868286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5349], device='cuda:0')), ('power', tensor([-20.5843], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.1372726857662201
epoch£º448	 i:1 	 global-step:8961	 l-p:0.10008883476257324
epoch£º448	 i:2 	 global-step:8962	 l-p:0.1482868641614914
epoch£º448	 i:3 	 global-step:8963	 l-p:0.022427605465054512
epoch£º448	 i:4 	 global-step:8964	 l-p:0.09714536368846893
epoch£º448	 i:5 	 global-step:8965	 l-p:0.09337476640939713
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13706699013710022
epoch£º448	 i:7 	 global-step:8967	 l-p:0.13389699161052704
epoch£º448	 i:8 	 global-step:8968	 l-p:0.22218680381774902
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13794372975826263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9202, 4.9181, 4.9201],
        [4.9202, 4.8408, 4.8254],
        [4.9202, 4.8807, 4.9027],
        [4.9202, 4.8723, 4.8941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.11861021816730499 
model_pd.l_d.mean(): -20.774391174316406 
model_pd.lagr.mean(): -20.655780792236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-21.4785], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.11861021816730499
epoch£º449	 i:1 	 global-step:8981	 l-p:0.1098950207233429
epoch£º449	 i:2 	 global-step:8982	 l-p:0.17106014490127563
epoch£º449	 i:3 	 global-step:8983	 l-p:0.1752152144908905
epoch£º449	 i:4 	 global-step:8984	 l-p:0.16147015988826752
epoch£º449	 i:5 	 global-step:8985	 l-p:0.06681820005178452
epoch£º449	 i:6 	 global-step:8986	 l-p:0.14246416091918945
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12292102724313736
epoch£º449	 i:8 	 global-step:8988	 l-p:0.18003340065479279
epoch£º449	 i:9 	 global-step:8989	 l-p:0.11159887164831161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9551, 4.9547, 4.9551],
        [4.9551, 4.9550, 4.9551],
        [4.9551, 4.9520, 4.9549],
        [4.9551, 5.2075, 5.1337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09579427540302277 
model_pd.l_d.mean(): -20.684324264526367 
model_pd.lagr.mean(): -20.588529586791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.3889], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.09579427540302277
epoch£º450	 i:1 	 global-step:9001	 l-p:0.1583734154701233
epoch£º450	 i:2 	 global-step:9002	 l-p:0.12063582986593246
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12815722823143005
epoch£º450	 i:4 	 global-step:9004	 l-p:0.08171737194061279
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.12384670227766037
epoch£º450	 i:6 	 global-step:9006	 l-p:0.17805497348308563
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14967526495456696
epoch£º450	 i:8 	 global-step:9008	 l-p:0.09631294012069702
epoch£º450	 i:9 	 global-step:9009	 l-p:0.12160281836986542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0001, 4.9778, 4.9941],
        [5.0001, 4.9484, 4.9682],
        [5.0001, 5.0001, 5.0001],
        [5.0001, 4.9999, 5.0001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13538497686386108 
model_pd.l_d.mean(): -18.771562576293945 
model_pd.lagr.mean(): -18.63617706298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5881], device='cuda:0')), ('power', tensor([-19.5775], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13538497686386108
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10890069603919983
epoch£º451	 i:2 	 global-step:9022	 l-p:0.14258699119091034
epoch£º451	 i:3 	 global-step:9023	 l-p:0.1594334840774536
epoch£º451	 i:4 	 global-step:9024	 l-p:0.07850714772939682
epoch£º451	 i:5 	 global-step:9025	 l-p:0.12007457762956619
epoch£º451	 i:6 	 global-step:9026	 l-p:-0.03988971561193466
epoch£º451	 i:7 	 global-step:9027	 l-p:0.1509958654642105
epoch£º451	 i:8 	 global-step:9028	 l-p:0.13570410013198853
epoch£º451	 i:9 	 global-step:9029	 l-p:0.05839196965098381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0076, 4.9457, 4.9594],
        [5.0076, 5.0062, 5.0075],
        [5.0076, 5.0048, 5.0074],
        [5.0076, 5.0076, 5.0076]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.2797389626502991 
model_pd.l_d.mean(): -20.09707260131836 
model_pd.lagr.mean(): -19.817333221435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5069], device='cuda:0')), ('power', tensor([-20.8345], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.2797389626502991
epoch£º452	 i:1 	 global-step:9041	 l-p:-0.13007992506027222
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10918468236923218
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12669843435287476
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12867029011249542
epoch£º452	 i:5 	 global-step:9045	 l-p:0.07619426399469376
epoch£º452	 i:6 	 global-step:9046	 l-p:0.12246061861515045
epoch£º452	 i:7 	 global-step:9047	 l-p:0.060291800647974014
epoch£º452	 i:8 	 global-step:9048	 l-p:0.13453148305416107
epoch£º452	 i:9 	 global-step:9049	 l-p:0.13714765012264252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0565, 5.0552, 5.0565],
        [5.0565, 5.0565, 5.0565],
        [5.0565, 5.0545, 5.0564],
        [5.0565, 5.0199, 5.0407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.26658880710601807 
model_pd.l_d.mean(): -20.69481658935547 
model_pd.lagr.mean(): -20.4282283782959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4326], device='cuda:0')), ('power', tensor([-21.3629], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.26658880710601807
epoch£º453	 i:1 	 global-step:9061	 l-p:0.14377644658088684
epoch£º453	 i:2 	 global-step:9062	 l-p:0.06947050988674164
epoch£º453	 i:3 	 global-step:9063	 l-p:0.0805269405245781
epoch£º453	 i:4 	 global-step:9064	 l-p:0.1269453912973404
epoch£º453	 i:5 	 global-step:9065	 l-p:0.13012348115444183
epoch£º453	 i:6 	 global-step:9066	 l-p:0.27418503165245056
epoch£º453	 i:7 	 global-step:9067	 l-p:0.07771050184965134
epoch£º453	 i:8 	 global-step:9068	 l-p:0.11060776561498642
epoch£º453	 i:9 	 global-step:9069	 l-p:0.13954734802246094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1219, 5.0552, 5.0547],
        [5.1219, 5.2655, 5.1399],
        [5.1219, 5.1192, 5.1217],
        [5.1219, 5.1103, 5.1200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1432914286851883 
model_pd.l_d.mean(): -20.399417877197266 
model_pd.lagr.mean(): -20.256126403808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.0469], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1432914286851883
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.14262494444847107
epoch£º454	 i:2 	 global-step:9082	 l-p:0.17031186819076538
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14704015851020813
epoch£º454	 i:4 	 global-step:9084	 l-p:0.1568877398967743
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12276584655046463
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11661124974489212
epoch£º454	 i:7 	 global-step:9087	 l-p:0.09895598888397217
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.03486216068267822
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13329844176769257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0211, 4.9503, 4.8744],
        [5.0211, 5.0206, 5.0211],
        [5.0211, 5.0210, 5.0211],
        [5.0211, 5.0211, 5.0211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.13008593022823334 
model_pd.l_d.mean(): -20.212087631225586 
model_pd.lagr.mean(): -20.082002639770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4917], device='cuda:0')), ('power', tensor([-20.9353], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.13008593022823334
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12091255933046341
epoch£º455	 i:2 	 global-step:9102	 l-p:0.15154393017292023
epoch£º455	 i:3 	 global-step:9103	 l-p:0.1264587789773941
epoch£º455	 i:4 	 global-step:9104	 l-p:0.09741085767745972
epoch£º455	 i:5 	 global-step:9105	 l-p:0.10982140898704529
epoch£º455	 i:6 	 global-step:9106	 l-p:0.12670908868312836
epoch£º455	 i:7 	 global-step:9107	 l-p:0.25866401195526123
epoch£º455	 i:8 	 global-step:9108	 l-p:0.16894206404685974
epoch£º455	 i:9 	 global-step:9109	 l-p:0.12958410382270813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8463, 4.8463, 4.8463],
        [4.8463, 4.7786, 4.7961],
        [4.8463, 4.8457, 4.8463],
        [4.8463, 4.8434, 4.8461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11474546790122986 
model_pd.l_d.mean(): -20.445697784423828 
model_pd.lagr.mean(): -20.330951690673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5084], device='cuda:0')), ('power', tensor([-21.1885], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11474546790122986
epoch£º456	 i:1 	 global-step:9121	 l-p:0.1627812683582306
epoch£º456	 i:2 	 global-step:9122	 l-p:0.1522594839334488
epoch£º456	 i:3 	 global-step:9123	 l-p:-0.09223988652229309
epoch£º456	 i:4 	 global-step:9124	 l-p:0.13531391322612762
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18063746392726898
epoch£º456	 i:6 	 global-step:9126	 l-p:0.14074312150478363
epoch£º456	 i:7 	 global-step:9127	 l-p:0.21433070302009583
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14727899432182312
epoch£º456	 i:9 	 global-step:9129	 l-p:0.0972609668970108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9037, 5.0761, 4.9617],
        [4.9037, 4.8324, 4.8442],
        [4.9037, 4.9085, 4.7590],
        [4.9037, 4.9037, 4.9038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.2037002146244049 
model_pd.l_d.mean(): -20.886159896850586 
model_pd.lagr.mean(): -20.682458877563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.5819], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.2037002146244049
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08734934777021408
epoch£º457	 i:2 	 global-step:9142	 l-p:0.14705152809619904
epoch£º457	 i:3 	 global-step:9143	 l-p:0.1411711722612381
epoch£º457	 i:4 	 global-step:9144	 l-p:0.10892412066459656
epoch£º457	 i:5 	 global-step:9145	 l-p:0.15573711693286896
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12772364914417267
epoch£º457	 i:7 	 global-step:9147	 l-p:0.11825323104858398
epoch£º457	 i:8 	 global-step:9148	 l-p:0.05402754619717598
epoch£º457	 i:9 	 global-step:9149	 l-p:0.1375080943107605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0582, 5.0581, 5.0582],
        [5.0582, 5.0170, 5.0386],
        [5.0582, 5.0582, 5.0582],
        [5.0582, 5.0581, 5.0582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.15605378150939941 
model_pd.l_d.mean(): -20.7276611328125 
model_pd.lagr.mean(): -20.57160758972168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.3916], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.15605378150939941
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11904015392065048
epoch£º458	 i:2 	 global-step:9162	 l-p:0.1332506686449051
epoch£º458	 i:3 	 global-step:9163	 l-p:0.19304393231868744
epoch£º458	 i:4 	 global-step:9164	 l-p:0.11533191055059433
epoch£º458	 i:5 	 global-step:9165	 l-p:0.12654927372932434
epoch£º458	 i:6 	 global-step:9166	 l-p:0.0007002019556239247
epoch£º458	 i:7 	 global-step:9167	 l-p:0.1333119124174118
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12386853992938995
epoch£º458	 i:9 	 global-step:9169	 l-p:0.2547559440135956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1046, 5.1046, 5.1046],
        [5.1046, 5.2419, 5.1128],
        [5.1046, 5.8579, 6.1449],
        [5.1046, 5.1046, 5.1046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.13981199264526367 
model_pd.l_d.mean(): -20.22161293029785 
model_pd.lagr.mean(): -20.08180046081543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-20.8749], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.13981199264526367
epoch£º459	 i:1 	 global-step:9181	 l-p:0.14422573149204254
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11612674593925476
epoch£º459	 i:3 	 global-step:9183	 l-p:0.25683489441871643
epoch£º459	 i:4 	 global-step:9184	 l-p:0.07499516010284424
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05970068648457527
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13634425401687622
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11337249726057053
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15179967880249023
epoch£º459	 i:9 	 global-step:9189	 l-p:0.04388193041086197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0355, 5.0261, 5.0342],
        [5.0355, 4.9623, 4.8877],
        [5.0355, 5.7061, 5.9279],
        [5.0355, 5.0691, 4.9219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.12343485653400421 
model_pd.l_d.mean(): -18.7839298248291 
model_pd.lagr.mean(): -18.66049575805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5694], device='cuda:0')), ('power', tensor([-19.5709], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.12343485653400421
epoch£º460	 i:1 	 global-step:9201	 l-p:0.13078859448432922
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12620629370212555
epoch£º460	 i:3 	 global-step:9203	 l-p:0.24811182916164398
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10871122032403946
epoch£º460	 i:5 	 global-step:9205	 l-p:0.1455339640378952
epoch£º460	 i:6 	 global-step:9206	 l-p:0.14412784576416016
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13811971247196198
epoch£º460	 i:8 	 global-step:9208	 l-p:0.10108593106269836
epoch£º460	 i:9 	 global-step:9209	 l-p:0.10935308039188385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9423, 4.9403, 4.9422],
        [4.9423, 4.8583, 4.7924],
        [4.9423, 4.8966, 4.9199],
        [4.9423, 5.0270, 4.8818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.16820941865444183 
model_pd.l_d.mean(): -18.577688217163086 
model_pd.lagr.mean(): -18.40947914123535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5818], device='cuda:0')), ('power', tensor([-19.3751], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.16820941865444183
epoch£º461	 i:1 	 global-step:9221	 l-p:0.03667671978473663
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11171384900808334
epoch£º461	 i:3 	 global-step:9223	 l-p:0.15938173234462738
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13893987238407135
epoch£º461	 i:5 	 global-step:9225	 l-p:0.12937618792057037
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15957002341747284
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07682663947343826
epoch£º461	 i:8 	 global-step:9228	 l-p:0.09752114862203598
epoch£º461	 i:9 	 global-step:9229	 l-p:0.1167856976389885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0075, 4.9684, 4.8408],
        [5.0075, 4.9422, 4.8426],
        [5.0075, 4.9671, 4.9895],
        [5.0075, 4.9676, 4.9900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09215221554040909 
model_pd.l_d.mean(): -20.466873168945312 
model_pd.lagr.mean(): -20.37472152709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-21.1738], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09215221554040909
epoch£º462	 i:1 	 global-step:9241	 l-p:0.28959736227989197
epoch£º462	 i:2 	 global-step:9242	 l-p:0.1458994597196579
epoch£º462	 i:3 	 global-step:9243	 l-p:0.09681293368339539
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.22641032934188843
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11962982267141342
epoch£º462	 i:6 	 global-step:9246	 l-p:0.1511956751346588
epoch£º462	 i:7 	 global-step:9247	 l-p:0.05596205219626427
epoch£º462	 i:8 	 global-step:9248	 l-p:0.0917002335190773
epoch£º462	 i:9 	 global-step:9249	 l-p:0.14061889052391052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0699, 5.0684, 5.0699],
        [5.0699, 5.0592, 5.0683],
        [5.0699, 5.0604, 5.0686],
        [5.0699, 5.0698, 5.0699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.18948616087436676 
model_pd.l_d.mean(): -18.224523544311523 
model_pd.lagr.mean(): -18.035037994384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5972], device='cuda:0')), ('power', tensor([-19.0338], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.18948616087436676
epoch£º463	 i:1 	 global-step:9261	 l-p:0.9430779218673706
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14516134560108185
epoch£º463	 i:3 	 global-step:9263	 l-p:0.04159841313958168
epoch£º463	 i:4 	 global-step:9264	 l-p:0.12644971907138824
epoch£º463	 i:5 	 global-step:9265	 l-p:0.118079274892807
epoch£º463	 i:6 	 global-step:9266	 l-p:0.12316376715898514
epoch£º463	 i:7 	 global-step:9267	 l-p:0.12990336120128632
epoch£º463	 i:8 	 global-step:9268	 l-p:0.0677630826830864
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11806951463222504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1079, 5.1043, 5.1076],
        [5.1079, 5.1984, 5.0560],
        [5.1079, 5.3133, 5.2091],
        [5.1079, 5.1079, 5.1079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): -0.02074536681175232 
model_pd.l_d.mean(): -20.583343505859375 
model_pd.lagr.mean(): -20.604089736938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-21.2441], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:-0.02074536681175232
epoch£º464	 i:1 	 global-step:9281	 l-p:0.18220935761928558
epoch£º464	 i:2 	 global-step:9282	 l-p:0.06187301501631737
epoch£º464	 i:3 	 global-step:9283	 l-p:0.1327114850282669
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11255136132240295
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12149935215711594
epoch£º464	 i:6 	 global-step:9286	 l-p:0.2453225702047348
epoch£º464	 i:7 	 global-step:9287	 l-p:0.14365893602371216
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17336542904376984
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13256700336933136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1018, 5.1018, 5.1018],
        [5.1018, 5.1003, 5.1017],
        [5.1018, 5.0819, 5.0970],
        [5.1018, 5.0240, 4.9795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12896455824375153 
model_pd.l_d.mean(): -20.12362289428711 
model_pd.lagr.mean(): -19.994657516479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4880], device='cuda:0')), ('power', tensor([-20.8421], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12896455824375153
epoch£º465	 i:1 	 global-step:9301	 l-p:0.304344117641449
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12312283366918564
epoch£º465	 i:3 	 global-step:9303	 l-p:0.1333165466785431
epoch£º465	 i:4 	 global-step:9304	 l-p:0.12002193927764893
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12885598838329315
epoch£º465	 i:6 	 global-step:9306	 l-p:0.13859505951404572
epoch£º465	 i:7 	 global-step:9307	 l-p:0.15638533234596252
epoch£º465	 i:8 	 global-step:9308	 l-p:0.14216098189353943
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09514423459768295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0056, 4.9242, 4.9135],
        [5.0056, 4.9206, 4.8777],
        [5.0056, 4.9901, 5.0026],
        [5.0056, 5.0030, 4.8562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.13784822821617126 
model_pd.l_d.mean(): -18.976043701171875 
model_pd.lagr.mean(): -18.83819580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5460], device='cuda:0')), ('power', tensor([-19.7412], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.13784822821617126
epoch£º466	 i:1 	 global-step:9321	 l-p:0.12539371848106384
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13692481815814972
epoch£º466	 i:3 	 global-step:9323	 l-p:0.12961611151695251
epoch£º466	 i:4 	 global-step:9324	 l-p:0.5679914951324463
epoch£º466	 i:5 	 global-step:9325	 l-p:0.044283926486968994
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11308170855045319
epoch£º466	 i:7 	 global-step:9327	 l-p:0.07728540897369385
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12515850365161896
epoch£º466	 i:9 	 global-step:9329	 l-p:0.11081879585981369
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0533, 5.0532, 5.0533],
        [5.0533, 5.1883, 5.0560],
        [5.0533, 5.0302, 5.0471],
        [5.0533, 4.9709, 4.9421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.1251785010099411 
model_pd.l_d.mean(): -20.326881408691406 
model_pd.lagr.mean(): -20.201702117919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-21.0255], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.1251785010099411
epoch£º467	 i:1 	 global-step:9341	 l-p:0.1847304105758667
epoch£º467	 i:2 	 global-step:9342	 l-p:0.29501694440841675
epoch£º467	 i:3 	 global-step:9343	 l-p:0.12103977054357529
epoch£º467	 i:4 	 global-step:9344	 l-p:0.14350007474422455
epoch£º467	 i:5 	 global-step:9345	 l-p:0.560338020324707
epoch£º467	 i:6 	 global-step:9346	 l-p:0.0497661754488945
epoch£º467	 i:7 	 global-step:9347	 l-p:0.049408234655857086
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11670371890068054
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11385789513587952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1816, 5.1491, 5.1691],
        [5.1816, 5.3489, 5.2273],
        [5.1816, 5.1304, 5.1496],
        [5.1816, 5.8659, 6.0859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.4249720573425293 
model_pd.l_d.mean(): -20.5190372467041 
model_pd.lagr.mean(): -20.094064712524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4175], device='cuda:0')), ('power', tensor([-21.1698], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.4249720573425293
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10660269856452942
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12665508687496185
epoch£º468	 i:3 	 global-step:9363	 l-p:0.20803771913051605
epoch£º468	 i:4 	 global-step:9364	 l-p:0.14554975926876068
epoch£º468	 i:5 	 global-step:9365	 l-p:0.1443694531917572
epoch£º468	 i:6 	 global-step:9366	 l-p:0.12977038323879242
epoch£º468	 i:7 	 global-step:9367	 l-p:0.11169629544019699
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.1385611742734909
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11348261684179306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1365, 5.0948, 5.1166],
        [5.1365, 5.1365, 5.1365],
        [5.1365, 5.1337, 5.1363],
        [5.1365, 5.1365, 5.1365]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09397003054618835 
model_pd.l_d.mean(): -20.474241256713867 
model_pd.lagr.mean(): -20.380271911621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4128], device='cuda:0')), ('power', tensor([-21.1197], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09397003054618835
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08635853230953217
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1293749064207077
epoch£º469	 i:3 	 global-step:9383	 l-p:0.1356172412633896
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.006811418570578098
epoch£º469	 i:5 	 global-step:9385	 l-p:0.6934100985527039
epoch£º469	 i:6 	 global-step:9386	 l-p:0.1862363964319229
epoch£º469	 i:7 	 global-step:9387	 l-p:0.13901996612548828
epoch£º469	 i:8 	 global-step:9388	 l-p:0.10766002535820007
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11723284423351288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0350, 5.0348, 5.0350],
        [5.0350, 4.9524, 4.9374],
        [5.0350, 4.9814, 5.0035],
        [5.0350, 5.0349, 5.0350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12250308692455292 
model_pd.l_d.mean(): -20.592504501342773 
model_pd.lagr.mean(): -20.470001220703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4402], device='cuda:0')), ('power', tensor([-21.2672], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12250308692455292
epoch£º470	 i:1 	 global-step:9401	 l-p:0.10466981679201126
epoch£º470	 i:2 	 global-step:9402	 l-p:0.13341324031352997
epoch£º470	 i:3 	 global-step:9403	 l-p:0.06409089267253876
epoch£º470	 i:4 	 global-step:9404	 l-p:0.1531328707933426
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09408150613307953
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16575461626052856
epoch£º470	 i:7 	 global-step:9407	 l-p:0.1516873836517334
epoch£º470	 i:8 	 global-step:9408	 l-p:0.09103697538375854
epoch£º470	 i:9 	 global-step:9409	 l-p:-0.632843554019928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0074, 5.0074, 5.0074],
        [5.0074, 5.0074, 5.0074],
        [5.0074, 5.0074, 5.0074],
        [5.0074, 5.0074, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.496518611907959 
model_pd.l_d.mean(): -20.064865112304688 
model_pd.lagr.mean(): -19.56834602355957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.8094], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.496518611907959
epoch£º471	 i:1 	 global-step:9421	 l-p:0.14307671785354614
epoch£º471	 i:2 	 global-step:9422	 l-p:-0.7006505131721497
epoch£º471	 i:3 	 global-step:9423	 l-p:0.13948707282543182
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.06627091020345688
epoch£º471	 i:5 	 global-step:9425	 l-p:0.1388915777206421
epoch£º471	 i:6 	 global-step:9426	 l-p:0.127314493060112
epoch£º471	 i:7 	 global-step:9427	 l-p:0.13344693183898926
epoch£º471	 i:8 	 global-step:9428	 l-p:0.06963996589183807
epoch£º471	 i:9 	 global-step:9429	 l-p:0.13775824010372162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0700, 5.0067, 4.9029],
        [5.0700, 5.0495, 5.0651],
        [5.0700, 5.4198, 5.3955],
        [5.0700, 5.0700, 5.0700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12873317301273346 
model_pd.l_d.mean(): -20.619216918945312 
model_pd.lagr.mean(): -20.4904842376709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.2863], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12873317301273346
epoch£º472	 i:1 	 global-step:9441	 l-p:0.3124179244041443
epoch£º472	 i:2 	 global-step:9442	 l-p:0.12708353996276855
epoch£º472	 i:3 	 global-step:9443	 l-p:0.13729292154312134
epoch£º472	 i:4 	 global-step:9444	 l-p:0.1433383822441101
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12313735485076904
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08304491639137268
epoch£º472	 i:7 	 global-step:9447	 l-p:0.1141357421875
epoch£º472	 i:8 	 global-step:9448	 l-p:0.19439220428466797
epoch£º472	 i:9 	 global-step:9449	 l-p:0.12135970592498779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9232, 4.8293, 4.7783],
        [4.9232, 4.9232, 4.9232],
        [4.9232, 4.9232, 4.9232],
        [4.9232, 4.9005, 4.9176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.12456072866916656 
model_pd.l_d.mean(): -19.676347732543945 
model_pd.lagr.mean(): -19.551786422729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5991], device='cuda:0')), ('power', tensor([-20.5034], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.12456072866916656
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12694130837917328
epoch£º473	 i:2 	 global-step:9462	 l-p:0.18433338403701782
epoch£º473	 i:3 	 global-step:9463	 l-p:0.1171417087316513
epoch£º473	 i:4 	 global-step:9464	 l-p:0.1338762789964676
epoch£º473	 i:5 	 global-step:9465	 l-p:0.14603720605373383
epoch£º473	 i:6 	 global-step:9466	 l-p:0.1731978803873062
epoch£º473	 i:7 	 global-step:9467	 l-p:0.14789724349975586
epoch£º473	 i:8 	 global-step:9468	 l-p:0.11626514047384262
epoch£º473	 i:9 	 global-step:9469	 l-p:0.1554814726114273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9012, 4.8991, 4.9011],
        [4.9012, 4.9012, 4.9012],
        [4.9012, 4.9012, 4.9012],
        [4.9012, 4.8382, 4.8610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.12728257477283478 
model_pd.l_d.mean(): -20.27151107788086 
model_pd.lagr.mean(): -20.144227981567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5271], device='cuda:0')), ('power', tensor([-21.0315], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.12728257477283478
epoch£º474	 i:1 	 global-step:9481	 l-p:0.14989706873893738
epoch£º474	 i:2 	 global-step:9482	 l-p:0.15173858404159546
epoch£º474	 i:3 	 global-step:9483	 l-p:0.1643250286579132
epoch£º474	 i:4 	 global-step:9484	 l-p:0.149312362074852
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12187396734952927
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12873651087284088
epoch£º474	 i:7 	 global-step:9487	 l-p:0.17298240959644318
epoch£º474	 i:8 	 global-step:9488	 l-p:0.11096026003360748
epoch£º474	 i:9 	 global-step:9489	 l-p:0.015514754690229893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9948, 4.9213, 4.9335],
        [4.9948, 4.9077, 4.8929],
        [4.9948, 4.9948, 4.9948],
        [4.9948, 4.9948, 4.9948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1422191709280014 
model_pd.l_d.mean(): -20.04716682434082 
model_pd.lagr.mean(): -19.90494728088379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5253], device='cuda:0')), ('power', tensor([-20.8029], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1422191709280014
epoch£º475	 i:1 	 global-step:9501	 l-p:0.1514623612165451
epoch£º475	 i:2 	 global-step:9502	 l-p:0.13704709708690643
epoch£º475	 i:3 	 global-step:9503	 l-p:0.09271582216024399
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10492516309022903
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18416304886341095
epoch£º475	 i:6 	 global-step:9506	 l-p:0.14979137480258942
epoch£º475	 i:7 	 global-step:9507	 l-p:0.19541239738464355
epoch£º475	 i:8 	 global-step:9508	 l-p:0.30543094873428345
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09903275221586227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 5.5367, 5.5326],
        [5.1489, 5.1489, 5.1489],
        [5.1489, 5.4686, 5.4213],
        [5.1489, 5.1002, 5.1221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.1203315481543541 
model_pd.l_d.mean(): -20.45720672607422 
model_pd.lagr.mean(): -20.336875915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4256], device='cuda:0')), ('power', tensor([-21.1155], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.1203315481543541
epoch£º476	 i:1 	 global-step:9521	 l-p:0.12878495454788208
epoch£º476	 i:2 	 global-step:9522	 l-p:0.21418483555316925
epoch£º476	 i:3 	 global-step:9523	 l-p:0.13412359356880188
epoch£º476	 i:4 	 global-step:9524	 l-p:0.14841392636299133
epoch£º476	 i:5 	 global-step:9525	 l-p:0.1378830373287201
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.14845794439315796
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12258364260196686
epoch£º476	 i:8 	 global-step:9528	 l-p:0.07071708887815475
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12468070536851883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1109, 5.1100, 5.1108],
        [5.1109, 5.2174, 5.0740],
        [5.1109, 5.0676, 5.0905],
        [5.1109, 5.0277, 4.9768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12939263880252838 
model_pd.l_d.mean(): -19.334381103515625 
model_pd.lagr.mean(): -19.204988479614258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5044], device='cuda:0')), ('power', tensor([-20.0609], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12939263880252838
epoch£º477	 i:1 	 global-step:9541	 l-p:0.13509415090084076
epoch£º477	 i:2 	 global-step:9542	 l-p:0.1106375977396965
epoch£º477	 i:3 	 global-step:9543	 l-p:0.13551262021064758
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11208382993936539
epoch£º477	 i:5 	 global-step:9545	 l-p:0.03636956959962845
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.04715583845973015
epoch£º477	 i:7 	 global-step:9547	 l-p:0.1314852237701416
epoch£º477	 i:8 	 global-step:9548	 l-p:0.16017790138721466
epoch£º477	 i:9 	 global-step:9549	 l-p:0.1622627079486847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9393, 4.9393, 4.9393],
        [4.9393, 5.3494, 5.3721],
        [4.9393, 4.9393, 4.9393],
        [4.9393, 4.8642, 4.7484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.0938538908958435 
model_pd.l_d.mean(): -18.750896453857422 
model_pd.lagr.mean(): -18.65704345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5957], device='cuda:0')), ('power', tensor([-19.5644], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.0938538908958435
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16578158736228943
epoch£º478	 i:2 	 global-step:9562	 l-p:0.1154978796839714
epoch£º478	 i:3 	 global-step:9563	 l-p:0.18795524537563324
epoch£º478	 i:4 	 global-step:9564	 l-p:0.1345459371805191
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13983319699764252
epoch£º478	 i:6 	 global-step:9566	 l-p:0.13203927874565125
epoch£º478	 i:7 	 global-step:9567	 l-p:0.16044510900974274
epoch£º478	 i:8 	 global-step:9568	 l-p:0.129279226064682
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15426583588123322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8931, 5.2788, 5.2869],
        [4.8931, 4.8762, 4.8899],
        [4.8931, 4.8114, 4.6960],
        [4.8931, 4.8215, 4.8419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.125279501080513 
model_pd.l_d.mean(): -19.89193344116211 
model_pd.lagr.mean(): -19.766653060913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5562], device='cuda:0')), ('power', tensor([-20.6776], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.125279501080513
epoch£º479	 i:1 	 global-step:9581	 l-p:0.12326612323522568
epoch£º479	 i:2 	 global-step:9582	 l-p:0.15563784539699554
epoch£º479	 i:3 	 global-step:9583	 l-p:0.10189295560121536
epoch£º479	 i:4 	 global-step:9584	 l-p:0.15663613379001617
epoch£º479	 i:5 	 global-step:9585	 l-p:0.1957458108663559
epoch£º479	 i:6 	 global-step:9586	 l-p:0.17405231297016144
epoch£º479	 i:7 	 global-step:9587	 l-p:0.20604558289051056
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.03347520902752876
epoch£º479	 i:9 	 global-step:9589	 l-p:0.23792226612567902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8583, 4.8583, 4.8583],
        [4.8583, 4.7626, 4.7549],
        [4.8583, 4.8583, 4.8583],
        [4.8583, 4.7949, 4.8200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12709638476371765 
model_pd.l_d.mean(): -19.346187591552734 
model_pd.lagr.mean(): -19.219091415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5471], device='cuda:0')), ('power', tensor([-20.1165], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12709638476371765
epoch£º480	 i:1 	 global-step:9601	 l-p:0.1358838975429535
epoch£º480	 i:2 	 global-step:9602	 l-p:0.17745624482631683
epoch£º480	 i:3 	 global-step:9603	 l-p:0.1537221372127533
epoch£º480	 i:4 	 global-step:9604	 l-p:0.08958546072244644
epoch£º480	 i:5 	 global-step:9605	 l-p:0.17579524219036102
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15414345264434814
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14524555206298828
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10974091291427612
epoch£º480	 i:9 	 global-step:9609	 l-p:0.09926904737949371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0303, 4.9959, 5.0180],
        [5.0303, 5.0303, 5.0303],
        [5.0303, 5.0260, 5.0300],
        [5.0303, 5.0302, 5.0303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.13670167326927185 
model_pd.l_d.mean(): -20.825008392333984 
model_pd.lagr.mean(): -20.68830680847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4289], device='cuda:0')), ('power', tensor([-21.4908], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.13670167326927185
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13685204088687897
epoch£º481	 i:2 	 global-step:9622	 l-p:0.1414233148097992
epoch£º481	 i:3 	 global-step:9623	 l-p:0.16160282492637634
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.08224569261074066
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14112725853919983
epoch£º481	 i:6 	 global-step:9626	 l-p:0.14079152047634125
epoch£º481	 i:7 	 global-step:9627	 l-p:0.13519488275051117
epoch£º481	 i:8 	 global-step:9628	 l-p:0.06898283213376999
epoch£º481	 i:9 	 global-step:9629	 l-p:0.1390172690153122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9654, 4.9427, 4.9599],
        [4.9654, 4.9163, 4.9416],
        [4.9654, 4.8781, 4.7835],
        [4.9654, 5.0338, 4.8776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.07533394545316696 
model_pd.l_d.mean(): -20.908899307250977 
model_pd.lagr.mean(): -20.83356475830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4307], device='cuda:0')), ('power', tensor([-21.5774], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.07533394545316696
epoch£º482	 i:1 	 global-step:9641	 l-p:0.11337234079837799
epoch£º482	 i:2 	 global-step:9642	 l-p:0.09838252514600754
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08843324333429337
epoch£º482	 i:4 	 global-step:9644	 l-p:0.17779630422592163
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11731843650341034
epoch£º482	 i:6 	 global-step:9646	 l-p:0.1409209817647934
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12876176834106445
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12215472012758255
epoch£º482	 i:9 	 global-step:9649	 l-p:0.09004081785678864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0774, 5.0774, 5.0774],
        [5.0774, 5.1669, 5.0168],
        [5.0774, 5.0769, 5.0774],
        [5.0774, 5.0774, 5.0774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.0742177739739418 
model_pd.l_d.mean(): -18.720680236816406 
model_pd.lagr.mean(): -18.794897079467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5587], device='cuda:0')), ('power', tensor([-19.4961], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.0742177739739418
epoch£º483	 i:1 	 global-step:9661	 l-p:0.21472415328025818
epoch£º483	 i:2 	 global-step:9662	 l-p:0.1501176953315735
epoch£º483	 i:3 	 global-step:9663	 l-p:0.12080284208059311
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13598574697971344
epoch£º483	 i:5 	 global-step:9665	 l-p:0.002598769497126341
epoch£º483	 i:6 	 global-step:9666	 l-p:0.12185394763946533
epoch£º483	 i:7 	 global-step:9667	 l-p:0.10930123925209045
epoch£º483	 i:8 	 global-step:9668	 l-p:0.15038064122200012
epoch£º483	 i:9 	 global-step:9669	 l-p:0.12631061673164368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.0859, 5.3132, 5.2148],
        [5.0859, 5.0067, 4.9170],
        [5.0859, 5.0477, 4.9095],
        [5.0859, 5.3386, 5.2533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.13038600981235504 
model_pd.l_d.mean(): -19.636035919189453 
model_pd.lagr.mean(): -19.50564956665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5459], device='cuda:0')), ('power', tensor([-20.4083], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.13038600981235504
epoch£º484	 i:1 	 global-step:9681	 l-p:0.14005428552627563
epoch£º484	 i:2 	 global-step:9682	 l-p:0.22077922523021698
epoch£º484	 i:3 	 global-step:9683	 l-p:0.0458182618021965
epoch£º484	 i:4 	 global-step:9684	 l-p:0.07428999245166779
epoch£º484	 i:5 	 global-step:9685	 l-p:-0.32484787702560425
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13321693241596222
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12848712503910065
epoch£º484	 i:8 	 global-step:9688	 l-p:0.1491292119026184
epoch£º484	 i:9 	 global-step:9689	 l-p:0.12468847632408142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0281, 4.9962, 5.0176],
        [5.0281, 4.9502, 4.9603],
        [5.0281, 5.0281, 5.0281],
        [5.0281, 5.0281, 5.0281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.13023236393928528 
model_pd.l_d.mean(): -21.11899757385254 
model_pd.lagr.mean(): -20.988765716552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3722], device='cuda:0')), ('power', tensor([-21.7300], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.13023236393928528
epoch£º485	 i:1 	 global-step:9701	 l-p:0.0875311940908432
epoch£º485	 i:2 	 global-step:9702	 l-p:0.1198170855641365
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12793616950511932
epoch£º485	 i:4 	 global-step:9704	 l-p:0.05127715319395065
epoch£º485	 i:5 	 global-step:9705	 l-p:0.1269957274198532
epoch£º485	 i:6 	 global-step:9706	 l-p:0.16343334317207336
epoch£º485	 i:7 	 global-step:9707	 l-p:0.2319965809583664
epoch£º485	 i:8 	 global-step:9708	 l-p:0.16303351521492004
epoch£º485	 i:9 	 global-step:9709	 l-p:0.3457728922367096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8729, 4.8170, 4.6680],
        [4.8729, 4.7692, 4.7373],
        [4.8729, 5.0298, 4.9016],
        [4.8729, 4.8130, 4.8394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.16269400715827942 
model_pd.l_d.mean(): -20.03476333618164 
model_pd.lagr.mean(): -19.872068405151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5664], device='cuda:0')), ('power', tensor([-20.8323], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.16269400715827942
epoch£º486	 i:1 	 global-step:9721	 l-p:0.12405181676149368
epoch£º486	 i:2 	 global-step:9722	 l-p:0.14431162178516388
epoch£º486	 i:3 	 global-step:9723	 l-p:0.21414348483085632
epoch£º486	 i:4 	 global-step:9724	 l-p:0.12948332726955414
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12295550107955933
epoch£º486	 i:6 	 global-step:9726	 l-p:0.1678609997034073
epoch£º486	 i:7 	 global-step:9727	 l-p:0.2335374802350998
epoch£º486	 i:8 	 global-step:9728	 l-p:0.098263680934906
epoch£º486	 i:9 	 global-step:9729	 l-p:0.13352817296981812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9170, 4.8158, 4.7824],
        [4.9170, 4.9168, 4.9170],
        [4.9170, 4.9170, 4.9170],
        [4.9170, 4.9375, 4.7730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.11899055540561676 
model_pd.l_d.mean(): -20.16006851196289 
model_pd.lagr.mean(): -20.041078567504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5204], device='cuda:0')), ('power', tensor([-20.9120], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.11899055540561676
epoch£º487	 i:1 	 global-step:9741	 l-p:0.18392764031887054
epoch£º487	 i:2 	 global-step:9742	 l-p:0.12452429533004761
epoch£º487	 i:3 	 global-step:9743	 l-p:0.012967299669981003
epoch£º487	 i:4 	 global-step:9744	 l-p:0.16631585359573364
epoch£º487	 i:5 	 global-step:9745	 l-p:0.11188308149576187
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13846206665039062
epoch£º487	 i:7 	 global-step:9747	 l-p:0.14526629447937012
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13182520866394043
epoch£º487	 i:9 	 global-step:9749	 l-p:0.1313132792711258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0368, 5.0356, 5.0368],
        [5.0368, 4.9771, 4.8477],
        [5.0368, 5.0149, 5.0316],
        [5.0368, 5.3928, 5.3710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13616684079170227 
model_pd.l_d.mean(): -20.399518966674805 
model_pd.lagr.mean(): -20.263351440429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.1095], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13616684079170227
epoch£º488	 i:1 	 global-step:9761	 l-p:0.08410627394914627
epoch£º488	 i:2 	 global-step:9762	 l-p:0.1583249419927597
epoch£º488	 i:3 	 global-step:9763	 l-p:0.11441351473331451
epoch£º488	 i:4 	 global-step:9764	 l-p:0.1524846851825714
epoch£º488	 i:5 	 global-step:9765	 l-p:0.00800760742276907
epoch£º488	 i:6 	 global-step:9766	 l-p:0.09676030278205872
epoch£º488	 i:7 	 global-step:9767	 l-p:0.10086904466152191
epoch£º488	 i:8 	 global-step:9768	 l-p:0.12309623509645462
epoch£º488	 i:9 	 global-step:9769	 l-p:0.16629116237163544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9671, 4.9341, 4.9563],
        [4.9671, 4.9667, 4.9671],
        [4.9671, 4.9311, 4.9544],
        [4.9671, 4.9671, 4.9671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.11854586005210876 
model_pd.l_d.mean(): -20.743633270263672 
model_pd.lagr.mean(): -20.62508773803711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-21.4229], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.11854586005210876
epoch£º489	 i:1 	 global-step:9781	 l-p:0.12355942279100418
epoch£º489	 i:2 	 global-step:9782	 l-p:0.051034968346357346
epoch£º489	 i:3 	 global-step:9783	 l-p:0.1322493851184845
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1894325166940689
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1906239241361618
epoch£º489	 i:6 	 global-step:9786	 l-p:0.14327985048294067
epoch£º489	 i:7 	 global-step:9787	 l-p:0.14435379207134247
epoch£º489	 i:8 	 global-step:9788	 l-p:0.14060036838054657
epoch£º489	 i:9 	 global-step:9789	 l-p:0.14009743928909302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 4.9684, 4.9684],
        [4.9684, 4.9679, 4.9684],
        [4.9684, 4.9676, 4.9684],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1425071507692337 
model_pd.l_d.mean(): -20.85320281982422 
model_pd.lagr.mean(): -20.710695266723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4362], device='cuda:0')), ('power', tensor([-21.5267], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1425071507692337
epoch£º490	 i:1 	 global-step:9801	 l-p:0.116996169090271
epoch£º490	 i:2 	 global-step:9802	 l-p:0.18796531856060028
epoch£º490	 i:3 	 global-step:9803	 l-p:0.12163568288087845
epoch£º490	 i:4 	 global-step:9804	 l-p:0.1253698617219925
epoch£º490	 i:5 	 global-step:9805	 l-p:0.1282072365283966
epoch£º490	 i:6 	 global-step:9806	 l-p:0.10805586725473404
epoch£º490	 i:7 	 global-step:9807	 l-p:0.12697072327136993
epoch£º490	 i:8 	 global-step:9808	 l-p:-0.012417620979249477
epoch£º490	 i:9 	 global-step:9809	 l-p:0.13444630801677704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0224, 5.4300, 5.4434],
        [5.0224, 4.9275, 4.9043],
        [5.0224, 4.9842, 5.0080],
        [5.0224, 5.1937, 5.0685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.002354600466787815 
model_pd.l_d.mean(): -19.43155860900879 
model_pd.lagr.mean(): -19.4292049407959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5049], device='cuda:0')), ('power', tensor([-20.1597], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.002354600466787815
epoch£º491	 i:1 	 global-step:9821	 l-p:0.1298820674419403
epoch£º491	 i:2 	 global-step:9822	 l-p:0.14100511372089386
epoch£º491	 i:3 	 global-step:9823	 l-p:0.14419113099575043
epoch£º491	 i:4 	 global-step:9824	 l-p:0.10870632529258728
epoch£º491	 i:5 	 global-step:9825	 l-p:0.1825575977563858
epoch£º491	 i:6 	 global-step:9826	 l-p:0.14433740079402924
epoch£º491	 i:7 	 global-step:9827	 l-p:0.08127035200595856
epoch£º491	 i:8 	 global-step:9828	 l-p:0.12782512605190277
epoch£º491	 i:9 	 global-step:9829	 l-p:0.13078783452510834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0384, 5.0384, 5.0384],
        [5.0384, 4.9682, 4.8456],
        [5.0384, 5.0309, 5.0376],
        [5.0384, 5.0329, 5.0379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.13858604431152344 
model_pd.l_d.mean(): -20.705873489379883 
model_pd.lagr.mean(): -20.56728744506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-21.3910], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.13858604431152344
epoch£º492	 i:1 	 global-step:9841	 l-p:0.14249595999717712
epoch£º492	 i:2 	 global-step:9842	 l-p:0.1121310219168663
epoch£º492	 i:3 	 global-step:9843	 l-p:0.023413173854351044
epoch£º492	 i:4 	 global-step:9844	 l-p:0.1561913639307022
epoch£º492	 i:5 	 global-step:9845	 l-p:0.11409766972064972
epoch£º492	 i:6 	 global-step:9846	 l-p:0.20907309651374817
epoch£º492	 i:7 	 global-step:9847	 l-p:0.16115184128284454
epoch£º492	 i:8 	 global-step:9848	 l-p:0.15903590619564056
epoch£º492	 i:9 	 global-step:9849	 l-p:0.11939111351966858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9005, 5.5539, 5.7643],
        [4.9005, 4.9002, 4.9005],
        [4.9005, 4.8011, 4.7928],
        [4.9005, 5.3927, 5.4750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.18495890498161316 
model_pd.l_d.mean(): -17.566383361816406 
model_pd.lagr.mean(): -17.381423950195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6891], device='cuda:0')), ('power', tensor([-18.4624], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.18495890498161316
epoch£º493	 i:1 	 global-step:9861	 l-p:0.1261938065290451
epoch£º493	 i:2 	 global-step:9862	 l-p:0.15180563926696777
epoch£º493	 i:3 	 global-step:9863	 l-p:0.17505107820034027
epoch£º493	 i:4 	 global-step:9864	 l-p:0.16349808871746063
epoch£º493	 i:5 	 global-step:9865	 l-p:0.16861404478549957
epoch£º493	 i:6 	 global-step:9866	 l-p:0.12626956403255463
epoch£º493	 i:7 	 global-step:9867	 l-p:0.14982883632183075
epoch£º493	 i:8 	 global-step:9868	 l-p:0.12150874733924866
epoch£º493	 i:9 	 global-step:9869	 l-p:0.11522310227155685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9164, 4.8298, 4.7063],
        [4.9164, 5.5080, 5.6665],
        [4.9164, 4.8308, 4.8442],
        [4.9164, 4.8982, 4.9129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.1310400515794754 
model_pd.l_d.mean(): -18.213821411132812 
model_pd.lagr.mean(): -18.082780838012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6512], device='cuda:0')), ('power', tensor([-19.0782], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.1310400515794754
epoch£º494	 i:1 	 global-step:9881	 l-p:0.19722038507461548
epoch£º494	 i:2 	 global-step:9882	 l-p:0.06165821850299835
epoch£º494	 i:3 	 global-step:9883	 l-p:0.14495722949504852
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1634499877691269
epoch£º494	 i:5 	 global-step:9885	 l-p:0.0856001228094101
epoch£º494	 i:6 	 global-step:9886	 l-p:0.09552393853664398
epoch£º494	 i:7 	 global-step:9887	 l-p:0.15063998103141785
epoch£º494	 i:8 	 global-step:9888	 l-p:0.10787126421928406
epoch£º494	 i:9 	 global-step:9889	 l-p:0.15715713798999786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0318, 4.9406, 4.9362],
        [5.0318, 5.0312, 5.0318],
        [5.0318, 5.3384, 5.2837],
        [5.0318, 5.0205, 5.0302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.137495219707489 
model_pd.l_d.mean(): -20.46092987060547 
model_pd.lagr.mean(): -20.323434829711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587], device='cuda:0')), ('power', tensor([-21.1531], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.137495219707489
epoch£º495	 i:1 	 global-step:9901	 l-p:0.13804294168949127
epoch£º495	 i:2 	 global-step:9902	 l-p:0.14860603213310242
epoch£º495	 i:3 	 global-step:9903	 l-p:0.13239416480064392
epoch£º495	 i:4 	 global-step:9904	 l-p:0.014400784857571125
epoch£º495	 i:5 	 global-step:9905	 l-p:0.16486571729183197
epoch£º495	 i:6 	 global-step:9906	 l-p:0.10384581983089447
epoch£º495	 i:7 	 global-step:9907	 l-p:0.12931466102600098
epoch£º495	 i:8 	 global-step:9908	 l-p:0.15262198448181152
epoch£º495	 i:9 	 global-step:9909	 l-p:0.14063698053359985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9392, 4.9392],
        [4.9392, 4.8765, 4.9029],
        [4.9392, 4.9392, 4.9392],
        [4.9392, 5.2193, 5.1522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.14888784289360046 
model_pd.l_d.mean(): -20.6256046295166 
model_pd.lagr.mean(): -20.476716995239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-21.3352], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.14888784289360046
epoch£º496	 i:1 	 global-step:9921	 l-p:0.13292145729064941
epoch£º496	 i:2 	 global-step:9922	 l-p:0.11183864623308182
epoch£º496	 i:3 	 global-step:9923	 l-p:0.11906053870916367
epoch£º496	 i:4 	 global-step:9924	 l-p:0.15094362199306488
epoch£º496	 i:5 	 global-step:9925	 l-p:0.13803592324256897
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13609734177589417
epoch£º496	 i:7 	 global-step:9927	 l-p:0.12445264309644699
epoch£º496	 i:8 	 global-step:9928	 l-p:0.26027581095695496
epoch£º496	 i:9 	 global-step:9929	 l-p:0.13909006118774414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9232, 4.8255, 4.7183],
        [4.9232, 4.9232, 4.9232],
        [4.9232, 4.9232, 4.9232],
        [4.9232, 4.9216, 4.9232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.16035017371177673 
model_pd.l_d.mean(): -20.340272903442383 
model_pd.lagr.mean(): -20.179922103881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5221], device='cuda:0')), ('power', tensor([-21.0960], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.16035017371177673
epoch£º497	 i:1 	 global-step:9941	 l-p:0.1331712156534195
epoch£º497	 i:2 	 global-step:9942	 l-p:0.12471963465213776
epoch£º497	 i:3 	 global-step:9943	 l-p:0.11944476515054703
epoch£º497	 i:4 	 global-step:9944	 l-p:0.1320464015007019
epoch£º497	 i:5 	 global-step:9945	 l-p:0.180190771818161
epoch£º497	 i:6 	 global-step:9946	 l-p:0.1501893550157547
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14831261336803436
epoch£º497	 i:8 	 global-step:9948	 l-p:0.10938429832458496
epoch£º497	 i:9 	 global-step:9949	 l-p:0.3106338083744049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8887, 4.8417, 4.8690],
        [4.8887, 5.2471, 5.2327],
        [4.8887, 5.0096, 4.8621],
        [4.8887, 4.8810, 4.8879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.17632068693637848 
model_pd.l_d.mean(): -20.405315399169922 
model_pd.lagr.mean(): -20.228994369506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-21.1570], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.17632068693637848
epoch£º498	 i:1 	 global-step:9961	 l-p:0.15777325630187988
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12448009103536606
epoch£º498	 i:3 	 global-step:9963	 l-p:0.1653222292661667
epoch£º498	 i:4 	 global-step:9964	 l-p:0.17121434211730957
epoch£º498	 i:5 	 global-step:9965	 l-p:0.13332810997962952
epoch£º498	 i:6 	 global-step:9966	 l-p:0.1628110110759735
epoch£º498	 i:7 	 global-step:9967	 l-p:0.10563323646783829
epoch£º498	 i:8 	 global-step:9968	 l-p:0.09011483192443848
epoch£º498	 i:9 	 global-step:9969	 l-p:0.11234387010335922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0041, 5.0033, 5.0040],
        [5.0041, 5.3654, 5.3463],
        [5.0041, 5.0041, 5.0041],
        [5.0041, 5.0041, 5.0041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.16338065266609192 
model_pd.l_d.mean(): -21.003332138061523 
model_pd.lagr.mean(): -20.839950561523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4000], device='cuda:0')), ('power', tensor([-21.6415], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.16338065266609192
epoch£º499	 i:1 	 global-step:9981	 l-p:0.1093035489320755
epoch£º499	 i:2 	 global-step:9982	 l-p:0.114552341401577
epoch£º499	 i:3 	 global-step:9983	 l-p:0.14720822870731354
epoch£º499	 i:4 	 global-step:9984	 l-p:0.020959820598363876
epoch£º499	 i:5 	 global-step:9985	 l-p:0.21982887387275696
epoch£º499	 i:6 	 global-step:9986	 l-p:0.1248871311545372
epoch£º499	 i:7 	 global-step:9987	 l-p:0.10273689776659012
epoch£º499	 i:8 	 global-step:9988	 l-p:0.1362578272819519
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12758754193782806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0708, 4.9943, 5.0111],
        [5.0708, 4.9846, 4.8793],
        [5.0708, 5.0262, 5.0517],
        [5.0708, 5.1691, 5.0146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.11670877039432526 
model_pd.l_d.mean(): -20.817615509033203 
model_pd.lagr.mean(): -20.70090675354004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4207], device='cuda:0')), ('power', tensor([-21.4749], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.11670877039432526
epoch£º500	 i:1 	 global-step:10001	 l-p:0.13638867437839508
epoch£º500	 i:2 	 global-step:10002	 l-p:0.40015050768852234
epoch£º500	 i:3 	 global-step:10003	 l-p:0.1410163789987564
epoch£º500	 i:4 	 global-step:10004	 l-p:0.12122388929128647
epoch£º500	 i:5 	 global-step:10005	 l-p:0.13135875761508942
epoch£º500	 i:6 	 global-step:10006	 l-p:0.12303060293197632
epoch£º500	 i:7 	 global-step:10007	 l-p:0.1692318320274353
epoch£º500	 i:8 	 global-step:10008	 l-p:0.04264918342232704
epoch£º500	 i:9 	 global-step:10009	 l-p:0.14374065399169922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9929, 4.8997, 4.9020],
        [4.9929, 4.9013, 4.7888],
        [4.9929, 4.9157, 4.7824],
        [4.9929, 4.9660, 4.9858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.0598672553896904 
model_pd.l_d.mean(): -19.995651245117188 
model_pd.lagr.mean(): -19.93578338623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5173], device='cuda:0')), ('power', tensor([-20.7426], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.0598672553896904
epoch£º501	 i:1 	 global-step:10021	 l-p:0.18439017236232758
epoch£º501	 i:2 	 global-step:10022	 l-p:0.1566701978445053
epoch£º501	 i:3 	 global-step:10023	 l-p:0.11176715791225433
epoch£º501	 i:4 	 global-step:10024	 l-p:0.1387106329202652
epoch£º501	 i:5 	 global-step:10025	 l-p:0.34816908836364746
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11580236256122589
epoch£º501	 i:7 	 global-step:10027	 l-p:0.13582338392734528
epoch£º501	 i:8 	 global-step:10028	 l-p:0.07969781011343002
epoch£º501	 i:9 	 global-step:10029	 l-p:0.1325892060995102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0356, 4.9332, 4.8645],
        [5.0356, 5.0356, 5.0356],
        [5.0356, 5.3924, 5.3678],
        [5.0356, 5.0354, 5.0356]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.14143405854701996 
model_pd.l_d.mean(): -19.596132278442383 
model_pd.lagr.mean(): -19.45469856262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5179], device='cuda:0')), ('power', tensor([-20.3394], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.14143405854701996
epoch£º502	 i:1 	 global-step:10041	 l-p:-0.02216896414756775
epoch£º502	 i:2 	 global-step:10042	 l-p:0.168365478515625
epoch£º502	 i:3 	 global-step:10043	 l-p:0.14722952246665955
epoch£º502	 i:4 	 global-step:10044	 l-p:0.11985664069652557
epoch£º502	 i:5 	 global-step:10045	 l-p:0.15029257535934448
epoch£º502	 i:6 	 global-step:10046	 l-p:0.008554692380130291
epoch£º502	 i:7 	 global-step:10047	 l-p:0.13374289870262146
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13488145172595978
epoch£º502	 i:9 	 global-step:10049	 l-p:0.12061076611280441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9515, 4.9514, 4.9515],
        [4.9515, 5.2640, 5.2146],
        [4.9515, 4.9515, 4.9515],
        [4.9515, 4.9276, 4.9459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.15928205847740173 
model_pd.l_d.mean(): -19.8164005279541 
model_pd.lagr.mean(): -19.65711784362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5128], device='cuda:0')), ('power', tensor([-20.5568], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.15928205847740173
epoch£º503	 i:1 	 global-step:10061	 l-p:0.11905033886432648
epoch£º503	 i:2 	 global-step:10062	 l-p:0.12171854078769684
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13752248883247375
epoch£º503	 i:4 	 global-step:10064	 l-p:0.10072854906320572
epoch£º503	 i:5 	 global-step:10065	 l-p:0.21875786781311035
epoch£º503	 i:6 	 global-step:10066	 l-p:0.2188626080751419
epoch£º503	 i:7 	 global-step:10067	 l-p:0.7213515043258667
epoch£º503	 i:8 	 global-step:10068	 l-p:0.1326368898153305
epoch£º503	 i:9 	 global-step:10069	 l-p:0.10031639039516449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8807, 4.8693, 4.8792],
        [4.8807, 4.7742, 4.6612],
        [4.8807, 4.8499, 4.8721],
        [4.8807, 4.7821, 4.7886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.1040213480591774 
model_pd.l_d.mean(): -20.45507049560547 
model_pd.lagr.mean(): -20.351049423217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5258], device='cuda:0')), ('power', tensor([-21.2158], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.1040213480591774
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11652549356222153
epoch£º504	 i:2 	 global-step:10082	 l-p:0.20484931766986847
epoch£º504	 i:3 	 global-step:10083	 l-p:0.8395435810089111
epoch£º504	 i:4 	 global-step:10084	 l-p:0.167058065533638
epoch£º504	 i:5 	 global-step:10085	 l-p:0.13690082728862762
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13333973288536072
epoch£º504	 i:7 	 global-step:10087	 l-p:0.1637464463710785
epoch£º504	 i:8 	 global-step:10088	 l-p:0.16622069478034973
epoch£º504	 i:9 	 global-step:10089	 l-p:0.16351594030857086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9402, 4.8336, 4.8185],
        [4.9402, 4.8898, 4.9179],
        [4.9402, 4.9356, 4.9398],
        [4.9402, 4.8721, 4.8993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.19710829854011536 
model_pd.l_d.mean(): -20.646669387817383 
model_pd.lagr.mean(): -20.449560165405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.3593], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.19710829854011536
epoch£º505	 i:1 	 global-step:10101	 l-p:0.16338953375816345
epoch£º505	 i:2 	 global-step:10102	 l-p:0.10877624899148941
epoch£º505	 i:3 	 global-step:10103	 l-p:0.15025946497917175
epoch£º505	 i:4 	 global-step:10104	 l-p:0.08955780416727066
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13351184129714966
epoch£º505	 i:6 	 global-step:10106	 l-p:0.08978088945150375
epoch£º505	 i:7 	 global-step:10107	 l-p:0.14325958490371704
epoch£º505	 i:8 	 global-step:10108	 l-p:0.11303405463695526
epoch£º505	 i:9 	 global-step:10109	 l-p:0.08509034663438797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1148, 5.1040, 5.1134],
        [5.1148, 5.7998, 6.0169],
        [5.1148, 5.6525, 5.7518],
        [5.1148, 5.0449, 4.9139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.0794689878821373 
model_pd.l_d.mean(): -19.715002059936523 
model_pd.lagr.mean(): -19.63553237915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5112], device='cuda:0')), ('power', tensor([-20.4527], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.0794689878821373
epoch£º506	 i:1 	 global-step:10121	 l-p:0.07624891400337219
epoch£º506	 i:2 	 global-step:10122	 l-p:0.18116432428359985
epoch£º506	 i:3 	 global-step:10123	 l-p:0.11113272607326508
epoch£º506	 i:4 	 global-step:10124	 l-p:0.24788959324359894
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11300100386142731
epoch£º506	 i:6 	 global-step:10126	 l-p:0.14395736157894135
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10417336970567703
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11652975529432297
epoch£º506	 i:9 	 global-step:10129	 l-p:0.13086476922035217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2544, 5.2127, 5.2368],
        [5.2544, 5.1634, 5.1168],
        [5.2544, 5.1709, 5.0860],
        [5.2544, 5.9215, 6.1136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.13685239851474762 
model_pd.l_d.mean(): -20.344478607177734 
model_pd.lagr.mean(): -20.207626342773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4221], device='cuda:0')), ('power', tensor([-20.9980], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.13685239851474762
epoch£º507	 i:1 	 global-step:10141	 l-p:0.12436521053314209
epoch£º507	 i:2 	 global-step:10142	 l-p:0.12004094570875168
epoch£º507	 i:3 	 global-step:10143	 l-p:0.17550590634346008
epoch£º507	 i:4 	 global-step:10144	 l-p:0.12269961833953857
epoch£º507	 i:5 	 global-step:10145	 l-p:0.07053324580192566
epoch£º507	 i:6 	 global-step:10146	 l-p:0.0622921921312809
epoch£º507	 i:7 	 global-step:10147	 l-p:0.1809225082397461
epoch£º507	 i:8 	 global-step:10148	 l-p:0.023747490718960762
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12140945345163345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.1491, 5.1665],
        [5.1719, 5.1697, 5.1718],
        [5.1719, 5.1713, 5.1719],
        [5.1719, 5.1886, 5.0233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09847798943519592 
model_pd.l_d.mean(): -19.465051651000977 
model_pd.lagr.mean(): -19.366573333740234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4953], device='cuda:0')), ('power', tensor([-20.1838], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09847798943519592
epoch£º508	 i:1 	 global-step:10161	 l-p:0.13309286534786224
epoch£º508	 i:2 	 global-step:10162	 l-p:0.2109023928642273
epoch£º508	 i:3 	 global-step:10163	 l-p:-0.001677503576502204
epoch£º508	 i:4 	 global-step:10164	 l-p:0.022059520706534386
epoch£º508	 i:5 	 global-step:10165	 l-p:0.14658141136169434
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.03772082179784775
epoch£º508	 i:7 	 global-step:10167	 l-p:0.10992458462715149
epoch£º508	 i:8 	 global-step:10168	 l-p:0.1460052877664566
epoch£º508	 i:9 	 global-step:10169	 l-p:0.1368447244167328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2106, 5.4624, 5.3675],
        [5.2106, 5.2101, 5.2106],
        [5.2106, 5.9496, 6.2045],
        [5.2106, 5.1727, 5.1964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.3107868432998657 
model_pd.l_d.mean(): -19.880544662475586 
model_pd.lagr.mean(): -19.56975746154785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-20.5574], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.3107868432998657
epoch£º509	 i:1 	 global-step:10181	 l-p:0.3306747078895569
epoch£º509	 i:2 	 global-step:10182	 l-p:0.11855660378932953
epoch£º509	 i:3 	 global-step:10183	 l-p:0.13766656816005707
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09413301199674606
epoch£º509	 i:5 	 global-step:10185	 l-p:0.12326036393642426
epoch£º509	 i:6 	 global-step:10186	 l-p:0.1146102324128151
epoch£º509	 i:7 	 global-step:10187	 l-p:-0.11597535759210587
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10815021395683289
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14048902690410614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1479, 5.1465, 5.1479],
        [5.1479, 5.2863, 5.1414],
        [5.1479, 5.3233, 5.1928],
        [5.1479, 5.1668, 4.9994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11648471653461456 
model_pd.l_d.mean(): -18.698102951049805 
model_pd.lagr.mean(): -18.58161735534668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-19.3819], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11648471653461456
epoch£º510	 i:1 	 global-step:10201	 l-p:0.09326642751693726
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13034076988697052
epoch£º510	 i:3 	 global-step:10203	 l-p:-0.04254025220870972
epoch£º510	 i:4 	 global-step:10204	 l-p:0.1266055405139923
epoch£º510	 i:5 	 global-step:10205	 l-p:0.13489219546318054
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14471489191055298
epoch£º510	 i:7 	 global-step:10207	 l-p:0.1282312124967575
epoch£º510	 i:8 	 global-step:10208	 l-p:0.15509310364723206
epoch£º510	 i:9 	 global-step:10209	 l-p:0.1207788810133934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9599, 4.8878, 4.9143],
        [4.9599, 5.0154, 4.8443],
        [4.9599, 4.9260, 4.9495],
        [4.9599, 4.9599, 4.9599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.16282668709754944 
model_pd.l_d.mean(): -20.08989715576172 
model_pd.lagr.mean(): -19.92707061767578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-20.8620], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.16282668709754944
epoch£º511	 i:1 	 global-step:10221	 l-p:0.13323020935058594
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12470674514770508
epoch£º511	 i:3 	 global-step:10223	 l-p:0.16202807426452637
epoch£º511	 i:4 	 global-step:10224	 l-p:0.1333644986152649
epoch£º511	 i:5 	 global-step:10225	 l-p:0.17201867699623108
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12710720300674438
epoch£º511	 i:7 	 global-step:10227	 l-p:0.15789523720741272
epoch£º511	 i:8 	 global-step:10228	 l-p:0.06370726227760315
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14766442775726318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9460, 4.9344, 4.9445],
        [4.9460, 4.9043, 4.9308],
        [4.9460, 4.9806, 4.8052],
        [4.9460, 4.8467, 4.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.16333089768886566 
model_pd.l_d.mean(): -20.65431785583496 
model_pd.lagr.mean(): -20.49098777770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.3516], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.16333089768886566
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10184669494628906
epoch£º512	 i:2 	 global-step:10242	 l-p:0.2057865858078003
epoch£º512	 i:3 	 global-step:10243	 l-p:0.14097608625888824
epoch£º512	 i:4 	 global-step:10244	 l-p:0.11403443664312363
epoch£º512	 i:5 	 global-step:10245	 l-p:0.11379062384366989
epoch£º512	 i:6 	 global-step:10246	 l-p:0.13066647946834564
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14574497938156128
epoch£º512	 i:8 	 global-step:10248	 l-p:0.16210238635540009
epoch£º512	 i:9 	 global-step:10249	 l-p:0.23592330515384674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9210, 4.8043, 4.7697],
        [4.9210, 5.2348, 5.1856],
        [4.9210, 4.8099, 4.7957],
        [4.9210, 4.9188, 4.9209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.1523894965648651 
model_pd.l_d.mean(): -20.486474990844727 
model_pd.lagr.mean(): -20.33408546447754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5074], device='cuda:0')), ('power', tensor([-21.2287], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.1523894965648651
epoch£º513	 i:1 	 global-step:10261	 l-p:0.16539226472377777
epoch£º513	 i:2 	 global-step:10262	 l-p:0.1381239891052246
epoch£º513	 i:3 	 global-step:10263	 l-p:0.14001910388469696
epoch£º513	 i:4 	 global-step:10264	 l-p:0.17828819155693054
epoch£º513	 i:5 	 global-step:10265	 l-p:0.1270282119512558
epoch£º513	 i:6 	 global-step:10266	 l-p:0.12455042451620102
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12723815441131592
epoch£º513	 i:8 	 global-step:10268	 l-p:0.04902129992842674
epoch£º513	 i:9 	 global-step:10269	 l-p:0.08943237364292145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0575, 5.6091, 5.7213],
        [5.0575, 5.3887, 5.3439],
        [5.0575, 4.9591, 4.9562],
        [5.0575, 5.0573, 5.0575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.14300285279750824 
model_pd.l_d.mean(): -20.446481704711914 
model_pd.lagr.mean(): -20.303478240966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-21.1341], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.14300285279750824
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14510144293308258
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15906772017478943
epoch£º514	 i:3 	 global-step:10283	 l-p:0.022330250591039658
epoch£º514	 i:4 	 global-step:10284	 l-p:0.06593111157417297
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13008756935596466
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11480392515659332
epoch£º514	 i:7 	 global-step:10287	 l-p:0.13951395452022552
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12062302231788635
epoch£º514	 i:9 	 global-step:10289	 l-p:0.13347259163856506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0390, 5.3322, 5.2643],
        [5.0390, 5.2827, 5.1860],
        [5.0390, 4.9876, 5.0157],
        [5.0390, 5.0120, 4.8417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.006231163628399372 
model_pd.l_d.mean(): -19.059175491333008 
model_pd.lagr.mean(): -19.05294418334961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5591], device='cuda:0')), ('power', tensor([-19.8386], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.006231163628399372
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10337118059396744
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11582014709711075
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15674243867397308
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13292750716209412
epoch£º515	 i:5 	 global-step:10305	 l-p:0.1336509734392166
epoch£º515	 i:6 	 global-step:10306	 l-p:0.09408216923475266
epoch£º515	 i:7 	 global-step:10307	 l-p:0.23716342449188232
epoch£º515	 i:8 	 global-step:10308	 l-p:0.23712147772312164
epoch£º515	 i:9 	 global-step:10309	 l-p:0.1476048082113266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9095, 4.9000, 4.9084],
        [4.9095, 4.7980, 4.7884],
        [4.9095, 5.1630, 5.0763],
        [4.9095, 4.8920, 4.9064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.2963423430919647 
model_pd.l_d.mean(): -20.007144927978516 
model_pd.lagr.mean(): -19.71080207824707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5616], device='cuda:0')), ('power', tensor([-20.7996], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.2963423430919647
epoch£º516	 i:1 	 global-step:10321	 l-p:0.19788861274719238
epoch£º516	 i:2 	 global-step:10322	 l-p:0.14006227254867554
epoch£º516	 i:3 	 global-step:10323	 l-p:0.1303117275238037
epoch£º516	 i:4 	 global-step:10324	 l-p:0.09770480543375015
epoch£º516	 i:5 	 global-step:10325	 l-p:0.18023860454559326
epoch£º516	 i:6 	 global-step:10326	 l-p:0.11035538464784622
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15607962012290955
epoch£º516	 i:8 	 global-step:10328	 l-p:0.13279540836811066
epoch£º516	 i:9 	 global-step:10329	 l-p:0.1419668197631836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9433, 4.9359, 4.9426],
        [4.9433, 4.8361, 4.7165],
        [4.9433, 4.8288, 4.7288],
        [4.9433, 4.9432, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.10874016582965851 
model_pd.l_d.mean(): -20.055707931518555 
model_pd.lagr.mean(): -19.94696807861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5053], device='cuda:0')), ('power', tensor([-20.7910], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.10874016582965851
epoch£º517	 i:1 	 global-step:10341	 l-p:0.131724014878273
epoch£º517	 i:2 	 global-step:10342	 l-p:0.1841699182987213
epoch£º517	 i:3 	 global-step:10343	 l-p:0.15422065556049347
epoch£º517	 i:4 	 global-step:10344	 l-p:0.12826810777187347
epoch£º517	 i:5 	 global-step:10345	 l-p:0.13056455552577972
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10383545607328415
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.1232975572347641
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08395100384950638
epoch£º517	 i:9 	 global-step:10349	 l-p:0.1395445168018341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1477, 5.3142, 5.1774],
        [5.1477, 5.1016, 4.9450],
        [5.1477, 5.0720, 5.0929],
        [5.1477, 5.0430, 4.9991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11567821353673935 
model_pd.l_d.mean(): -20.300071716308594 
model_pd.lagr.mean(): -20.18439292907715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-20.9943], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11567821353673935
epoch£º518	 i:1 	 global-step:10361	 l-p:0.045329608023166656
epoch£º518	 i:2 	 global-step:10362	 l-p:0.03265374153852463
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12201298773288727
epoch£º518	 i:4 	 global-step:10364	 l-p:0.19215373694896698
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13568928837776184
epoch£º518	 i:6 	 global-step:10366	 l-p:0.0914725661277771
epoch£º518	 i:7 	 global-step:10367	 l-p:0.12505625188350677
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12040223926305771
epoch£º518	 i:9 	 global-step:10369	 l-p:0.2313026785850525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2341, 5.2314, 5.2339],
        [5.2341, 5.9167, 6.1203],
        [5.2341, 5.2312, 5.2339],
        [5.2341, 5.1718, 5.1967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.1264955699443817 
model_pd.l_d.mean(): -20.41261100769043 
model_pd.lagr.mean(): -20.286115646362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4059], device='cuda:0')), ('power', tensor([-21.0503], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.1264955699443817
epoch£º519	 i:1 	 global-step:10381	 l-p:0.17142155766487122
epoch£º519	 i:2 	 global-step:10382	 l-p:0.1105625256896019
epoch£º519	 i:3 	 global-step:10383	 l-p:0.14906112849712372
epoch£º519	 i:4 	 global-step:10384	 l-p:-0.0631885901093483
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.18064112961292267
epoch£º519	 i:6 	 global-step:10386	 l-p:0.1199118047952652
epoch£º519	 i:7 	 global-step:10387	 l-p:0.07087259739637375
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11357712745666504
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11646141111850739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1589, 5.1589],
        [5.1589, 5.6933, 5.7844],
        [5.1589, 5.1589, 5.1589],
        [5.1589, 5.1519, 5.1583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13412532210350037 
model_pd.l_d.mean(): -19.566051483154297 
model_pd.lagr.mean(): -19.431926727294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4455], device='cuda:0')), ('power', tensor([-20.2350], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13412532210350037
epoch£º520	 i:1 	 global-step:10401	 l-p:0.07031229138374329
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12128528207540512
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.15806467831134796
epoch£º520	 i:4 	 global-step:10404	 l-p:0.15776708722114563
epoch£º520	 i:5 	 global-step:10405	 l-p:0.19557338953018188
epoch£º520	 i:6 	 global-step:10406	 l-p:0.132811039686203
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12934254109859467
epoch£º520	 i:8 	 global-step:10408	 l-p:0.1398451179265976
epoch£º520	 i:9 	 global-step:10409	 l-p:0.16657817363739014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0748, 4.9643, 4.8935],
        [5.0748, 5.0479, 5.0680],
        [5.0748, 5.6645, 5.8039],
        [5.0748, 4.9652, 4.9327]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.13497506082057953 
model_pd.l_d.mean(): -20.729848861694336 
model_pd.lagr.mean(): -20.594873428344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-21.3836], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.13497506082057953
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12513917684555054
epoch£º521	 i:2 	 global-step:10422	 l-p:-0.06593456119298935
epoch£º521	 i:3 	 global-step:10423	 l-p:0.15396298468112946
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11446142941713333
epoch£º521	 i:5 	 global-step:10425	 l-p:0.15293876826763153
epoch£º521	 i:6 	 global-step:10426	 l-p:0.14671778678894043
epoch£º521	 i:7 	 global-step:10427	 l-p:0.1462714523077011
epoch£º521	 i:8 	 global-step:10428	 l-p:0.1172531321644783
epoch£º521	 i:9 	 global-step:10429	 l-p:0.258587509393692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9123, 4.9086, 4.9121],
        [4.9123, 4.9123, 4.9123],
        [4.9123, 4.9038, 4.9114],
        [4.9123, 4.9027, 4.9112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.11639552563428879 
model_pd.l_d.mean(): -20.754934310913086 
model_pd.lagr.mean(): -20.638538360595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4806], device='cuda:0')), ('power', tensor([-21.4727], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.11639552563428879
epoch£º522	 i:1 	 global-step:10441	 l-p:0.20747147500514984
epoch£º522	 i:2 	 global-step:10442	 l-p:0.25540709495544434
epoch£º522	 i:3 	 global-step:10443	 l-p:0.15021449327468872
epoch£º522	 i:4 	 global-step:10444	 l-p:0.18257610499858856
epoch£º522	 i:5 	 global-step:10445	 l-p:0.16856957972049713
epoch£º522	 i:6 	 global-step:10446	 l-p:0.07965949922800064
epoch£º522	 i:7 	 global-step:10447	 l-p:0.11027725040912628
epoch£º522	 i:8 	 global-step:10448	 l-p:0.13318167626857758
epoch£º522	 i:9 	 global-step:10449	 l-p:0.11136627197265625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0359, 5.0182, 5.0327],
        [5.0359, 5.0359, 5.0359],
        [5.0359, 5.6095, 5.7383],
        [5.0359, 5.0357, 5.0359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.1055126041173935 
model_pd.l_d.mean(): -18.285654067993164 
model_pd.lagr.mean(): -18.18014144897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6164], device='cuda:0')), ('power', tensor([-19.1152], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.1055126041173935
epoch£º523	 i:1 	 global-step:10461	 l-p:0.0932828038930893
epoch£º523	 i:2 	 global-step:10462	 l-p:0.8814058303833008
epoch£º523	 i:3 	 global-step:10463	 l-p:0.14968757331371307
epoch£º523	 i:4 	 global-step:10464	 l-p:0.1353432685136795
epoch£º523	 i:5 	 global-step:10465	 l-p:-0.0002855777565855533
epoch£º523	 i:6 	 global-step:10466	 l-p:0.1623297780752182
epoch£º523	 i:7 	 global-step:10467	 l-p:0.14912298321723938
epoch£º523	 i:8 	 global-step:10468	 l-p:0.1425163745880127
epoch£º523	 i:9 	 global-step:10469	 l-p:0.14073286950588226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9977, 4.9867, 4.9963],
        [4.9977, 5.2673, 5.1847],
        [4.9977, 4.9974, 4.9976],
        [4.9977, 4.9873, 4.9964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.1961246281862259 
model_pd.l_d.mean(): -20.32461166381836 
model_pd.lagr.mean(): -20.12848663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-21.0432], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.1961246281862259
epoch£º524	 i:1 	 global-step:10481	 l-p:0.12939532101154327
epoch£º524	 i:2 	 global-step:10482	 l-p:0.03885635733604431
epoch£º524	 i:3 	 global-step:10483	 l-p:0.11679170280694962
epoch£º524	 i:4 	 global-step:10484	 l-p:0.1353760063648224
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13329696655273438
epoch£º524	 i:6 	 global-step:10486	 l-p:0.13443779945373535
epoch£º524	 i:7 	 global-step:10487	 l-p:0.12518855929374695
epoch£º524	 i:8 	 global-step:10488	 l-p:0.15222501754760742
epoch£º524	 i:9 	 global-step:10489	 l-p:0.11464657634496689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9816, 4.9793, 4.9815],
        [4.9816, 4.9816, 4.9816],
        [4.9816, 4.9784, 4.9814],
        [4.9816, 4.8607, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16551242768764496 
model_pd.l_d.mean(): -19.725370407104492 
model_pd.lagr.mean(): -19.559858322143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5195], device='cuda:0')), ('power', tensor([-20.4717], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16551242768764496
epoch£º525	 i:1 	 global-step:10501	 l-p:0.08061276376247406
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12359589338302612
epoch£º525	 i:3 	 global-step:10503	 l-p:0.12366117537021637
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12348794937133789
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12444104999303818
epoch£º525	 i:6 	 global-step:10506	 l-p:0.18393218517303467
epoch£º525	 i:7 	 global-step:10507	 l-p:0.1689292937517166
epoch£º525	 i:8 	 global-step:10508	 l-p:0.1324942409992218
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10205753147602081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0059, 4.9951, 5.0046],
        [5.0059, 4.8954, 4.8859],
        [5.0059, 5.0770, 4.9040],
        [5.0059, 5.1492, 5.0007]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.14820195734500885 
model_pd.l_d.mean(): -20.656940460205078 
model_pd.lagr.mean(): -20.508737564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-21.3614], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.14820195734500885
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14387105405330658
epoch£º526	 i:2 	 global-step:10522	 l-p:0.057753514498472214
epoch£º526	 i:3 	 global-step:10523	 l-p:0.1263558715581894
epoch£º526	 i:4 	 global-step:10524	 l-p:0.1257157176733017
epoch£º526	 i:5 	 global-step:10525	 l-p:0.1378898322582245
epoch£º526	 i:6 	 global-step:10526	 l-p:0.19976800680160522
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13940653204917908
epoch£º526	 i:8 	 global-step:10528	 l-p:-0.3692484200000763
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13558486104011536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1461, 5.1461, 5.1461],
        [5.1461, 5.0487, 4.9359],
        [5.1461, 5.1461, 5.1461],
        [5.1461, 5.1455, 5.1461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12865802645683289 
model_pd.l_d.mean(): -20.508460998535156 
model_pd.lagr.mean(): -20.379802703857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4208], device='cuda:0')), ('power', tensor([-21.1625], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12865802645683289
epoch£º527	 i:1 	 global-step:10541	 l-p:0.1354893296957016
epoch£º527	 i:2 	 global-step:10542	 l-p:0.16163897514343262
epoch£º527	 i:3 	 global-step:10543	 l-p:0.1415330469608307
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.5334088802337646
epoch£º527	 i:5 	 global-step:10545	 l-p:0.1313076913356781
epoch£º527	 i:6 	 global-step:10546	 l-p:0.1332068294286728
epoch£º527	 i:7 	 global-step:10547	 l-p:0.2066659927368164
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12370914220809937
epoch£º527	 i:9 	 global-step:10549	 l-p:0.1352059692144394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0748, 5.0315, 5.0586],
        [5.0748, 5.0684, 5.0743],
        [5.0748, 5.3951, 5.3392],
        [5.0748, 5.3953, 5.3395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): -0.14012502133846283 
model_pd.l_d.mean(): -19.775135040283203 
model_pd.lagr.mean(): -19.915260314941406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5159], device='cuda:0')), ('power', tensor([-20.5182], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:-0.14012502133846283
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12102734297513962
epoch£º528	 i:2 	 global-step:10562	 l-p:0.12885333597660065
epoch£º528	 i:3 	 global-step:10563	 l-p:0.13820156455039978
epoch£º528	 i:4 	 global-step:10564	 l-p:0.1221390888094902
epoch£º528	 i:5 	 global-step:10565	 l-p:0.0714913085103035
epoch£º528	 i:6 	 global-step:10566	 l-p:0.1505293995141983
epoch£º528	 i:7 	 global-step:10567	 l-p:0.13882045447826385
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13248905539512634
epoch£º528	 i:9 	 global-step:10569	 l-p:0.1562691628932953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9488, 4.8345, 4.7085],
        [4.9488, 4.9027, 4.7203],
        [4.9488, 4.8724, 4.9017],
        [4.9488, 5.1573, 5.0402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.10667333006858826 
model_pd.l_d.mean(): -20.693262100219727 
model_pd.lagr.mean(): -20.58658790588379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-21.4103], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.10667333006858826
epoch£º529	 i:1 	 global-step:10581	 l-p:0.101796455681324
epoch£º529	 i:2 	 global-step:10582	 l-p:0.146120086312294
epoch£º529	 i:3 	 global-step:10583	 l-p:0.14563608169555664
epoch£º529	 i:4 	 global-step:10584	 l-p:0.23021510243415833
epoch£º529	 i:5 	 global-step:10585	 l-p:0.09604066610336304
epoch£º529	 i:6 	 global-step:10586	 l-p:0.17566360533237457
epoch£º529	 i:7 	 global-step:10587	 l-p:0.17080295085906982
epoch£º529	 i:8 	 global-step:10588	 l-p:0.1859581023454666
epoch£º529	 i:9 	 global-step:10589	 l-p:0.15293176472187042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9910, 4.9909, 4.9910],
        [4.9910, 4.8665, 4.7963],
        [4.9910, 4.9910, 4.9910],
        [4.9910, 5.1014, 4.9385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13656365871429443 
model_pd.l_d.mean(): -20.339080810546875 
model_pd.lagr.mean(): -20.202516555786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-21.0705], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13656365871429443
epoch£º530	 i:1 	 global-step:10601	 l-p:0.1309315264225006
epoch£º530	 i:2 	 global-step:10602	 l-p:0.12388038635253906
epoch£º530	 i:3 	 global-step:10603	 l-p:0.09085261821746826
epoch£º530	 i:4 	 global-step:10604	 l-p:0.42397814989089966
epoch£º530	 i:5 	 global-step:10605	 l-p:0.1301826387643814
epoch£º530	 i:6 	 global-step:10606	 l-p:0.15674170851707458
epoch£º530	 i:7 	 global-step:10607	 l-p:0.09738407284021378
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13008356094360352
epoch£º530	 i:9 	 global-step:10609	 l-p:-0.09316104650497437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1370, 5.1321, 5.1366],
        [5.1370, 5.1370, 5.1370],
        [5.1370, 5.1370, 5.1370],
        [5.1370, 5.1370, 5.1370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.1386648267507553 
model_pd.l_d.mean(): -19.412471771240234 
model_pd.lagr.mean(): -19.273807525634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4653], device='cuda:0')), ('power', tensor([-20.1000], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.1386648267507553
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12367042154073715
epoch£º531	 i:2 	 global-step:10622	 l-p:0.10492590069770813
epoch£º531	 i:3 	 global-step:10623	 l-p:0.1318599283695221
epoch£º531	 i:4 	 global-step:10624	 l-p:0.060122184455394745
epoch£º531	 i:5 	 global-step:10625	 l-p:0.10486672818660736
epoch£º531	 i:6 	 global-step:10626	 l-p:0.23526692390441895
epoch£º531	 i:7 	 global-step:10627	 l-p:0.132082000374794
epoch£º531	 i:8 	 global-step:10628	 l-p:0.13229738175868988
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13139978051185608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1958, 5.2220, 5.0444],
        [5.1958, 5.1518, 5.1787],
        [5.1958, 5.2920, 5.1259],
        [5.1958, 5.1932, 5.1957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): -0.17103958129882812 
model_pd.l_d.mean(): -19.411073684692383 
model_pd.lagr.mean(): -19.58211326599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-20.1008], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:-0.17103958129882812
epoch£º532	 i:1 	 global-step:10641	 l-p:0.1274646669626236
epoch£º532	 i:2 	 global-step:10642	 l-p:0.1951005905866623
epoch£º532	 i:3 	 global-step:10643	 l-p:0.12399712949991226
epoch£º532	 i:4 	 global-step:10644	 l-p:-7.8773627281188965
epoch£º532	 i:5 	 global-step:10645	 l-p:0.14109304547309875
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11587884277105331
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11384589970111847
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10205335170030594
epoch£º532	 i:9 	 global-step:10649	 l-p:0.19208744168281555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2747, 5.6082, 5.5515],
        [5.2747, 5.2716, 5.2745],
        [5.2747, 5.2516, 5.2694],
        [5.2747, 5.1697, 5.1246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12252990156412125 
model_pd.l_d.mean(): -19.708932876586914 
model_pd.lagr.mean(): -19.586402893066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4224], device='cuda:0')), ('power', tensor([-20.3558], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12252990156412125
epoch£º533	 i:1 	 global-step:10661	 l-p:0.11455083638429642
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11547181755304337
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14067305624485016
epoch£º533	 i:4 	 global-step:10664	 l-p:0.70613032579422
epoch£º533	 i:5 	 global-step:10665	 l-p:0.13496936857700348
epoch£º533	 i:6 	 global-step:10666	 l-p:0.0435740165412426
epoch£º533	 i:7 	 global-step:10667	 l-p:0.07737627625465393
epoch£º533	 i:8 	 global-step:10668	 l-p:0.1365627646446228
epoch£º533	 i:9 	 global-step:10669	 l-p:0.30937254428863525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.0447, 5.0100],
        [5.1572, 5.0901, 4.9328],
        [5.1572, 5.1495, 5.1564],
        [5.1572, 5.0450, 4.9653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.07262767851352692 
model_pd.l_d.mean(): -19.954010009765625 
model_pd.lagr.mean(): -19.88138198852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-20.6788], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.07262767851352692
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13584186136722565
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12515205144882202
epoch£º534	 i:3 	 global-step:10683	 l-p:0.127425879240036
epoch£º534	 i:4 	 global-step:10684	 l-p:0.1114128828048706
epoch£º534	 i:5 	 global-step:10685	 l-p:0.25453466176986694
epoch£º534	 i:6 	 global-step:10686	 l-p:0.07137542217969894
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11569925397634506
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14457297325134277
epoch£º534	 i:9 	 global-step:10689	 l-p:0.1522517055273056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0593, 5.0591, 5.0593],
        [5.0593, 5.0391, 5.0554],
        [5.0593, 5.0593, 5.0593],
        [5.0593, 5.0158, 5.0435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): 0.5973442792892456 
model_pd.l_d.mean(): -20.324377059936523 
model_pd.lagr.mean(): -19.727033615112305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587], device='cuda:0')), ('power', tensor([-21.0151], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:0.5973442792892456
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14238354563713074
epoch£º535	 i:2 	 global-step:10702	 l-p:0.15229281783103943
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16094960272312164
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12421374768018723
epoch£º535	 i:5 	 global-step:10705	 l-p:0.15228839218616486
epoch£º535	 i:6 	 global-step:10706	 l-p:0.13036951422691345
epoch£º535	 i:7 	 global-step:10707	 l-p:0.11408747732639313
epoch£º535	 i:8 	 global-step:10708	 l-p:0.1083260104060173
epoch£º535	 i:9 	 global-step:10709	 l-p:0.15003982186317444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9559, 4.9450, 4.9546],
        [4.9559, 4.9505, 4.9555],
        [4.9559, 4.9459, 4.9547],
        [4.9559, 5.0502, 4.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.16618973016738892 
model_pd.l_d.mean(): -19.574928283691406 
model_pd.lagr.mean(): -19.40873908996582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5423], device='cuda:0')), ('power', tensor([-20.3429], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.16618973016738892
epoch£º536	 i:1 	 global-step:10721	 l-p:0.08913583308458328
epoch£º536	 i:2 	 global-step:10722	 l-p:0.0926205962896347
epoch£º536	 i:3 	 global-step:10723	 l-p:0.11256422847509384
epoch£º536	 i:4 	 global-step:10724	 l-p:0.12634986639022827
epoch£º536	 i:5 	 global-step:10725	 l-p:0.13468804955482483
epoch£º536	 i:6 	 global-step:10726	 l-p:0.1525000035762787
epoch£º536	 i:7 	 global-step:10727	 l-p:0.1656631976366043
epoch£º536	 i:8 	 global-step:10728	 l-p:0.13392798602581024
epoch£º536	 i:9 	 global-step:10729	 l-p:0.11950984597206116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0870, 5.0869, 5.0870],
        [5.0870, 4.9657, 4.9123],
        [5.0870, 4.9666, 4.9236],
        [5.0870, 5.0870, 5.0870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.1140233650803566 
model_pd.l_d.mean(): -20.841411590576172 
model_pd.lagr.mean(): -20.727388381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.4727], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.1140233650803566
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12579117715358734
epoch£º537	 i:2 	 global-step:10742	 l-p:0.21167322993278503
epoch£º537	 i:3 	 global-step:10743	 l-p:0.07976561039686203
epoch£º537	 i:4 	 global-step:10744	 l-p:0.1354660838842392
epoch£º537	 i:5 	 global-step:10745	 l-p:0.08931811898946762
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12614135444164276
epoch£º537	 i:7 	 global-step:10747	 l-p:0.10368474572896957
epoch£º537	 i:8 	 global-step:10748	 l-p:0.12941372394561768
epoch£º537	 i:9 	 global-step:10749	 l-p:0.1703050136566162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2081, 5.1768, 5.1992],
        [5.2081, 5.2076, 5.2081],
        [5.2081, 5.2081, 5.2081],
        [5.2081, 5.1584, 5.1867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.12960048019886017 
model_pd.l_d.mean(): -20.619842529296875 
model_pd.lagr.mean(): -20.49024200439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3974], device='cuda:0')), ('power', tensor([-21.2512], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.12960048019886017
epoch£º538	 i:1 	 global-step:10761	 l-p:0.0032637715339660645
epoch£º538	 i:2 	 global-step:10762	 l-p:0.13373279571533203
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11165887862443924
epoch£º538	 i:4 	 global-step:10764	 l-p:-0.01166904903948307
epoch£º538	 i:5 	 global-step:10765	 l-p:0.16418857872486115
epoch£º538	 i:6 	 global-step:10766	 l-p:0.13048005104064941
epoch£º538	 i:7 	 global-step:10767	 l-p:0.2236911952495575
epoch£º538	 i:8 	 global-step:10768	 l-p:0.13306547701358795
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.03795430064201355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2217, 5.1423, 5.1650],
        [5.2217, 5.2216, 5.2217],
        [5.2217, 5.2092, 5.2199],
        [5.2217, 5.2216, 5.2217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11372581869363785 
model_pd.l_d.mean(): -20.481863021850586 
model_pd.lagr.mean(): -20.36813735961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4174], device='cuda:0')), ('power', tensor([-21.1321], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11372581869363785
epoch£º539	 i:1 	 global-step:10781	 l-p:0.6082056164741516
epoch£º539	 i:2 	 global-step:10782	 l-p:0.127504363656044
epoch£º539	 i:3 	 global-step:10783	 l-p:0.14924561977386475
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12325317412614822
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12390363216400146
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11598849296569824
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11789537221193314
epoch£º539	 i:8 	 global-step:10788	 l-p:0.18805032968521118
epoch£º539	 i:9 	 global-step:10789	 l-p:0.04374260827898979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1634, 5.0735, 4.9327],
        [5.1634, 5.1634, 5.1634],
        [5.1634, 5.0618, 5.0652],
        [5.1634, 5.3026, 5.1477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.1631917953491211 
model_pd.l_d.mean(): -19.292097091674805 
model_pd.lagr.mean(): -19.12890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5455], device='cuda:0')), ('power', tensor([-20.0602], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.1631917953491211
epoch£º540	 i:1 	 global-step:10801	 l-p:0.19890029728412628
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13539479672908783
epoch£º540	 i:3 	 global-step:10803	 l-p:0.12522195279598236
epoch£º540	 i:4 	 global-step:10804	 l-p:0.06857237964868546
epoch£º540	 i:5 	 global-step:10805	 l-p:0.12138902395963669
epoch£º540	 i:6 	 global-step:10806	 l-p:0.12631724774837494
epoch£º540	 i:7 	 global-step:10807	 l-p:0.1340077519416809
epoch£º540	 i:8 	 global-step:10808	 l-p:0.13677120208740234
epoch£º540	 i:9 	 global-step:10809	 l-p:0.5330525040626526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0938, 5.0579, 5.0828],
        [5.0938, 5.0651, 5.0865],
        [5.0938, 5.5863, 5.6449],
        [5.0938, 5.0938, 5.0938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.1156526505947113 
model_pd.l_d.mean(): -19.221912384033203 
model_pd.lagr.mean(): -19.106260299682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-19.9346], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.1156526505947113
epoch£º541	 i:1 	 global-step:10821	 l-p:0.13911618292331696
epoch£º541	 i:2 	 global-step:10822	 l-p:-0.18838977813720703
epoch£º541	 i:3 	 global-step:10823	 l-p:0.0955108031630516
epoch£º541	 i:4 	 global-step:10824	 l-p:0.15155985951423645
epoch£º541	 i:5 	 global-step:10825	 l-p:0.13883067667484283
epoch£º541	 i:6 	 global-step:10826	 l-p:0.12149672955274582
epoch£º541	 i:7 	 global-step:10827	 l-p:0.10904296487569809
epoch£º541	 i:8 	 global-step:10828	 l-p:0.12465087324380875
epoch£º541	 i:9 	 global-step:10829	 l-p:0.15953056514263153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9293, 4.9293, 4.9293],
        [4.9293, 4.7992, 4.6797],
        [4.9293, 5.5437, 5.7063],
        [4.9293, 4.8247, 4.6618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.16132020950317383 
model_pd.l_d.mean(): -18.580833435058594 
model_pd.lagr.mean(): -18.419513702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5925], device='cuda:0')), ('power', tensor([-19.3892], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.16132020950317383
epoch£º542	 i:1 	 global-step:10841	 l-p:0.1823805570602417
epoch£º542	 i:2 	 global-step:10842	 l-p:0.13534954190254211
epoch£º542	 i:3 	 global-step:10843	 l-p:0.3126669228076935
epoch£º542	 i:4 	 global-step:10844	 l-p:0.09848976880311966
epoch£º542	 i:5 	 global-step:10845	 l-p:0.14097058773040771
epoch£º542	 i:6 	 global-step:10846	 l-p:0.13712574541568756
epoch£º542	 i:7 	 global-step:10847	 l-p:0.17290247976779938
epoch£º542	 i:8 	 global-step:10848	 l-p:0.15274041891098022
epoch£º542	 i:9 	 global-step:10849	 l-p:0.1319464147090912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9513, 4.8440, 4.6863],
        [4.9513, 5.5128, 5.6320],
        [4.9513, 4.8741, 4.9059],
        [4.9513, 4.9500, 4.9513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.1359037309885025 
model_pd.l_d.mean(): -20.091930389404297 
model_pd.lagr.mean(): -19.956026077270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5155], device='cuda:0')), ('power', tensor([-20.8381], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.1359037309885025
epoch£º543	 i:1 	 global-step:10861	 l-p:0.08786267042160034
epoch£º543	 i:2 	 global-step:10862	 l-p:0.16562137007713318
epoch£º543	 i:3 	 global-step:10863	 l-p:0.13836278021335602
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12897837162017822
epoch£º543	 i:5 	 global-step:10865	 l-p:0.12259453535079956
epoch£º543	 i:6 	 global-step:10866	 l-p:0.1614023894071579
epoch£º543	 i:7 	 global-step:10867	 l-p:0.2230246365070343
epoch£º543	 i:8 	 global-step:10868	 l-p:0.267848938703537
epoch£º543	 i:9 	 global-step:10869	 l-p:0.2049213945865631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9609, 4.9598, 4.9609],
        [4.9609, 4.8760, 4.6992],
        [4.9609, 4.9609, 4.9609],
        [4.9609, 5.3061, 5.2677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.11261473596096039 
model_pd.l_d.mean(): -20.597511291503906 
model_pd.lagr.mean(): -20.484895706176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4650], device='cuda:0')), ('power', tensor([-21.2976], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.11261473596096039
epoch£º544	 i:1 	 global-step:10881	 l-p:0.1371532827615738
epoch£º544	 i:2 	 global-step:10882	 l-p:0.10644563287496567
epoch£º544	 i:3 	 global-step:10883	 l-p:0.1505756378173828
epoch£º544	 i:4 	 global-step:10884	 l-p:0.139907568693161
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1410125494003296
epoch£º544	 i:6 	 global-step:10886	 l-p:0.09677369892597198
epoch£º544	 i:7 	 global-step:10887	 l-p:0.12576302886009216
epoch£º544	 i:8 	 global-step:10888	 l-p:0.14672717452049255
epoch£º544	 i:9 	 global-step:10889	 l-p:0.13131386041641235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0551, 5.0435, 5.0537],
        [5.0551, 5.0101, 5.0388],
        [5.0551, 4.9376, 4.9279],
        [5.0551, 4.9397, 4.9345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.10023953020572662 
model_pd.l_d.mean(): -20.757354736328125 
model_pd.lagr.mean(): -20.657115936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.4302], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.10023953020572662
epoch£º545	 i:1 	 global-step:10901	 l-p:0.14235809445381165
epoch£º545	 i:2 	 global-step:10902	 l-p:0.132009819149971
epoch£º545	 i:3 	 global-step:10903	 l-p:0.11501649767160416
epoch£º545	 i:4 	 global-step:10904	 l-p:0.0711025819182396
epoch£º545	 i:5 	 global-step:10905	 l-p:0.5531760454177856
epoch£º545	 i:6 	 global-step:10906	 l-p:0.14944536983966827
epoch£º545	 i:7 	 global-step:10907	 l-p:0.09689794480800629
epoch£º545	 i:8 	 global-step:10908	 l-p:0.15076148509979248
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11355113983154297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1925, 5.1669, 5.1865],
        [5.1925, 5.1680, 5.1869],
        [5.1925, 5.1923, 5.1925],
        [5.1925, 5.0842, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.12612098455429077 
model_pd.l_d.mean(): -19.800642013549805 
model_pd.lagr.mean(): -19.67452049255371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3989], device='cuda:0')), ('power', tensor([-20.4245], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.12612098455429077
epoch£º546	 i:1 	 global-step:10921	 l-p:0.13300225138664246
epoch£º546	 i:2 	 global-step:10922	 l-p:0.12742972373962402
epoch£º546	 i:3 	 global-step:10923	 l-p:0.07218162715435028
epoch£º546	 i:4 	 global-step:10924	 l-p:0.03529832139611244
epoch£º546	 i:5 	 global-step:10925	 l-p:0.17927499115467072
epoch£º546	 i:6 	 global-step:10926	 l-p:0.13754305243492126
epoch£º546	 i:7 	 global-step:10927	 l-p:0.14145106077194214
epoch£º546	 i:8 	 global-step:10928	 l-p:0.11947128921747208
epoch£º546	 i:9 	 global-step:10929	 l-p:0.13099555671215057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 5.0811, 5.0993],
        [5.1730, 5.7069, 5.7904],
        [5.1730, 5.1721, 5.1730],
        [5.1730, 5.1730, 5.1730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.12029846012592316 
model_pd.l_d.mean(): -20.49131965637207 
model_pd.lagr.mean(): -20.371021270751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4258], device='cuda:0')), ('power', tensor([-21.1503], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.12029846012592316
epoch£º547	 i:1 	 global-step:10941	 l-p:0.1511056274175644
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11406610906124115
epoch£º547	 i:3 	 global-step:10943	 l-p:0.0908220186829567
epoch£º547	 i:4 	 global-step:10944	 l-p:0.16773472726345062
epoch£º547	 i:5 	 global-step:10945	 l-p:0.15036620199680328
epoch£º547	 i:6 	 global-step:10946	 l-p:0.13739974796772003
epoch£º547	 i:7 	 global-step:10947	 l-p:0.01909554749727249
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13448525965213776
epoch£º547	 i:9 	 global-step:10949	 l-p:0.11211468279361725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0858, 5.0856, 5.0858],
        [5.0858, 4.9992, 5.0250],
        [5.0858, 5.0550, 5.0777],
        [5.0858, 5.0409, 5.0695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.12088906764984131 
model_pd.l_d.mean(): -19.42947769165039 
model_pd.lagr.mean(): -19.3085880279541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5154], device='cuda:0')), ('power', tensor([-20.1683], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.12088906764984131
epoch£º548	 i:1 	 global-step:10961	 l-p:0.12723813951015472
epoch£º548	 i:2 	 global-step:10962	 l-p:0.15029871463775635
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12454555183649063
epoch£º548	 i:4 	 global-step:10964	 l-p:0.17486320436000824
epoch£º548	 i:5 	 global-step:10965	 l-p:0.12644653022289276
epoch£º548	 i:6 	 global-step:10966	 l-p:0.13421641290187836
epoch£º548	 i:7 	 global-step:10967	 l-p:0.09447351843118668
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12078142166137695
epoch£º548	 i:9 	 global-step:10969	 l-p:0.0964168906211853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9912, 5.3236, 5.2744],
        [4.9912, 4.9861, 4.9908],
        [4.9912, 4.9832, 4.9904],
        [4.9912, 4.8586, 4.8207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.11250820010900497 
model_pd.l_d.mean(): -20.58384132385254 
model_pd.lagr.mean(): -20.471332550048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4573], device='cuda:0')), ('power', tensor([-21.2759], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.11250820010900497
epoch£º549	 i:1 	 global-step:10981	 l-p:0.1462002545595169
epoch£º549	 i:2 	 global-step:10982	 l-p:0.1567521095275879
epoch£º549	 i:3 	 global-step:10983	 l-p:0.13532383739948273
epoch£º549	 i:4 	 global-step:10984	 l-p:0.10096891224384308
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11590764671564102
epoch£º549	 i:6 	 global-step:10986	 l-p:0.15670491755008698
epoch£º549	 i:7 	 global-step:10987	 l-p:0.14490637183189392
epoch£º549	 i:8 	 global-step:10988	 l-p:0.14049865305423737
epoch£º549	 i:9 	 global-step:10989	 l-p:0.12769410014152527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9582, 4.9245, 4.9491],
        [4.9582, 4.8260, 4.7036],
        [4.9582, 4.8186, 4.7490],
        [4.9582, 4.9033, 4.9355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.14287149906158447 
model_pd.l_d.mean(): -18.535978317260742 
model_pd.lagr.mean(): -18.39310646057129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5957], device='cuda:0')), ('power', tensor([-19.3471], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.14287149906158447
epoch£º550	 i:1 	 global-step:11001	 l-p:0.10090439766645432
epoch£º550	 i:2 	 global-step:11002	 l-p:0.225746288895607
epoch£º550	 i:3 	 global-step:11003	 l-p:0.1049456000328064
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12110531330108643
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13746556639671326
epoch£º550	 i:6 	 global-step:11006	 l-p:0.1653318554162979
epoch£º550	 i:7 	 global-step:11007	 l-p:0.13402040302753448
epoch£º550	 i:8 	 global-step:11008	 l-p:0.13702720403671265
epoch£º550	 i:9 	 global-step:11009	 l-p:0.12007544934749603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0048, 4.8958, 4.9087],
        [5.0048, 5.0967, 4.9198],
        [5.0048, 5.5696, 5.6860],
        [5.0048, 5.0048, 5.0048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.13556428253650665 
model_pd.l_d.mean(): -18.180002212524414 
model_pd.lagr.mean(): -18.044437408447266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5963], device='cuda:0')), ('power', tensor([-18.9878], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.13556428253650665
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14638490974903107
epoch£º551	 i:2 	 global-step:11022	 l-p:0.11037062108516693
epoch£º551	 i:3 	 global-step:11023	 l-p:0.1664971113204956
epoch£º551	 i:4 	 global-step:11024	 l-p:0.12592335045337677
epoch£º551	 i:5 	 global-step:11025	 l-p:0.16012586653232574
epoch£º551	 i:6 	 global-step:11026	 l-p:0.1413251906633377
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12737233936786652
epoch£º551	 i:8 	 global-step:11028	 l-p:0.11530755460262299
epoch£º551	 i:9 	 global-step:11029	 l-p:0.13636945188045502
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9769, 4.8461, 4.7199],
        [4.9769, 4.9769, 4.9769],
        [4.9769, 4.9769, 4.9769],
        [4.9769, 4.9769, 4.9769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.1106707826256752 
model_pd.l_d.mean(): -19.94987678527832 
model_pd.lagr.mean(): -19.83920669555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5244], device='cuda:0')), ('power', tensor([-20.7036], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.1106707826256752
epoch£º552	 i:1 	 global-step:11041	 l-p:0.1910555213689804
epoch£º552	 i:2 	 global-step:11042	 l-p:0.16840150952339172
epoch£º552	 i:3 	 global-step:11043	 l-p:0.1472281962633133
epoch£º552	 i:4 	 global-step:11044	 l-p:0.13683561980724335
epoch£º552	 i:5 	 global-step:11045	 l-p:0.11817814409732819
epoch£º552	 i:6 	 global-step:11046	 l-p:0.12641790509223938
epoch£º552	 i:7 	 global-step:11047	 l-p:0.12330754846334457
epoch£º552	 i:8 	 global-step:11048	 l-p:0.16797372698783875
epoch£º552	 i:9 	 global-step:11049	 l-p:0.18863902986049652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9440, 4.9440, 4.9440],
        [4.9440, 4.8099, 4.6799],
        [4.9440, 5.4476, 5.5202],
        [4.9440, 4.8990, 4.9286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.24295435845851898 
model_pd.l_d.mean(): -20.270235061645508 
model_pd.lagr.mean(): -20.027280807495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5399], device='cuda:0')), ('power', tensor([-21.0433], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.24295435845851898
epoch£º553	 i:1 	 global-step:11061	 l-p:0.11795854568481445
epoch£º553	 i:2 	 global-step:11062	 l-p:0.11281280964612961
epoch£º553	 i:3 	 global-step:11063	 l-p:0.17771241068840027
epoch£º553	 i:4 	 global-step:11064	 l-p:0.1417730450630188
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08517832309007645
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13474245369434357
epoch£º553	 i:7 	 global-step:11067	 l-p:0.13502126932144165
epoch£º553	 i:8 	 global-step:11068	 l-p:0.10785287618637085
epoch£º553	 i:9 	 global-step:11069	 l-p:0.18342483043670654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0370, 5.0158, 5.0330],
        [5.0370, 5.0361, 5.0370],
        [5.0370, 5.0108, 5.0311],
        [5.0370, 4.9813, 4.7901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.08012931793928146 
model_pd.l_d.mean(): -20.0240535736084 
model_pd.lagr.mean(): -19.943923950195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.7148], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.08012931793928146
epoch£º554	 i:1 	 global-step:11081	 l-p:0.1297319084405899
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09366240352392197
epoch£º554	 i:3 	 global-step:11083	 l-p:0.10887234658002853
epoch£º554	 i:4 	 global-step:11084	 l-p:0.1405380368232727
epoch£º554	 i:5 	 global-step:11085	 l-p:0.28516116738319397
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12231139093637466
epoch£º554	 i:7 	 global-step:11087	 l-p:0.128682941198349
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12327827513217926
epoch£º554	 i:9 	 global-step:11089	 l-p:0.13749060034751892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1374, 5.1373, 5.1374],
        [5.1374, 5.0636, 5.0945],
        [5.1374, 5.1247, 5.1357],
        [5.1374, 5.0494, 5.0751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.13991017639636993 
model_pd.l_d.mean(): -20.422990798950195 
model_pd.lagr.mean(): -20.2830810546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4339], device='cuda:0')), ('power', tensor([-21.0895], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.13991017639636993
epoch£º555	 i:1 	 global-step:11101	 l-p:0.09538618475198746
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10741091519594193
epoch£º555	 i:3 	 global-step:11103	 l-p:0.18822965025901794
epoch£º555	 i:4 	 global-step:11104	 l-p:0.23510755598545074
epoch£º555	 i:5 	 global-step:11105	 l-p:0.022053563967347145
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14182539284229279
epoch£º555	 i:7 	 global-step:11107	 l-p:0.11429967731237411
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11204896867275238
epoch£º555	 i:9 	 global-step:11109	 l-p:0.12921810150146484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2745, 5.1599, 5.0648],
        [5.2745, 5.2745, 5.2745],
        [5.2745, 5.3546, 5.1759],
        [5.2745, 5.2727, 5.2744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.16110968589782715 
model_pd.l_d.mean(): -20.559959411621094 
model_pd.lagr.mean(): -20.398849487304688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3975], device='cuda:0')), ('power', tensor([-21.1907], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.16110968589782715
epoch£º556	 i:1 	 global-step:11121	 l-p:0.2806476354598999
epoch£º556	 i:2 	 global-step:11122	 l-p:0.2201271951198578
epoch£º556	 i:3 	 global-step:11123	 l-p:0.1190263032913208
epoch£º556	 i:4 	 global-step:11124	 l-p:0.13472597301006317
epoch£º556	 i:5 	 global-step:11125	 l-p:0.14656688272953033
epoch£º556	 i:6 	 global-step:11126	 l-p:-0.1252148151397705
epoch£º556	 i:7 	 global-step:11127	 l-p:0.12552829086780548
epoch£º556	 i:8 	 global-step:11128	 l-p:0.1138286367058754
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13451549410820007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1698, 5.1597, 5.1687],
        [5.1698, 5.0839, 4.9191],
        [5.1698, 5.1125, 5.1439],
        [5.1698, 5.2377, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.12247031182050705 
model_pd.l_d.mean(): -19.957374572753906 
model_pd.lagr.mean(): -19.834903717041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4948], device='cuda:0')), ('power', tensor([-20.6810], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.12247031182050705
epoch£º557	 i:1 	 global-step:11141	 l-p:0.12921305000782013
epoch£º557	 i:2 	 global-step:11142	 l-p:0.0064940070733428
epoch£º557	 i:3 	 global-step:11143	 l-p:0.12982875108718872
epoch£º557	 i:4 	 global-step:11144	 l-p:0.1302139014005661
epoch£º557	 i:5 	 global-step:11145	 l-p:0.13970957696437836
epoch£º557	 i:6 	 global-step:11146	 l-p:-0.6265619397163391
epoch£º557	 i:7 	 global-step:11147	 l-p:0.1415853351354599
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11740818619728088
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12149249762296677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0515, 4.9135, 4.8327],
        [5.0515, 5.0595, 4.8611],
        [5.0515, 5.0224, 5.0445],
        [5.0515, 5.0446, 5.0509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08616428077220917 
model_pd.l_d.mean(): -19.917028427124023 
model_pd.lagr.mean(): -19.83086395263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-20.6293], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.08616428077220917
epoch£º558	 i:1 	 global-step:11161	 l-p:0.056905362755060196
epoch£º558	 i:2 	 global-step:11162	 l-p:0.1275094598531723
epoch£º558	 i:3 	 global-step:11163	 l-p:0.1349208652973175
epoch£º558	 i:4 	 global-step:11164	 l-p:0.11697179824113846
epoch£º558	 i:5 	 global-step:11165	 l-p:0.13894781470298767
epoch£º558	 i:6 	 global-step:11166	 l-p:0.1255750209093094
epoch£º558	 i:7 	 global-step:11167	 l-p:0.16334465146064758
epoch£º558	 i:8 	 global-step:11168	 l-p:0.1538989543914795
epoch£º558	 i:9 	 global-step:11169	 l-p:0.1546963006258011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9919, 4.9206, 4.7253],
        [4.9919, 4.9827, 4.9910],
        [4.9919, 4.9919, 4.9919],
        [4.9919, 4.9912, 4.9919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.14561550319194794 
model_pd.l_d.mean(): -19.89202880859375 
model_pd.lagr.mean(): -19.746414184570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5664], device='cuda:0')), ('power', tensor([-20.6881], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.14561550319194794
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15170516073703766
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10761669278144836
epoch£º559	 i:3 	 global-step:11183	 l-p:0.16311408579349518
epoch£º559	 i:4 	 global-step:11184	 l-p:0.1740635186433792
epoch£º559	 i:5 	 global-step:11185	 l-p:0.14117814600467682
epoch£º559	 i:6 	 global-step:11186	 l-p:0.16528892517089844
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15424998104572296
epoch£º559	 i:8 	 global-step:11188	 l-p:0.0516425222158432
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11922303587198257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0762, 5.7121, 5.8768],
        [5.0762, 4.9524, 4.8142],
        [5.0762, 5.7188, 5.8888],
        [5.0762, 5.1092, 4.9134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.11865827441215515 
model_pd.l_d.mean(): -18.702720642089844 
model_pd.lagr.mean(): -18.584062576293945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5463], device='cuda:0')), ('power', tensor([-19.4652], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.11865827441215515
epoch£º560	 i:1 	 global-step:11201	 l-p:0.2632942795753479
epoch£º560	 i:2 	 global-step:11202	 l-p:0.0805332288146019
epoch£º560	 i:3 	 global-step:11203	 l-p:2.552898406982422
epoch£º560	 i:4 	 global-step:11204	 l-p:0.16582484543323517
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11190816760063171
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13211371004581451
epoch£º560	 i:7 	 global-step:11207	 l-p:0.19257739186286926
epoch£º560	 i:8 	 global-step:11208	 l-p:0.14322318136692047
epoch£º560	 i:9 	 global-step:11209	 l-p:0.11887871474027634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3438, 5.3438, 5.3438],
        [5.3438, 5.2411, 5.1184],
        [5.3438, 5.3351, 5.3429],
        [5.3438, 5.4453, 5.2708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.158741757273674 
model_pd.l_d.mean(): -19.668495178222656 
model_pd.lagr.mean(): -19.509754180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4053], device='cuda:0')), ('power', tensor([-20.2975], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.158741757273674
epoch£º561	 i:1 	 global-step:11221	 l-p:0.13375918567180634
epoch£º561	 i:2 	 global-step:11222	 l-p:0.1356257200241089
epoch£º561	 i:3 	 global-step:11223	 l-p:0.11278248578310013
epoch£º561	 i:4 	 global-step:11224	 l-p:0.12211116403341293
epoch£º561	 i:5 	 global-step:11225	 l-p:0.11728450655937195
epoch£º561	 i:6 	 global-step:11226	 l-p:0.06794135272502899
epoch£º561	 i:7 	 global-step:11227	 l-p:0.12754607200622559
epoch£º561	 i:8 	 global-step:11228	 l-p:0.14634889364242554
epoch£º561	 i:9 	 global-step:11229	 l-p:0.13924899697303772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3406, 5.3045, 5.3294],
        [5.3406, 5.3400, 5.3406],
        [5.3406, 5.3405, 5.3406],
        [5.3406, 5.3017, 5.1219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.12595956027507782 
model_pd.l_d.mean(): -19.89164924621582 
model_pd.lagr.mean(): -19.765689849853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3914], device='cuda:0')), ('power', tensor([-20.5088], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.12595956027507782
epoch£º562	 i:1 	 global-step:11241	 l-p:0.12154193967580795
epoch£º562	 i:2 	 global-step:11242	 l-p:0.1337490677833557
epoch£º562	 i:3 	 global-step:11243	 l-p:0.2087673544883728
epoch£º562	 i:4 	 global-step:11244	 l-p:0.10299751907587051
epoch£º562	 i:5 	 global-step:11245	 l-p:0.12793947756290436
epoch£º562	 i:6 	 global-step:11246	 l-p:0.13344083726406097
epoch£º562	 i:7 	 global-step:11247	 l-p:0.12873731553554535
epoch£º562	 i:8 	 global-step:11248	 l-p:0.3871256113052368
epoch£º562	 i:9 	 global-step:11249	 l-p:0.04564649984240532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0911, 5.0839, 5.0904],
        [5.0911, 4.9932, 5.0179],
        [5.0911, 5.0214, 5.0551],
        [5.0911, 5.0584, 5.0825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): -0.024161415174603462 
model_pd.l_d.mean(): -20.5721492767334 
model_pd.lagr.mean(): -20.596311569213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4502], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:-0.024161415174603462
epoch£º563	 i:1 	 global-step:11261	 l-p:0.13933050632476807
epoch£º563	 i:2 	 global-step:11262	 l-p:0.09929341822862625
epoch£º563	 i:3 	 global-step:11263	 l-p:0.15534278750419617
epoch£º563	 i:4 	 global-step:11264	 l-p:-0.016939936205744743
epoch£º563	 i:5 	 global-step:11265	 l-p:0.1432969570159912
epoch£º563	 i:6 	 global-step:11266	 l-p:0.13848164677619934
epoch£º563	 i:7 	 global-step:11267	 l-p:0.14786723256111145
epoch£º563	 i:8 	 global-step:11268	 l-p:0.1131075993180275
epoch£º563	 i:9 	 global-step:11269	 l-p:0.12885217368602753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0053, 4.8953, 4.7203],
        [5.0053, 5.0203, 4.8173],
        [5.0053, 4.8587, 4.7779],
        [5.0053, 4.8926, 4.9098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.08103258162736893 
model_pd.l_d.mean(): -19.697065353393555 
model_pd.lagr.mean(): -19.61603355407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-20.4155], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.08103258162736893
epoch£º564	 i:1 	 global-step:11281	 l-p:0.24131819605827332
epoch£º564	 i:2 	 global-step:11282	 l-p:0.1361372023820877
epoch£º564	 i:3 	 global-step:11283	 l-p:0.1458885222673416
epoch£º564	 i:4 	 global-step:11284	 l-p:0.21426790952682495
epoch£º564	 i:5 	 global-step:11285	 l-p:0.11246225982904434
epoch£º564	 i:6 	 global-step:11286	 l-p:0.13394340872764587
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12988904118537903
epoch£º564	 i:8 	 global-step:11288	 l-p:0.13740044832229614
epoch£º564	 i:9 	 global-step:11289	 l-p:0.1269998848438263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8597, 4.8546, 4.8594],
        [4.8597, 4.7537, 4.7828],
        [4.8597, 5.3449, 5.4043],
        [4.8597, 5.3500, 5.4132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.08136942982673645 
model_pd.l_d.mean(): -20.17652130126953 
model_pd.lagr.mean(): -20.095151901245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5574], device='cuda:0')), ('power', tensor([-20.9664], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.08136942982673645
epoch£º565	 i:1 	 global-step:11301	 l-p:1.387371301651001
epoch£º565	 i:2 	 global-step:11302	 l-p:0.06855683773756027
epoch£º565	 i:3 	 global-step:11303	 l-p:0.1666080802679062
epoch£º565	 i:4 	 global-step:11304	 l-p:-0.5061266422271729
epoch£º565	 i:5 	 global-step:11305	 l-p:0.21258628368377686
epoch£º565	 i:6 	 global-step:11306	 l-p:0.16364124417304993
epoch£º565	 i:7 	 global-step:11307	 l-p:0.17098766565322876
epoch£º565	 i:8 	 global-step:11308	 l-p:0.08215386420488358
epoch£º565	 i:9 	 global-step:11309	 l-p:0.09200431406497955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0761, 5.0630, 5.0743],
        [5.0761, 5.0744, 5.0760],
        [5.0761, 4.9806, 5.0081],
        [5.0761, 5.0273, 5.0582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.021432170644402504 
model_pd.l_d.mean(): -20.338211059570312 
model_pd.lagr.mean(): -20.3167781829834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4710], device='cuda:0')), ('power', tensor([-21.0416], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.021432170644402504
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11619588732719421
epoch£º566	 i:2 	 global-step:11322	 l-p:0.07740221172571182
epoch£º566	 i:3 	 global-step:11323	 l-p:0.14270539581775665
epoch£º566	 i:4 	 global-step:11324	 l-p:0.14015431702136993
epoch£º566	 i:5 	 global-step:11325	 l-p:0.10992176085710526
epoch£º566	 i:6 	 global-step:11326	 l-p:0.1415480226278305
epoch£º566	 i:7 	 global-step:11327	 l-p:0.17329569160938263
epoch£º566	 i:8 	 global-step:11328	 l-p:0.560552716255188
epoch£º566	 i:9 	 global-step:11329	 l-p:0.12382110208272934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.2792, 5.9321, 6.0961],
        [5.2792, 5.8633, 5.9733],
        [5.2792, 5.1708, 5.1746],
        [5.2792, 5.6692, 5.6398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.1270073503255844 
model_pd.l_d.mean(): -18.51715660095215 
model_pd.lagr.mean(): -18.39015007019043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5245], device='cuda:0')), ('power', tensor([-19.2553], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.1270073503255844
epoch£º567	 i:1 	 global-step:11341	 l-p:0.12839670479297638
epoch£º567	 i:2 	 global-step:11342	 l-p:0.13935792446136475
epoch£º567	 i:3 	 global-step:11343	 l-p:0.12542535364627838
epoch£º567	 i:4 	 global-step:11344	 l-p:0.2603614628314972
epoch£º567	 i:5 	 global-step:11345	 l-p:0.12306205183267593
epoch£º567	 i:6 	 global-step:11346	 l-p:0.3317205607891083
epoch£º567	 i:7 	 global-step:11347	 l-p:0.13587148487567902
epoch£º567	 i:8 	 global-step:11348	 l-p:0.12991340458393097
epoch£º567	 i:9 	 global-step:11349	 l-p:-0.003930871374905109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2220, 5.1067, 4.9727],
        [5.2220, 5.2132, 5.2211],
        [5.2220, 5.4414, 5.3124],
        [5.2220, 5.1161, 4.9668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.04767021909356117 
model_pd.l_d.mean(): -20.043363571166992 
model_pd.lagr.mean(): -19.99569320678711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-20.7361], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.04767021909356117
epoch£º568	 i:1 	 global-step:11361	 l-p:0.14574585855007172
epoch£º568	 i:2 	 global-step:11362	 l-p:0.11274150758981705
epoch£º568	 i:3 	 global-step:11363	 l-p:0.113536037504673
epoch£º568	 i:4 	 global-step:11364	 l-p:0.3837496340274811
epoch£º568	 i:5 	 global-step:11365	 l-p:0.11894403398036957
epoch£º568	 i:6 	 global-step:11366	 l-p:0.12113585323095322
epoch£º568	 i:7 	 global-step:11367	 l-p:0.15736974775791168
epoch£º568	 i:8 	 global-step:11368	 l-p:-0.46407780051231384
epoch£º568	 i:9 	 global-step:11369	 l-p:0.08986564725637436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1097, 5.1097, 5.1097],
        [5.1097, 5.0783, 5.1017],
        [5.1097, 5.6810, 5.7907],
        [5.1097, 5.1097, 5.1097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.12414867430925369 
model_pd.l_d.mean(): -19.570615768432617 
model_pd.lagr.mean(): -19.44646644592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-20.2877], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.12414867430925369
epoch£º569	 i:1 	 global-step:11381	 l-p:-0.009582476690411568
epoch£º569	 i:2 	 global-step:11382	 l-p:0.1214190423488617
epoch£º569	 i:3 	 global-step:11383	 l-p:0.12922891974449158
epoch£º569	 i:4 	 global-step:11384	 l-p:0.055826280266046524
epoch£º569	 i:5 	 global-step:11385	 l-p:0.13827097415924072
epoch£º569	 i:6 	 global-step:11386	 l-p:0.1422746479511261
epoch£º569	 i:7 	 global-step:11387	 l-p:0.18164248764514923
epoch£º569	 i:8 	 global-step:11388	 l-p:0.09682741016149521
epoch£º569	 i:9 	 global-step:11389	 l-p:0.13801322877407074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0238, 5.0134, 5.0226],
        [5.0238, 5.0238, 5.0238],
        [5.0238, 4.8801, 4.7608],
        [5.0238, 5.2975, 5.2039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.14549703896045685 
model_pd.l_d.mean(): -20.109710693359375 
model_pd.lagr.mean(): -19.964214324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.8278], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.14549703896045685
epoch£º570	 i:1 	 global-step:11401	 l-p:0.15060313045978546
epoch£º570	 i:2 	 global-step:11402	 l-p:0.11009185761213303
epoch£º570	 i:3 	 global-step:11403	 l-p:0.09375062584877014
epoch£º570	 i:4 	 global-step:11404	 l-p:0.1418972611427307
epoch£º570	 i:5 	 global-step:11405	 l-p:0.1905130296945572
epoch£º570	 i:6 	 global-step:11406	 l-p:0.1476747840642929
epoch£º570	 i:7 	 global-step:11407	 l-p:0.14360840618610382
epoch£º570	 i:8 	 global-step:11408	 l-p:0.10951997339725494
epoch£º570	 i:9 	 global-step:11409	 l-p:0.22494514286518097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9738, 4.8237, 4.7760],
        [4.9738, 4.9738, 4.9738],
        [4.9738, 4.9431, 4.9664],
        [4.9738, 4.9683, 4.9734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.20938310027122498 
model_pd.l_d.mean(): -19.376651763916016 
model_pd.lagr.mean(): -19.167268753051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5262], device='cuda:0')), ('power', tensor([-20.1260], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.20938310027122498
epoch£º571	 i:1 	 global-step:11421	 l-p:0.08431326597929001
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11778909713029861
epoch£º571	 i:3 	 global-step:11423	 l-p:0.11366555839776993
epoch£º571	 i:4 	 global-step:11424	 l-p:0.1076076328754425
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12657369673252106
epoch£º571	 i:6 	 global-step:11426	 l-p:0.16049499809741974
epoch£º571	 i:7 	 global-step:11427	 l-p:0.1020045280456543
epoch£º571	 i:8 	 global-step:11428	 l-p:0.15745602548122406
epoch£º571	 i:9 	 global-step:11429	 l-p:0.12344542145729065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0085, 5.0075, 5.0084],
        [5.0085, 5.6058, 5.7403],
        [5.0085, 5.5109, 5.5729],
        [5.0085, 4.9958, 5.0069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.13020265102386475 
model_pd.l_d.mean(): -19.582555770874023 
model_pd.lagr.mean(): -19.45235252380371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5394], device='cuda:0')), ('power', tensor([-20.3476], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.13020265102386475
epoch£º572	 i:1 	 global-step:11441	 l-p:0.11260721832513809
epoch£º572	 i:2 	 global-step:11442	 l-p:0.12731322646141052
epoch£º572	 i:3 	 global-step:11443	 l-p:0.23878325521945953
epoch£º572	 i:4 	 global-step:11444	 l-p:0.3434090316295624
epoch£º572	 i:5 	 global-step:11445	 l-p:0.14301960170269012
epoch£º572	 i:6 	 global-step:11446	 l-p:0.10409620404243469
epoch£º572	 i:7 	 global-step:11447	 l-p:0.10222112387418747
epoch£º572	 i:8 	 global-step:11448	 l-p:-0.6591532230377197
epoch£º572	 i:9 	 global-step:11449	 l-p:-0.6516808271408081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8668, 4.8080, 4.8436],
        [4.8668, 4.7195, 4.7075],
        [4.8668, 4.7855, 4.8234],
        [4.8668, 4.8665, 4.8668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): -0.3563113212585449 
model_pd.l_d.mean(): -19.62778663635254 
model_pd.lagr.mean(): -19.984098434448242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-20.3819], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:-0.3563113212585449
epoch£º573	 i:1 	 global-step:11461	 l-p:0.23956158757209778
epoch£º573	 i:2 	 global-step:11462	 l-p:0.12092532962560654
epoch£º573	 i:3 	 global-step:11463	 l-p:0.13036203384399414
epoch£º573	 i:4 	 global-step:11464	 l-p:0.12432337552309036
epoch£º573	 i:5 	 global-step:11465	 l-p:0.10953968018293381
epoch£º573	 i:6 	 global-step:11466	 l-p:0.15600226819515228
epoch£º573	 i:7 	 global-step:11467	 l-p:0.13419006764888763
epoch£º573	 i:8 	 global-step:11468	 l-p:0.22725380957126617
epoch£º573	 i:9 	 global-step:11469	 l-p:0.1126871109008789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0403, 4.8911, 4.7843],
        [5.0403, 5.0910, 4.8912],
        [5.0403, 5.0403, 5.0403],
        [5.0403, 5.0364, 5.0401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.14069414138793945 
model_pd.l_d.mean(): -20.348119735717773 
model_pd.lagr.mean(): -20.207426071166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-21.0483], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.14069414138793945
epoch£º574	 i:1 	 global-step:11481	 l-p:0.14117451012134552
epoch£º574	 i:2 	 global-step:11482	 l-p:0.08522647619247437
epoch£º574	 i:3 	 global-step:11483	 l-p:0.11144313216209412
epoch£º574	 i:4 	 global-step:11484	 l-p:0.10761494189500809
epoch£º574	 i:5 	 global-step:11485	 l-p:0.1564577966928482
epoch£º574	 i:6 	 global-step:11486	 l-p:0.1389499306678772
epoch£º574	 i:7 	 global-step:11487	 l-p:-0.3873901963233948
epoch£º574	 i:8 	 global-step:11488	 l-p:0.15226416289806366
epoch£º574	 i:9 	 global-step:11489	 l-p:0.11561398953199387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1147, 5.1029, 5.1133],
        [5.1147, 4.9848, 4.9772],
        [5.1147, 5.1147, 5.1147],
        [5.1147, 5.1013, 5.1129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.11653326451778412 
model_pd.l_d.mean(): -20.35138511657715 
model_pd.lagr.mean(): -20.234851837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4829], device='cuda:0')), ('power', tensor([-21.0671], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.11653326451778412
epoch£º575	 i:1 	 global-step:11501	 l-p:0.11070722341537476
epoch£º575	 i:2 	 global-step:11502	 l-p:0.10429877042770386
epoch£º575	 i:3 	 global-step:11503	 l-p:0.11792673170566559
epoch£º575	 i:4 	 global-step:11504	 l-p:0.2246595025062561
epoch£º575	 i:5 	 global-step:11505	 l-p:0.12757247686386108
epoch£º575	 i:6 	 global-step:11506	 l-p:0.12720704078674316
epoch£º575	 i:7 	 global-step:11507	 l-p:-0.011823243461549282
epoch£º575	 i:8 	 global-step:11508	 l-p:0.09215893596410751
epoch£º575	 i:9 	 global-step:11509	 l-p:0.14994968473911285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1021, 5.1018, 5.1021],
        [5.1021, 5.0987, 5.1020],
        [5.1021, 5.5964, 5.6458],
        [5.1021, 5.0380, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.10701774060726166 
model_pd.l_d.mean(): -20.312633514404297 
model_pd.lagr.mean(): -20.205615997314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.0180], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.10701774060726166
epoch£º576	 i:1 	 global-step:11521	 l-p:0.1419018656015396
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13589167594909668
epoch£º576	 i:3 	 global-step:11523	 l-p:0.10962459444999695
epoch£º576	 i:4 	 global-step:11524	 l-p:0.12900523841381073
epoch£º576	 i:5 	 global-step:11525	 l-p:0.14063261449337006
epoch£º576	 i:6 	 global-step:11526	 l-p:0.15380547940731049
epoch£º576	 i:7 	 global-step:11527	 l-p:0.09645871073007584
epoch£º576	 i:8 	 global-step:11528	 l-p:0.11393867433071136
epoch£º576	 i:9 	 global-step:11529	 l-p:0.22801436483860016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9607, 4.8588, 4.8906],
        [4.9607, 4.9607, 4.9607],
        [4.9607, 5.0109, 4.8078],
        [4.9607, 4.9607, 4.9607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.11442543566226959 
model_pd.l_d.mean(): -20.731008529663086 
model_pd.lagr.mean(): -20.6165828704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-21.4432], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.11442543566226959
epoch£º577	 i:1 	 global-step:11541	 l-p:0.256868839263916
epoch£º577	 i:2 	 global-step:11542	 l-p:0.24384945631027222
epoch£º577	 i:3 	 global-step:11543	 l-p:0.12828879058361053
epoch£º577	 i:4 	 global-step:11544	 l-p:0.11121731251478195
epoch£º577	 i:5 	 global-step:11545	 l-p:0.15420176088809967
epoch£º577	 i:6 	 global-step:11546	 l-p:0.16165012121200562
epoch£º577	 i:7 	 global-step:11547	 l-p:0.13667833805084229
epoch£º577	 i:8 	 global-step:11548	 l-p:0.10485006868839264
epoch£º577	 i:9 	 global-step:11549	 l-p:0.3623279631137848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8961, 4.8962, 4.8962],
        [4.8961, 4.9103, 4.6956],
        [4.8961, 4.7400, 4.7112],
        [4.8961, 4.7373, 4.5890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.14081139862537384 
model_pd.l_d.mean(): -20.59187889099121 
model_pd.lagr.mean(): -20.451066970825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4985], device='cuda:0')), ('power', tensor([-21.3262], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.14081139862537384
epoch£º578	 i:1 	 global-step:11561	 l-p:0.1829284429550171
epoch£º578	 i:2 	 global-step:11562	 l-p:0.09975744038820267
epoch£º578	 i:3 	 global-step:11563	 l-p:0.11998197436332703
epoch£º578	 i:4 	 global-step:11564	 l-p:-0.02235465869307518
epoch£º578	 i:5 	 global-step:11565	 l-p:0.20303381979465485
epoch£º578	 i:6 	 global-step:11566	 l-p:0.18765221536159515
epoch£º578	 i:7 	 global-step:11567	 l-p:0.14582909643650055
epoch£º578	 i:8 	 global-step:11568	 l-p:0.21954742074012756
epoch£º578	 i:9 	 global-step:11569	 l-p:0.15137982368469238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0228, 5.5663, 5.6556],
        [5.0228, 4.9477, 4.9845],
        [5.0228, 4.8735, 4.7366],
        [5.0228, 4.9680, 5.0019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.14589297771453857 
model_pd.l_d.mean(): -20.472951889038086 
model_pd.lagr.mean(): -20.327058792114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4719], device='cuda:0')), ('power', tensor([-21.1788], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.14589297771453857
epoch£º579	 i:1 	 global-step:11581	 l-p:0.04862339422106743
epoch£º579	 i:2 	 global-step:11582	 l-p:0.14253608882427216
epoch£º579	 i:3 	 global-step:11583	 l-p:0.0993841364979744
epoch£º579	 i:4 	 global-step:11584	 l-p:0.11637815833091736
epoch£º579	 i:5 	 global-step:11585	 l-p:0.1529088318347931
epoch£º579	 i:6 	 global-step:11586	 l-p:0.08049780875444412
epoch£º579	 i:7 	 global-step:11587	 l-p:0.11309468001127243
epoch£º579	 i:8 	 global-step:11588	 l-p:0.15142475068569183
epoch£º579	 i:9 	 global-step:11589	 l-p:0.021660059690475464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1095, 5.0893, 5.1060],
        [5.1095, 5.1038, 5.1091],
        [5.1095, 5.0506, 4.8431],
        [5.1095, 5.0837, 5.1041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.10933595150709152 
model_pd.l_d.mean(): -20.94124984741211 
model_pd.lagr.mean(): -20.831912994384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900], device='cuda:0')), ('power', tensor([-21.5685], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.10933595150709152
epoch£º580	 i:1 	 global-step:11601	 l-p:0.14193095266819
epoch£º580	 i:2 	 global-step:11602	 l-p:0.13043519854545593
epoch£º580	 i:3 	 global-step:11603	 l-p:0.11419995874166489
epoch£º580	 i:4 	 global-step:11604	 l-p:0.07622625678777695
epoch£º580	 i:5 	 global-step:11605	 l-p:0.1971527636051178
epoch£º580	 i:6 	 global-step:11606	 l-p:0.19234944880008698
epoch£º580	 i:7 	 global-step:11607	 l-p:0.07556606829166412
epoch£º580	 i:8 	 global-step:11608	 l-p:0.1438256949186325
epoch£º580	 i:9 	 global-step:11609	 l-p:0.1236424595117569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0490, 5.0490, 5.0490],
        [5.0490, 5.2431, 5.0992],
        [5.0490, 4.8979, 4.7704],
        [5.0490, 4.9536, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.171904519200325 
model_pd.l_d.mean(): -20.289081573486328 
model_pd.lagr.mean(): -20.117177963256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-21.0189], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.171904519200325
epoch£º581	 i:1 	 global-step:11621	 l-p:0.10393880307674408
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12484313547611237
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13229049742221832
epoch£º581	 i:4 	 global-step:11624	 l-p:0.11685498803853989
epoch£º581	 i:5 	 global-step:11625	 l-p:0.06049645319581032
epoch£º581	 i:6 	 global-step:11626	 l-p:0.08569779992103577
epoch£º581	 i:7 	 global-step:11627	 l-p:0.1546730101108551
epoch£º581	 i:8 	 global-step:11628	 l-p:0.1323791742324829
epoch£º581	 i:9 	 global-step:11629	 l-p:0.13714911043643951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0829, 5.0330, 5.0652],
        [5.0829, 5.6009, 5.6667],
        [5.0829, 5.0825, 5.0829],
        [5.0829, 5.0721, 5.0817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.14212173223495483 
model_pd.l_d.mean(): -18.500507354736328 
model_pd.lagr.mean(): -18.35838508605957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5579], device='cuda:0')), ('power', tensor([-19.2726], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.14212173223495483
epoch£º582	 i:1 	 global-step:11641	 l-p:0.13213995099067688
epoch£º582	 i:2 	 global-step:11642	 l-p:0.11992169916629791
epoch£º582	 i:3 	 global-step:11643	 l-p:0.13276824355125427
epoch£º582	 i:4 	 global-step:11644	 l-p:0.13478311896324158
epoch£º582	 i:5 	 global-step:11645	 l-p:-0.4062995910644531
epoch£º582	 i:6 	 global-step:11646	 l-p:0.06280616670846939
epoch£º582	 i:7 	 global-step:11647	 l-p:0.11176525056362152
epoch£º582	 i:8 	 global-step:11648	 l-p:0.09755497425794601
epoch£º582	 i:9 	 global-step:11649	 l-p:0.1356632113456726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0931, 5.0594, 5.0845],
        [5.0931, 5.2065, 5.0237],
        [5.0931, 5.2287, 5.0550],
        [5.0931, 4.9587, 4.7984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.1315315216779709 
model_pd.l_d.mean(): -20.439695358276367 
model_pd.lagr.mean(): -20.308164596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4761], device='cuda:0')), ('power', tensor([-21.1494], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.1315315216779709
epoch£º583	 i:1 	 global-step:11661	 l-p:0.13622497022151947
epoch£º583	 i:2 	 global-step:11662	 l-p:0.09886050224304199
epoch£º583	 i:3 	 global-step:11663	 l-p:0.09743140637874603
epoch£º583	 i:4 	 global-step:11664	 l-p:0.16207341849803925
epoch£º583	 i:5 	 global-step:11665	 l-p:0.1582755595445633
epoch£º583	 i:6 	 global-step:11666	 l-p:0.07525691390037537
epoch£º583	 i:7 	 global-step:11667	 l-p:0.1268543154001236
epoch£º583	 i:8 	 global-step:11668	 l-p:0.12677815556526184
epoch£º583	 i:9 	 global-step:11669	 l-p:0.20115473866462708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9825, 4.9614, 4.9788],
        [4.9825, 4.9825, 4.9825],
        [4.9825, 4.9269, 4.9617],
        [4.9825, 4.9161, 4.9533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.2505897581577301 
model_pd.l_d.mean(): -20.704917907714844 
model_pd.lagr.mean(): -20.454328536987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-21.4115], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.2505897581577301
epoch£º584	 i:1 	 global-step:11681	 l-p:0.12974072992801666
epoch£º584	 i:2 	 global-step:11682	 l-p:0.16612428426742554
epoch£º584	 i:3 	 global-step:11683	 l-p:0.1559298038482666
epoch£º584	 i:4 	 global-step:11684	 l-p:0.11826066672801971
epoch£º584	 i:5 	 global-step:11685	 l-p:0.1139780804514885
epoch£º584	 i:6 	 global-step:11686	 l-p:0.15551435947418213
epoch£º584	 i:7 	 global-step:11687	 l-p:-0.17281606793403625
epoch£º584	 i:8 	 global-step:11688	 l-p:0.09879248589277267
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10947969555854797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 5.0738, 5.1098],
        [5.1466, 5.1394, 5.1460],
        [5.1466, 5.1451, 5.1466],
        [5.1466, 5.1371, 5.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.03385080769658089 
model_pd.l_d.mean(): -20.446401596069336 
model_pd.lagr.mean(): -20.41254997253418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4250], device='cuda:0')), ('power', tensor([-21.1040], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.03385080769658089
epoch£º585	 i:1 	 global-step:11701	 l-p:0.12793125212192535
epoch£º585	 i:2 	 global-step:11702	 l-p:0.30169984698295593
epoch£º585	 i:3 	 global-step:11703	 l-p:0.03491736203432083
epoch£º585	 i:4 	 global-step:11704	 l-p:0.11833923310041428
epoch£º585	 i:5 	 global-step:11705	 l-p:0.12438403815031052
epoch£º585	 i:6 	 global-step:11706	 l-p:0.14110645651817322
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11614576727151871
epoch£º585	 i:8 	 global-step:11708	 l-p:0.1347966343164444
epoch£º585	 i:9 	 global-step:11709	 l-p:0.06878673285245895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1951, 5.1945, 5.1951],
        [5.1951, 5.7451, 5.8278],
        [5.1951, 5.0694, 5.0718],
        [5.1951, 5.1642, 5.1876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.13577891886234283 
model_pd.l_d.mean(): -20.211206436157227 
model_pd.lagr.mean(): -20.075428009033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4443], device='cuda:0')), ('power', tensor([-20.8859], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.13577891886234283
epoch£º586	 i:1 	 global-step:11721	 l-p:0.14563022553920746
epoch£º586	 i:2 	 global-step:11722	 l-p:0.08803091198205948
epoch£º586	 i:3 	 global-step:11723	 l-p:0.19750399887561798
epoch£º586	 i:4 	 global-step:11724	 l-p:0.8107812404632568
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11341606080532074
epoch£º586	 i:6 	 global-step:11726	 l-p:0.03460392728447914
epoch£º586	 i:7 	 global-step:11727	 l-p:0.13555338978767395
epoch£º586	 i:8 	 global-step:11728	 l-p:0.11623439192771912
epoch£º586	 i:9 	 global-step:11729	 l-p:0.14045247435569763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 4.9782, 4.8478],
        [5.1253, 5.2092, 5.0145],
        [5.1253, 5.1479, 4.9372],
        [5.1253, 5.0723, 5.1056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12334384769201279 
model_pd.l_d.mean(): -20.171390533447266 
model_pd.lagr.mean(): -20.048046112060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4857], device='cuda:0')), ('power', tensor([-20.8880], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12334384769201279
epoch£º587	 i:1 	 global-step:11741	 l-p:0.05125721916556358
epoch£º587	 i:2 	 global-step:11742	 l-p:-0.24149535596370697
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12798938155174255
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1317087560892105
epoch£º587	 i:5 	 global-step:11745	 l-p:0.183115154504776
epoch£º587	 i:6 	 global-step:11746	 l-p:0.166168212890625
epoch£º587	 i:7 	 global-step:11747	 l-p:0.1617937982082367
epoch£º587	 i:8 	 global-step:11748	 l-p:0.12573586404323578
epoch£º587	 i:9 	 global-step:11749	 l-p:0.27142706513404846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9703, 4.9703, 4.9703],
        [4.9703, 4.8847, 4.9232],
        [4.9703, 4.8050, 4.7455],
        [4.9703, 4.8968, 4.9353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.1336003839969635 
model_pd.l_d.mean(): -20.491369247436523 
model_pd.lagr.mean(): -20.357769012451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-21.2242], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.1336003839969635
epoch£º588	 i:1 	 global-step:11761	 l-p:0.11897571384906769
epoch£º588	 i:2 	 global-step:11762	 l-p:0.1177118644118309
epoch£º588	 i:3 	 global-step:11763	 l-p:0.12404825538396835
epoch£º588	 i:4 	 global-step:11764	 l-p:0.06727591156959534
epoch£º588	 i:5 	 global-step:11765	 l-p:0.17987193167209625
epoch£º588	 i:6 	 global-step:11766	 l-p:0.03750289976596832
epoch£º588	 i:7 	 global-step:11767	 l-p:0.11361302435398102
epoch£º588	 i:8 	 global-step:11768	 l-p:0.1554037183523178
epoch£º588	 i:9 	 global-step:11769	 l-p:0.09625760465860367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8407, 4.8258, 4.8388],
        [4.8407, 4.8399, 4.8407],
        [4.8407, 4.7962, 4.8276],
        [4.8407, 4.8407, 4.8407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.1132780984044075 
model_pd.l_d.mean(): -20.27164649963379 
model_pd.lagr.mean(): -20.158369064331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5650], device='cuda:0')), ('power', tensor([-21.0704], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.1132780984044075
epoch£º589	 i:1 	 global-step:11781	 l-p:0.13496853411197662
epoch£º589	 i:2 	 global-step:11782	 l-p:0.19115222990512848
epoch£º589	 i:3 	 global-step:11783	 l-p:0.12338968366384506
epoch£º589	 i:4 	 global-step:11784	 l-p:0.07654958218336105
epoch£º589	 i:5 	 global-step:11785	 l-p:0.25755125284194946
epoch£º589	 i:6 	 global-step:11786	 l-p:0.17416074872016907
epoch£º589	 i:7 	 global-step:11787	 l-p:0.14348052442073822
epoch£º589	 i:8 	 global-step:11788	 l-p:0.1568506360054016
epoch£º589	 i:9 	 global-step:11789	 l-p:0.13349133729934692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0534, 5.0197, 5.0450],
        [5.0534, 5.0190, 4.7985],
        [5.0534, 4.9976, 5.0323],
        [5.0534, 5.0436, 5.0524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.16146662831306458 
model_pd.l_d.mean(): -20.35980796813965 
model_pd.lagr.mean(): -20.198341369628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4853], device='cuda:0')), ('power', tensor([-21.0781], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.16146662831306458
epoch£º590	 i:1 	 global-step:11801	 l-p:0.13132716715335846
epoch£º590	 i:2 	 global-step:11802	 l-p:0.12577366828918457
epoch£º590	 i:3 	 global-step:11803	 l-p:0.1331769973039627
epoch£º590	 i:4 	 global-step:11804	 l-p:0.13760600984096527
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13218913972377777
epoch£º590	 i:6 	 global-step:11806	 l-p:0.12676553428173065
epoch£º590	 i:7 	 global-step:11807	 l-p:0.06627611815929413
epoch£º590	 i:8 	 global-step:11808	 l-p:-0.055189475417137146
epoch£º590	 i:9 	 global-step:11809	 l-p:0.13659143447875977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0729, 5.1563, 4.9593],
        [5.0729, 4.9166, 4.7891],
        [5.0729, 5.0728, 5.0729],
        [5.0729, 5.0725, 5.0729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12366979569196701 
model_pd.l_d.mean(): -18.83618927001953 
model_pd.lagr.mean(): -18.7125186920166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5271], device='cuda:0')), ('power', tensor([-19.5805], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12366979569196701
epoch£º591	 i:1 	 global-step:11821	 l-p:0.16688001155853271
epoch£º591	 i:2 	 global-step:11822	 l-p:0.11972443014383316
epoch£º591	 i:3 	 global-step:11823	 l-p:0.11304489523172379
epoch£º591	 i:4 	 global-step:11824	 l-p:0.04706602916121483
epoch£º591	 i:5 	 global-step:11825	 l-p:0.10883574187755585
epoch£º591	 i:6 	 global-step:11826	 l-p:0.10062113404273987
epoch£º591	 i:7 	 global-step:11827	 l-p:0.17439685761928558
epoch£º591	 i:8 	 global-step:11828	 l-p:0.09248830378055573
epoch£º591	 i:9 	 global-step:11829	 l-p:0.13218457996845245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.1213, 4.9710, 4.9363],
        [5.1213, 4.9709, 4.9360],
        [5.1213, 5.7303, 5.8605],
        [5.1213, 4.9653, 4.9039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12641894817352295 
model_pd.l_d.mean(): -20.1563777923584 
model_pd.lagr.mean(): -20.029958724975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4439], device='cuda:0')), ('power', tensor([-20.8301], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12641894817352295
epoch£º592	 i:1 	 global-step:11841	 l-p:0.07116676867008209
epoch£º592	 i:2 	 global-step:11842	 l-p:0.6515985727310181
epoch£º592	 i:3 	 global-step:11843	 l-p:0.13978227972984314
epoch£º592	 i:4 	 global-step:11844	 l-p:-0.008227214217185974
epoch£º592	 i:5 	 global-step:11845	 l-p:0.12411650270223618
epoch£º592	 i:6 	 global-step:11846	 l-p:0.13825955986976624
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13033843040466309
epoch£º592	 i:8 	 global-step:11848	 l-p:0.1712467521429062
epoch£º592	 i:9 	 global-step:11849	 l-p:0.12114308774471283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0768, 5.0071, 5.0447],
        [5.0768, 4.9704, 5.0013],
        [5.0768, 5.0767, 5.0768],
        [5.0768, 5.0768, 5.0768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.012404403649270535 
model_pd.l_d.mean(): -20.772451400756836 
model_pd.lagr.mean(): -20.760047912597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.4390], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.012404403649270535
epoch£º593	 i:1 	 global-step:11861	 l-p:0.10774385184049606
epoch£º593	 i:2 	 global-step:11862	 l-p:0.10910573601722717
epoch£º593	 i:3 	 global-step:11863	 l-p:0.18745791912078857
epoch£º593	 i:4 	 global-step:11864	 l-p:0.14126047492027283
epoch£º593	 i:5 	 global-step:11865	 l-p:0.13441775739192963
epoch£º593	 i:6 	 global-step:11866	 l-p:0.13062983751296997
epoch£º593	 i:7 	 global-step:11867	 l-p:0.18186046183109283
epoch£º593	 i:8 	 global-step:11868	 l-p:0.1533430516719818
epoch£º593	 i:9 	 global-step:11869	 l-p:0.10238603502511978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0585, 4.9588, 4.9933],
        [5.0585, 5.0585, 5.0585],
        [5.0585, 4.9984, 5.0345],
        [5.0585, 5.3089, 5.1921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.10745290666818619 
model_pd.l_d.mean(): -20.333280563354492 
model_pd.lagr.mean(): -20.225828170776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5055], device='cuda:0')), ('power', tensor([-21.0719], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.10745290666818619
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13077159225940704
epoch£º594	 i:2 	 global-step:11882	 l-p:0.11507274210453033
epoch£º594	 i:3 	 global-step:11883	 l-p:0.13224107027053833
epoch£º594	 i:4 	 global-step:11884	 l-p:0.11507043242454529
epoch£º594	 i:5 	 global-step:11885	 l-p:0.023301180452108383
epoch£º594	 i:6 	 global-step:11886	 l-p:0.13821743428707123
epoch£º594	 i:7 	 global-step:11887	 l-p:0.17881318926811218
epoch£º594	 i:8 	 global-step:11888	 l-p:0.12212220579385757
epoch£º594	 i:9 	 global-step:11889	 l-p:0.1384470909833908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0680, 5.3243, 5.2104],
        [5.0680, 4.9120, 4.8782],
        [5.0680, 5.0679, 5.0680],
        [5.0680, 5.0664, 5.0679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.14441512525081635 
model_pd.l_d.mean(): -20.61735725402832 
model_pd.lagr.mean(): -20.472942352294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-21.2989], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.14441512525081635
epoch£º595	 i:1 	 global-step:11901	 l-p:0.12178550660610199
epoch£º595	 i:2 	 global-step:11902	 l-p:0.1609647572040558
epoch£º595	 i:3 	 global-step:11903	 l-p:0.18464063107967377
epoch£º595	 i:4 	 global-step:11904	 l-p:0.12274454534053802
epoch£º595	 i:5 	 global-step:11905	 l-p:0.1270950883626938
epoch£º595	 i:6 	 global-step:11906	 l-p:0.12491948157548904
epoch£º595	 i:7 	 global-step:11907	 l-p:0.11660194396972656
epoch£º595	 i:8 	 global-step:11908	 l-p:0.12068063765764236
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11790092289447784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9822, 4.9452, 4.9726],
        [4.9822, 4.9814, 4.9822],
        [4.9822, 4.8763, 4.6584],
        [4.9822, 4.9822, 4.9822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11407646536827087 
model_pd.l_d.mean(): -20.57505226135254 
model_pd.lagr.mean(): -20.460975646972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-21.2811], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11407646536827087
epoch£º596	 i:1 	 global-step:11921	 l-p:0.14591045677661896
epoch£º596	 i:2 	 global-step:11922	 l-p:0.1550169587135315
epoch£º596	 i:3 	 global-step:11923	 l-p:-2.790886878967285
epoch£º596	 i:4 	 global-step:11924	 l-p:0.12461194396018982
epoch£º596	 i:5 	 global-step:11925	 l-p:-0.3061502277851105
epoch£º596	 i:6 	 global-step:11926	 l-p:0.5017895698547363
epoch£º596	 i:7 	 global-step:11927	 l-p:3.4645984172821045
epoch£º596	 i:8 	 global-step:11928	 l-p:0.09183598309755325
epoch£º596	 i:9 	 global-step:11929	 l-p:0.13214625418186188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9325, 4.7803, 4.7779],
        [4.9325, 4.9077, 4.9278],
        [4.9325, 4.9324, 4.9325],
        [4.9325, 4.9308, 4.9324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.15061482787132263 
model_pd.l_d.mean(): -20.48334312438965 
model_pd.lagr.mean(): -20.332727432250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-21.2129], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.15061482787132263
epoch£º597	 i:1 	 global-step:11941	 l-p:0.14420804381370544
epoch£º597	 i:2 	 global-step:11942	 l-p:0.13236412405967712
epoch£º597	 i:3 	 global-step:11943	 l-p:0.023058176040649414
epoch£º597	 i:4 	 global-step:11944	 l-p:0.23822173476219177
epoch£º597	 i:5 	 global-step:11945	 l-p:0.9482417106628418
epoch£º597	 i:6 	 global-step:11946	 l-p:0.08924032747745514
epoch£º597	 i:7 	 global-step:11947	 l-p:0.17745192348957062
epoch£º597	 i:8 	 global-step:11948	 l-p:0.13339905440807343
epoch£º597	 i:9 	 global-step:11949	 l-p:0.14851342141628265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0220, 5.0213, 5.0220],
        [5.0220, 5.1461, 4.9621],
        [5.0220, 4.8934, 4.9132],
        [5.0220, 5.4451, 5.4402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11588733643293381 
model_pd.l_d.mean(): -19.295555114746094 
model_pd.lagr.mean(): -19.179668426513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5402], device='cuda:0')), ('power', tensor([-20.0582], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11588733643293381
epoch£º598	 i:1 	 global-step:11961	 l-p:0.1200675442814827
epoch£º598	 i:2 	 global-step:11962	 l-p:0.1104217991232872
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13191351294517517
epoch£º598	 i:4 	 global-step:11964	 l-p:0.11949135363101959
epoch£º598	 i:5 	 global-step:11965	 l-p:0.12865948677062988
epoch£º598	 i:6 	 global-step:11966	 l-p:0.1485099494457245
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13960425555706024
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15603677928447723
epoch£º598	 i:9 	 global-step:11969	 l-p:0.1315850168466568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0843, 5.0843, 5.0843],
        [5.0843, 4.9521, 4.9650],
        [5.0843, 5.3394, 5.2227],
        [5.0843, 5.0805, 5.0841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.13903279602527618 
model_pd.l_d.mean(): -19.838651657104492 
model_pd.lagr.mean(): -19.69961929321289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5481], device='cuda:0')), ('power', tensor([-20.6154], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.13903279602527618
epoch£º599	 i:1 	 global-step:11981	 l-p:0.15715402364730835
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12185049802064896
epoch£º599	 i:3 	 global-step:11983	 l-p:0.1679111123085022
epoch£º599	 i:4 	 global-step:11984	 l-p:0.09145866334438324
epoch£º599	 i:5 	 global-step:11985	 l-p:0.07572878897190094
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13480444252490997
epoch£º599	 i:7 	 global-step:11987	 l-p:0.1325284093618393
epoch£º599	 i:8 	 global-step:11988	 l-p:0.05101730301976204
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12151232361793518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0782, 5.1434, 4.9353],
        [5.0782, 5.0782, 5.0783],
        [5.0782, 5.0783, 5.0783],
        [5.0782, 5.0783, 5.0783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.12997227907180786 
model_pd.l_d.mean(): -19.630887985229492 
model_pd.lagr.mean(): -19.50091552734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-20.3257], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.12997227907180786
epoch£º600	 i:1 	 global-step:12001	 l-p:0.13610875606536865
epoch£º600	 i:2 	 global-step:12002	 l-p:0.1516445279121399
epoch£º600	 i:3 	 global-step:12003	 l-p:0.06895212084054947
epoch£º600	 i:4 	 global-step:12004	 l-p:0.0722852572798729
epoch£º600	 i:5 	 global-step:12005	 l-p:0.12036013603210449
epoch£º600	 i:6 	 global-step:12006	 l-p:0.20109540224075317
epoch£º600	 i:7 	 global-step:12007	 l-p:0.11387235671281815
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12580880522727966
epoch£º600	 i:9 	 global-step:12009	 l-p:0.12838612496852875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0579, 5.0319, 4.8039],
        [5.0579, 4.8932, 4.7513],
        [5.0579, 5.0379, 5.0547],
        [5.0579, 5.0573, 5.0579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.08499247580766678 
model_pd.l_d.mean(): -20.648334503173828 
model_pd.lagr.mean(): -20.56334114074707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.3160], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.08499247580766678
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12540987133979797
epoch£º601	 i:2 	 global-step:12022	 l-p:0.17519418895244598
epoch£º601	 i:3 	 global-step:12023	 l-p:0.14991241693496704
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13024383783340454
epoch£º601	 i:5 	 global-step:12025	 l-p:0.10733495652675629
epoch£º601	 i:6 	 global-step:12026	 l-p:0.19664865732192993
epoch£º601	 i:7 	 global-step:12027	 l-p:0.10774500668048859
epoch£º601	 i:8 	 global-step:12028	 l-p:0.1046198233962059
epoch£º601	 i:9 	 global-step:12029	 l-p:0.13032957911491394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0330, 5.0330, 5.0330],
        [5.0330, 5.0669, 4.8481],
        [5.0330, 5.0092, 5.0286],
        [5.0330, 5.0232, 4.7954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.1372767835855484 
model_pd.l_d.mean(): -20.41288948059082 
model_pd.lagr.mean(): -20.275611877441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4845], device='cuda:0')), ('power', tensor([-21.1310], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.1372767835855484
epoch£º602	 i:1 	 global-step:12041	 l-p:0.08032897114753723
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11914999037981033
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11304496228694916
epoch£º602	 i:4 	 global-step:12044	 l-p:0.11549229174852371
epoch£º602	 i:5 	 global-step:12045	 l-p:0.2082279473543167
epoch£º602	 i:6 	 global-step:12046	 l-p:0.1307830661535263
epoch£º602	 i:7 	 global-step:12047	 l-p:0.10102444142103195
epoch£º602	 i:8 	 global-step:12048	 l-p:0.15251395106315613
epoch£º602	 i:9 	 global-step:12049	 l-p:0.17279639840126038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0633, 4.9455, 4.9738],
        [5.0633, 5.0525, 5.0622],
        [5.0633, 5.0632, 5.0633],
        [5.0633, 4.9530, 4.9855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.1079316958785057 
model_pd.l_d.mean(): -20.772802352905273 
model_pd.lagr.mean(): -20.664871215820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4240], device='cuda:0')), ('power', tensor([-21.4330], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.1079316958785057
epoch£º603	 i:1 	 global-step:12061	 l-p:0.17875243723392487
epoch£º603	 i:2 	 global-step:12062	 l-p:0.13537655770778656
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12868748605251312
epoch£º603	 i:4 	 global-step:12064	 l-p:0.09694799035787582
epoch£º603	 i:5 	 global-step:12065	 l-p:0.12842054665088654
epoch£º603	 i:6 	 global-step:12066	 l-p:0.1404445469379425
epoch£º603	 i:7 	 global-step:12067	 l-p:-0.7048640251159668
epoch£º603	 i:8 	 global-step:12068	 l-p:0.1142522394657135
epoch£º603	 i:9 	 global-step:12069	 l-p:0.12123485654592514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1340, 5.4340, 5.3415],
        [5.1340, 5.1340, 5.1340],
        [5.1340, 5.0709, 5.1078],
        [5.1340, 4.9730, 4.9224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.29824355244636536 
model_pd.l_d.mean(): -20.52640724182129 
model_pd.lagr.mean(): -20.22816276550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4367], device='cuda:0')), ('power', tensor([-21.1969], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.29824355244636536
epoch£º604	 i:1 	 global-step:12081	 l-p:0.1511652022600174
epoch£º604	 i:2 	 global-step:12082	 l-p:0.10355237871408463
epoch£º604	 i:3 	 global-step:12083	 l-p:0.06327875703573227
epoch£º604	 i:4 	 global-step:12084	 l-p:0.11870169639587402
epoch£º604	 i:5 	 global-step:12085	 l-p:0.1108861044049263
epoch£º604	 i:6 	 global-step:12086	 l-p:0.25960665941238403
epoch£º604	 i:7 	 global-step:12087	 l-p:0.10076698660850525
epoch£º604	 i:8 	 global-step:12088	 l-p:0.13736365735530853
epoch£º604	 i:9 	 global-step:12089	 l-p:0.1181197315454483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2076, 5.1936, 4.9724],
        [5.2076, 5.1061, 5.1385],
        [5.2076, 5.1554, 5.1890],
        [5.2076, 5.1650, 5.1948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.10726288706064224 
model_pd.l_d.mean(): -20.413787841796875 
model_pd.lagr.mean(): -20.3065242767334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.0665], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.10726288706064224
epoch£º605	 i:1 	 global-step:12101	 l-p:0.09341171383857727
epoch£º605	 i:2 	 global-step:12102	 l-p:0.13171039521694183
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12392742186784744
epoch£º605	 i:4 	 global-step:12104	 l-p:-0.5289281606674194
epoch£º605	 i:5 	 global-step:12105	 l-p:0.13664545118808746
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11024080216884613
epoch£º605	 i:7 	 global-step:12107	 l-p:0.19248881936073303
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12264646589756012
epoch£º605	 i:9 	 global-step:12109	 l-p:0.30193403363227844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 5.1717, 5.1717],
        [5.1717, 5.4524, 5.3466],
        [5.1717, 5.0132, 4.9628],
        [5.1717, 5.1714, 5.1717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.03205855190753937 
model_pd.l_d.mean(): -20.671308517456055 
model_pd.lagr.mean(): -20.639249801635742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4088], device='cuda:0')), ('power', tensor([-21.3149], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.03205855190753937
epoch£º606	 i:1 	 global-step:12121	 l-p:0.12187028676271439
epoch£º606	 i:2 	 global-step:12122	 l-p:0.12444344162940979
epoch£º606	 i:3 	 global-step:12123	 l-p:0.07549095898866653
epoch£º606	 i:4 	 global-step:12124	 l-p:0.12456994503736496
epoch£º606	 i:5 	 global-step:12125	 l-p:0.49187251925468445
epoch£º606	 i:6 	 global-step:12126	 l-p:0.1363363265991211
epoch£º606	 i:7 	 global-step:12127	 l-p:0.13478179275989532
epoch£º606	 i:8 	 global-step:12128	 l-p:0.12626561522483826
epoch£º606	 i:9 	 global-step:12129	 l-p:0.22541774809360504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1491, 5.1401, 5.1482],
        [5.1491, 4.9851, 4.9107],
        [5.1491, 5.1478, 5.1491],
        [5.1491, 4.9928, 4.9584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.1379474252462387 
model_pd.l_d.mean(): -20.45220375061035 
model_pd.lagr.mean(): -20.31425666809082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4451], device='cuda:0')), ('power', tensor([-21.1304], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.1379474252462387
epoch£º607	 i:1 	 global-step:12141	 l-p:0.12497350573539734
epoch£º607	 i:2 	 global-step:12142	 l-p:-0.0345722958445549
epoch£º607	 i:3 	 global-step:12143	 l-p:0.13189192116260529
epoch£º607	 i:4 	 global-step:12144	 l-p:0.45918163657188416
epoch£º607	 i:5 	 global-step:12145	 l-p:0.10022968053817749
epoch£º607	 i:6 	 global-step:12146	 l-p:0.07081690430641174
epoch£º607	 i:7 	 global-step:12147	 l-p:0.11793260276317596
epoch£º607	 i:8 	 global-step:12148	 l-p:0.11611313372850418
epoch£º607	 i:9 	 global-step:12149	 l-p:0.14598701894283295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1012, 5.0997, 5.1011],
        [5.1012, 5.0957, 5.1008],
        [5.1012, 5.0402, 5.0770],
        [5.1012, 5.3359, 5.2051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.15387371182441711 
model_pd.l_d.mean(): -19.572547912597656 
model_pd.lagr.mean(): -19.41867446899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.3318], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.15387371182441711
epoch£º608	 i:1 	 global-step:12161	 l-p:0.1379811316728592
epoch£º608	 i:2 	 global-step:12162	 l-p:0.11568629741668701
epoch£º608	 i:3 	 global-step:12163	 l-p:0.15236873924732208
epoch£º608	 i:4 	 global-step:12164	 l-p:-0.021423500031232834
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11304804682731628
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13331858813762665
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11264720559120178
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15072759985923767
epoch£º608	 i:9 	 global-step:12169	 l-p:0.13106699287891388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0414, 5.0414, 5.0414],
        [5.0414, 5.0241, 5.0389],
        [5.0414, 5.2583, 5.1188],
        [5.0414, 4.9321, 4.9667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.14258049428462982 
model_pd.l_d.mean(): -19.25135040283203 
model_pd.lagr.mean(): -19.1087703704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5643], device='cuda:0')), ('power', tensor([-20.0382], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.14258049428462982
epoch£º609	 i:1 	 global-step:12181	 l-p:0.13729526102542877
epoch£º609	 i:2 	 global-step:12182	 l-p:0.16517089307308197
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13406798243522644
epoch£º609	 i:4 	 global-step:12184	 l-p:0.11713525652885437
epoch£º609	 i:5 	 global-step:12185	 l-p:0.14640147984027863
epoch£º609	 i:6 	 global-step:12186	 l-p:0.18356256186962128
epoch£º609	 i:7 	 global-step:12187	 l-p:0.1412002295255661
epoch£º609	 i:8 	 global-step:12188	 l-p:0.12735550105571747
epoch£º609	 i:9 	 global-step:12189	 l-p:0.1585192233324051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9891, 4.8780, 4.9136],
        [4.9891, 4.9888, 4.9891],
        [4.9891, 4.9891, 4.9891],
        [4.9891, 4.9551, 4.9810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.09836620092391968 
model_pd.l_d.mean(): -20.42862892150879 
model_pd.lagr.mean(): -20.330263137817383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4884], device='cuda:0')), ('power', tensor([-21.1509], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.09836620092391968
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14843694865703583
epoch£º610	 i:2 	 global-step:12202	 l-p:0.14880409836769104
epoch£º610	 i:3 	 global-step:12203	 l-p:0.13740256428718567
epoch£º610	 i:4 	 global-step:12204	 l-p:0.28117725253105164
epoch£º610	 i:5 	 global-step:12205	 l-p:0.1577494740486145
epoch£º610	 i:6 	 global-step:12206	 l-p:0.09126650542020798
epoch£º610	 i:7 	 global-step:12207	 l-p:0.245681032538414
epoch£º610	 i:8 	 global-step:12208	 l-p:0.16546913981437683
epoch£º610	 i:9 	 global-step:12209	 l-p:0.15429052710533142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0332, 4.8609, 4.8060],
        [5.0332, 5.3111, 5.2073],
        [5.0332, 5.5385, 5.5894],
        [5.0332, 5.0321, 5.0331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.11243186891078949 
model_pd.l_d.mean(): -20.610328674316406 
model_pd.lagr.mean(): -20.497896194458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.2802], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.11243186891078949
epoch£º611	 i:1 	 global-step:12221	 l-p:0.12224940210580826
epoch£º611	 i:2 	 global-step:12222	 l-p:0.1245998963713646
epoch£º611	 i:3 	 global-step:12223	 l-p:0.1763835996389389
epoch£º611	 i:4 	 global-step:12224	 l-p:0.14832104742527008
epoch£º611	 i:5 	 global-step:12225	 l-p:0.12599001824855804
epoch£º611	 i:6 	 global-step:12226	 l-p:0.12975449860095978
epoch£º611	 i:7 	 global-step:12227	 l-p:0.1325272023677826
epoch£º611	 i:8 	 global-step:12228	 l-p:0.057798437774181366
epoch£º611	 i:9 	 global-step:12229	 l-p:0.15235549211502075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0783, 5.0343, 4.8028],
        [5.0783, 5.0783, 5.0783],
        [5.0783, 4.9171, 4.8888],
        [5.0783, 5.0774, 5.0783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14890152215957642 
model_pd.l_d.mean(): -19.507984161376953 
model_pd.lagr.mean(): -19.35908317565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4948], device='cuda:0')), ('power', tensor([-20.2267], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14890152215957642
epoch£º612	 i:1 	 global-step:12241	 l-p:0.13105608522891998
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09973345696926117
epoch£º612	 i:3 	 global-step:12243	 l-p:0.13077157735824585
epoch£º612	 i:4 	 global-step:12244	 l-p:0.11468326300382614
epoch£º612	 i:5 	 global-step:12245	 l-p:0.1350412368774414
epoch£º612	 i:6 	 global-step:12246	 l-p:0.14605720341205597
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14593419432640076
epoch£º612	 i:8 	 global-step:12248	 l-p:0.18929500877857208
epoch£º612	 i:9 	 global-step:12249	 l-p:0.05570119619369507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0601, 5.0600, 5.0601],
        [5.0601, 5.0360, 5.0557],
        [5.0601, 5.0570, 5.0600],
        [5.0601, 5.0043, 5.0401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.19844385981559753 
model_pd.l_d.mean(): -20.584585189819336 
model_pd.lagr.mean(): -20.386140823364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4574], device='cuda:0')), ('power', tensor([-21.2768], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.19844385981559753
epoch£º613	 i:1 	 global-step:12261	 l-p:0.1618518829345703
epoch£º613	 i:2 	 global-step:12262	 l-p:0.11974060535430908
epoch£º613	 i:3 	 global-step:12263	 l-p:0.033885277807712555
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11308538168668747
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12202499806880951
epoch£º613	 i:6 	 global-step:12266	 l-p:0.1405353844165802
epoch£º613	 i:7 	 global-step:12267	 l-p:0.09005583822727203
epoch£º613	 i:8 	 global-step:12268	 l-p:0.1068783700466156
epoch£º613	 i:9 	 global-step:12269	 l-p:0.13131967186927795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0861, 4.9907, 4.7670],
        [5.0861, 4.9124, 4.8209],
        [5.0861, 5.0828, 5.0859],
        [5.0861, 5.0844, 5.0860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13682952523231506 
model_pd.l_d.mean(): -20.442123413085938 
model_pd.lagr.mean(): -20.305294036865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.1454], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13682952523231506
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14873504638671875
epoch£º614	 i:2 	 global-step:12282	 l-p:0.031754426658153534
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12801159918308258
epoch£º614	 i:4 	 global-step:12284	 l-p:0.1291218250989914
epoch£º614	 i:5 	 global-step:12285	 l-p:0.14054955542087555
epoch£º614	 i:6 	 global-step:12286	 l-p:0.18239615857601166
epoch£º614	 i:7 	 global-step:12287	 l-p:0.11178372800350189
epoch£º614	 i:8 	 global-step:12288	 l-p:0.15474063158035278
epoch£º614	 i:9 	 global-step:12289	 l-p:0.1248423382639885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0446, 4.8668, 4.7559],
        [5.0446, 4.8671, 4.7827],
        [5.0446, 5.0896, 4.8706],
        [5.0446, 5.0446, 5.0446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.1341896653175354 
model_pd.l_d.mean(): -19.14018440246582 
model_pd.lagr.mean(): -19.00599479675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5226], device='cuda:0')), ('power', tensor([-19.8832], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.1341896653175354
epoch£º615	 i:1 	 global-step:12301	 l-p:0.10424748808145523
epoch£º615	 i:2 	 global-step:12302	 l-p:0.14156289398670197
epoch£º615	 i:3 	 global-step:12303	 l-p:0.10626214742660522
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13230322301387787
epoch£º615	 i:5 	 global-step:12305	 l-p:0.13112564384937286
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14947465062141418
epoch£º615	 i:7 	 global-step:12307	 l-p:0.18123702704906464
epoch£º615	 i:8 	 global-step:12308	 l-p:0.14085502922534943
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11918521672487259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0400, 5.0380, 5.0399],
        [5.0400, 5.0075, 5.0326],
        [5.0400, 5.1971, 5.0243],
        [5.0400, 4.8973, 4.6955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.13503135740756989 
model_pd.l_d.mean(): -18.740718841552734 
model_pd.lagr.mean(): -18.605688095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6058], device='cuda:0')), ('power', tensor([-19.5644], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.13503135740756989
epoch£º616	 i:1 	 global-step:12321	 l-p:0.16098390519618988
epoch£º616	 i:2 	 global-step:12322	 l-p:0.1618063896894455
epoch£º616	 i:3 	 global-step:12323	 l-p:0.15075768530368805
epoch£º616	 i:4 	 global-step:12324	 l-p:0.07541729509830475
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10706698149442673
epoch£º616	 i:6 	 global-step:12326	 l-p:0.1317099630832672
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12354177236557007
epoch£º616	 i:8 	 global-step:12328	 l-p:0.1364024579524994
epoch£º616	 i:9 	 global-step:12329	 l-p:0.12919993698596954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0601, 5.0013, 5.0381],
        [5.0601, 4.9206, 4.9341],
        [5.0601, 5.6027, 5.6787],
        [5.0601, 5.0023, 5.0388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.14379705488681793 
model_pd.l_d.mean(): -20.254371643066406 
model_pd.lagr.mean(): -20.11057472229004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4779], device='cuda:0')), ('power', tensor([-20.9639], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.14379705488681793
epoch£º617	 i:1 	 global-step:12341	 l-p:0.10869108885526657
epoch£º617	 i:2 	 global-step:12342	 l-p:0.16587281227111816
epoch£º617	 i:3 	 global-step:12343	 l-p:0.11514491587877274
epoch£º617	 i:4 	 global-step:12344	 l-p:0.1293277144432068
epoch£º617	 i:5 	 global-step:12345	 l-p:0.1412399262189865
epoch£º617	 i:6 	 global-step:12346	 l-p:0.1253235638141632
epoch£º617	 i:7 	 global-step:12347	 l-p:0.19580644369125366
epoch£º617	 i:8 	 global-step:12348	 l-p:0.1193644180893898
epoch£º617	 i:9 	 global-step:12349	 l-p:0.13722313940525055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0320, 4.8514, 4.7482],
        [5.0320, 4.9715, 5.0090],
        [5.0320, 5.5408, 5.5928],
        [5.0320, 4.8651, 4.8363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.128205344080925 
model_pd.l_d.mean(): -20.29783058166504 
model_pd.lagr.mean(): -20.16962432861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4866], device='cuda:0')), ('power', tensor([-21.0168], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.128205344080925
epoch£º618	 i:1 	 global-step:12361	 l-p:0.16903716325759888
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10305726528167725
epoch£º618	 i:3 	 global-step:12363	 l-p:0.24603687226772308
epoch£º618	 i:4 	 global-step:12364	 l-p:0.11492118239402771
epoch£º618	 i:5 	 global-step:12365	 l-p:0.154794842004776
epoch£º618	 i:6 	 global-step:12366	 l-p:0.1348452866077423
epoch£º618	 i:7 	 global-step:12367	 l-p:0.15821072459220886
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08622811734676361
epoch£º618	 i:9 	 global-step:12369	 l-p:0.14275699853897095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0243, 4.9659, 5.0028],
        [5.0243, 4.9109, 4.9460],
        [5.0243, 4.9070, 4.9402],
        [5.0243, 5.1130, 4.9086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.16637584567070007 
model_pd.l_d.mean(): -20.35788345336914 
model_pd.lagr.mean(): -20.19150733947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5075], device='cuda:0')), ('power', tensor([-21.0989], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.16637584567070007
epoch£º619	 i:1 	 global-step:12381	 l-p:0.14463981986045837
epoch£º619	 i:2 	 global-step:12382	 l-p:0.11998260021209717
epoch£º619	 i:3 	 global-step:12383	 l-p:0.08089938759803772
epoch£º619	 i:4 	 global-step:12384	 l-p:0.17466728389263153
epoch£º619	 i:5 	 global-step:12385	 l-p:0.10590679198503494
epoch£º619	 i:6 	 global-step:12386	 l-p:0.14270351827144623
epoch£º619	 i:7 	 global-step:12387	 l-p:0.1103440672159195
epoch£º619	 i:8 	 global-step:12388	 l-p:0.12039079517126083
epoch£º619	 i:9 	 global-step:12389	 l-p:0.1617335081100464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0737, 5.0513, 5.0698],
        [5.0737, 4.9366, 4.9525],
        [5.0737, 5.0737, 5.0737],
        [5.0737, 5.0467, 4.8129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.12853439152240753 
model_pd.l_d.mean(): -20.86421775817871 
model_pd.lagr.mean(): -20.73568344116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4287], device='cuda:0')), ('power', tensor([-21.5301], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.12853439152240753
epoch£º620	 i:1 	 global-step:12401	 l-p:0.16425222158432007
epoch£º620	 i:2 	 global-step:12402	 l-p:0.1253156214952469
epoch£º620	 i:3 	 global-step:12403	 l-p:0.09266221523284912
epoch£º620	 i:4 	 global-step:12404	 l-p:0.122222900390625
epoch£º620	 i:5 	 global-step:12405	 l-p:0.06023631989955902
epoch£º620	 i:6 	 global-step:12406	 l-p:0.14523564279079437
epoch£º620	 i:7 	 global-step:12407	 l-p:0.1306731402873993
epoch£º620	 i:8 	 global-step:12408	 l-p:0.13589218258857727
epoch£º620	 i:9 	 global-step:12409	 l-p:0.10773669183254242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0938, 5.1559, 4.9419],
        [5.0938, 5.0929, 5.0938],
        [5.0938, 5.0938, 5.0938],
        [5.0938, 5.0938, 5.0938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.14318852126598358 
model_pd.l_d.mean(): -20.862529754638672 
model_pd.lagr.mean(): -20.719341278076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4026], device='cuda:0')), ('power', tensor([-21.5018], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.14318852126598358
epoch£º621	 i:1 	 global-step:12421	 l-p:0.11941974610090256
epoch£º621	 i:2 	 global-step:12422	 l-p:0.11458851397037506
epoch£º621	 i:3 	 global-step:12423	 l-p:0.16070842742919922
epoch£º621	 i:4 	 global-step:12424	 l-p:0.16408997774124146
epoch£º621	 i:5 	 global-step:12425	 l-p:0.043982576578855515
epoch£º621	 i:6 	 global-step:12426	 l-p:0.1105118915438652
epoch£º621	 i:7 	 global-step:12427	 l-p:0.1245466023683548
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14156094193458557
epoch£º621	 i:9 	 global-step:12429	 l-p:-0.6373056173324585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1195, 5.1195, 5.1195],
        [5.1195, 4.9514, 4.9011],
        [5.1195, 5.0257, 4.8011],
        [5.1195, 4.9727, 4.9732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.1518932729959488 
model_pd.l_d.mean(): -20.75387191772461 
model_pd.lagr.mean(): -20.601978302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.4074], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.1518932729959488
epoch£º622	 i:1 	 global-step:12441	 l-p:0.12882950901985168
epoch£º622	 i:2 	 global-step:12442	 l-p:0.11779683083295822
epoch£º622	 i:3 	 global-step:12443	 l-p:0.08990132808685303
epoch£º622	 i:4 	 global-step:12444	 l-p:0.1194041296839714
epoch£º622	 i:5 	 global-step:12445	 l-p:-0.18961071968078613
epoch£º622	 i:6 	 global-step:12446	 l-p:0.14991231262683868
epoch£º622	 i:7 	 global-step:12447	 l-p:0.14168280363082886
epoch£º622	 i:8 	 global-step:12448	 l-p:0.1154070645570755
epoch£º622	 i:9 	 global-step:12449	 l-p:0.11226858198642731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0917, 5.0586, 5.0840],
        [5.0917, 4.9816, 5.0164],
        [5.0917, 5.0855, 5.0913],
        [5.0917, 5.0916, 5.0917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.14582888782024384 
model_pd.l_d.mean(): -20.288976669311523 
model_pd.lagr.mean(): -20.14314842224121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-20.9519], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.14582888782024384
epoch£º623	 i:1 	 global-step:12461	 l-p:0.12201496213674545
epoch£º623	 i:2 	 global-step:12462	 l-p:0.16014514863491058
epoch£º623	 i:3 	 global-step:12463	 l-p:0.1816553771495819
epoch£º623	 i:4 	 global-step:12464	 l-p:0.11438972502946854
epoch£º623	 i:5 	 global-step:12465	 l-p:0.11759765446186066
epoch£º623	 i:6 	 global-step:12466	 l-p:0.12465286999940872
epoch£º623	 i:7 	 global-step:12467	 l-p:0.1130807101726532
epoch£º623	 i:8 	 global-step:12468	 l-p:0.09261784702539444
epoch£º623	 i:9 	 global-step:12469	 l-p:0.11663391441106796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0576, 5.0493, 5.0569],
        [5.0576, 5.4291, 5.3824],
        [5.0576, 5.0559, 5.0575],
        [5.0576, 5.0576, 5.0576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.17063280940055847 
model_pd.l_d.mean(): -20.494670867919922 
model_pd.lagr.mean(): -20.324037551879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-21.1952], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.17063280940055847
epoch£º624	 i:1 	 global-step:12481	 l-p:0.12821346521377563
epoch£º624	 i:2 	 global-step:12482	 l-p:0.14793266355991364
epoch£º624	 i:3 	 global-step:12483	 l-p:0.07170882821083069
epoch£º624	 i:4 	 global-step:12484	 l-p:0.1724361926317215
epoch£º624	 i:5 	 global-step:12485	 l-p:0.14231982827186584
epoch£º624	 i:6 	 global-step:12486	 l-p:0.14594507217407227
epoch£º624	 i:7 	 global-step:12487	 l-p:0.0967986211180687
epoch£º624	 i:8 	 global-step:12488	 l-p:0.11161443591117859
epoch£º624	 i:9 	 global-step:12489	 l-p:0.16170430183410645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0332, 5.0332, 5.0332],
        [5.0332, 5.0209, 5.0318],
        [5.0332, 5.0320, 5.0331],
        [5.0332, 4.8566, 4.7052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.16624297201633453 
model_pd.l_d.mean(): -20.33502769470215 
model_pd.lagr.mean(): -20.168785095214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-21.0609], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.16624297201633453
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09927651286125183
epoch£º625	 i:2 	 global-step:12502	 l-p:0.1085226908326149
epoch£º625	 i:3 	 global-step:12503	 l-p:0.13781391084194183
epoch£º625	 i:4 	 global-step:12504	 l-p:0.16101598739624023
epoch£º625	 i:5 	 global-step:12505	 l-p:0.1625451296567917
epoch£º625	 i:6 	 global-step:12506	 l-p:0.13261950016021729
epoch£º625	 i:7 	 global-step:12507	 l-p:0.13115698099136353
epoch£º625	 i:8 	 global-step:12508	 l-p:0.1305895447731018
epoch£º625	 i:9 	 global-step:12509	 l-p:0.14044100046157837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0620, 5.0209, 5.0507],
        [5.0620, 4.8873, 4.8343],
        [5.0620, 5.0620, 5.0620],
        [5.0620, 5.4533, 5.4193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.07777740061283112 
model_pd.l_d.mean(): -20.843952178955078 
model_pd.lagr.mean(): -20.76617431640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194], device='cuda:0')), ('power', tensor([-21.5002], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.07777740061283112
epoch£º626	 i:1 	 global-step:12521	 l-p:0.1710340529680252
epoch£º626	 i:2 	 global-step:12522	 l-p:0.12825696170330048
epoch£º626	 i:3 	 global-step:12523	 l-p:0.1379138082265854
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10612580925226212
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11260190606117249
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10017851740121841
epoch£º626	 i:7 	 global-step:12527	 l-p:0.15612462162971497
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11870173364877701
epoch£º626	 i:9 	 global-step:12529	 l-p:0.12971271574497223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1231, 5.0303, 5.0698],
        [5.1231, 4.9570, 4.8023],
        [5.1231, 5.0373, 5.0776],
        [5.1231, 5.0922, 5.1163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.10512088239192963 
model_pd.l_d.mean(): -20.74380111694336 
model_pd.lagr.mean(): -20.63867950439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4055], device='cuda:0')), ('power', tensor([-21.3847], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.10512088239192963
epoch£º627	 i:1 	 global-step:12541	 l-p:0.08073348551988602
epoch£º627	 i:2 	 global-step:12542	 l-p:0.15145954489707947
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11428874731063843
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12618470191955566
epoch£º627	 i:5 	 global-step:12545	 l-p:0.12062888592481613
epoch£º627	 i:6 	 global-step:12546	 l-p:0.147714301943779
epoch£º627	 i:7 	 global-step:12547	 l-p:-0.00921487808227539
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15244054794311523
epoch£º627	 i:9 	 global-step:12549	 l-p:0.146913543343544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0834, 4.9039, 4.8096],
        [5.0834, 5.0830, 5.0834],
        [5.0834, 5.0825, 5.0833],
        [5.0834, 5.0196, 5.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.041374366730451584 
model_pd.l_d.mean(): -20.77775001525879 
model_pd.lagr.mean(): -20.73637580871582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4210], device='cuda:0')), ('power', tensor([-21.4349], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.041374366730451584
epoch£º628	 i:1 	 global-step:12561	 l-p:0.14310906827449799
epoch£º628	 i:2 	 global-step:12562	 l-p:0.1720455139875412
epoch£º628	 i:3 	 global-step:12563	 l-p:0.16645750403404236
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10136887431144714
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14098243415355682
epoch£º628	 i:6 	 global-step:12566	 l-p:0.1300978809595108
epoch£º628	 i:7 	 global-step:12567	 l-p:0.1237388551235199
epoch£º628	 i:8 	 global-step:12568	 l-p:0.16476181149482727
epoch£º628	 i:9 	 global-step:12569	 l-p:0.11225783824920654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0244, 5.0235, 5.0244],
        [5.0244, 4.8466, 4.7983],
        [5.0244, 5.0199, 5.0241],
        [5.0244, 5.2227, 5.0694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.1474675089120865 
model_pd.l_d.mean(): -21.058317184448242 
model_pd.lagr.mean(): -20.910850524902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3995], device='cuda:0')), ('power', tensor([-21.6966], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.1474675089120865
epoch£º629	 i:1 	 global-step:12581	 l-p:0.13984601199626923
epoch£º629	 i:2 	 global-step:12582	 l-p:0.13302896916866302
epoch£º629	 i:3 	 global-step:12583	 l-p:0.12953054904937744
epoch£º629	 i:4 	 global-step:12584	 l-p:0.24015535414218903
epoch£º629	 i:5 	 global-step:12585	 l-p:0.0998806431889534
epoch£º629	 i:6 	 global-step:12586	 l-p:0.1188022568821907
epoch£º629	 i:7 	 global-step:12587	 l-p:0.1222396269440651
epoch£º629	 i:8 	 global-step:12588	 l-p:0.28506702184677124
epoch£º629	 i:9 	 global-step:12589	 l-p:0.19521045684814453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9880, 4.9880, 4.9880],
        [4.9880, 4.8071, 4.7596],
        [4.9880, 4.7980, 4.6864],
        [4.9880, 4.9880, 4.9880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.17650845646858215 
model_pd.l_d.mean(): -20.074787139892578 
model_pd.lagr.mean(): -19.898279190063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5086], device='cuda:0')), ('power', tensor([-20.8137], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.17650845646858215
epoch£º630	 i:1 	 global-step:12601	 l-p:0.2896820902824402
epoch£º630	 i:2 	 global-step:12602	 l-p:0.2496546506881714
epoch£º630	 i:3 	 global-step:12603	 l-p:0.11184264719486237
epoch£º630	 i:4 	 global-step:12604	 l-p:0.14214952290058136
epoch£º630	 i:5 	 global-step:12605	 l-p:0.143483504652977
epoch£º630	 i:6 	 global-step:12606	 l-p:0.10741820931434631
epoch£º630	 i:7 	 global-step:12607	 l-p:0.12323980033397675
epoch£º630	 i:8 	 global-step:12608	 l-p:0.14103883504867554
epoch£º630	 i:9 	 global-step:12609	 l-p:0.12799173593521118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0176, 5.0011, 5.0153],
        [5.0176, 4.9498, 4.7076],
        [5.0176, 5.0176, 5.0176],
        [5.0176, 4.8922, 4.9235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.12176891416311264 
model_pd.l_d.mean(): -21.003610610961914 
model_pd.lagr.mean(): -20.8818416595459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4064], device='cuda:0')), ('power', tensor([-21.6483], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.12176891416311264
epoch£º631	 i:1 	 global-step:12621	 l-p:0.13363949954509735
epoch£º631	 i:2 	 global-step:12622	 l-p:0.16895291209220886
epoch£º631	 i:3 	 global-step:12623	 l-p:0.16592290997505188
epoch£º631	 i:4 	 global-step:12624	 l-p:0.24469079077243805
epoch£º631	 i:5 	 global-step:12625	 l-p:0.22549742460250854
epoch£º631	 i:6 	 global-step:12626	 l-p:0.09962853044271469
epoch£º631	 i:7 	 global-step:12627	 l-p:0.12749028205871582
epoch£º631	 i:8 	 global-step:12628	 l-p:0.13139598071575165
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12971118092536926
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0111, 4.9572, 4.9930],
        [5.0111, 4.9944, 5.0088],
        [5.0111, 5.0060, 5.0108],
        [5.0111, 5.0070, 5.0109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.14027753472328186 
model_pd.l_d.mean(): -19.509140014648438 
model_pd.lagr.mean(): -19.36886215209961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.2454], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.14027753472328186
epoch£º632	 i:1 	 global-step:12641	 l-p:0.16518431901931763
epoch£º632	 i:2 	 global-step:12642	 l-p:0.13272735476493835
epoch£º632	 i:3 	 global-step:12643	 l-p:0.1531629115343094
epoch£º632	 i:4 	 global-step:12644	 l-p:0.16129133105278015
epoch£º632	 i:5 	 global-step:12645	 l-p:0.1375267505645752
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12752792239189148
epoch£º632	 i:7 	 global-step:12647	 l-p:0.3254833221435547
epoch£º632	 i:8 	 global-step:12648	 l-p:0.12255101650953293
epoch£º632	 i:9 	 global-step:12649	 l-p:0.26056772470474243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9773, 4.9631, 4.9756],
        [4.9773, 4.7869, 4.6515],
        [4.9773, 4.8671, 4.9067],
        [4.9773, 5.5057, 5.5720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.1393308788537979 
model_pd.l_d.mean(): -19.877822875976562 
model_pd.lagr.mean(): -19.73849105834961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5224], device='cuda:0')), ('power', tensor([-20.6287], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.1393308788537979
epoch£º633	 i:1 	 global-step:12661	 l-p:0.19882862269878387
epoch£º633	 i:2 	 global-step:12662	 l-p:0.13915669918060303
epoch£º633	 i:3 	 global-step:12663	 l-p:0.2610151469707489
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12815764546394348
epoch£º633	 i:5 	 global-step:12665	 l-p:0.8166537880897522
epoch£º633	 i:6 	 global-step:12666	 l-p:0.15942005813121796
epoch£º633	 i:7 	 global-step:12667	 l-p:0.22116710245609283
epoch£º633	 i:8 	 global-step:12668	 l-p:0.16097599267959595
epoch£º633	 i:9 	 global-step:12669	 l-p:0.10382930189371109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0050, 5.0050, 5.0050],
        [5.0050, 5.0012, 5.0048],
        [5.0050, 4.9039, 4.9451],
        [5.0050, 4.9135, 4.9559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.1274678260087967 
model_pd.l_d.mean(): -20.08669662475586 
model_pd.lagr.mean(): -19.959228515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5132], device='cuda:0')), ('power', tensor([-20.8305], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.1274678260087967
epoch£º634	 i:1 	 global-step:12681	 l-p:0.2150450497865677
epoch£º634	 i:2 	 global-step:12682	 l-p:0.18722467124462128
epoch£º634	 i:3 	 global-step:12683	 l-p:0.17131052911281586
epoch£º634	 i:4 	 global-step:12684	 l-p:0.07008923590183258
epoch£º634	 i:5 	 global-step:12685	 l-p:0.15364506840705872
epoch£º634	 i:6 	 global-step:12686	 l-p:0.14425265789031982
epoch£º634	 i:7 	 global-step:12687	 l-p:0.07857443392276764
epoch£º634	 i:8 	 global-step:12688	 l-p:0.11542545258998871
epoch£º634	 i:9 	 global-step:12689	 l-p:0.1425260603427887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0884, 4.9462, 4.9602],
        [5.0884, 4.9555, 4.9786],
        [5.0884, 5.0875, 5.0884],
        [5.0884, 4.9621, 4.7424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.1449252814054489 
model_pd.l_d.mean(): -20.624162673950195 
model_pd.lagr.mean(): -20.479236602783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-21.3135], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.1449252814054489
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13534528017044067
epoch£º635	 i:2 	 global-step:12702	 l-p:0.11621059477329254
epoch£º635	 i:3 	 global-step:12703	 l-p:0.1430843323469162
epoch£º635	 i:4 	 global-step:12704	 l-p:0.07263165712356567
epoch£º635	 i:5 	 global-step:12705	 l-p:0.10909447073936462
epoch£º635	 i:6 	 global-step:12706	 l-p:0.15109078586101532
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12450434267520905
epoch£º635	 i:8 	 global-step:12708	 l-p:0.1131131500005722
epoch£º635	 i:9 	 global-step:12709	 l-p:-0.3248171806335449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1272, 5.0689, 5.1058],
        [5.1272, 5.1101, 5.1248],
        [5.1272, 5.0795, 4.8427],
        [5.1272, 5.1271, 5.1272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11688964813947678 
model_pd.l_d.mean(): -20.502033233642578 
model_pd.lagr.mean(): -20.385143280029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4252], device='cuda:0')), ('power', tensor([-21.1605], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11688964813947678
epoch£º636	 i:1 	 global-step:12721	 l-p:0.12238412350416183
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10571125149726868
epoch£º636	 i:3 	 global-step:12723	 l-p:0.1226242259144783
epoch£º636	 i:4 	 global-step:12724	 l-p:0.07392564415931702
epoch£º636	 i:5 	 global-step:12725	 l-p:0.0761648416519165
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13439546525478363
epoch£º636	 i:7 	 global-step:12727	 l-p:0.035385798662900925
epoch£º636	 i:8 	 global-step:12728	 l-p:0.18618662655353546
epoch£º636	 i:9 	 global-step:12729	 l-p:0.1620721071958542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1016, 4.9337, 4.9039],
        [5.1016, 5.6692, 5.7588],
        [5.1016, 4.9773, 5.0065],
        [5.1016, 5.0851, 5.0993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15946801006793976 
model_pd.l_d.mean(): -19.469093322753906 
model_pd.lagr.mean(): -19.30962562561035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.1811], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15946801006793976
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12763215601444244
epoch£º637	 i:2 	 global-step:12742	 l-p:0.1006719097495079
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14608517289161682
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11841341853141785
epoch£º637	 i:5 	 global-step:12745	 l-p:0.1253305971622467
epoch£º637	 i:6 	 global-step:12746	 l-p:0.022495245561003685
epoch£º637	 i:7 	 global-step:12747	 l-p:0.12514273822307587
epoch£º637	 i:8 	 global-step:12748	 l-p:0.17234988510608673
epoch£º637	 i:9 	 global-step:12749	 l-p:0.1013653352856636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0830, 5.0827, 5.0830],
        [5.0830, 5.0830, 5.0830],
        [5.0830, 5.2933, 5.1439],
        [5.0830, 5.4964, 5.4748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.142240509390831 
model_pd.l_d.mean(): -20.424522399902344 
model_pd.lagr.mean(): -20.28228187561035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-21.1631], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.142240509390831
epoch£º638	 i:1 	 global-step:12761	 l-p:0.09016351401805878
epoch£º638	 i:2 	 global-step:12762	 l-p:0.05838513374328613
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12305301427841187
epoch£º638	 i:4 	 global-step:12764	 l-p:0.1432138830423355
epoch£º638	 i:5 	 global-step:12765	 l-p:0.11045677214860916
epoch£º638	 i:6 	 global-step:12766	 l-p:0.1605505347251892
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14847666025161743
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14483417570590973
epoch£º638	 i:9 	 global-step:12769	 l-p:0.13182710111141205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0817, 5.5295, 5.5317],
        [5.0817, 5.0779, 5.0815],
        [5.0817, 4.9023, 4.8407],
        [5.0817, 4.9090, 4.8720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.10439085960388184 
model_pd.l_d.mean(): -18.345684051513672 
model_pd.lagr.mean(): -18.24129295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6341], device='cuda:0')), ('power', tensor([-19.1940], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.10439085960388184
epoch£º639	 i:1 	 global-step:12781	 l-p:0.1851673126220703
epoch£º639	 i:2 	 global-step:12782	 l-p:0.1289670765399933
epoch£º639	 i:3 	 global-step:12783	 l-p:0.0879739373922348
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13951453566551208
epoch£º639	 i:5 	 global-step:12785	 l-p:0.06920330226421356
epoch£º639	 i:6 	 global-step:12786	 l-p:0.1399405300617218
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14023850858211517
epoch£º639	 i:8 	 global-step:12788	 l-p:3.2374093532562256
epoch£º639	 i:9 	 global-step:12789	 l-p:0.1358538269996643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1389, 4.9661, 4.9156],
        [5.1389, 4.9632, 4.8279],
        [5.1389, 5.1389, 5.1389],
        [5.1389, 5.1217, 5.1364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.0969221442937851 
model_pd.l_d.mean(): -20.041053771972656 
model_pd.lagr.mean(): -19.94413185119629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5196], device='cuda:0')), ('power', tensor([-20.7909], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.0969221442937851
epoch£º640	 i:1 	 global-step:12801	 l-p:0.14039210975170135
epoch£º640	 i:2 	 global-step:12802	 l-p:-1.5017226934432983
epoch£º640	 i:3 	 global-step:12803	 l-p:0.08775510638952255
epoch£º640	 i:4 	 global-step:12804	 l-p:0.16423314809799194
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11780422180891037
epoch£º640	 i:6 	 global-step:12806	 l-p:0.12498420476913452
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12060264497995377
epoch£º640	 i:8 	 global-step:12808	 l-p:0.12512297928333282
epoch£º640	 i:9 	 global-step:12809	 l-p:0.13761353492736816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1340, 5.1326, 5.1340],
        [5.1340, 5.1047, 4.8671],
        [5.1340, 5.1340, 5.1340],
        [5.1340, 5.0754, 5.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): -0.004992417059838772 
model_pd.l_d.mean(): -20.63368034362793 
model_pd.lagr.mean(): -20.638671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-21.3134], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:-0.004992417059838772
epoch£º641	 i:1 	 global-step:12821	 l-p:0.08982931077480316
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10589992254972458
epoch£º641	 i:3 	 global-step:12823	 l-p:0.10719431936740875
epoch£º641	 i:4 	 global-step:12824	 l-p:0.1485588103532791
epoch£º641	 i:5 	 global-step:12825	 l-p:0.17367877066135406
epoch£º641	 i:6 	 global-step:12826	 l-p:0.13326668739318848
epoch£º641	 i:7 	 global-step:12827	 l-p:-0.1886679083108902
epoch£º641	 i:8 	 global-step:12828	 l-p:0.13151240348815918
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13515274226665497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1072, 5.1070, 5.1072],
        [5.1072, 4.9351, 4.8977],
        [5.1072, 5.1072, 5.1072],
        [5.1072, 5.5849, 5.6066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): -0.025774385780096054 
model_pd.l_d.mean(): -19.193220138549805 
model_pd.lagr.mean(): -19.218994140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5649], device='cuda:0')), ('power', tensor([-19.9801], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:-0.025774385780096054
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15666864812374115
epoch£º642	 i:2 	 global-step:12842	 l-p:0.18225635588169098
epoch£º642	 i:3 	 global-step:12843	 l-p:0.1303362101316452
epoch£º642	 i:4 	 global-step:12844	 l-p:0.09479783475399017
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14474400877952576
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12890465557575226
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11409173905849457
epoch£º642	 i:8 	 global-step:12848	 l-p:0.09639894217252731
epoch£º642	 i:9 	 global-step:12849	 l-p:0.18321412801742554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0390, 4.9732, 5.0130],
        [5.0390, 5.0390, 5.0390],
        [5.0390, 4.9134, 4.6824],
        [5.0390, 5.0390, 5.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.10268311202526093 
model_pd.l_d.mean(): -19.040071487426758 
model_pd.lagr.mean(): -18.937387466430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-19.8226], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.10268311202526093
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12238091975450516
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13260547816753387
epoch£º643	 i:3 	 global-step:12863	 l-p:0.1093544140458107
epoch£º643	 i:4 	 global-step:12864	 l-p:0.23591910302639008
epoch£º643	 i:5 	 global-step:12865	 l-p:0.10323429107666016
epoch£º643	 i:6 	 global-step:12866	 l-p:0.18527324497699738
epoch£º643	 i:7 	 global-step:12867	 l-p:0.1237606406211853
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13791726529598236
epoch£º643	 i:9 	 global-step:12869	 l-p:0.15760311484336853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0555, 4.8673, 4.7748],
        [5.0555, 4.9320, 4.7013],
        [5.0555, 4.8778, 4.8374],
        [5.0555, 4.9583, 5.0001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.1508674919605255 
model_pd.l_d.mean(): -20.281837463378906 
model_pd.lagr.mean(): -20.130970001220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-21.0154], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.1508674919605255
epoch£º644	 i:1 	 global-step:12881	 l-p:0.18247196078300476
epoch£º644	 i:2 	 global-step:12882	 l-p:0.14642538130283356
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12806569039821625
epoch£º644	 i:4 	 global-step:12884	 l-p:0.1600572019815445
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14772050082683563
epoch£º644	 i:6 	 global-step:12886	 l-p:0.13331247866153717
epoch£º644	 i:7 	 global-step:12887	 l-p:0.03828642889857292
epoch£º644	 i:8 	 global-step:12888	 l-p:0.06884356588125229
epoch£º644	 i:9 	 global-step:12889	 l-p:0.06477415561676025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1138, 5.0571, 5.0938],
        [5.1138, 4.9508, 4.9365],
        [5.1138, 5.0949, 4.8558],
        [5.1138, 5.1107, 5.1137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14512884616851807 
model_pd.l_d.mean(): -20.061559677124023 
model_pd.lagr.mean(): -19.916431427001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.7909], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14512884616851807
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.07193484902381897
epoch£º645	 i:2 	 global-step:12902	 l-p:0.11723380535840988
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11239675432443619
epoch£º645	 i:4 	 global-step:12904	 l-p:0.1222492977976799
epoch£º645	 i:5 	 global-step:12905	 l-p:0.016135891899466515
epoch£º645	 i:6 	 global-step:12906	 l-p:0.1420460343360901
epoch£º645	 i:7 	 global-step:12907	 l-p:0.0797123908996582
epoch£º645	 i:8 	 global-step:12908	 l-p:0.10061473399400711
epoch£º645	 i:9 	 global-step:12909	 l-p:0.12579487264156342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1841, 5.1177, 4.8817],
        [5.1841, 5.6816, 5.7133],
        [5.1841, 5.7218, 5.7831],
        [5.1841, 5.0185, 4.9877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10145285725593567 
model_pd.l_d.mean(): -19.39392852783203 
model_pd.lagr.mean(): -19.2924747467041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5250], device='cuda:0')), ('power', tensor([-20.1422], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10145285725593567
epoch£º646	 i:1 	 global-step:12921	 l-p:0.15863175690174103
epoch£º646	 i:2 	 global-step:12922	 l-p:0.124461829662323
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13525065779685974
epoch£º646	 i:4 	 global-step:12924	 l-p:2.9240167140960693
epoch£º646	 i:5 	 global-step:12925	 l-p:0.03983799368143082
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12056326866149902
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12390214949846268
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13409756124019623
epoch£º646	 i:9 	 global-step:12929	 l-p:0.19667857885360718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1618, 5.1618, 5.1618],
        [5.1618, 5.1615, 5.1618],
        [5.1618, 5.0490, 4.8238],
        [5.1618, 5.1615, 5.1618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.13504579663276672 
model_pd.l_d.mean(): -20.067209243774414 
model_pd.lagr.mean(): -19.93216323852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5025], device='cuda:0')), ('power', tensor([-20.7999], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.13504579663276672
epoch£º647	 i:1 	 global-step:12941	 l-p:0.13078537583351135
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12373054027557373
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12052212655544281
epoch£º647	 i:4 	 global-step:12944	 l-p:0.1097436472773552
epoch£º647	 i:5 	 global-step:12945	 l-p:0.1276129186153412
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.04596959426999092
epoch£º647	 i:7 	 global-step:12947	 l-p:0.09810229390859604
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14630456268787384
epoch£º647	 i:9 	 global-step:12949	 l-p:0.08916914463043213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0852, 5.0637, 4.8221],
        [5.0852, 5.0852, 5.0852],
        [5.0852, 5.0481, 5.0760],
        [5.0852, 5.0807, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.1172923743724823 
model_pd.l_d.mean(): -20.784032821655273 
model_pd.lagr.mean(): -20.66674041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4365], device='cuda:0')), ('power', tensor([-21.4571], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.1172923743724823
epoch£º648	 i:1 	 global-step:12961	 l-p:0.12693198025226593
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17048461735248566
epoch£º648	 i:3 	 global-step:12963	 l-p:0.14217375218868256
epoch£º648	 i:4 	 global-step:12964	 l-p:0.17877425253391266
epoch£º648	 i:5 	 global-step:12965	 l-p:0.14373405277729034
epoch£º648	 i:6 	 global-step:12966	 l-p:0.13191621005535126
epoch£º648	 i:7 	 global-step:12967	 l-p:0.09875638782978058
epoch£º648	 i:8 	 global-step:12968	 l-p:0.12057523429393768
epoch£º648	 i:9 	 global-step:12969	 l-p:0.09674712270498276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0661, 5.0296, 5.0572],
        [5.0661, 5.0632, 5.0660],
        [5.0661, 5.0639, 5.0660],
        [5.0661, 4.8876, 4.8472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12285812199115753 
model_pd.l_d.mean(): -20.087081909179688 
model_pd.lagr.mean(): -19.964223861694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765], device='cuda:0')), ('power', tensor([-20.7933], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12285812199115753
epoch£º649	 i:1 	 global-step:12981	 l-p:0.12194491922855377
epoch£º649	 i:2 	 global-step:12982	 l-p:0.13042159378528595
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15181408822536469
epoch£º649	 i:4 	 global-step:12984	 l-p:0.18632987141609192
epoch£º649	 i:5 	 global-step:12985	 l-p:0.13437971472740173
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13339029252529144
epoch£º649	 i:7 	 global-step:12987	 l-p:0.15542210638523102
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06533806771039963
epoch£º649	 i:9 	 global-step:12989	 l-p:0.22687922418117523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0298, 4.9368, 4.9799],
        [5.0298, 4.9822, 5.0156],
        [5.0298, 4.8684, 4.6591],
        [5.0298, 4.8817, 4.8984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.1105707585811615 
model_pd.l_d.mean(): -20.732004165649414 
model_pd.lagr.mean(): -20.62143325805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4549], device='cuda:0')), ('power', tensor([-21.4233], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.1105707585811615
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11358451843261719
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16827644407749176
epoch£º650	 i:3 	 global-step:13003	 l-p:0.06797593832015991
epoch£º650	 i:4 	 global-step:13004	 l-p:0.14026030898094177
epoch£º650	 i:5 	 global-step:13005	 l-p:0.15505748987197876
epoch£º650	 i:6 	 global-step:13006	 l-p:0.1545220911502838
epoch£º650	 i:7 	 global-step:13007	 l-p:0.1546095609664917
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14926454424858093
epoch£º650	 i:9 	 global-step:13009	 l-p:0.1543417125940323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0685, 5.0685, 5.0685],
        [5.0685, 5.5076, 5.5020],
        [5.0685, 5.0685, 5.0685],
        [5.0685, 4.9012, 4.8879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.15914925932884216 
model_pd.l_d.mean(): -20.302640914916992 
model_pd.lagr.mean(): -20.143491744995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-21.0324], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.15914925932884216
epoch£º651	 i:1 	 global-step:13021	 l-p:0.11834640055894852
epoch£º651	 i:2 	 global-step:13022	 l-p:0.08383271843194962
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11798739433288574
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12812966108322144
epoch£º651	 i:5 	 global-step:13025	 l-p:0.1367141753435135
epoch£º651	 i:6 	 global-step:13026	 l-p:0.1596307009458542
epoch£º651	 i:7 	 global-step:13027	 l-p:0.18088886141777039
epoch£º651	 i:8 	 global-step:13028	 l-p:0.23192940652370453
epoch£º651	 i:9 	 global-step:13029	 l-p:0.09917113929986954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0360, 4.9253, 4.9656],
        [5.0360, 5.0244, 5.0348],
        [5.0360, 5.0360, 5.0360],
        [5.0360, 5.0347, 5.0359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.10554540157318115 
model_pd.l_d.mean(): -20.563068389892578 
model_pd.lagr.mean(): -20.457523345947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4515], device='cuda:0')), ('power', tensor([-21.2491], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.10554540157318115
epoch£º652	 i:1 	 global-step:13041	 l-p:0.15851673483848572
epoch£º652	 i:2 	 global-step:13042	 l-p:0.12516215443611145
epoch£º652	 i:3 	 global-step:13043	 l-p:0.15565600991249084
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08160045742988586
epoch£º652	 i:5 	 global-step:13045	 l-p:0.17940931022167206
epoch£º652	 i:6 	 global-step:13046	 l-p:0.15210296213626862
epoch£º652	 i:7 	 global-step:13047	 l-p:0.15734988451004028
epoch£º652	 i:8 	 global-step:13048	 l-p:0.12577533721923828
epoch£º652	 i:9 	 global-step:13049	 l-p:0.10276445746421814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1145, 5.2460, 5.0526],
        [5.1145, 5.0209, 4.7812],
        [5.1145, 5.2538, 5.0641],
        [5.1145, 5.0489, 5.0885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.16652366518974304 
model_pd.l_d.mean(): -20.181838989257812 
model_pd.lagr.mean(): -20.015316009521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.8905], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.16652366518974304
epoch£º653	 i:1 	 global-step:13061	 l-p:0.1195899248123169
epoch£º653	 i:2 	 global-step:13062	 l-p:0.14753089845180511
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12538105249404907
epoch£º653	 i:4 	 global-step:13064	 l-p:0.14095881581306458
epoch£º653	 i:5 	 global-step:13065	 l-p:0.006751599255949259
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08472929149866104
epoch£º653	 i:7 	 global-step:13067	 l-p:0.12988866865634918
epoch£º653	 i:8 	 global-step:13068	 l-p:0.11737906187772751
epoch£º653	 i:9 	 global-step:13069	 l-p:0.06686057895421982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1328, 5.2473, 5.0461],
        [5.1328, 4.9790, 4.9823],
        [5.1328, 5.1328, 5.1328],
        [5.1328, 4.9620, 4.9340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12374419718980789 
model_pd.l_d.mean(): -19.661678314208984 
model_pd.lagr.mean(): -19.537933349609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-20.3774], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12374419718980789
epoch£º654	 i:1 	 global-step:13081	 l-p:0.052581191062927246
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14206910133361816
epoch£º654	 i:3 	 global-step:13083	 l-p:0.08663852512836456
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13797293603420258
epoch£º654	 i:5 	 global-step:13085	 l-p:0.07298450917005539
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12874735891819
epoch£º654	 i:7 	 global-step:13087	 l-p:0.11281163990497589
epoch£º654	 i:8 	 global-step:13088	 l-p:0.30054596066474915
epoch£º654	 i:9 	 global-step:13089	 l-p:0.1369948387145996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1561, 4.9756, 4.8367],
        [5.1561, 5.0041, 4.8029],
        [5.1561, 4.9820, 4.9409],
        [5.1561, 5.0313, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11580970138311386 
model_pd.l_d.mean(): -20.544931411743164 
model_pd.lagr.mean(): -20.429121017456055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.2113], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11580970138311386
epoch£º655	 i:1 	 global-step:13101	 l-p:0.06421397626399994
epoch£º655	 i:2 	 global-step:13102	 l-p:0.14323337376117706
epoch£º655	 i:3 	 global-step:13103	 l-p:0.17680984735488892
epoch£º655	 i:4 	 global-step:13104	 l-p:0.04879068210721016
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12996409833431244
epoch£º655	 i:6 	 global-step:13106	 l-p:0.13092471659183502
epoch£º655	 i:7 	 global-step:13107	 l-p:0.1065620556473732
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.1167113184928894
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12354480475187302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1161, 5.0959, 5.1130],
        [5.1161, 5.0840, 5.1091],
        [5.1161, 5.1015, 5.1143],
        [5.1161, 5.0110, 4.7732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11827243864536285 
model_pd.l_d.mean(): -20.598520278930664 
model_pd.lagr.mean(): -20.480247497558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.2667], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11827243864536285
epoch£º656	 i:1 	 global-step:13121	 l-p:0.13075374066829681
epoch£º656	 i:2 	 global-step:13122	 l-p:0.05194179341197014
epoch£º656	 i:3 	 global-step:13123	 l-p:0.1360481083393097
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13973216712474823
epoch£º656	 i:5 	 global-step:13125	 l-p:0.22191490232944489
epoch£º656	 i:6 	 global-step:13126	 l-p:0.1468527466058731
epoch£º656	 i:7 	 global-step:13127	 l-p:0.06065548583865166
epoch£º656	 i:8 	 global-step:13128	 l-p:0.14251503348350525
epoch£º656	 i:9 	 global-step:13129	 l-p:0.12129926681518555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0776, 4.9017, 4.8730],
        [5.0776, 4.8862, 4.7636],
        [5.0776, 5.0651, 4.8217],
        [5.0776, 4.9820, 4.7378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.1417079120874405 
model_pd.l_d.mean(): -20.439838409423828 
model_pd.lagr.mean(): -20.29813003540039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-21.1547], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.1417079120874405
epoch£º657	 i:1 	 global-step:13141	 l-p:0.09103531390428543
epoch£º657	 i:2 	 global-step:13142	 l-p:0.17346306145191193
epoch£º657	 i:3 	 global-step:13143	 l-p:0.1425638496875763
epoch£º657	 i:4 	 global-step:13144	 l-p:0.11905530840158463
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10172268003225327
epoch£º657	 i:6 	 global-step:13146	 l-p:0.1383209526538849
epoch£º657	 i:7 	 global-step:13147	 l-p:0.12502411007881165
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1434657871723175
epoch£º657	 i:9 	 global-step:13149	 l-p:0.01188984327018261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1146, 5.1146, 5.1146],
        [5.1146, 4.9301, 4.7825],
        [5.1146, 4.9321, 4.8759],
        [5.1146, 4.9283, 4.8532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.0768480971455574 
model_pd.l_d.mean(): -20.224533081054688 
model_pd.lagr.mean(): -20.14768409729004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4831], device='cuda:0')), ('power', tensor([-20.9391], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.0768480971455574
epoch£º658	 i:1 	 global-step:13161	 l-p:0.1497703194618225
epoch£º658	 i:2 	 global-step:13162	 l-p:0.16247940063476562
epoch£º658	 i:3 	 global-step:13163	 l-p:0.10360722243785858
epoch£º658	 i:4 	 global-step:13164	 l-p:0.13812527060508728
epoch£º658	 i:5 	 global-step:13165	 l-p:0.10827569663524628
epoch£º658	 i:6 	 global-step:13166	 l-p:-0.0004630565526895225
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12783406674861908
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14914624392986298
epoch£º658	 i:9 	 global-step:13169	 l-p:0.12440978735685349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1170, 5.0956, 4.8520],
        [5.1170, 5.1167, 5.1170],
        [5.1170, 4.9283, 4.8130],
        [5.1170, 5.1161, 5.1170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12217947095632553 
model_pd.l_d.mean(): -20.48119354248047 
model_pd.lagr.mean(): -20.3590145111084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.1372], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12217947095632553
epoch£º659	 i:1 	 global-step:13181	 l-p:0.11014232039451599
epoch£º659	 i:2 	 global-step:13182	 l-p:0.16223818063735962
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13316890597343445
epoch£º659	 i:4 	 global-step:13184	 l-p:0.11800399422645569
epoch£º659	 i:5 	 global-step:13185	 l-p:0.13195666670799255
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10901132225990295
epoch£º659	 i:7 	 global-step:13187	 l-p:0.13012030720710754
epoch£º659	 i:8 	 global-step:13188	 l-p:0.19782352447509766
epoch£º659	 i:9 	 global-step:13189	 l-p:0.1327056884765625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0368, 5.2185, 5.0504],
        [5.0368, 4.9189, 4.9578],
        [5.0368, 5.0174, 4.7701],
        [5.0368, 5.0367, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14523547887802124 
model_pd.l_d.mean(): -19.68292999267578 
model_pd.lagr.mean(): -19.537694931030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5670], device='cuda:0')), ('power', tensor([-20.4773], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14523547887802124
epoch£º660	 i:1 	 global-step:13201	 l-p:0.1613161861896515
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13792361319065094
epoch£º660	 i:3 	 global-step:13203	 l-p:0.12709225714206696
epoch£º660	 i:4 	 global-step:13204	 l-p:0.16960129141807556
epoch£º660	 i:5 	 global-step:13205	 l-p:0.1320253163576126
epoch£º660	 i:6 	 global-step:13206	 l-p:0.1156255379319191
epoch£º660	 i:7 	 global-step:13207	 l-p:0.12448032200336456
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12503120303153992
epoch£º660	 i:9 	 global-step:13209	 l-p:0.21260663866996765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0438, 5.0428, 5.0437],
        [5.0438, 4.9435, 4.9868],
        [5.0438, 5.0172, 5.0388],
        [5.0438, 4.8988, 4.9208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13810907304286957 
model_pd.l_d.mean(): -19.97213363647461 
model_pd.lagr.mean(): -19.83402442932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.6921], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13810907304286957
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13293252885341644
epoch£º661	 i:2 	 global-step:13222	 l-p:0.15509141981601715
epoch£º661	 i:3 	 global-step:13223	 l-p:0.1942276954650879
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11442286521196365
epoch£º661	 i:5 	 global-step:13225	 l-p:0.11053180694580078
epoch£º661	 i:6 	 global-step:13226	 l-p:0.13521043956279755
epoch£º661	 i:7 	 global-step:13227	 l-p:0.17141887545585632
epoch£º661	 i:8 	 global-step:13228	 l-p:0.10865125060081482
epoch£º661	 i:9 	 global-step:13229	 l-p:0.1865200400352478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0475, 5.0475, 5.0475],
        [5.0475, 5.0475, 5.0475],
        [5.0475, 5.0257, 5.0440],
        [5.0475, 4.8755, 4.8631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13610270619392395 
model_pd.l_d.mean(): -20.906444549560547 
model_pd.lagr.mean(): -20.770341873168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4073], device='cuda:0')), ('power', tensor([-21.5510], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13610270619392395
epoch£º662	 i:1 	 global-step:13241	 l-p:0.10303600132465363
epoch£º662	 i:2 	 global-step:13242	 l-p:0.1495642364025116
epoch£º662	 i:3 	 global-step:13243	 l-p:0.0994366928935051
epoch£º662	 i:4 	 global-step:13244	 l-p:0.10692708939313889
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12826459109783173
epoch£º662	 i:6 	 global-step:13246	 l-p:0.15564216673374176
epoch£º662	 i:7 	 global-step:13247	 l-p:0.15239474177360535
epoch£º662	 i:8 	 global-step:13248	 l-p:0.14420825242996216
epoch£º662	 i:9 	 global-step:13249	 l-p:0.147262841463089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0873, 5.2128, 5.0149],
        [5.0873, 5.0277, 5.0660],
        [5.0873, 4.8940, 4.7704],
        [5.0873, 5.0745, 5.0859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.1282341033220291 
model_pd.l_d.mean(): -20.588830947875977 
model_pd.lagr.mean(): -20.460596084594727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-21.2529], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.1282341033220291
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11580440402030945
epoch£º663	 i:2 	 global-step:13262	 l-p:0.08357356488704681
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10859306156635284
epoch£º663	 i:4 	 global-step:13264	 l-p:0.13601578772068024
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19909729063510895
epoch£º663	 i:6 	 global-step:13266	 l-p:0.17160087823867798
epoch£º663	 i:7 	 global-step:13267	 l-p:0.1022753119468689
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10515796393156052
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12465322017669678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0922, 5.0922, 5.0922],
        [5.0922, 4.9023, 4.7539],
        [5.0922, 5.0905, 5.0922],
        [5.0922, 4.9293, 4.9279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.13284330070018768 
model_pd.l_d.mean(): -20.383350372314453 
model_pd.lagr.mean(): -20.250507354736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-21.0893], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.13284330070018768
epoch£º664	 i:1 	 global-step:13281	 l-p:0.16009105741977692
epoch£º664	 i:2 	 global-step:13282	 l-p:0.125817209482193
epoch£º664	 i:3 	 global-step:13283	 l-p:0.11479217559099197
epoch£º664	 i:4 	 global-step:13284	 l-p:0.129484161734581
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12773217260837555
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16603326797485352
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12659606337547302
epoch£º664	 i:8 	 global-step:13288	 l-p:0.17071124911308289
epoch£º664	 i:9 	 global-step:13289	 l-p:0.05479889735579491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0605, 5.0582, 5.0604],
        [5.0605, 5.0581, 5.0604],
        [5.0605, 4.8649, 4.7251],
        [5.0605, 4.9873, 5.0295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.09744471311569214 
model_pd.l_d.mean(): -20.407651901245117 
model_pd.lagr.mean(): -20.31020736694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4962], device='cuda:0')), ('power', tensor([-21.1376], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.09744471311569214
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13781307637691498
epoch£º665	 i:2 	 global-step:13302	 l-p:0.17814840376377106
epoch£º665	 i:3 	 global-step:13303	 l-p:0.127890482544899
epoch£º665	 i:4 	 global-step:13304	 l-p:0.1242322027683258
epoch£º665	 i:5 	 global-step:13305	 l-p:0.1361699402332306
epoch£º665	 i:6 	 global-step:13306	 l-p:0.13657917082309723
epoch£º665	 i:7 	 global-step:13307	 l-p:0.1750630885362625
epoch£º665	 i:8 	 global-step:13308	 l-p:0.10068725049495697
epoch£º665	 i:9 	 global-step:13309	 l-p:0.1431688666343689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0555, 5.0555, 5.0555],
        [5.0555, 4.9731, 4.7208],
        [5.0555, 4.9894, 4.7365],
        [5.0555, 4.8960, 4.9037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12453652918338776 
model_pd.l_d.mean(): -18.876657485961914 
model_pd.lagr.mean(): -18.752120971679688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5544], device='cuda:0')), ('power', tensor([-19.6493], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12453652918338776
epoch£º666	 i:1 	 global-step:13321	 l-p:0.09287860989570618
epoch£º666	 i:2 	 global-step:13322	 l-p:0.1325216293334961
epoch£º666	 i:3 	 global-step:13323	 l-p:0.1553506851196289
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10705125331878662
epoch£º666	 i:5 	 global-step:13325	 l-p:0.1612933725118637
epoch£º666	 i:6 	 global-step:13326	 l-p:0.16120243072509766
epoch£º666	 i:7 	 global-step:13327	 l-p:0.13845399022102356
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1432403177022934
epoch£º666	 i:9 	 global-step:13329	 l-p:0.20605027675628662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0462, 5.5288, 5.5519],
        [5.0462, 4.8520, 4.7864],
        [5.0462, 5.2912, 5.1570],
        [5.0462, 4.8506, 4.6937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.1303534358739853 
model_pd.l_d.mean(): -20.27382469177246 
model_pd.lagr.mean(): -20.143470764160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5135], device='cuda:0')), ('power', tensor([-21.0200], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.1303534358739853
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11532199382781982
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14134946465492249
epoch£º667	 i:3 	 global-step:13343	 l-p:0.17776456475257874
epoch£º667	 i:4 	 global-step:13344	 l-p:0.1650126427412033
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17942020297050476
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11896135658025742
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11162622272968292
epoch£º667	 i:8 	 global-step:13348	 l-p:0.1382489800453186
epoch£º667	 i:9 	 global-step:13349	 l-p:0.1203131452202797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0570, 5.0276, 5.0511],
        [5.0570, 5.0569, 5.0570],
        [5.0570, 4.8739, 4.8424],
        [5.0570, 4.9533, 4.7029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11718886345624924 
model_pd.l_d.mean(): -19.9702091217041 
model_pd.lagr.mean(): -19.85301971435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5200], device='cuda:0')), ('power', tensor([-20.7197], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11718886345624924
epoch£º668	 i:1 	 global-step:13361	 l-p:0.14162078499794006
epoch£º668	 i:2 	 global-step:13362	 l-p:0.13164114952087402
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15838679671287537
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12212325632572174
epoch£º668	 i:5 	 global-step:13365	 l-p:0.16415855288505554
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11951110512018204
epoch£º668	 i:7 	 global-step:13367	 l-p:0.16848383843898773
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11307651549577713
epoch£º668	 i:9 	 global-step:13369	 l-p:0.25729647278785706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0287, 4.8742, 4.8911],
        [5.0287, 5.0286, 5.0287],
        [5.0287, 5.0287, 5.0287],
        [5.0287, 4.8332, 4.6626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.1372302919626236 
model_pd.l_d.mean(): -20.10453224182129 
model_pd.lagr.mean(): -19.967302322387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4381], device='cuda:0')), ('power', tensor([-20.7718], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.1372302919626236
epoch£º669	 i:1 	 global-step:13381	 l-p:0.16643546521663666
epoch£º669	 i:2 	 global-step:13382	 l-p:0.13412360846996307
epoch£º669	 i:3 	 global-step:13383	 l-p:0.1597919464111328
epoch£º669	 i:4 	 global-step:13384	 l-p:0.16960951685905457
epoch£º669	 i:5 	 global-step:13385	 l-p:0.1094973236322403
epoch£º669	 i:6 	 global-step:13386	 l-p:0.15306809544563293
epoch£º669	 i:7 	 global-step:13387	 l-p:0.18575705587863922
epoch£º669	 i:8 	 global-step:13388	 l-p:0.18010425567626953
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11090637743473053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0471, 5.0471, 5.0471],
        [5.0471, 4.9772, 5.0191],
        [5.0471, 5.0457, 5.0471],
        [5.0471, 5.0471, 5.0471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.23593242466449738 
model_pd.l_d.mean(): -20.61826515197754 
model_pd.lagr.mean(): -20.38233184814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-21.3356], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.23593242466449738
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11708435416221619
epoch£º670	 i:2 	 global-step:13402	 l-p:0.1381980925798416
epoch£º670	 i:3 	 global-step:13403	 l-p:0.13216149806976318
epoch£º670	 i:4 	 global-step:13404	 l-p:0.14758086204528809
epoch£º670	 i:5 	 global-step:13405	 l-p:0.09917508810758591
epoch£º670	 i:6 	 global-step:13406	 l-p:0.14942628145217896
epoch£º670	 i:7 	 global-step:13407	 l-p:0.12459146231412888
epoch£º670	 i:8 	 global-step:13408	 l-p:0.13954663276672363
epoch£º670	 i:9 	 global-step:13409	 l-p:0.1534007042646408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0250, 5.0347],
        [5.0357, 5.0321, 5.0355],
        [5.0357, 4.8436, 4.7947],
        [5.0357, 5.0320, 5.0355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.14231324195861816 
model_pd.l_d.mean(): -20.74148941040039 
model_pd.lagr.mean(): -20.59917640686035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-21.4126], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.14231324195861816
epoch£º671	 i:1 	 global-step:13421	 l-p:0.1646328717470169
epoch£º671	 i:2 	 global-step:13422	 l-p:0.1158648282289505
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16397444903850555
epoch£º671	 i:4 	 global-step:13424	 l-p:0.2395104169845581
epoch£º671	 i:5 	 global-step:13425	 l-p:0.19810882210731506
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09542867541313171
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12328594923019409
epoch£º671	 i:8 	 global-step:13428	 l-p:0.17969264090061188
epoch£º671	 i:9 	 global-step:13429	 l-p:0.1496845930814743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0261, 5.0051, 5.0229],
        [5.0261, 4.8870, 4.6435],
        [5.0261, 5.0261, 5.0261],
        [5.0261, 4.8413, 4.8139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.2382020205259323 
model_pd.l_d.mean(): -20.128612518310547 
model_pd.lagr.mean(): -19.890411376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5152], device='cuda:0')), ('power', tensor([-20.8749], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.2382020205259323
epoch£º672	 i:1 	 global-step:13441	 l-p:0.13576653599739075
epoch£º672	 i:2 	 global-step:13442	 l-p:0.15677163004875183
epoch£º672	 i:3 	 global-step:13443	 l-p:0.15445281565189362
epoch£º672	 i:4 	 global-step:13444	 l-p:0.10590357333421707
epoch£º672	 i:5 	 global-step:13445	 l-p:0.11647935956716537
epoch£º672	 i:6 	 global-step:13446	 l-p:0.14454211294651031
epoch£º672	 i:7 	 global-step:13447	 l-p:0.1374080628156662
epoch£º672	 i:8 	 global-step:13448	 l-p:0.11972907930612564
epoch£º672	 i:9 	 global-step:13449	 l-p:0.22963227331638336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0246, 5.0243, 5.0246],
        [5.0246, 5.0246, 5.0246],
        [5.0246, 5.0139, 4.7634],
        [5.0246, 5.0141, 5.0236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.1340438574552536 
model_pd.l_d.mean(): -20.247102737426758 
model_pd.lagr.mean(): -20.11305809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-20.9462], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.1340438574552536
epoch£º673	 i:1 	 global-step:13461	 l-p:0.16240474581718445
epoch£º673	 i:2 	 global-step:13462	 l-p:0.19320113956928253
epoch£º673	 i:3 	 global-step:13463	 l-p:0.28832319378852844
epoch£º673	 i:4 	 global-step:13464	 l-p:0.13143521547317505
epoch£º673	 i:5 	 global-step:13465	 l-p:0.1580529361963272
epoch£º673	 i:6 	 global-step:13466	 l-p:0.17538024485111237
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09528820961713791
epoch£º673	 i:8 	 global-step:13468	 l-p:0.1578451544046402
epoch£º673	 i:9 	 global-step:13469	 l-p:0.0979723408818245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0288, 4.9180, 4.9612],
        [5.0288, 4.9322, 4.9773],
        [5.0288, 4.8821, 4.9073],
        [5.0288, 4.8241, 4.7064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.17120423913002014 
model_pd.l_d.mean(): -19.55916976928711 
model_pd.lagr.mean(): -19.38796615600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5601], device='cuda:0')), ('power', tensor([-20.3451], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.17120423913002014
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13269685208797455
epoch£º674	 i:2 	 global-step:13482	 l-p:0.10834572464227676
epoch£º674	 i:3 	 global-step:13483	 l-p:0.13470111787319183
epoch£º674	 i:4 	 global-step:13484	 l-p:0.07691410183906555
epoch£º674	 i:5 	 global-step:13485	 l-p:0.13789187371730804
epoch£º674	 i:6 	 global-step:13486	 l-p:0.2234411984682083
epoch£º674	 i:7 	 global-step:13487	 l-p:0.16791868209838867
epoch£º674	 i:8 	 global-step:13488	 l-p:0.1316709816455841
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13980633020401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0895, 4.9254, 4.9284],
        [5.0895, 4.8921, 4.8061],
        [5.0895, 4.9963, 5.0406],
        [5.0895, 5.0181, 5.0602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10076969116926193 
model_pd.l_d.mean(): -20.163911819458008 
model_pd.lagr.mean(): -20.063142776489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-20.8626], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10076969116926193
epoch£º675	 i:1 	 global-step:13501	 l-p:0.13319654762744904
epoch£º675	 i:2 	 global-step:13502	 l-p:0.13590022921562195
epoch£º675	 i:3 	 global-step:13503	 l-p:0.12966212630271912
epoch£º675	 i:4 	 global-step:13504	 l-p:0.15985362231731415
epoch£º675	 i:5 	 global-step:13505	 l-p:0.15020078420639038
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12131473422050476
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12209019064903259
epoch£º675	 i:8 	 global-step:13508	 l-p:0.15782243013381958
epoch£º675	 i:9 	 global-step:13509	 l-p:0.04175455495715141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0995, 5.0995, 5.0995],
        [5.0995, 4.9118, 4.7330],
        [5.0995, 5.0876, 5.0983],
        [5.0995, 5.0956, 5.0993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.10141976177692413 
model_pd.l_d.mean(): -20.46120834350586 
model_pd.lagr.mean(): -20.35978889465332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4478], device='cuda:0')), ('power', tensor([-21.1423], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.10141976177692413
epoch£º676	 i:1 	 global-step:13521	 l-p:0.14904363453388214
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18526093661785126
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12584777176380157
epoch£º676	 i:4 	 global-step:13524	 l-p:0.14628435671329498
epoch£º676	 i:5 	 global-step:13525	 l-p:0.07592575997114182
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19311150908470154
epoch£º676	 i:7 	 global-step:13527	 l-p:0.1371186375617981
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12130098789930344
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12649093568325043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0549, 5.0549, 5.0549],
        [5.0549, 4.9568, 5.0017],
        [5.0549, 4.8657, 4.8273],
        [5.0549, 5.0387, 5.0529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.1293192207813263 
model_pd.l_d.mean(): -20.59308624267578 
model_pd.lagr.mean(): -20.46376609802246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4586], device='cuda:0')), ('power', tensor([-21.2866], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.1293192207813263
epoch£º677	 i:1 	 global-step:13541	 l-p:0.1480988711118698
epoch£º677	 i:2 	 global-step:13542	 l-p:0.11472785472869873
epoch£º677	 i:3 	 global-step:13543	 l-p:0.14332690834999084
epoch£º677	 i:4 	 global-step:13544	 l-p:0.1843409538269043
epoch£º677	 i:5 	 global-step:13545	 l-p:0.13100561499595642
epoch£º677	 i:6 	 global-step:13546	 l-p:0.42850610613822937
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.2616919279098511
epoch£º677	 i:8 	 global-step:13548	 l-p:0.1602848768234253
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11240342259407043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9862, 4.9548, 4.9797],
        [4.9862, 4.9846, 4.9861],
        [4.9862, 4.9586, 4.9810],
        [4.9862, 4.9466, 4.9765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.09478948265314102 
model_pd.l_d.mean(): -19.923959732055664 
model_pd.lagr.mean(): -20.018749237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5696], device='cuda:0')), ('power', tensor([-20.7236], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.09478948265314102
epoch£º678	 i:1 	 global-step:13561	 l-p:0.1792486011981964
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13023364543914795
epoch£º678	 i:3 	 global-step:13563	 l-p:0.22961322963237762
epoch£º678	 i:4 	 global-step:13564	 l-p:0.1296636015176773
epoch£º678	 i:5 	 global-step:13565	 l-p:0.1349084973335266
epoch£º678	 i:6 	 global-step:13566	 l-p:0.17112155258655548
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14643023908138275
epoch£º678	 i:8 	 global-step:13568	 l-p:0.14455905556678772
epoch£º678	 i:9 	 global-step:13569	 l-p:0.21342584490776062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9949, 4.9949, 4.9949],
        [4.9949, 4.9530, 4.6933],
        [4.9949, 4.9683, 4.9901],
        [4.9949, 4.9451, 4.9803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.18197669088840485 
model_pd.l_d.mean(): -20.443553924560547 
model_pd.lagr.mean(): -20.261577606201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-21.1691], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.18197669088840485
epoch£º679	 i:1 	 global-step:13581	 l-p:0.16470977663993835
epoch£º679	 i:2 	 global-step:13582	 l-p:0.1705898642539978
epoch£º679	 i:3 	 global-step:13583	 l-p:0.8678462505340576
epoch£º679	 i:4 	 global-step:13584	 l-p:0.2207428365945816
epoch£º679	 i:5 	 global-step:13585	 l-p:0.1193905621767044
epoch£º679	 i:6 	 global-step:13586	 l-p:0.1630462110042572
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11926382035017014
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12273364514112473
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11460031569004059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0315, 5.0247, 5.0310],
        [5.0315, 5.0290, 5.0314],
        [5.0315, 4.9942, 5.0227],
        [5.0315, 5.2224, 5.0554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.2078099548816681 
model_pd.l_d.mean(): -20.949329376220703 
model_pd.lagr.mean(): -20.741519927978516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.6135], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.2078099548816681
epoch£º680	 i:1 	 global-step:13601	 l-p:0.14127451181411743
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15293365716934204
epoch£º680	 i:3 	 global-step:13603	 l-p:0.16643336415290833
epoch£º680	 i:4 	 global-step:13604	 l-p:0.1985519528388977
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15213507413864136
epoch£º680	 i:6 	 global-step:13606	 l-p:0.13592766225337982
epoch£º680	 i:7 	 global-step:13607	 l-p:0.11455461382865906
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12349870055913925
epoch£º680	 i:9 	 global-step:13609	 l-p:0.1175522431731224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0906, 4.9147, 4.9015],
        [5.0906, 5.4807, 5.4348],
        [5.0906, 5.0897, 5.0905],
        [5.0906, 5.0896, 5.0905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.10400871932506561 
model_pd.l_d.mean(): -20.37457847595215 
model_pd.lagr.mean(): -20.27056884765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.0704], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.10400871932506561
epoch£º681	 i:1 	 global-step:13621	 l-p:0.1326378732919693
epoch£º681	 i:2 	 global-step:13622	 l-p:0.17045317590236664
epoch£º681	 i:3 	 global-step:13623	 l-p:0.145344078540802
epoch£º681	 i:4 	 global-step:13624	 l-p:0.14628395438194275
epoch£º681	 i:5 	 global-step:13625	 l-p:0.08379598706960678
epoch£º681	 i:6 	 global-step:13626	 l-p:0.045047663152217865
epoch£º681	 i:7 	 global-step:13627	 l-p:0.14551304280757904
epoch£º681	 i:8 	 global-step:13628	 l-p:0.1574518233537674
epoch£º681	 i:9 	 global-step:13629	 l-p:0.10944020748138428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0892, 4.9951, 5.0399],
        [5.0892, 4.9678, 5.0072],
        [5.0892, 5.0868, 5.0891],
        [5.0892, 5.0743, 5.0874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14006227254867554 
model_pd.l_d.mean(): -20.538480758666992 
model_pd.lagr.mean(): -20.398418426513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4596], device='cuda:0')), ('power', tensor([-21.2325], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14006227254867554
epoch£º682	 i:1 	 global-step:13641	 l-p:0.12582902610301971
epoch£º682	 i:2 	 global-step:13642	 l-p:0.12314116209745407
epoch£º682	 i:3 	 global-step:13643	 l-p:0.18561717867851257
epoch£º682	 i:4 	 global-step:13644	 l-p:0.16929946839809418
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12856462597846985
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13617528975009918
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14026892185211182
epoch£º682	 i:8 	 global-step:13648	 l-p:0.10932767391204834
epoch£º682	 i:9 	 global-step:13649	 l-p:0.5056272149085999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9818, 4.8632, 4.9065],
        [4.9818, 4.9458, 4.9736],
        [4.9818, 4.9818, 4.9818],
        [4.9818, 4.8301, 4.8560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.14598815143108368 
model_pd.l_d.mean(): -20.863845825195312 
model_pd.lagr.mean(): -20.717857360839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.5621], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.14598815143108368
epoch£º683	 i:1 	 global-step:13661	 l-p:3.2459938526153564
epoch£º683	 i:2 	 global-step:13662	 l-p:0.11249004304409027
epoch£º683	 i:3 	 global-step:13663	 l-p:0.1345929503440857
epoch£º683	 i:4 	 global-step:13664	 l-p:0.5410587787628174
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12484104186296463
epoch£º683	 i:6 	 global-step:13666	 l-p:0.06503342092037201
epoch£º683	 i:7 	 global-step:13667	 l-p:0.16096128523349762
epoch£º683	 i:8 	 global-step:13668	 l-p:0.20220261812210083
epoch£º683	 i:9 	 global-step:13669	 l-p:0.11424408853054047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9790, 4.8203, 4.5754],
        [4.9790, 4.9790, 4.9790],
        [4.9790, 5.2048, 5.0586],
        [4.9790, 4.9768, 4.9790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.1214793473482132 
model_pd.l_d.mean(): -20.361492156982422 
model_pd.lagr.mean(): -20.240013122558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-21.0984], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.1214793473482132
epoch£º684	 i:1 	 global-step:13681	 l-p:0.12390358000993729
epoch£º684	 i:2 	 global-step:13682	 l-p:0.13073012232780457
epoch£º684	 i:3 	 global-step:13683	 l-p:0.21299758553504944
epoch£º684	 i:4 	 global-step:13684	 l-p:0.12215499579906464
epoch£º684	 i:5 	 global-step:13685	 l-p:0.27879413962364197
epoch£º684	 i:6 	 global-step:13686	 l-p:0.3186606466770172
epoch£º684	 i:7 	 global-step:13687	 l-p:0.7047443389892578
epoch£º684	 i:8 	 global-step:13688	 l-p:0.1308310329914093
epoch£º684	 i:9 	 global-step:13689	 l-p:-0.8670828342437744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0046, 4.9262, 4.9707],
        [5.0046, 5.3599, 5.2936],
        [5.0046, 4.7935, 4.6685],
        [5.0046, 5.0045, 5.0046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.19358965754508972 
model_pd.l_d.mean(): -20.6300048828125 
model_pd.lagr.mean(): -20.43641471862793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4830], device='cuda:0')), ('power', tensor([-21.3489], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.19358965754508972
epoch£º685	 i:1 	 global-step:13701	 l-p:0.172841876745224
epoch£º685	 i:2 	 global-step:13702	 l-p:0.11724990606307983
epoch£º685	 i:3 	 global-step:13703	 l-p:0.2633177936077118
epoch£º685	 i:4 	 global-step:13704	 l-p:0.1290622055530548
epoch£º685	 i:5 	 global-step:13705	 l-p:0.11519290506839752
epoch£º685	 i:6 	 global-step:13706	 l-p:0.07233569771051407
epoch£º685	 i:7 	 global-step:13707	 l-p:0.16251155734062195
epoch£º685	 i:8 	 global-step:13708	 l-p:0.09991492331027985
epoch£º685	 i:9 	 global-step:13709	 l-p:0.12587293982505798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1206, 5.1203, 5.1206],
        [5.1206, 5.1201, 5.1206],
        [5.1206, 5.0904, 5.1145],
        [5.1206, 5.3641, 5.2238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.045969828963279724 
model_pd.l_d.mean(): -19.038619995117188 
model_pd.lagr.mean(): -18.992650985717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5806], device='cuda:0')), ('power', tensor([-19.8398], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.045969828963279724
epoch£º686	 i:1 	 global-step:13721	 l-p:0.13495144248008728
epoch£º686	 i:2 	 global-step:13722	 l-p:0.08417120575904846
epoch£º686	 i:3 	 global-step:13723	 l-p:0.14516766369342804
epoch£º686	 i:4 	 global-step:13724	 l-p:0.1078905239701271
epoch£º686	 i:5 	 global-step:13725	 l-p:0.10282847285270691
epoch£º686	 i:6 	 global-step:13726	 l-p:0.09241949766874313
epoch£º686	 i:7 	 global-step:13727	 l-p:0.13114270567893982
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13353566825389862
epoch£º686	 i:9 	 global-step:13729	 l-p:0.13863800466060638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1534, 5.3676, 5.2095],
        [5.1534, 4.9850, 4.7760],
        [5.1534, 4.9683, 4.9277],
        [5.1534, 5.2521, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.04797061160206795 
model_pd.l_d.mean(): -19.74765396118164 
model_pd.lagr.mean(): -19.699684143066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.5180], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.04797061160206795
epoch£º687	 i:1 	 global-step:13741	 l-p:0.16940473020076752
epoch£º687	 i:2 	 global-step:13742	 l-p:0.1360168308019638
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13038410246372223
epoch£º687	 i:4 	 global-step:13744	 l-p:0.1484294831752777
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10512556880712509
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08621323108673096
epoch£º687	 i:7 	 global-step:13747	 l-p:0.13816338777542114
epoch£º687	 i:8 	 global-step:13748	 l-p:0.14087216556072235
epoch£º687	 i:9 	 global-step:13749	 l-p:0.020138436928391457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1129, 5.1129, 5.1129],
        [5.1129, 4.9194, 4.7467],
        [5.1129, 4.9586, 4.9756],
        [5.1129, 5.1128, 5.1129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.047264307737350464 
model_pd.l_d.mean(): -19.472414016723633 
model_pd.lagr.mean(): -19.42514991760254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5271], device='cuda:0')), ('power', tensor([-20.2237], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.047264307737350464
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13521656394004822
epoch£º688	 i:2 	 global-step:13762	 l-p:0.15126270055770874
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17914924025535583
epoch£º688	 i:4 	 global-step:13764	 l-p:0.1517665684223175
epoch£º688	 i:5 	 global-step:13765	 l-p:0.122261643409729
epoch£º688	 i:6 	 global-step:13766	 l-p:0.04710204899311066
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13447144627571106
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12252822518348694
epoch£º688	 i:9 	 global-step:13769	 l-p:0.13202916085720062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0951, 5.0305, 5.0712],
        [5.0951, 5.0880, 5.0946],
        [5.0951, 5.0951, 5.0951],
        [5.0951, 5.0951, 5.0951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.17815232276916504 
model_pd.l_d.mean(): -20.672710418701172 
model_pd.lagr.mean(): -20.494558334350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-21.3530], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.17815232276916504
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13672645390033722
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12139153480529785
epoch£º689	 i:3 	 global-step:13783	 l-p:0.12750723958015442
epoch£º689	 i:4 	 global-step:13784	 l-p:0.12150180339813232
epoch£º689	 i:5 	 global-step:13785	 l-p:0.12147564440965652
epoch£º689	 i:6 	 global-step:13786	 l-p:0.19402694702148438
epoch£º689	 i:7 	 global-step:13787	 l-p:0.09461740404367447
epoch£º689	 i:8 	 global-step:13788	 l-p:0.2637600898742676
epoch£º689	 i:9 	 global-step:13789	 l-p:0.1269000619649887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0248, 5.0247, 5.0248],
        [5.0248, 5.4385, 5.4100],
        [5.0248, 5.0242, 5.0248],
        [5.0248, 5.0248, 5.0248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13947397470474243 
model_pd.l_d.mean(): -20.614151000976562 
model_pd.lagr.mean(): -20.47467613220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4762], device='cuda:0')), ('power', tensor([-21.3259], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13947397470474243
epoch£º690	 i:1 	 global-step:13801	 l-p:0.08007514476776123
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12501263618469238
epoch£º690	 i:3 	 global-step:13803	 l-p:0.4250156283378601
epoch£º690	 i:4 	 global-step:13804	 l-p:0.1325165182352066
epoch£º690	 i:5 	 global-step:13805	 l-p:0.19342494010925293
epoch£º690	 i:6 	 global-step:13806	 l-p:0.22948837280273438
epoch£º690	 i:7 	 global-step:13807	 l-p:0.17325584590435028
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13671943545341492
epoch£º690	 i:9 	 global-step:13809	 l-p:0.13370881974697113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0371, 5.0371, 5.0371],
        [5.0371, 4.9326, 4.9783],
        [5.0371, 4.9366, 4.6744],
        [5.0371, 5.0349, 5.0370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.11141900718212128 
model_pd.l_d.mean(): -19.58020782470703 
model_pd.lagr.mean(): -19.468788146972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-20.3269], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.11141900718212128
epoch£º691	 i:1 	 global-step:13821	 l-p:0.09845911711454391
epoch£º691	 i:2 	 global-step:13822	 l-p:0.17088103294372559
epoch£º691	 i:3 	 global-step:13823	 l-p:0.170237198472023
epoch£º691	 i:4 	 global-step:13824	 l-p:0.1914043426513672
epoch£º691	 i:5 	 global-step:13825	 l-p:0.10042355954647064
epoch£º691	 i:6 	 global-step:13826	 l-p:0.20769740641117096
epoch£º691	 i:7 	 global-step:13827	 l-p:0.11123832315206528
epoch£º691	 i:8 	 global-step:13828	 l-p:0.1170496717095375
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11847245693206787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0979, 5.0976, 5.0979],
        [5.0979, 5.0974, 5.0979],
        [5.0979, 5.0356, 5.0756],
        [5.0979, 5.0979, 5.0979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.07867708057165146 
model_pd.l_d.mean(): -20.843847274780273 
model_pd.lagr.mean(): -20.76517105102539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-21.4867], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.07867708057165146
epoch£º692	 i:1 	 global-step:13841	 l-p:0.12463860958814621
epoch£º692	 i:2 	 global-step:13842	 l-p:0.15784665942192078
epoch£º692	 i:3 	 global-step:13843	 l-p:0.14207978546619415
epoch£º692	 i:4 	 global-step:13844	 l-p:0.10517760366201401
epoch£º692	 i:5 	 global-step:13845	 l-p:0.11923787742853165
epoch£º692	 i:6 	 global-step:13846	 l-p:0.1447197049856186
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12329444289207458
epoch£º692	 i:8 	 global-step:13848	 l-p:0.19573579728603363
epoch£º692	 i:9 	 global-step:13849	 l-p:0.11199288070201874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0617, 5.0580, 5.0615],
        [5.0617, 5.1755, 4.9660],
        [5.0617, 5.4305, 5.3694],
        [5.0617, 5.4709, 5.4372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.09465927630662918 
model_pd.l_d.mean(): -19.557315826416016 
model_pd.lagr.mean(): -19.462656021118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5808], device='cuda:0')), ('power', tensor([-20.3644], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.09465927630662918
epoch£º693	 i:1 	 global-step:13861	 l-p:0.07432727515697479
epoch£º693	 i:2 	 global-step:13862	 l-p:0.1564282327890396
epoch£º693	 i:3 	 global-step:13863	 l-p:0.1583479344844818
epoch£º693	 i:4 	 global-step:13864	 l-p:0.17330169677734375
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13157129287719727
epoch£º693	 i:6 	 global-step:13866	 l-p:0.12147270143032074
epoch£º693	 i:7 	 global-step:13867	 l-p:0.20195268094539642
epoch£º693	 i:8 	 global-step:13868	 l-p:0.19825002551078796
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11461355537176132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0578, 4.9944, 5.0351],
        [5.0578, 4.8524, 4.6856],
        [5.0578, 5.0577, 5.0578],
        [5.0578, 5.0578, 5.0578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.10000325739383698 
model_pd.l_d.mean(): -19.195505142211914 
model_pd.lagr.mean(): -19.095500946044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5555], device='cuda:0')), ('power', tensor([-19.9728], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.10000325739383698
epoch£º694	 i:1 	 global-step:13881	 l-p:0.17644844949245453
epoch£º694	 i:2 	 global-step:13882	 l-p:0.1320621818304062
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12803979218006134
epoch£º694	 i:4 	 global-step:13884	 l-p:0.10765150934457779
epoch£º694	 i:5 	 global-step:13885	 l-p:0.18131686747074127
epoch£º694	 i:6 	 global-step:13886	 l-p:0.144312784075737
epoch£º694	 i:7 	 global-step:13887	 l-p:0.1467287689447403
epoch£º694	 i:8 	 global-step:13888	 l-p:0.13115599751472473
epoch£º694	 i:9 	 global-step:13889	 l-p:0.13328590989112854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0829, 4.8774, 4.7907],
        [5.0829, 4.9342, 4.6898],
        [5.0829, 4.9428, 4.6938],
        [5.0829, 5.0826, 5.0829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12568898499011993 
model_pd.l_d.mean(): -19.547151565551758 
model_pd.lagr.mean(): -19.421463012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-20.2672], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12568898499011993
epoch£º695	 i:1 	 global-step:13901	 l-p:0.17650334537029266
epoch£º695	 i:2 	 global-step:13902	 l-p:0.09372291713953018
epoch£º695	 i:3 	 global-step:13903	 l-p:0.13365867733955383
epoch£º695	 i:4 	 global-step:13904	 l-p:0.16974030435085297
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12142688781023026
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13796260952949524
epoch£º695	 i:7 	 global-step:13907	 l-p:0.1007014736533165
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13214117288589478
epoch£º695	 i:9 	 global-step:13909	 l-p:0.13518927991390228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0815, 5.0727, 5.0807],
        [5.0815, 4.9321, 4.6873],
        [5.0815, 5.0376, 5.0698],
        [5.0815, 5.4336, 5.3603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.1972958892583847 
model_pd.l_d.mean(): -20.808271408081055 
model_pd.lagr.mean(): -20.61097526550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.4751], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.1972958892583847
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11958374083042145
epoch£º696	 i:2 	 global-step:13922	 l-p:0.16698643565177917
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11995693296194077
epoch£º696	 i:4 	 global-step:13924	 l-p:0.14305877685546875
epoch£º696	 i:5 	 global-step:13925	 l-p:0.2005366086959839
epoch£º696	 i:6 	 global-step:13926	 l-p:0.1339615434408188
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11856832355260849
epoch£º696	 i:8 	 global-step:13928	 l-p:0.1377749890089035
epoch£º696	 i:9 	 global-step:13929	 l-p:0.11717943102121353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0075, 5.0075, 5.0075],
        [5.0075, 5.0074, 5.0075],
        [5.0075, 5.0896, 4.8654],
        [5.0075, 5.0066, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.1332310140132904 
model_pd.l_d.mean(): -20.119831085205078 
model_pd.lagr.mean(): -19.986600875854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4708], device='cuda:0')), ('power', tensor([-20.8206], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.1332310140132904
epoch£º697	 i:1 	 global-step:13941	 l-p:0.16788020730018616
epoch£º697	 i:2 	 global-step:13942	 l-p:-0.20092040300369263
epoch£º697	 i:3 	 global-step:13943	 l-p:0.13167601823806763
epoch£º697	 i:4 	 global-step:13944	 l-p:0.32707610726356506
epoch£º697	 i:5 	 global-step:13945	 l-p:0.14900869131088257
epoch£º697	 i:6 	 global-step:13946	 l-p:0.18881919980049133
epoch£º697	 i:7 	 global-step:13947	 l-p:0.1655844897031784
epoch£º697	 i:8 	 global-step:13948	 l-p:0.14290836453437805
epoch£º697	 i:9 	 global-step:13949	 l-p:0.19133159518241882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0199, 4.9836, 5.0117],
        [5.0199, 5.0174, 5.0198],
        [5.0199, 4.8353, 4.8248],
        [5.0199, 4.8043, 4.6851]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.14636115729808807 
model_pd.l_d.mean(): -20.58735466003418 
model_pd.lagr.mean(): -20.440994262695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-21.2883], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.14636115729808807
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11943814903497696
epoch£º698	 i:2 	 global-step:13962	 l-p:0.22389674186706543
epoch£º698	 i:3 	 global-step:13963	 l-p:0.15111279487609863
epoch£º698	 i:4 	 global-step:13964	 l-p:0.13273541629314423
epoch£º698	 i:5 	 global-step:13965	 l-p:0.17563268542289734
epoch£º698	 i:6 	 global-step:13966	 l-p:0.3163108229637146
epoch£º698	 i:7 	 global-step:13967	 l-p:0.16821959614753723
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09555663913488388
epoch£º698	 i:9 	 global-step:13969	 l-p:0.12020835280418396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0527, 5.0457, 5.0522],
        [5.0527, 4.9896, 5.0303],
        [5.0527, 4.8406, 4.6993],
        [5.0527, 5.0527, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.14081981778144836 
model_pd.l_d.mean(): -20.441823959350586 
model_pd.lagr.mean(): -20.30100440979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4837], device='cuda:0')), ('power', tensor([-21.1594], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.14081981778144836
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12368717044591904
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14931292831897736
epoch£º699	 i:3 	 global-step:13983	 l-p:0.13287478685379028
epoch£º699	 i:4 	 global-step:13984	 l-p:0.17752671241760254
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06378310173749924
epoch£º699	 i:6 	 global-step:13986	 l-p:0.10114084184169769
epoch£º699	 i:7 	 global-step:13987	 l-p:0.17803500592708588
epoch£º699	 i:8 	 global-step:13988	 l-p:0.14638324081897736
epoch£º699	 i:9 	 global-step:13989	 l-p:0.13425473868846893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1218, 5.1168, 5.1215],
        [5.1218, 4.9194, 4.8430],
        [5.1218, 4.9342, 4.9057],
        [5.1218, 5.0576, 5.0984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.1102306917309761 
model_pd.l_d.mean(): -20.435489654541016 
model_pd.lagr.mean(): -20.325258255004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4311], device='cuda:0')), ('power', tensor([-21.0992], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.1102306917309761
epoch£º700	 i:1 	 global-step:14001	 l-p:0.1206616535782814
epoch£º700	 i:2 	 global-step:14002	 l-p:0.08080169558525085
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12421663105487823
epoch£º700	 i:4 	 global-step:14004	 l-p:0.1212771013379097
epoch£º700	 i:5 	 global-step:14005	 l-p:0.15756668150424957
epoch£º700	 i:6 	 global-step:14006	 l-p:0.07242303341627121
epoch£º700	 i:7 	 global-step:14007	 l-p:0.13762159645557404
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11590060591697693
epoch£º700	 i:9 	 global-step:14009	 l-p:0.15482407808303833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1322, 5.1251, 5.1317],
        [5.1322, 5.0541, 5.0983],
        [5.1322, 5.1322, 5.1322],
        [5.1322, 4.9696, 4.9810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.12223853915929794 
model_pd.l_d.mean(): -20.121862411499023 
model_pd.lagr.mean(): -19.999624252319336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4832], device='cuda:0')), ('power', tensor([-20.8354], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.12223853915929794
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14451530575752258
epoch£º701	 i:2 	 global-step:14022	 l-p:0.13307052850723267
epoch£º701	 i:3 	 global-step:14023	 l-p:0.1582617312669754
epoch£º701	 i:4 	 global-step:14024	 l-p:0.08650941401720047
epoch£º701	 i:5 	 global-step:14025	 l-p:0.07288204878568649
epoch£º701	 i:6 	 global-step:14026	 l-p:0.13942626118659973
epoch£º701	 i:7 	 global-step:14027	 l-p:0.07255932688713074
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12837058305740356
epoch£º701	 i:9 	 global-step:14029	 l-p:0.15476179122924805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1169, 5.0543, 5.0946],
        [5.1169, 5.2938, 5.1132],
        [5.1169, 5.0562, 5.0959],
        [5.1169, 5.1169, 5.1169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.12607991695404053 
model_pd.l_d.mean(): -20.293498992919922 
model_pd.lagr.mean(): -20.16741943359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4770], device='cuda:0')), ('power', tensor([-21.0026], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.12607991695404053
epoch£º702	 i:1 	 global-step:14041	 l-p:0.18331582844257355
epoch£º702	 i:2 	 global-step:14042	 l-p:0.021683814004063606
epoch£º702	 i:3 	 global-step:14043	 l-p:0.09556494653224945
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1317405253648758
epoch£º702	 i:5 	 global-step:14045	 l-p:0.13944867253303528
epoch£º702	 i:6 	 global-step:14046	 l-p:0.10337241739034653
epoch£º702	 i:7 	 global-step:14047	 l-p:0.16317711770534515
epoch£º702	 i:8 	 global-step:14048	 l-p:0.15144455432891846
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12555484473705292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0834, 5.0806, 5.0833],
        [5.0834, 5.3275, 5.1853],
        [5.0834, 5.0711, 5.0822],
        [5.0834, 5.3523, 5.2248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.1425127536058426 
model_pd.l_d.mean(): -20.45989990234375 
model_pd.lagr.mean(): -20.317386627197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-21.1824], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.1425127536058426
epoch£º703	 i:1 	 global-step:14061	 l-p:0.12815475463867188
epoch£º703	 i:2 	 global-step:14062	 l-p:0.08503551036119461
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13657893240451813
epoch£º703	 i:4 	 global-step:14064	 l-p:0.19707797467708588
epoch£º703	 i:5 	 global-step:14065	 l-p:0.1831083595752716
epoch£º703	 i:6 	 global-step:14066	 l-p:0.11769693344831467
epoch£º703	 i:7 	 global-step:14067	 l-p:0.12146715819835663
epoch£º703	 i:8 	 global-step:14068	 l-p:0.06410222500562668
epoch£º703	 i:9 	 global-step:14069	 l-p:0.15175345540046692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1053, 5.1053, 5.1053],
        [5.1053, 5.0610, 5.0935],
        [5.1053, 5.0254, 5.0704],
        [5.1053, 4.9358, 4.7034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13199464976787567 
model_pd.l_d.mean(): -21.06192398071289 
model_pd.lagr.mean(): -20.929929733276367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3720], device='cuda:0')), ('power', tensor([-21.6721], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13199464976787567
epoch£º704	 i:1 	 global-step:14081	 l-p:0.08959528058767319
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16382171213626862
epoch£º704	 i:3 	 global-step:14083	 l-p:0.0814191922545433
epoch£º704	 i:4 	 global-step:14084	 l-p:0.14875392615795135
epoch£º704	 i:5 	 global-step:14085	 l-p:0.1257515549659729
epoch£º704	 i:6 	 global-step:14086	 l-p:0.10493337363004684
epoch£º704	 i:7 	 global-step:14087	 l-p:0.14076265692710876
epoch£º704	 i:8 	 global-step:14088	 l-p:0.10953240841627121
epoch£º704	 i:9 	 global-step:14089	 l-p:0.133848175406456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 5.1606, 5.1606],
        [5.1606, 5.1562, 5.1604],
        [5.1606, 5.0087, 4.7698],
        [5.1606, 5.2689, 5.0534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): -0.5237323045730591 
model_pd.l_d.mean(): -20.13909149169922 
model_pd.lagr.mean(): -20.662824630737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5008], device='cuda:0')), ('power', tensor([-20.8708], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:-0.5237323045730591
epoch£º705	 i:1 	 global-step:14101	 l-p:0.14968502521514893
epoch£º705	 i:2 	 global-step:14102	 l-p:0.13392890989780426
epoch£º705	 i:3 	 global-step:14103	 l-p:0.14660148322582245
epoch£º705	 i:4 	 global-step:14104	 l-p:0.1119827851653099
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11316612362861633
epoch£º705	 i:6 	 global-step:14106	 l-p:0.1287819743156433
epoch£º705	 i:7 	 global-step:14107	 l-p:0.13932986557483673
epoch£º705	 i:8 	 global-step:14108	 l-p:0.07723576575517654
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12258840352296829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1194, 5.0453, 5.0891],
        [5.1194, 5.3082, 5.1329],
        [5.1194, 5.0834, 5.1113],
        [5.1194, 4.9095, 4.7791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.15456323325634003 
model_pd.l_d.mean(): -20.51092529296875 
model_pd.lagr.mean(): -20.356361389160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.2099], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.15456323325634003
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12512539327144623
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14576031267642975
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10715789347887039
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10337159037590027
epoch£º706	 i:5 	 global-step:14125	 l-p:0.20713011920452118
epoch£º706	 i:6 	 global-step:14126	 l-p:0.12349160760641098
epoch£º706	 i:7 	 global-step:14127	 l-p:0.3023104965686798
epoch£º706	 i:8 	 global-step:14128	 l-p:0.21699531376361847
epoch£º706	 i:9 	 global-step:14129	 l-p:0.1376163214445114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0157, 5.0154, 5.0157],
        [5.0157, 5.0157, 5.0157],
        [5.0157, 5.0157, 5.0157],
        [5.0157, 4.9076, 4.9551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.9971683025360107 
model_pd.l_d.mean(): -20.918872833251953 
model_pd.lagr.mean(): -19.92170524597168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4383], device='cuda:0')), ('power', tensor([-21.5953], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.9971683025360107
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11279626935720444
epoch£º707	 i:2 	 global-step:14142	 l-p:0.2212202250957489
epoch£º707	 i:3 	 global-step:14143	 l-p:0.1364164650440216
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12810161709785461
epoch£º707	 i:5 	 global-step:14145	 l-p:0.27056047320365906
epoch£º707	 i:6 	 global-step:14146	 l-p:0.19068288803100586
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14587783813476562
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12637914717197418
epoch£º707	 i:9 	 global-step:14149	 l-p:0.11417738348245621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0364, 4.9629, 5.0073],
        [5.0364, 4.8669, 4.8806],
        [5.0364, 4.9865, 4.7178],
        [5.0364, 4.9621, 5.0066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13564592599868774 
model_pd.l_d.mean(): -20.378103256225586 
model_pd.lagr.mean(): -20.242456436157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-21.1092], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13564592599868774
epoch£º708	 i:1 	 global-step:14161	 l-p:0.1139766275882721
epoch£º708	 i:2 	 global-step:14162	 l-p:0.16618341207504272
epoch£º708	 i:3 	 global-step:14163	 l-p:0.1740841120481491
epoch£º708	 i:4 	 global-step:14164	 l-p:0.13892677426338196
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06639290601015091
epoch£º708	 i:6 	 global-step:14166	 l-p:0.31329816579818726
epoch£º708	 i:7 	 global-step:14167	 l-p:0.19090647995471954
epoch£º708	 i:8 	 global-step:14168	 l-p:0.14585073292255402
epoch£º708	 i:9 	 global-step:14169	 l-p:0.1716955155134201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0817, 4.8762, 4.8178],
        [5.0817, 5.0443, 5.0731],
        [5.0817, 5.5481, 5.5500],
        [5.0817, 4.9098, 4.6707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10521616786718369 
model_pd.l_d.mean(): -20.186134338378906 
model_pd.lagr.mean(): -20.080917358398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.8950], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10521616786718369
epoch£º709	 i:1 	 global-step:14181	 l-p:0.15029622614383698
epoch£º709	 i:2 	 global-step:14182	 l-p:0.06840458512306213
epoch£º709	 i:3 	 global-step:14183	 l-p:0.09638303518295288
epoch£º709	 i:4 	 global-step:14184	 l-p:0.15609245002269745
epoch£º709	 i:5 	 global-step:14185	 l-p:0.11632776260375977
epoch£º709	 i:6 	 global-step:14186	 l-p:0.13827715814113617
epoch£º709	 i:7 	 global-step:14187	 l-p:0.14749500155448914
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12491649389266968
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15825101733207703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1265, 5.1095, 5.1243],
        [5.1265, 4.9255, 4.8715],
        [5.1265, 4.9563, 4.9631],
        [5.1265, 5.0994, 5.1216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12204490602016449 
model_pd.l_d.mean(): -20.503496170043945 
model_pd.lagr.mean(): -20.381450653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.1679], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12204490602016449
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12647666037082672
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12608692049980164
epoch£º710	 i:3 	 global-step:14203	 l-p:0.07026701420545578
epoch£º710	 i:4 	 global-step:14204	 l-p:0.15537241101264954
epoch£º710	 i:5 	 global-step:14205	 l-p:0.09849336743354797
epoch£º710	 i:6 	 global-step:14206	 l-p:0.2168726921081543
epoch£º710	 i:7 	 global-step:14207	 l-p:0.14535096287727356
epoch£º710	 i:8 	 global-step:14208	 l-p:0.15752318501472473
epoch£º710	 i:9 	 global-step:14209	 l-p:0.1372087001800537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0770, 5.0661, 5.0759],
        [5.0770, 5.1435, 4.9087],
        [5.0770, 5.0697, 4.8100],
        [5.0770, 5.0770, 5.0770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.1509007066488266 
model_pd.l_d.mean(): -19.751340866088867 
model_pd.lagr.mean(): -19.600440979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5435], device='cuda:0')), ('power', tensor([-20.5224], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.1509007066488266
epoch£º711	 i:1 	 global-step:14221	 l-p:0.19040237367153168
epoch£º711	 i:2 	 global-step:14222	 l-p:0.10422708094120026
epoch£º711	 i:3 	 global-step:14223	 l-p:0.1590183824300766
epoch£º711	 i:4 	 global-step:14224	 l-p:0.148151233792305
epoch£º711	 i:5 	 global-step:14225	 l-p:0.15386046469211578
epoch£º711	 i:6 	 global-step:14226	 l-p:0.028387947008013725
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12215159833431244
epoch£º711	 i:8 	 global-step:14228	 l-p:0.0869264081120491
epoch£º711	 i:9 	 global-step:14229	 l-p:0.1188388392329216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.1152, 4.8516],
        [5.1624, 5.2657, 5.0461],
        [5.1624, 5.0904, 5.1336],
        [5.1624, 5.3987, 5.2477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): -0.2846023142337799 
model_pd.l_d.mean(): -20.211416244506836 
model_pd.lagr.mean(): -20.49601936340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5099], device='cuda:0')), ('power', tensor([-20.9532], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:-0.2846023142337799
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11697578430175781
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13281859457492828
epoch£º712	 i:3 	 global-step:14243	 l-p:0.14036709070205688
epoch£º712	 i:4 	 global-step:14244	 l-p:0.11687372624874115
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12375989556312561
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13848954439163208
epoch£º712	 i:7 	 global-step:14247	 l-p:0.08568666130304337
epoch£º712	 i:8 	 global-step:14248	 l-p:0.07488815486431122
epoch£º712	 i:9 	 global-step:14249	 l-p:0.1242920383810997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 5.1446, 5.1511],
        [5.1516, 4.9424, 4.7970],
        [5.1516, 5.1515, 5.1516],
        [5.1516, 4.9775, 4.7492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08786344528198242 
model_pd.l_d.mean(): -20.886043548583984 
model_pd.lagr.mean(): -20.798179626464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3857], device='cuda:0')), ('power', tensor([-21.5083], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.08786344528198242
epoch£º713	 i:1 	 global-step:14261	 l-p:0.04169168695807457
epoch£º713	 i:2 	 global-step:14262	 l-p:0.13587935268878937
epoch£º713	 i:3 	 global-step:14263	 l-p:0.1327105462551117
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15706327557563782
epoch£º713	 i:5 	 global-step:14265	 l-p:0.14181719720363617
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12078098952770233
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12431613355875015
epoch£º713	 i:8 	 global-step:14268	 l-p:0.0702735036611557
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18534427881240845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0965, 5.0965, 5.0965],
        [5.0965, 5.0342, 4.7651],
        [5.0965, 4.9286, 4.6848],
        [5.0965, 5.0923, 5.0963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15068191289901733 
model_pd.l_d.mean(): -20.60407066345215 
model_pd.lagr.mean(): -20.453388214111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.2899], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15068191289901733
epoch£º714	 i:1 	 global-step:14281	 l-p:0.17095735669136047
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13863301277160645
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14241257309913635
epoch£º714	 i:4 	 global-step:14284	 l-p:0.19232402741909027
epoch£º714	 i:5 	 global-step:14285	 l-p:0.19363348186016083
epoch£º714	 i:6 	 global-step:14286	 l-p:0.10827118903398514
epoch£º714	 i:7 	 global-step:14287	 l-p:0.0982314869761467
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12509776651859283
epoch£º714	 i:9 	 global-step:14289	 l-p:0.09667608141899109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0536, 5.0416, 5.0524],
        [5.0536, 5.4239, 5.3596],
        [5.0536, 4.8328, 4.6915],
        [5.0536, 5.0536, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.1343049854040146 
model_pd.l_d.mean(): -20.388877868652344 
model_pd.lagr.mean(): -20.254573822021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4837], device='cuda:0')), ('power', tensor([-21.1058], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.1343049854040146
epoch£º715	 i:1 	 global-step:14301	 l-p:0.19131354987621307
epoch£º715	 i:2 	 global-step:14302	 l-p:0.11917568743228912
epoch£º715	 i:3 	 global-step:14303	 l-p:0.1569504588842392
epoch£º715	 i:4 	 global-step:14304	 l-p:0.1809227019548416
epoch£º715	 i:5 	 global-step:14305	 l-p:0.17568236589431763
epoch£º715	 i:6 	 global-step:14306	 l-p:0.1299765408039093
epoch£º715	 i:7 	 global-step:14307	 l-p:0.21892505884170532
epoch£º715	 i:8 	 global-step:14308	 l-p:0.1265241950750351
epoch£º715	 i:9 	 global-step:14309	 l-p:0.10913721472024918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0628, 4.9499, 4.6790],
        [5.0628, 5.0603, 5.0627],
        [5.0628, 5.0628, 5.0628],
        [5.0628, 5.4165, 5.3407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.1705358475446701 
model_pd.l_d.mean(): -20.3513240814209 
model_pd.lagr.mean(): -20.180788040161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-21.0832], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.1705358475446701
epoch£º716	 i:1 	 global-step:14321	 l-p:0.1364876925945282
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14087322354316711
epoch£º716	 i:3 	 global-step:14323	 l-p:0.129690021276474
epoch£º716	 i:4 	 global-step:14324	 l-p:0.20011748373508453
epoch£º716	 i:5 	 global-step:14325	 l-p:0.10803839564323425
epoch£º716	 i:6 	 global-step:14326	 l-p:0.1300276517868042
epoch£º716	 i:7 	 global-step:14327	 l-p:0.16122762858867645
epoch£º716	 i:8 	 global-step:14328	 l-p:0.07445504516363144
epoch£º716	 i:9 	 global-step:14329	 l-p:0.12928104400634766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0906, 4.9907, 5.0383],
        [5.0906, 4.9635, 5.0062],
        [5.0906, 5.0854, 5.0903],
        [5.0906, 5.2145, 5.0041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.1470090001821518 
model_pd.l_d.mean(): -20.85897445678711 
model_pd.lagr.mean(): -20.711965560913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-21.5110], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.1470090001821518
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10865171998739243
epoch£º717	 i:2 	 global-step:14342	 l-p:0.1968240737915039
epoch£º717	 i:3 	 global-step:14343	 l-p:0.1037590354681015
epoch£º717	 i:4 	 global-step:14344	 l-p:0.12842117249965668
epoch£º717	 i:5 	 global-step:14345	 l-p:0.06969182193279266
epoch£º717	 i:6 	 global-step:14346	 l-p:0.1599193513393402
epoch£º717	 i:7 	 global-step:14347	 l-p:0.19474802911281586
epoch£º717	 i:8 	 global-step:14348	 l-p:0.25024092197418213
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12125175446271896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0601, 4.8605, 4.8300],
        [5.0601, 4.8953, 4.9154],
        [5.0601, 5.4671, 5.4267],
        [5.0601, 4.9227, 4.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.13098903000354767 
model_pd.l_d.mean(): -20.873865127563477 
model_pd.lagr.mean(): -20.742876052856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4163], device='cuda:0')), ('power', tensor([-21.5273], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.13098903000354767
epoch£º718	 i:1 	 global-step:14361	 l-p:0.10406814515590668
epoch£º718	 i:2 	 global-step:14362	 l-p:0.22744214534759521
epoch£º718	 i:3 	 global-step:14363	 l-p:0.15439748764038086
epoch£º718	 i:4 	 global-step:14364	 l-p:0.17707854509353638
epoch£º718	 i:5 	 global-step:14365	 l-p:0.10639183968305588
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12365309149026871
epoch£º718	 i:7 	 global-step:14367	 l-p:0.1072639524936676
epoch£º718	 i:8 	 global-step:14368	 l-p:0.24816730618476868
epoch£º718	 i:9 	 global-step:14369	 l-p:0.11845042556524277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0675, 4.9199, 4.9535],
        [5.0675, 4.9778, 4.7046],
        [5.0675, 5.1501, 4.9205],
        [5.0675, 5.0453, 5.0641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.12299205362796783 
model_pd.l_d.mean(): -18.661611557006836 
model_pd.lagr.mean(): -18.538619995117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6014], device='cuda:0')), ('power', tensor([-19.4800], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.12299205362796783
epoch£º719	 i:1 	 global-step:14381	 l-p:0.17835824191570282
epoch£º719	 i:2 	 global-step:14382	 l-p:0.12849025428295135
epoch£º719	 i:3 	 global-step:14383	 l-p:0.10267289727926254
epoch£º719	 i:4 	 global-step:14384	 l-p:0.13949817419052124
epoch£º719	 i:5 	 global-step:14385	 l-p:0.16800543665885925
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13547219336032867
epoch£º719	 i:7 	 global-step:14387	 l-p:0.09857247769832611
epoch£º719	 i:8 	 global-step:14388	 l-p:0.15236760675907135
epoch£º719	 i:9 	 global-step:14389	 l-p:0.141646608710289
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1195, 5.0508, 5.0937],
        [5.1195, 4.9232, 4.8913],
        [5.1195, 4.9481, 4.7076],
        [5.1195, 4.9052, 4.8060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.10898658633232117 
model_pd.l_d.mean(): -20.35159683227539 
model_pd.lagr.mean(): -20.242610931396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4596], device='cuda:0')), ('power', tensor([-21.0435], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.10898658633232117
epoch£º720	 i:1 	 global-step:14401	 l-p:0.12137242406606674
epoch£º720	 i:2 	 global-step:14402	 l-p:0.036360323429107666
epoch£º720	 i:3 	 global-step:14403	 l-p:0.13314668834209442
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12237734347581863
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13734126091003418
epoch£º720	 i:6 	 global-step:14406	 l-p:0.14127130806446075
epoch£º720	 i:7 	 global-step:14407	 l-p:0.18174022436141968
epoch£º720	 i:8 	 global-step:14408	 l-p:0.1441059410572052
epoch£º720	 i:9 	 global-step:14409	 l-p:0.11532671004533768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.1167, 4.9841, 4.7209],
        [5.1167, 5.5411, 5.5101],
        [5.1167, 4.9001, 4.7811],
        [5.1167, 4.9046, 4.7340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11659884452819824 
model_pd.l_d.mean(): -19.605989456176758 
model_pd.lagr.mean(): -19.489391326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5071], device='cuda:0')), ('power', tensor([-20.3383], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11659884452819824
epoch£º721	 i:1 	 global-step:14421	 l-p:0.08397526293992996
epoch£º721	 i:2 	 global-step:14422	 l-p:0.14339102804660797
epoch£º721	 i:3 	 global-step:14423	 l-p:0.1492803692817688
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10823746770620346
epoch£º721	 i:5 	 global-step:14425	 l-p:0.1331758201122284
epoch£º721	 i:6 	 global-step:14426	 l-p:0.15938745439052582
epoch£º721	 i:7 	 global-step:14427	 l-p:0.12465766817331314
epoch£º721	 i:8 	 global-step:14428	 l-p:0.13767565786838531
epoch£º721	 i:9 	 global-step:14429	 l-p:0.18747004866600037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0672, 5.0672, 5.0672],
        [5.0672, 5.0647, 5.0671],
        [5.0672, 5.0672, 5.0672],
        [5.0672, 5.0205, 5.0546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.10265371203422546 
model_pd.l_d.mean(): -20.22943115234375 
model_pd.lagr.mean(): -20.12677764892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5038], device='cuda:0')), ('power', tensor([-20.9651], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.10265371203422546
epoch£º722	 i:1 	 global-step:14441	 l-p:0.1415240466594696
epoch£º722	 i:2 	 global-step:14442	 l-p:0.16656829416751862
epoch£º722	 i:3 	 global-step:14443	 l-p:0.13199257850646973
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13782653212547302
epoch£º722	 i:5 	 global-step:14445	 l-p:0.27739188075065613
epoch£º722	 i:6 	 global-step:14446	 l-p:0.12166179716587067
epoch£º722	 i:7 	 global-step:14447	 l-p:0.1336846947669983
epoch£º722	 i:8 	 global-step:14448	 l-p:0.13565701246261597
epoch£º722	 i:9 	 global-step:14449	 l-p:1.1013809442520142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0166, 4.9519, 4.9941],
        [5.0166, 4.9208, 4.6426],
        [5.0166, 5.0164, 5.0166],
        [5.0166, 5.0166, 5.0166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.117448590695858 
model_pd.l_d.mean(): -20.534832000732422 
model_pd.lagr.mean(): -20.417383193969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4892], device='cuda:0')), ('power', tensor([-21.2590], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.117448590695858
epoch£º723	 i:1 	 global-step:14461	 l-p:0.19747577607631683
epoch£º723	 i:2 	 global-step:14462	 l-p:0.22006013989448547
epoch£º723	 i:3 	 global-step:14463	 l-p:0.12976841628551483
epoch£º723	 i:4 	 global-step:14464	 l-p:0.4092806577682495
epoch£º723	 i:5 	 global-step:14465	 l-p:0.19857291877269745
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10549242049455643
epoch£º723	 i:7 	 global-step:14467	 l-p:0.1273864507675171
epoch£º723	 i:8 	 global-step:14468	 l-p:0.5111203193664551
epoch£º723	 i:9 	 global-step:14469	 l-p:0.13174070417881012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0408, 5.0381, 5.0407],
        [5.0408, 4.9826, 4.7080],
        [5.0408, 5.0290, 5.0396],
        [5.0408, 4.8273, 4.7694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.11939666420221329 
model_pd.l_d.mean(): -19.823301315307617 
model_pd.lagr.mean(): -19.70390510559082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5133], device='cuda:0')), ('power', tensor([-20.5643], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.11939666420221329
epoch£º724	 i:1 	 global-step:14481	 l-p:0.10912154614925385
epoch£º724	 i:2 	 global-step:14482	 l-p:0.21105891466140747
epoch£º724	 i:3 	 global-step:14483	 l-p:0.14768798649311066
epoch£º724	 i:4 	 global-step:14484	 l-p:0.12138618528842926
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14616553485393524
epoch£º724	 i:6 	 global-step:14486	 l-p:0.1314818561077118
epoch£º724	 i:7 	 global-step:14487	 l-p:0.14273099601268768
epoch£º724	 i:8 	 global-step:14488	 l-p:0.21506644785404205
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10406342893838882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0889, 5.0847, 5.0887],
        [5.0889, 4.9494, 4.6838],
        [5.0889, 4.9117, 4.6678],
        [5.0889, 4.9603, 5.0033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13102921843528748 
model_pd.l_d.mean(): -20.569292068481445 
model_pd.lagr.mean(): -20.438262939453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.2337], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13102921843528748
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12517280876636505
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13018636405467987
epoch£º725	 i:3 	 global-step:14503	 l-p:0.15789498388767242
epoch£º725	 i:4 	 global-step:14504	 l-p:0.1298999935388565
epoch£º725	 i:5 	 global-step:14505	 l-p:0.26289793848991394
epoch£º725	 i:6 	 global-step:14506	 l-p:0.19149470329284668
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10289206355810165
epoch£º725	 i:8 	 global-step:14508	 l-p:0.11258724331855774
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12584485113620758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0434, 4.9201, 4.9661],
        [5.0434, 4.8173, 4.6856],
        [5.0434, 5.0030, 4.7307],
        [5.0434, 5.2491, 5.0813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.16930349171161652 
model_pd.l_d.mean(): -19.222393035888672 
model_pd.lagr.mean(): -19.053089141845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5707], device='cuda:0')), ('power', tensor([-20.0155], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.16930349171161652
epoch£º726	 i:1 	 global-step:14521	 l-p:0.18047355115413666
epoch£º726	 i:2 	 global-step:14522	 l-p:0.1901969611644745
epoch£º726	 i:3 	 global-step:14523	 l-p:0.18127784132957458
epoch£º726	 i:4 	 global-step:14524	 l-p:0.14591805636882782
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13061945140361786
epoch£º726	 i:6 	 global-step:14526	 l-p:0.20629504323005676
epoch£º726	 i:7 	 global-step:14527	 l-p:0.06460227072238922
epoch£º726	 i:8 	 global-step:14528	 l-p:0.1508837789297104
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12233424931764603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0906, 5.0906, 5.0906],
        [5.0906, 5.2437, 5.0462],
        [5.0906, 5.5068, 5.4702],
        [5.0906, 5.0906, 5.0906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.15795280039310455 
model_pd.l_d.mean(): -20.527301788330078 
model_pd.lagr.mean(): -20.369348526000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4715], device='cuda:0')), ('power', tensor([-21.2333], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.15795280039310455
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12341362982988358
epoch£º727	 i:2 	 global-step:14542	 l-p:0.1379377692937851
epoch£º727	 i:3 	 global-step:14543	 l-p:0.114386647939682
epoch£º727	 i:4 	 global-step:14544	 l-p:0.13553445041179657
epoch£º727	 i:5 	 global-step:14545	 l-p:0.20416201651096344
epoch£º727	 i:6 	 global-step:14546	 l-p:0.12589821219444275
epoch£º727	 i:7 	 global-step:14547	 l-p:0.16086997091770172
epoch£º727	 i:8 	 global-step:14548	 l-p:0.15035182237625122
epoch£º727	 i:9 	 global-step:14549	 l-p:0.0719057247042656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1125, 5.0992, 5.1111],
        [5.1125, 5.1118, 5.1125],
        [5.1125, 4.9360, 4.6934],
        [5.1125, 5.5665, 5.5551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.13014961779117584 
model_pd.l_d.mean(): -20.468181610107422 
model_pd.lagr.mean(): -20.338031768798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-21.1630], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.13014961779117584
epoch£º728	 i:1 	 global-step:14561	 l-p:0.10215915739536285
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12619511783123016
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1460012048482895
epoch£º728	 i:4 	 global-step:14564	 l-p:0.1386675089597702
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13276217877864838
epoch£º728	 i:6 	 global-step:14566	 l-p:-0.37957778573036194
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13744772970676422
epoch£º728	 i:8 	 global-step:14568	 l-p:0.1139579564332962
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10289213061332703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1762, 5.0424, 5.0812],
        [5.1762, 5.1041, 5.1479],
        [5.1762, 5.0150, 5.0341],
        [5.1762, 5.1762, 5.1762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): -2.5300843715667725 
model_pd.l_d.mean(): -19.30739402770996 
model_pd.lagr.mean(): -21.837478637695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5677], device='cuda:0')), ('power', tensor([-20.0984], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:-2.5300843715667725
epoch£º729	 i:1 	 global-step:14581	 l-p:0.11377952247858047
epoch£º729	 i:2 	 global-step:14582	 l-p:0.1612357795238495
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12778231501579285
epoch£º729	 i:4 	 global-step:14584	 l-p:0.14144489169120789
epoch£º729	 i:5 	 global-step:14585	 l-p:0.11494612693786621
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12527047097682953
epoch£º729	 i:7 	 global-step:14587	 l-p:0.03760001063346863
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13593894243240356
epoch£º729	 i:9 	 global-step:14589	 l-p:0.12120171636343002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1098, 5.0355, 4.7616],
        [5.1098, 5.3207, 5.1536],
        [5.1098, 4.9365, 4.6896],
        [5.1098, 4.9456, 4.6921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05362643301486969 
model_pd.l_d.mean(): -19.29609489440918 
model_pd.lagr.mean(): -19.242467880249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5556], device='cuda:0')), ('power', tensor([-20.0746], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05362643301486969
epoch£º730	 i:1 	 global-step:14601	 l-p:0.14032652974128723
epoch£º730	 i:2 	 global-step:14602	 l-p:0.18144340813159943
epoch£º730	 i:3 	 global-step:14603	 l-p:0.12746959924697876
epoch£º730	 i:4 	 global-step:14604	 l-p:0.14740732312202454
epoch£º730	 i:5 	 global-step:14605	 l-p:0.13525722920894623
epoch£º730	 i:6 	 global-step:14606	 l-p:0.13902589678764343
epoch£º730	 i:7 	 global-step:14607	 l-p:0.1403724104166031
epoch£º730	 i:8 	 global-step:14608	 l-p:0.37967512011528015
epoch£º730	 i:9 	 global-step:14609	 l-p:0.09347809851169586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0214, 5.3278, 5.2205],
        [5.0214, 4.8397, 4.8481],
        [5.0214, 4.7913, 4.6489],
        [5.0214, 5.0105, 5.0204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.12029866129159927 
model_pd.l_d.mean(): -20.277666091918945 
model_pd.lagr.mean(): -20.157367706298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-21.0319], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.12029866129159927
epoch£º731	 i:1 	 global-step:14621	 l-p:0.13093386590480804
epoch£º731	 i:2 	 global-step:14622	 l-p:0.21576888859272003
epoch£º731	 i:3 	 global-step:14623	 l-p:0.3370382487773895
epoch£º731	 i:4 	 global-step:14624	 l-p:0.1391262412071228
epoch£º731	 i:5 	 global-step:14625	 l-p:0.32138460874557495
epoch£º731	 i:6 	 global-step:14626	 l-p:0.10177183151245117
epoch£º731	 i:7 	 global-step:14627	 l-p:-0.10891184210777283
epoch£º731	 i:8 	 global-step:14628	 l-p:0.05605466663837433
epoch£º731	 i:9 	 global-step:14629	 l-p:0.12172795087099075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9753, 4.8980, 4.6157],
        [4.9753, 4.9729, 4.9752],
        [4.9753, 4.9753, 4.9753],
        [4.9753, 4.9716, 4.9751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): -1.0213972330093384 
model_pd.l_d.mean(): -19.534770965576172 
model_pd.lagr.mean(): -20.556167602539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5602], device='cuda:0')), ('power', tensor([-20.3206], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:-1.0213972330093384
epoch£º732	 i:1 	 global-step:14641	 l-p:0.433916300535202
epoch£º732	 i:2 	 global-step:14642	 l-p:0.12656794488430023
epoch£º732	 i:3 	 global-step:14643	 l-p:0.13774476945400238
epoch£º732	 i:4 	 global-step:14644	 l-p:0.8124796152114868
epoch£º732	 i:5 	 global-step:14645	 l-p:0.12100060284137726
epoch£º732	 i:6 	 global-step:14646	 l-p:0.19897733628749847
epoch£º732	 i:7 	 global-step:14647	 l-p:0.0971413403749466
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13878555595874786
epoch£º732	 i:9 	 global-step:14649	 l-p:0.2819361686706543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0514, 5.0514, 5.0514],
        [5.0514, 5.0395, 5.0502],
        [5.0514, 4.9738, 5.0199],
        [5.0514, 5.0188, 5.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13341380655765533 
model_pd.l_d.mean(): -19.853574752807617 
model_pd.lagr.mean(): -19.72016143798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5178], device='cuda:0')), ('power', tensor([-20.5995], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13341380655765533
epoch£º733	 i:1 	 global-step:14661	 l-p:0.1089518740773201
epoch£º733	 i:2 	 global-step:14662	 l-p:0.2297087460756302
epoch£º733	 i:3 	 global-step:14663	 l-p:0.1768224537372589
epoch£º733	 i:4 	 global-step:14664	 l-p:0.07224003970623016
epoch£º733	 i:5 	 global-step:14665	 l-p:0.15111804008483887
epoch£º733	 i:6 	 global-step:14666	 l-p:0.1758294701576233
epoch£º733	 i:7 	 global-step:14667	 l-p:0.10790418088436127
epoch£º733	 i:8 	 global-step:14668	 l-p:0.16875103116035461
epoch£º733	 i:9 	 global-step:14669	 l-p:0.10783971846103668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1062, 5.1050, 5.1062],
        [5.1062, 5.2261, 5.0108],
        [5.1062, 5.4433, 5.3525],
        [5.1062, 4.9067, 4.6870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.15929833054542542 
model_pd.l_d.mean(): -20.442594528198242 
model_pd.lagr.mean(): -20.283296585083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.1376], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.15929833054542542
epoch£º734	 i:1 	 global-step:14681	 l-p:0.11354369670152664
epoch£º734	 i:2 	 global-step:14682	 l-p:0.11064185947179794
epoch£º734	 i:3 	 global-step:14683	 l-p:0.16966022551059723
epoch£º734	 i:4 	 global-step:14684	 l-p:0.12220308184623718
epoch£º734	 i:5 	 global-step:14685	 l-p:0.1458985060453415
epoch£º734	 i:6 	 global-step:14686	 l-p:0.08738760650157928
epoch£º734	 i:7 	 global-step:14687	 l-p:0.10754463821649551
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13205963373184204
epoch£º734	 i:9 	 global-step:14689	 l-p:0.24485120177268982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 4.8624, 4.7629],
        [5.0849, 5.0024, 5.0494],
        [5.0849, 4.9975, 4.7203],
        [5.0849, 4.9671, 4.6920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.14626996219158173 
model_pd.l_d.mean(): -20.022172927856445 
model_pd.lagr.mean(): -19.87590217590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-20.7304], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.14626996219158173
epoch£º735	 i:1 	 global-step:14701	 l-p:0.0974486917257309
epoch£º735	 i:2 	 global-step:14702	 l-p:0.10511106252670288
epoch£º735	 i:3 	 global-step:14703	 l-p:0.13084186613559723
epoch£º735	 i:4 	 global-step:14704	 l-p:0.2213868796825409
epoch£º735	 i:5 	 global-step:14705	 l-p:0.13168077170848846
epoch£º735	 i:6 	 global-step:14706	 l-p:0.112418532371521
epoch£º735	 i:7 	 global-step:14707	 l-p:0.15524686872959137
epoch£º735	 i:8 	 global-step:14708	 l-p:0.14499478042125702
epoch£º735	 i:9 	 global-step:14709	 l-p:0.22895099222660065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0755, 5.4107, 5.3192],
        [5.0755, 5.0095, 5.0522],
        [5.0755, 4.8497, 4.6978],
        [5.0755, 4.8492, 4.7061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16324573755264282 
model_pd.l_d.mean(): -20.444683074951172 
model_pd.lagr.mean(): -20.281436920166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4893], device='cuda:0')), ('power', tensor([-21.1680], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16324573755264282
epoch£º736	 i:1 	 global-step:14721	 l-p:0.10675468295812607
epoch£º736	 i:2 	 global-step:14722	 l-p:0.19114923477172852
epoch£º736	 i:3 	 global-step:14723	 l-p:0.1397709846496582
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09738500416278839
epoch£º736	 i:5 	 global-step:14725	 l-p:0.13085772097110748
epoch£º736	 i:6 	 global-step:14726	 l-p:0.13902254402637482
epoch£º736	 i:7 	 global-step:14727	 l-p:0.13414449989795685
epoch£º736	 i:8 	 global-step:14728	 l-p:0.09537994116544724
epoch£º736	 i:9 	 global-step:14729	 l-p:0.18283028900623322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0930, 5.0599, 5.0862],
        [5.0930, 5.5294, 5.5049],
        [5.0930, 4.9900, 4.7126],
        [5.0930, 5.4619, 5.3916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12373223155736923 
model_pd.l_d.mean(): -20.690479278564453 
model_pd.lagr.mean(): -20.566747665405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4226], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12373223155736923
epoch£º737	 i:1 	 global-step:14741	 l-p:0.2020931988954544
epoch£º737	 i:2 	 global-step:14742	 l-p:0.08686435222625732
epoch£º737	 i:3 	 global-step:14743	 l-p:0.110548235476017
epoch£º737	 i:4 	 global-step:14744	 l-p:0.18241149187088013
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12810732424259186
epoch£º737	 i:6 	 global-step:14746	 l-p:0.14317269623279572
epoch£º737	 i:7 	 global-step:14747	 l-p:0.1337662637233734
epoch£º737	 i:8 	 global-step:14748	 l-p:0.14402838051319122
epoch£º737	 i:9 	 global-step:14749	 l-p:0.12257184833288193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1052, 5.0930, 5.1039],
        [5.1052, 5.1027, 5.1051],
        [5.1052, 5.1044, 5.1052],
        [5.1052, 5.1052, 5.1052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.10378232598304749 
model_pd.l_d.mean(): -20.27227020263672 
model_pd.lagr.mean(): -20.168487548828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.9922], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.10378232598304749
epoch£º738	 i:1 	 global-step:14761	 l-p:0.13650794327259064
epoch£º738	 i:2 	 global-step:14762	 l-p:0.09022485464811325
epoch£º738	 i:3 	 global-step:14763	 l-p:0.1442689448595047
epoch£º738	 i:4 	 global-step:14764	 l-p:0.1209535226225853
epoch£º738	 i:5 	 global-step:14765	 l-p:0.1454474776983261
epoch£º738	 i:6 	 global-step:14766	 l-p:0.1313125491142273
epoch£º738	 i:7 	 global-step:14767	 l-p:0.1518394649028778
epoch£º738	 i:8 	 global-step:14768	 l-p:0.14646440744400024
epoch£º738	 i:9 	 global-step:14769	 l-p:0.16490063071250916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0947, 5.0947, 5.0947],
        [5.0947, 5.0947, 5.0947],
        [5.0947, 5.0342, 4.7572],
        [5.0947, 4.9630, 5.0068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.1512739509344101 
model_pd.l_d.mean(): -19.719472885131836 
model_pd.lagr.mean(): -19.568199157714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5293], device='cuda:0')), ('power', tensor([-20.4757], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.1512739509344101
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13050813972949982
epoch£º739	 i:2 	 global-step:14782	 l-p:0.18009474873542786
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15885311365127563
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12646044790744781
epoch£º739	 i:5 	 global-step:14785	 l-p:0.18195535242557526
epoch£º739	 i:6 	 global-step:14786	 l-p:0.0933803841471672
epoch£º739	 i:7 	 global-step:14787	 l-p:0.13242411613464355
epoch£º739	 i:8 	 global-step:14788	 l-p:0.08367228507995605
epoch£º739	 i:9 	 global-step:14789	 l-p:0.09549941122531891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 5.1237, 5.1237],
        [5.1237, 5.1125, 5.1226],
        [5.1237, 4.9140, 4.8641],
        [5.1237, 4.9217, 4.7017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.1315605193376541 
model_pd.l_d.mean(): -19.21927833557129 
model_pd.lagr.mean(): -19.087717056274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-19.9720], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.1315605193376541
epoch£º740	 i:1 	 global-step:14801	 l-p:0.0830211490392685
epoch£º740	 i:2 	 global-step:14802	 l-p:0.10189256072044373
epoch£º740	 i:3 	 global-step:14803	 l-p:0.07570799440145493
epoch£º740	 i:4 	 global-step:14804	 l-p:0.13934580981731415
epoch£º740	 i:5 	 global-step:14805	 l-p:0.13586002588272095
epoch£º740	 i:6 	 global-step:14806	 l-p:0.14829638600349426
epoch£º740	 i:7 	 global-step:14807	 l-p:0.15910767018795013
epoch£º740	 i:8 	 global-step:14808	 l-p:0.13690896332263947
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15325607359409332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1243, 4.9625, 4.7023],
        [5.1243, 5.5402, 5.4995],
        [5.1243, 4.9705, 4.7062],
        [5.1243, 5.1243, 5.1243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11703762412071228 
model_pd.l_d.mean(): -20.337596893310547 
model_pd.lagr.mean(): -20.22056007385254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-21.0544], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11703762412071228
epoch£º741	 i:1 	 global-step:14821	 l-p:0.11096828430891037
epoch£º741	 i:2 	 global-step:14822	 l-p:0.12853509187698364
epoch£º741	 i:3 	 global-step:14823	 l-p:0.15798310935497284
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07306566834449768
epoch£º741	 i:5 	 global-step:14825	 l-p:0.11505008488893509
epoch£º741	 i:6 	 global-step:14826	 l-p:0.1469302773475647
epoch£º741	 i:7 	 global-step:14827	 l-p:0.14711695909500122
epoch£º741	 i:8 	 global-step:14828	 l-p:0.16048884391784668
epoch£º741	 i:9 	 global-step:14829	 l-p:0.1481328308582306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0887, 5.4951, 5.4490],
        [5.0887, 4.9630, 5.0092],
        [5.0887, 5.0847, 5.0886],
        [5.0887, 4.8647, 4.7750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.13457052409648895 
model_pd.l_d.mean(): -19.844234466552734 
model_pd.lagr.mean(): -19.70966339111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5503], device='cuda:0')), ('power', tensor([-20.6233], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.13457052409648895
epoch£º742	 i:1 	 global-step:14841	 l-p:0.11435342580080032
epoch£º742	 i:2 	 global-step:14842	 l-p:0.1143592819571495
epoch£º742	 i:3 	 global-step:14843	 l-p:0.1317344307899475
epoch£º742	 i:4 	 global-step:14844	 l-p:0.22937797009944916
epoch£º742	 i:5 	 global-step:14845	 l-p:0.14457517862319946
epoch£º742	 i:6 	 global-step:14846	 l-p:0.19655346870422363
epoch£º742	 i:7 	 global-step:14847	 l-p:0.1214764341711998
epoch£º742	 i:8 	 global-step:14848	 l-p:0.06731522083282471
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12502643465995789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1144, 5.1144, 5.1144],
        [5.1144, 5.0503, 5.0923],
        [5.1144, 5.0793, 4.8056],
        [5.1144, 4.9432, 4.6859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.05203114449977875 
model_pd.l_d.mean(): -19.88136100769043 
model_pd.lagr.mean(): -19.829330444335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5193], device='cuda:0')), ('power', tensor([-20.6292], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.05203114449977875
epoch£º743	 i:1 	 global-step:14861	 l-p:0.12883639335632324
epoch£º743	 i:2 	 global-step:14862	 l-p:0.13284586369991302
epoch£º743	 i:3 	 global-step:14863	 l-p:0.1327429711818695
epoch£º743	 i:4 	 global-step:14864	 l-p:0.12372443825006485
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12613922357559204
epoch£º743	 i:6 	 global-step:14866	 l-p:0.16238828003406525
epoch£º743	 i:7 	 global-step:14867	 l-p:0.14142195880413055
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12383300811052322
epoch£º743	 i:9 	 global-step:14869	 l-p:0.16019207239151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1031, 5.0188, 5.0665],
        [5.1031, 4.9690, 5.0125],
        [5.1031, 5.0203, 5.0677],
        [5.1031, 5.0650, 5.0945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.13289383053779602 
model_pd.l_d.mean(): -20.917301177978516 
model_pd.lagr.mean(): -20.784406661987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3928], device='cuda:0')), ('power', tensor([-21.5471], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.13289383053779602
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13067105412483215
epoch£º744	 i:2 	 global-step:14882	 l-p:0.10996560752391815
epoch£º744	 i:3 	 global-step:14883	 l-p:0.1114840880036354
epoch£º744	 i:4 	 global-step:14884	 l-p:0.2101779580116272
epoch£º744	 i:5 	 global-step:14885	 l-p:0.09418321400880814
epoch£º744	 i:6 	 global-step:14886	 l-p:0.15125000476837158
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13351523876190186
epoch£º744	 i:8 	 global-step:14888	 l-p:-0.4793141186237335
epoch£º744	 i:9 	 global-step:14889	 l-p:0.2813877761363983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0210, 4.9877, 5.0143],
        [5.0210, 5.0209, 5.0210],
        [5.0210, 5.4258, 5.3803],
        [5.0210, 4.8651, 4.5869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.3072100877761841 
model_pd.l_d.mean(): -19.69388198852539 
model_pd.lagr.mean(): -19.38667106628418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5540], device='cuda:0')), ('power', tensor([-20.4750], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.3072100877761841
epoch£º745	 i:1 	 global-step:14901	 l-p:0.13942335546016693
epoch£º745	 i:2 	 global-step:14902	 l-p:0.26840609312057495
epoch£º745	 i:3 	 global-step:14903	 l-p:0.11005423963069916
epoch£º745	 i:4 	 global-step:14904	 l-p:0.1338634341955185
epoch£º745	 i:5 	 global-step:14905	 l-p:0.12117315083742142
epoch£º745	 i:6 	 global-step:14906	 l-p:0.3170328438282013
epoch£º745	 i:7 	 global-step:14907	 l-p:0.15592217445373535
epoch£º745	 i:8 	 global-step:14908	 l-p:0.18121658265590668
epoch£º745	 i:9 	 global-step:14909	 l-p:0.11842664331197739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0780, 5.0714, 5.0775],
        [5.0780, 5.0684, 5.0772],
        [5.0780, 5.1237, 4.8728],
        [5.0780, 5.0780, 5.0780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.14593413472175598 
model_pd.l_d.mean(): -19.259384155273438 
model_pd.lagr.mean(): -19.113449096679688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5536], device='cuda:0')), ('power', tensor([-20.0354], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.14593413472175598
epoch£º746	 i:1 	 global-step:14921	 l-p:0.19922293722629547
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12727759778499603
epoch£º746	 i:3 	 global-step:14923	 l-p:0.1325867623090744
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12293517589569092
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12827977538108826
epoch£º746	 i:6 	 global-step:14926	 l-p:0.13347814977169037
epoch£º746	 i:7 	 global-step:14927	 l-p:0.10752321034669876
epoch£º746	 i:8 	 global-step:14928	 l-p:0.11020051687955856
epoch£º746	 i:9 	 global-step:14929	 l-p:0.12128152698278427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.0832, 5.1206],
        [5.1361, 5.0982, 5.1275],
        [5.1361, 5.1460, 4.8831],
        [5.1361, 5.1667, 4.9106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18084609508514404 
model_pd.l_d.mean(): -19.934932708740234 
model_pd.lagr.mean(): -19.754087448120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.6641], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18084609508514404
epoch£º747	 i:1 	 global-step:14941	 l-p:0.1632770299911499
epoch£º747	 i:2 	 global-step:14942	 l-p:0.1355373114347458
epoch£º747	 i:3 	 global-step:14943	 l-p:0.10485366731882095
epoch£º747	 i:4 	 global-step:14944	 l-p:0.1326868236064911
epoch£º747	 i:5 	 global-step:14945	 l-p:0.10021047294139862
epoch£º747	 i:6 	 global-step:14946	 l-p:0.1534663587808609
epoch£º747	 i:7 	 global-step:14947	 l-p:0.08191241323947906
epoch£º747	 i:8 	 global-step:14948	 l-p:0.1473883092403412
epoch£º747	 i:9 	 global-step:14949	 l-p:0.143027201294899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0768, 4.8846, 4.8819],
        [5.0768, 5.4749, 5.4224],
        [5.0768, 4.9670, 5.0168],
        [5.0768, 4.8702, 4.8430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.16131697595119476 
model_pd.l_d.mean(): -20.736190795898438 
model_pd.lagr.mean(): -20.574872970581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-21.4121], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.16131697595119476
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15119722485542297
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1790851205587387
epoch£º748	 i:3 	 global-step:14963	 l-p:0.13056448101997375
epoch£º748	 i:4 	 global-step:14964	 l-p:0.09189306199550629
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2516704499721527
epoch£º748	 i:6 	 global-step:14966	 l-p:0.1023520827293396
epoch£º748	 i:7 	 global-step:14967	 l-p:0.16736644506454468
epoch£º748	 i:8 	 global-step:14968	 l-p:0.125417560338974
epoch£º748	 i:9 	 global-step:14969	 l-p:0.12508951127529144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0751, 5.4324, 5.3526],
        [5.0751, 5.0521, 5.0715],
        [5.0751, 5.0479, 5.0704],
        [5.0751, 4.8523, 4.6495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.1381993144750595 
model_pd.l_d.mean(): -20.092220306396484 
model_pd.lagr.mean(): -19.954021453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5269], device='cuda:0')), ('power', tensor([-20.8500], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.1381993144750595
epoch£º749	 i:1 	 global-step:14981	 l-p:0.13000020384788513
epoch£º749	 i:2 	 global-step:14982	 l-p:0.13086393475532532
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13454000651836395
epoch£º749	 i:4 	 global-step:14984	 l-p:0.11005246639251709
epoch£º749	 i:5 	 global-step:14985	 l-p:0.15321826934814453
epoch£º749	 i:6 	 global-step:14986	 l-p:0.28246352076530457
epoch£º749	 i:7 	 global-step:14987	 l-p:0.2007560282945633
epoch£º749	 i:8 	 global-step:14988	 l-p:0.14343209564685822
epoch£º749	 i:9 	 global-step:14989	 l-p:0.124918133020401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0762, 4.9826, 4.6982],
        [5.0762, 5.3916, 5.2846],
        [5.0762, 5.0718, 5.0760],
        [5.0762, 5.0708, 5.0759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.12005768716335297 
model_pd.l_d.mean(): -20.414012908935547 
model_pd.lagr.mean(): -20.293954849243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-21.1256], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.12005768716335297
epoch£º750	 i:1 	 global-step:15001	 l-p:0.1973593533039093
epoch£º750	 i:2 	 global-step:15002	 l-p:0.19916746020317078
epoch£º750	 i:3 	 global-step:15003	 l-p:0.20068645477294922
epoch£º750	 i:4 	 global-step:15004	 l-p:0.039528265595436096
epoch£º750	 i:5 	 global-step:15005	 l-p:0.1368119716644287
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12452926486730576
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12250041961669922
epoch£º750	 i:8 	 global-step:15008	 l-p:0.12430774420499802
epoch£º750	 i:9 	 global-step:15009	 l-p:0.1334875077009201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1133, 5.0011, 5.0503],
        [5.1133, 5.0494, 4.7687],
        [5.1133, 5.1133, 5.1133],
        [5.1133, 4.9432, 4.9643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.15297958254814148 
model_pd.l_d.mean(): -20.676429748535156 
model_pd.lagr.mean(): -20.52345085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566], device='cuda:0')), ('power', tensor([-21.3689], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.15297958254814148
epoch£º751	 i:1 	 global-step:15021	 l-p:0.12697890400886536
epoch£º751	 i:2 	 global-step:15022	 l-p:0.16146358847618103
epoch£º751	 i:3 	 global-step:15023	 l-p:0.17253875732421875
epoch£º751	 i:4 	 global-step:15024	 l-p:0.08179223537445068
epoch£º751	 i:5 	 global-step:15025	 l-p:0.11794769018888474
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14491280913352966
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13112521171569824
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14893002808094025
epoch£º751	 i:9 	 global-step:15029	 l-p:0.09796116501092911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1066, 5.1066, 5.1066],
        [5.1066, 5.0660, 5.0970],
        [5.1066, 4.9797, 5.0264],
        [5.1066, 5.1023, 5.1064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.10517173260450363 
model_pd.l_d.mean(): -19.461015701293945 
model_pd.lagr.mean(): -19.355844497680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5424], device='cuda:0')), ('power', tensor([-20.2279], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.10517173260450363
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12218067049980164
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09298568218946457
epoch£º752	 i:3 	 global-step:15043	 l-p:0.12897205352783203
epoch£º752	 i:4 	 global-step:15044	 l-p:0.16963215172290802
epoch£º752	 i:5 	 global-step:15045	 l-p:0.1765325516462326
epoch£º752	 i:6 	 global-step:15046	 l-p:0.13008685410022736
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13622571527957916
epoch£º752	 i:8 	 global-step:15048	 l-p:0.1292032152414322
epoch£º752	 i:9 	 global-step:15049	 l-p:0.12104097008705139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 5.1306, 5.1325],
        [5.1325, 5.1325, 5.1325],
        [5.1325, 5.0005, 5.0451],
        [5.1325, 4.9173, 4.8634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13507339358329773 
model_pd.l_d.mean(): -20.58638572692871 
model_pd.lagr.mean(): -20.451313018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4584], device='cuda:0')), ('power', tensor([-21.2797], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13507339358329773
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14207464456558228
epoch£º753	 i:2 	 global-step:15062	 l-p:0.12983262538909912
epoch£º753	 i:3 	 global-step:15063	 l-p:0.15607139468193054
epoch£º753	 i:4 	 global-step:15064	 l-p:0.154682919383049
epoch£º753	 i:5 	 global-step:15065	 l-p:0.06871452927589417
epoch£º753	 i:6 	 global-step:15066	 l-p:0.2013481855392456
epoch£º753	 i:7 	 global-step:15067	 l-p:0.15484875440597534
epoch£º753	 i:8 	 global-step:15068	 l-p:0.14009158313274384
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12763404846191406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0805, 5.0750, 5.0802],
        [5.0805, 5.0764, 5.0803],
        [5.0805, 5.1743, 4.9423],
        [5.0805, 5.0803, 5.0805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.11910838633775711 
model_pd.l_d.mean(): -20.517871856689453 
model_pd.lagr.mean(): -20.39876365661621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4588], device='cuda:0')), ('power', tensor([-21.2108], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.11910838633775711
epoch£º754	 i:1 	 global-step:15081	 l-p:0.11124148964881897
epoch£º754	 i:2 	 global-step:15082	 l-p:0.1615152657032013
epoch£º754	 i:3 	 global-step:15083	 l-p:0.22940701246261597
epoch£º754	 i:4 	 global-step:15084	 l-p:0.19945506751537323
epoch£º754	 i:5 	 global-step:15085	 l-p:0.1237151250243187
epoch£º754	 i:6 	 global-step:15086	 l-p:0.13057532906532288
epoch£º754	 i:7 	 global-step:15087	 l-p:0.15073463320732117
epoch£º754	 i:8 	 global-step:15088	 l-p:0.14608187973499298
epoch£º754	 i:9 	 global-step:15089	 l-p:0.11301881819963455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0670, 4.9031, 4.9333],
        [5.0670, 5.0249, 5.0569],
        [5.0670, 4.9624, 4.6753],
        [5.0670, 4.8387, 4.7618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10940391570329666 
model_pd.l_d.mean(): -20.120513916015625 
model_pd.lagr.mean(): -20.011110305786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5042], device='cuda:0')), ('power', tensor([-20.8555], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10940391570329666
epoch£º755	 i:1 	 global-step:15101	 l-p:0.14759431779384613
epoch£º755	 i:2 	 global-step:15102	 l-p:0.15636399388313293
epoch£º755	 i:3 	 global-step:15103	 l-p:0.17240524291992188
epoch£º755	 i:4 	 global-step:15104	 l-p:0.15070658922195435
epoch£º755	 i:5 	 global-step:15105	 l-p:-0.1300811916589737
epoch£º755	 i:6 	 global-step:15106	 l-p:0.12324880063533783
epoch£º755	 i:7 	 global-step:15107	 l-p:0.1445079743862152
epoch£º755	 i:8 	 global-step:15108	 l-p:-0.0792224258184433
epoch£º755	 i:9 	 global-step:15109	 l-p:1.6976624727249146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9913, 4.8454, 4.5559],
        [4.9913, 4.9913, 4.9913],
        [4.9913, 4.7491, 4.5653],
        [4.9913, 4.7534, 4.6656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.12519845366477966 
model_pd.l_d.mean(): -20.65827751159668 
model_pd.lagr.mean(): -20.533079147338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.3625], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.12519845366477966
epoch£º756	 i:1 	 global-step:15121	 l-p:-0.0016321658622473478
epoch£º756	 i:2 	 global-step:15122	 l-p:-0.546269953250885
epoch£º756	 i:3 	 global-step:15123	 l-p:0.23825177550315857
epoch£º756	 i:4 	 global-step:15124	 l-p:0.36810633540153503
epoch£º756	 i:5 	 global-step:15125	 l-p:0.1776810586452484
epoch£º756	 i:6 	 global-step:15126	 l-p:0.11329814791679382
epoch£º756	 i:7 	 global-step:15127	 l-p:0.142966628074646
epoch£º756	 i:8 	 global-step:15128	 l-p:0.08065108209848404
epoch£º756	 i:9 	 global-step:15129	 l-p:0.16203594207763672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0660, 4.8662, 4.6147],
        [5.0660, 4.9603, 4.6725],
        [5.0660, 4.9313, 4.9772],
        [5.0660, 5.0660, 5.0660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.14720246195793152 
model_pd.l_d.mean(): -18.72988510131836 
model_pd.lagr.mean(): -18.582683563232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5654], device='cuda:0')), ('power', tensor([-19.5122], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.14720246195793152
epoch£º757	 i:1 	 global-step:15141	 l-p:0.15763303637504578
epoch£º757	 i:2 	 global-step:15142	 l-p:0.1482110172510147
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12059648334980011
epoch£º757	 i:4 	 global-step:15144	 l-p:0.1602536141872406
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13142657279968262
epoch£º757	 i:6 	 global-step:15146	 l-p:0.11168387532234192
epoch£º757	 i:7 	 global-step:15147	 l-p:0.068110391497612
epoch£º757	 i:8 	 global-step:15148	 l-p:0.11040212213993073
epoch£º757	 i:9 	 global-step:15149	 l-p:0.15608283877372742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1640, 4.9778, 4.9794],
        [5.1640, 5.0036, 5.0323],
        [5.1640, 5.0936, 5.1380],
        [5.1640, 4.9386, 4.8371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.006055021192878485 
model_pd.l_d.mean(): -20.756046295166016 
model_pd.lagr.mean(): -20.749990463256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4065], device='cuda:0')), ('power', tensor([-21.3982], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.006055021192878485
epoch£º758	 i:1 	 global-step:15161	 l-p:0.1327737271785736
epoch£º758	 i:2 	 global-step:15162	 l-p:0.14987343549728394
epoch£º758	 i:3 	 global-step:15163	 l-p:0.07946842163801193
epoch£º758	 i:4 	 global-step:15164	 l-p:0.16581472754478455
epoch£º758	 i:5 	 global-step:15165	 l-p:0.10764572024345398
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11079463362693787
epoch£º758	 i:7 	 global-step:15167	 l-p:0.07899931818246841
epoch£º758	 i:8 	 global-step:15168	 l-p:-0.07941503077745438
epoch£º758	 i:9 	 global-step:15169	 l-p:0.1251036822795868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2239, 5.2238, 5.2239],
        [5.2239, 5.2239, 5.2239],
        [5.2239, 5.1332, 4.8555],
        [5.2239, 5.2239, 5.2239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): -0.1805564910173416 
model_pd.l_d.mean(): -19.833423614501953 
model_pd.lagr.mean(): -20.013980865478516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4417], device='cuda:0')), ('power', tensor([-20.5014], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:-0.1805564910173416
epoch£º759	 i:1 	 global-step:15181	 l-p:0.1284858137369156
epoch£º759	 i:2 	 global-step:15182	 l-p:0.11097702383995056
epoch£º759	 i:3 	 global-step:15183	 l-p:0.13652557134628296
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11481279134750366
epoch£º759	 i:5 	 global-step:15185	 l-p:0.14464417099952698
epoch£º759	 i:6 	 global-step:15186	 l-p:-0.00763406278565526
epoch£º759	 i:7 	 global-step:15187	 l-p:0.12464887648820877
epoch£º759	 i:8 	 global-step:15188	 l-p:0.17060701549053192
epoch£º759	 i:9 	 global-step:15189	 l-p:0.09886357933282852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2574, 5.2256, 5.2511],
        [5.2574, 5.0761, 5.0765],
        [5.2574, 5.0444, 4.8670],
        [5.2574, 5.1899, 5.2328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.16285225749015808 
model_pd.l_d.mean(): -19.252872467041016 
model_pd.lagr.mean(): -19.09002113342285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5212], device='cuda:0')), ('power', tensor([-19.9957], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.16285225749015808
epoch£º760	 i:1 	 global-step:15201	 l-p:0.08675367385149002
epoch£º760	 i:2 	 global-step:15202	 l-p:0.19221234321594238
epoch£º760	 i:3 	 global-step:15203	 l-p:0.1303970068693161
epoch£º760	 i:4 	 global-step:15204	 l-p:0.09049808233976364
epoch£º760	 i:5 	 global-step:15205	 l-p:0.11653581261634827
epoch£º760	 i:6 	 global-step:15206	 l-p:0.205062597990036
epoch£º760	 i:7 	 global-step:15207	 l-p:0.12244129180908203
epoch£º760	 i:8 	 global-step:15208	 l-p:0.12718597054481506
epoch£º760	 i:9 	 global-step:15209	 l-p:0.10102090239524841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3312, 5.3152, 5.3293],
        [5.3312, 5.1730, 4.9271],
        [5.3312, 5.1362, 5.1018],
        [5.3312, 5.3355, 5.0733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.019741715863347054 
model_pd.l_d.mean(): -18.410839080810547 
model_pd.lagr.mean(): -18.391098022460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5505], device='cuda:0')), ('power', tensor([-19.1744], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.019741715863347054
epoch£º761	 i:1 	 global-step:15221	 l-p:-0.0192826259881258
epoch£º761	 i:2 	 global-step:15222	 l-p:0.11199305951595306
epoch£º761	 i:3 	 global-step:15223	 l-p:0.12284500896930695
epoch£º761	 i:4 	 global-step:15224	 l-p:0.11793860793113708
epoch£º761	 i:5 	 global-step:15225	 l-p:0.12180411070585251
epoch£º761	 i:6 	 global-step:15226	 l-p:0.1320972740650177
epoch£º761	 i:7 	 global-step:15227	 l-p:0.12238814681768417
epoch£º761	 i:8 	 global-step:15228	 l-p:0.14482074975967407
epoch£º761	 i:9 	 global-step:15229	 l-p:0.1327812373638153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2809, 5.0939, 5.0833],
        [5.2809, 5.0837, 5.0532],
        [5.2809, 5.0879, 4.8697],
        [5.2809, 5.2809, 5.2809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.12030051648616791 
model_pd.l_d.mean(): -20.6240177154541 
model_pd.lagr.mean(): -20.50371742248535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3952], device='cuda:0')), ('power', tensor([-21.2531], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.12030051648616791
epoch£º762	 i:1 	 global-step:15241	 l-p:0.15166041254997253
epoch£º762	 i:2 	 global-step:15242	 l-p:0.32965487241744995
epoch£º762	 i:3 	 global-step:15243	 l-p:0.11411307752132416
epoch£º762	 i:4 	 global-step:15244	 l-p:0.11951451003551483
epoch£º762	 i:5 	 global-step:15245	 l-p:0.09860900044441223
epoch£º762	 i:6 	 global-step:15246	 l-p:0.05987054854631424
epoch£º762	 i:7 	 global-step:15247	 l-p:0.13869871199131012
epoch£º762	 i:8 	 global-step:15248	 l-p:-0.5615771412849426
epoch£º762	 i:9 	 global-step:15249	 l-p:0.13469281792640686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1836, 4.9565, 4.8211],
        [5.1836, 5.1783, 5.1833],
        [5.1836, 5.1431, 5.1740],
        [5.1836, 5.4188, 5.2578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.1118384301662445 
model_pd.l_d.mean(): -19.43094825744629 
model_pd.lagr.mean(): -19.319108963012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-20.1508], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.1118384301662445
epoch£º763	 i:1 	 global-step:15261	 l-p:0.14920201897621155
epoch£º763	 i:2 	 global-step:15262	 l-p:0.07835040241479874
epoch£º763	 i:3 	 global-step:15263	 l-p:-0.24189622700214386
epoch£º763	 i:4 	 global-step:15264	 l-p:0.13344645500183105
epoch£º763	 i:5 	 global-step:15265	 l-p:0.11020296066999435
epoch£º763	 i:6 	 global-step:15266	 l-p:0.15072856843471527
epoch£º763	 i:7 	 global-step:15267	 l-p:0.11538669466972351
epoch£º763	 i:8 	 global-step:15268	 l-p:0.13299943506717682
epoch£º763	 i:9 	 global-step:15269	 l-p:0.12501738965511322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1826, 5.1079, 5.1535],
        [5.1826, 5.1789, 5.1824],
        [5.1826, 4.9768, 4.7529],
        [5.1826, 4.9769, 4.9452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.14334061741828918 
model_pd.l_d.mean(): -20.764612197875977 
model_pd.lagr.mean(): -20.62127113342285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.4141], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.14334061741828918
epoch£º764	 i:1 	 global-step:15281	 l-p:0.08313851058483124
epoch£º764	 i:2 	 global-step:15282	 l-p:0.1397239714860916
epoch£º764	 i:3 	 global-step:15283	 l-p:0.14165309071540833
epoch£º764	 i:4 	 global-step:15284	 l-p:0.1255141794681549
epoch£º764	 i:5 	 global-step:15285	 l-p:0.12384249269962311
epoch£º764	 i:6 	 global-step:15286	 l-p:0.12981775403022766
epoch£º764	 i:7 	 global-step:15287	 l-p:0.1743692010641098
epoch£º764	 i:8 	 global-step:15288	 l-p:0.07451942563056946
epoch£º764	 i:9 	 global-step:15289	 l-p:0.08599533885717392
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1183, 5.0136, 5.0641],
        [5.1183, 5.1124, 5.1179],
        [5.1183, 5.1182, 5.1183],
        [5.1183, 4.9997, 5.0492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.12279132753610611 
model_pd.l_d.mean(): -18.579648971557617 
model_pd.lagr.mean(): -18.456857681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5808], device='cuda:0')), ('power', tensor([-19.3761], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.12279132753610611
epoch£º765	 i:1 	 global-step:15301	 l-p:0.19982893764972687
epoch£º765	 i:2 	 global-step:15302	 l-p:0.1216030865907669
epoch£º765	 i:3 	 global-step:15303	 l-p:0.12241329997777939
epoch£º765	 i:4 	 global-step:15304	 l-p:0.17515704035758972
epoch£º765	 i:5 	 global-step:15305	 l-p:0.1365026980638504
epoch£º765	 i:6 	 global-step:15306	 l-p:0.03908484801650047
epoch£º765	 i:7 	 global-step:15307	 l-p:0.13824462890625
epoch£º765	 i:8 	 global-step:15308	 l-p:0.09828710556030273
epoch£º765	 i:9 	 global-step:15309	 l-p:0.09987238049507141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1500, 5.5015, 5.4124],
        [5.1500, 4.9179, 4.7925],
        [5.1500, 5.1026, 5.1375],
        [5.1500, 5.2601, 5.0326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12789390981197357 
model_pd.l_d.mean(): -20.541364669799805 
model_pd.lagr.mean(): -20.413471221923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-21.2238], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12789390981197357
epoch£º766	 i:1 	 global-step:15321	 l-p:0.14216016232967377
epoch£º766	 i:2 	 global-step:15322	 l-p:0.15173301100730896
epoch£º766	 i:3 	 global-step:15323	 l-p:0.05259740725159645
epoch£º766	 i:4 	 global-step:15324	 l-p:0.16970176994800568
epoch£º766	 i:5 	 global-step:15325	 l-p:0.1328580379486084
epoch£º766	 i:6 	 global-step:15326	 l-p:0.12389843910932541
epoch£º766	 i:7 	 global-step:15327	 l-p:0.12142571061849594
epoch£º766	 i:8 	 global-step:15328	 l-p:0.10891687124967575
epoch£º766	 i:9 	 global-step:15329	 l-p:0.13674353063106537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1182, 5.0550, 5.0972],
        [5.1182, 5.1181, 5.1182],
        [5.1182, 5.1181, 5.1182],
        [5.1182, 5.3569, 5.1985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.12607663869857788 
model_pd.l_d.mean(): -20.82363510131836 
model_pd.lagr.mean(): -20.697559356689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.4748], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.12607663869857788
epoch£º767	 i:1 	 global-step:15341	 l-p:0.08827076852321625
epoch£º767	 i:2 	 global-step:15342	 l-p:0.12105817347764969
epoch£º767	 i:3 	 global-step:15343	 l-p:0.15901142358779907
epoch£º767	 i:4 	 global-step:15344	 l-p:0.14067746698856354
epoch£º767	 i:5 	 global-step:15345	 l-p:0.15321463346481323
epoch£º767	 i:6 	 global-step:15346	 l-p:0.2523896396160126
epoch£º767	 i:7 	 global-step:15347	 l-p:0.5275420546531677
epoch£º767	 i:8 	 global-step:15348	 l-p:-0.05253211781382561
epoch£º767	 i:9 	 global-step:15349	 l-p:0.09940774738788605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0143, 4.7672, 4.6350],
        [5.0143, 5.4657, 5.4479],
        [5.0143, 4.9656, 5.0015],
        [5.0143, 4.9185, 4.6234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.010297966189682484 
model_pd.l_d.mean(): -19.518421173095703 
model_pd.lagr.mean(): -19.50812339782715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5335], device='cuda:0')), ('power', tensor([-20.2768], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.010297966189682484
epoch£º768	 i:1 	 global-step:15361	 l-p:0.1309533268213272
epoch£º768	 i:2 	 global-step:15362	 l-p:0.5538896322250366
epoch£º768	 i:3 	 global-step:15363	 l-p:0.11393605917692184
epoch£º768	 i:4 	 global-step:15364	 l-p:0.1298072636127472
epoch£º768	 i:5 	 global-step:15365	 l-p:0.2041664868593216
epoch£º768	 i:6 	 global-step:15366	 l-p:0.160605326294899
epoch£º768	 i:7 	 global-step:15367	 l-p:0.07832992821931839
epoch£º768	 i:8 	 global-step:15368	 l-p:0.1726321429014206
epoch£º768	 i:9 	 global-step:15369	 l-p:0.11947225034236908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9883, 4.7413, 4.5480],
        [4.9883, 4.9322, 4.6419],
        [4.9883, 4.9242, 4.6321],
        [4.9883, 4.9741, 4.9868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.1358986496925354 
model_pd.l_d.mean(): -20.304229736328125 
model_pd.lagr.mean(): -20.168331146240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5162], device='cuda:0')), ('power', tensor([-21.0534], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.1358986496925354
epoch£º769	 i:1 	 global-step:15381	 l-p:0.34376245737075806
epoch£º769	 i:2 	 global-step:15382	 l-p:0.11570239812135696
epoch£º769	 i:3 	 global-step:15383	 l-p:0.016912847757339478
epoch£º769	 i:4 	 global-step:15384	 l-p:0.28406286239624023
epoch£º769	 i:5 	 global-step:15385	 l-p:0.15354666113853455
epoch£º769	 i:6 	 global-step:15386	 l-p:-0.24009305238723755
epoch£º769	 i:7 	 global-step:15387	 l-p:0.1463886946439743
epoch£º769	 i:8 	 global-step:15388	 l-p:0.2812578082084656
epoch£º769	 i:9 	 global-step:15389	 l-p:0.1382192075252533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0335, 5.0334, 5.0335],
        [5.0335, 4.9645, 4.6733],
        [5.0335, 4.7984, 4.5830],
        [5.0335, 4.9171, 4.9688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): -0.281158447265625 
model_pd.l_d.mean(): -18.339706420898438 
model_pd.lagr.mean(): -18.620864868164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6323], device='cuda:0')), ('power', tensor([-19.1861], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:-0.281158447265625
epoch£º770	 i:1 	 global-step:15401	 l-p:0.15348204970359802
epoch£º770	 i:2 	 global-step:15402	 l-p:0.11912959069013596
epoch£º770	 i:3 	 global-step:15403	 l-p:0.23732835054397583
epoch£º770	 i:4 	 global-step:15404	 l-p:0.1546991765499115
epoch£º770	 i:5 	 global-step:15405	 l-p:0.12640388309955597
epoch£º770	 i:6 	 global-step:15406	 l-p:0.12623876333236694
epoch£º770	 i:7 	 global-step:15407	 l-p:0.14779093861579895
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10672041028738022
epoch£º770	 i:9 	 global-step:15409	 l-p:0.1810864955186844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0663, 5.0662, 5.0663],
        [5.0663, 5.0235, 5.0560],
        [5.0663, 5.0617, 5.0661],
        [5.0663, 4.8257, 4.6484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.12533116340637207 
model_pd.l_d.mean(): -20.79226303100586 
model_pd.lagr.mean(): -20.66693115234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.4490], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.12533116340637207
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11226055026054382
epoch£º771	 i:2 	 global-step:15422	 l-p:0.17531509697437286
epoch£º771	 i:3 	 global-step:15423	 l-p:0.13450518250465393
epoch£º771	 i:4 	 global-step:15424	 l-p:3.6644930839538574
epoch£º771	 i:5 	 global-step:15425	 l-p:0.1382851004600525
epoch£º771	 i:6 	 global-step:15426	 l-p:0.13095802068710327
epoch£º771	 i:7 	 global-step:15427	 l-p:0.16280531883239746
epoch£º771	 i:8 	 global-step:15428	 l-p:0.19193385541439056
epoch£º771	 i:9 	 global-step:15429	 l-p:0.1625453382730484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0564, 5.0149, 5.0467],
        [5.0564, 4.9833, 5.0295],
        [5.0564, 5.3489, 5.2241],
        [5.0564, 5.0134, 5.0461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.38716307282447815 
model_pd.l_d.mean(): -20.810514450073242 
model_pd.lagr.mean(): -20.423351287841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4392], device='cuda:0')), ('power', tensor([-21.4866], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.38716307282447815
epoch£º772	 i:1 	 global-step:15441	 l-p:0.14432482421398163
epoch£º772	 i:2 	 global-step:15442	 l-p:0.08538192510604858
epoch£º772	 i:3 	 global-step:15443	 l-p:0.14298364520072937
epoch£º772	 i:4 	 global-step:15444	 l-p:0.18000824749469757
epoch£º772	 i:5 	 global-step:15445	 l-p:0.11339150369167328
epoch£º772	 i:6 	 global-step:15446	 l-p:0.11507707089185715
epoch£º772	 i:7 	 global-step:15447	 l-p:0.1734059900045395
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07387051731348038
epoch£º772	 i:9 	 global-step:15449	 l-p:0.1618146449327469
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1329, 5.1329, 5.1329],
        [5.1329, 5.0957, 5.1248],
        [5.1329, 4.9041, 4.8258],
        [5.1329, 4.9068, 4.8385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13483010232448578 
model_pd.l_d.mean(): -21.015382766723633 
model_pd.lagr.mean(): -20.880552291870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3822], device='cuda:0')), ('power', tensor([-21.6355], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13483010232448578
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13196586072444916
epoch£º773	 i:2 	 global-step:15462	 l-p:0.09045687317848206
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07716328650712967
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14161160588264465
epoch£º773	 i:5 	 global-step:15465	 l-p:0.16048963367938995
epoch£º773	 i:6 	 global-step:15466	 l-p:-0.991430938243866
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13249756395816803
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11699516326189041
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10999413579702377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2205, 5.1235, 5.1731],
        [5.2205, 5.1810, 5.2114],
        [5.2205, 4.9922, 4.8660],
        [5.2205, 5.2015, 5.2179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.08744464069604874 
model_pd.l_d.mean(): -20.721649169921875 
model_pd.lagr.mean(): -20.634204864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-21.3632], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.08744464069604874
epoch£º774	 i:1 	 global-step:15481	 l-p:0.1192568764090538
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12828204035758972
epoch£º774	 i:3 	 global-step:15483	 l-p:0.22012880444526672
epoch£º774	 i:4 	 global-step:15484	 l-p:0.13621972501277924
epoch£º774	 i:5 	 global-step:15485	 l-p:0.0681648924946785
epoch£º774	 i:6 	 global-step:15486	 l-p:0.13720561563968658
epoch£º774	 i:7 	 global-step:15487	 l-p:0.932860255241394
epoch£º774	 i:8 	 global-step:15488	 l-p:0.1258891522884369
epoch£º774	 i:9 	 global-step:15489	 l-p:0.11729627102613449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2626, 5.0407, 4.9448],
        [5.2626, 5.0552, 5.0140],
        [5.2626, 5.2626, 5.2626],
        [5.2626, 5.3982, 5.1802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.12590469419956207 
model_pd.l_d.mean(): -18.810213088989258 
model_pd.lagr.mean(): -18.684309005737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5982], device='cuda:0')), ('power', tensor([-19.6269], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.12590469419956207
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14222270250320435
epoch£º775	 i:2 	 global-step:15502	 l-p:0.09580842405557632
epoch£º775	 i:3 	 global-step:15503	 l-p:0.3858735263347626
epoch£º775	 i:4 	 global-step:15504	 l-p:0.09176892787218094
epoch£º775	 i:5 	 global-step:15505	 l-p:0.1132703348994255
epoch£º775	 i:6 	 global-step:15506	 l-p:0.1736139953136444
epoch£º775	 i:7 	 global-step:15507	 l-p:0.139029860496521
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13884270191192627
epoch£º775	 i:9 	 global-step:15509	 l-p:0.05314600467681885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2254, 5.0552, 5.0764],
        [5.2254, 5.0096, 4.9542],
        [5.2254, 5.0150, 4.9745],
        [5.2254, 5.5977, 5.5180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.2272690087556839 
model_pd.l_d.mean(): -20.611371994018555 
model_pd.lagr.mean(): -20.384103775024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4268], device='cuda:0')), ('power', tensor([-21.2726], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.2272690087556839
epoch£º776	 i:1 	 global-step:15521	 l-p:0.12009839713573456
epoch£º776	 i:2 	 global-step:15522	 l-p:0.13832594454288483
epoch£º776	 i:3 	 global-step:15523	 l-p:0.1276397854089737
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12559203803539276
epoch£º776	 i:5 	 global-step:15525	 l-p:0.11341754347085953
epoch£º776	 i:6 	 global-step:15526	 l-p:0.13111992180347443
epoch£º776	 i:7 	 global-step:15527	 l-p:0.09750688076019287
epoch£º776	 i:8 	 global-step:15528	 l-p:0.1541839987039566
epoch£º776	 i:9 	 global-step:15529	 l-p:0.13481953740119934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1389, 4.9290, 4.9042],
        [5.1389, 5.0553, 5.1039],
        [5.1389, 4.9181, 4.8690],
        [5.1389, 4.9010, 4.7503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.11898992210626602 
model_pd.l_d.mean(): -20.61281394958496 
model_pd.lagr.mean(): -20.493824005126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4392], device='cuda:0')), ('power', tensor([-21.2867], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.11898992210626602
epoch£º777	 i:1 	 global-step:15541	 l-p:0.14300145208835602
epoch£º777	 i:2 	 global-step:15542	 l-p:0.15073516964912415
epoch£º777	 i:3 	 global-step:15543	 l-p:0.16037750244140625
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16055123507976532
epoch£º777	 i:5 	 global-step:15545	 l-p:0.0872957780957222
epoch£º777	 i:6 	 global-step:15546	 l-p:0.13777607679367065
epoch£º777	 i:7 	 global-step:15547	 l-p:0.20461833477020264
epoch£º777	 i:8 	 global-step:15548	 l-p:0.04090636968612671
epoch£º777	 i:9 	 global-step:15549	 l-p:0.15790340304374695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1041, 5.1031, 5.1041],
        [5.1041, 5.0260, 5.0736],
        [5.1041, 5.0872, 5.1020],
        [5.1041, 5.0973, 5.1037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.16537746787071228 
model_pd.l_d.mean(): -19.045352935791016 
model_pd.lagr.mean(): -18.879976272583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6079], device='cuda:0')), ('power', tensor([-19.8745], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.16537746787071228
epoch£º778	 i:1 	 global-step:15561	 l-p:0.1479366272687912
epoch£º778	 i:2 	 global-step:15562	 l-p:0.08654260635375977
epoch£º778	 i:3 	 global-step:15563	 l-p:0.18403179943561554
epoch£º778	 i:4 	 global-step:15564	 l-p:0.09496653079986572
epoch£º778	 i:5 	 global-step:15565	 l-p:0.0740567296743393
epoch£º778	 i:6 	 global-step:15566	 l-p:0.1698116660118103
epoch£º778	 i:7 	 global-step:15567	 l-p:0.1431715041399002
epoch£º778	 i:8 	 global-step:15568	 l-p:0.13692818582057953
epoch£º778	 i:9 	 global-step:15569	 l-p:0.1482015699148178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1407, 4.9837, 5.0201],
        [5.1407, 4.9060, 4.7222],
        [5.1407, 5.1406, 5.1407],
        [5.1407, 5.0631, 5.1104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.11027047783136368 
model_pd.l_d.mean(): -20.493900299072266 
model_pd.lagr.mean(): -20.383630752563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4490], device='cuda:0')), ('power', tensor([-21.1765], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.11027047783136368
epoch£º779	 i:1 	 global-step:15581	 l-p:0.16569837927818298
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13222402334213257
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12251222133636475
epoch£º779	 i:4 	 global-step:15584	 l-p:0.1538032740354538
epoch£º779	 i:5 	 global-step:15585	 l-p:0.058820292353630066
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13225260376930237
epoch£º779	 i:7 	 global-step:15587	 l-p:0.13932070136070251
epoch£º779	 i:8 	 global-step:15588	 l-p:0.15407802164554596
epoch£º779	 i:9 	 global-step:15589	 l-p:0.16876114904880524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1086, 5.1033, 5.1083],
        [5.1086, 5.0901, 5.1062],
        [5.1086, 5.2555, 5.0443],
        [5.1086, 5.2488, 5.0342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.17812350392341614 
model_pd.l_d.mean(): -18.6492977142334 
model_pd.lagr.mean(): -18.471174240112305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5736], device='cuda:0')), ('power', tensor([-19.4391], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.17812350392341614
epoch£º780	 i:1 	 global-step:15601	 l-p:0.06930819153785706
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14129133522510529
epoch£º780	 i:3 	 global-step:15603	 l-p:0.11320510506629944
epoch£º780	 i:4 	 global-step:15604	 l-p:0.15943829715251923
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11434870213270187
epoch£º780	 i:6 	 global-step:15606	 l-p:0.1786063313484192
epoch£º780	 i:7 	 global-step:15607	 l-p:0.11910655349493027
epoch£º780	 i:8 	 global-step:15608	 l-p:0.0795779898762703
epoch£º780	 i:9 	 global-step:15609	 l-p:0.13045674562454224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1743, 5.3374, 5.1334],
        [5.1743, 5.1743, 5.1743],
        [5.1743, 5.1683, 5.1739],
        [5.1743, 5.1716, 5.1742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.16800841689109802 
model_pd.l_d.mean(): -20.506479263305664 
model_pd.lagr.mean(): -20.338470458984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4531], device='cuda:0')), ('power', tensor([-21.1934], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.16800841689109802
epoch£º781	 i:1 	 global-step:15621	 l-p:0.12423200160264969
epoch£º781	 i:2 	 global-step:15622	 l-p:0.12004788964986801
epoch£º781	 i:3 	 global-step:15623	 l-p:0.10000640898942947
epoch£º781	 i:4 	 global-step:15624	 l-p:0.13234248757362366
epoch£º781	 i:5 	 global-step:15625	 l-p:0.07432635873556137
epoch£º781	 i:6 	 global-step:15626	 l-p:0.11157263815402985
epoch£º781	 i:7 	 global-step:15627	 l-p:0.07410596311092377
epoch£º781	 i:8 	 global-step:15628	 l-p:0.14324066042900085
epoch£º781	 i:9 	 global-step:15629	 l-p:0.12142035365104675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1468, 5.1287, 5.1445],
        [5.1468, 5.2002, 4.9448],
        [5.1468, 5.0830, 5.1257],
        [5.1468, 5.0792, 5.1233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.06852994114160538 
model_pd.l_d.mean(): -20.136701583862305 
model_pd.lagr.mean(): -20.068172454833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4338], device='cuda:0')), ('power', tensor([-20.7999], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.06852994114160538
epoch£º782	 i:1 	 global-step:15641	 l-p:0.15377554297447205
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12642504274845123
epoch£º782	 i:3 	 global-step:15643	 l-p:0.13624721765518188
epoch£º782	 i:4 	 global-step:15644	 l-p:0.14460235834121704
epoch£º782	 i:5 	 global-step:15645	 l-p:0.13024234771728516
epoch£º782	 i:6 	 global-step:15646	 l-p:0.10929715633392334
epoch£º782	 i:7 	 global-step:15647	 l-p:0.12986880540847778
epoch£º782	 i:8 	 global-step:15648	 l-p:0.17628879845142365
epoch£º782	 i:9 	 global-step:15649	 l-p:0.18654413521289825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1092, 5.0564, 4.7674],
        [5.1092, 4.8815, 4.8250],
        [5.1092, 5.0228, 4.7294],
        [5.1092, 5.0648, 5.0983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.16520795226097107 
model_pd.l_d.mean(): -20.67865753173828 
model_pd.lagr.mean(): -20.51344871520996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-21.3385], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.16520795226097107
epoch£º783	 i:1 	 global-step:15661	 l-p:0.1646783947944641
epoch£º783	 i:2 	 global-step:15662	 l-p:0.12616798281669617
epoch£º783	 i:3 	 global-step:15663	 l-p:0.11747764050960541
epoch£º783	 i:4 	 global-step:15664	 l-p:0.12849225103855133
epoch£º783	 i:5 	 global-step:15665	 l-p:0.10055473446846008
epoch£º783	 i:6 	 global-step:15666	 l-p:0.13904784619808197
epoch£º783	 i:7 	 global-step:15667	 l-p:0.14458955824375153
epoch£º783	 i:8 	 global-step:15668	 l-p:0.07896611094474792
epoch£º783	 i:9 	 global-step:15669	 l-p:0.17083688080310822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1218, 5.0690, 4.7803],
        [5.1218, 5.1218, 5.1218],
        [5.1218, 5.0202, 5.0718],
        [5.1218, 5.0124, 5.0641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.10644982010126114 
model_pd.l_d.mean(): -20.222082138061523 
model_pd.lagr.mean(): -20.115633010864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-20.9636], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.10644982010126114
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13600215315818787
epoch£º784	 i:2 	 global-step:15682	 l-p:0.10229696333408356
epoch£º784	 i:3 	 global-step:15683	 l-p:0.17822158336639404
epoch£º784	 i:4 	 global-step:15684	 l-p:0.09626992791891098
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13567855954170227
epoch£º784	 i:6 	 global-step:15686	 l-p:0.15895536541938782
epoch£º784	 i:7 	 global-step:15687	 l-p:0.15014515817165375
epoch£º784	 i:8 	 global-step:15688	 l-p:0.10557996481657028
epoch£º784	 i:9 	 global-step:15689	 l-p:0.1288936734199524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.1360, 5.1360],
        [5.1360, 5.1315, 5.1358],
        [5.1360, 4.9263, 4.6784],
        [5.1360, 4.9677, 4.9976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.09515468031167984 
model_pd.l_d.mean(): -19.939565658569336 
model_pd.lagr.mean(): -19.844411849975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.6197], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.09515468031167984
epoch£º785	 i:1 	 global-step:15701	 l-p:0.1503339260816574
epoch£º785	 i:2 	 global-step:15702	 l-p:0.08471585065126419
epoch£º785	 i:3 	 global-step:15703	 l-p:0.1109074205160141
epoch£º785	 i:4 	 global-step:15704	 l-p:0.1342785358428955
epoch£º785	 i:5 	 global-step:15705	 l-p:0.2248736470937729
epoch£º785	 i:6 	 global-step:15706	 l-p:0.16799478232860565
epoch£º785	 i:7 	 global-step:15707	 l-p:0.19382983446121216
epoch£º785	 i:8 	 global-step:15708	 l-p:0.12460273504257202
epoch£º785	 i:9 	 global-step:15709	 l-p:0.17807963490486145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0844, 5.0844, 5.0844],
        [5.0844, 5.4600, 5.3855],
        [5.0844, 5.0585, 4.7743],
        [5.0844, 5.0195, 5.0630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.1329643279314041 
model_pd.l_d.mean(): -20.737401962280273 
model_pd.lagr.mean(): -20.60443687438965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4336], device='cuda:0')), ('power', tensor([-21.4070], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.1329643279314041
epoch£º786	 i:1 	 global-step:15721	 l-p:0.14084407687187195
epoch£º786	 i:2 	 global-step:15722	 l-p:0.1664152890443802
epoch£º786	 i:3 	 global-step:15723	 l-p:0.24257151782512665
epoch£º786	 i:4 	 global-step:15724	 l-p:0.12478036433458328
epoch£º786	 i:5 	 global-step:15725	 l-p:0.12642881274223328
epoch£º786	 i:6 	 global-step:15726	 l-p:0.12917667627334595
epoch£º786	 i:7 	 global-step:15727	 l-p:0.16478939354419708
epoch£º786	 i:8 	 global-step:15728	 l-p:0.11646377295255661
epoch£º786	 i:9 	 global-step:15729	 l-p:0.15725268423557281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1211, 5.0607, 4.7700],
        [5.1211, 5.0767, 5.1102],
        [5.1211, 5.1028, 5.1187],
        [5.1211, 5.0416, 5.0897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.13079465925693512 
model_pd.l_d.mean(): -19.421619415283203 
model_pd.lagr.mean(): -19.29082489013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5667], device='cuda:0')), ('power', tensor([-20.2128], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.13079465925693512
epoch£º787	 i:1 	 global-step:15741	 l-p:0.11526632308959961
epoch£º787	 i:2 	 global-step:15742	 l-p:0.1423729509115219
epoch£º787	 i:3 	 global-step:15743	 l-p:0.10205838084220886
epoch£º787	 i:4 	 global-step:15744	 l-p:0.10907620936632156
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14277154207229614
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11854182183742523
epoch£º787	 i:7 	 global-step:15747	 l-p:0.1552208513021469
epoch£º787	 i:8 	 global-step:15748	 l-p:0.16274093091487885
epoch£º787	 i:9 	 global-step:15749	 l-p:0.13721050322055817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1164, 4.8783, 4.7880],
        [5.1164, 5.1164, 5.1164],
        [5.1164, 4.8732, 4.7497],
        [5.1164, 5.1994, 4.9557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.13817067444324493 
model_pd.l_d.mean(): -20.710363388061523 
model_pd.lagr.mean(): -20.572193145751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4215], device='cuda:0')), ('power', tensor([-21.3673], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.13817067444324493
epoch£º788	 i:1 	 global-step:15761	 l-p:0.14952042698860168
epoch£º788	 i:2 	 global-step:15762	 l-p:0.11715423315763474
epoch£º788	 i:3 	 global-step:15763	 l-p:0.11277680844068527
epoch£º788	 i:4 	 global-step:15764	 l-p:0.1953287422657013
epoch£º788	 i:5 	 global-step:15765	 l-p:0.14834274351596832
epoch£º788	 i:6 	 global-step:15766	 l-p:0.13156962394714355
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12494517862796783
epoch£º788	 i:8 	 global-step:15768	 l-p:0.14920616149902344
epoch£º788	 i:9 	 global-step:15769	 l-p:0.1366998553276062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 5.0595, 5.0939],
        [5.1054, 4.8734, 4.8101],
        [5.1054, 5.1054, 5.1054],
        [5.1054, 5.1054, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.11781860142946243 
model_pd.l_d.mean(): -20.421825408935547 
model_pd.lagr.mean(): -20.304006576538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4646], device='cuda:0')), ('power', tensor([-21.1196], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.11781860142946243
epoch£º789	 i:1 	 global-step:15781	 l-p:0.1458507776260376
epoch£º789	 i:2 	 global-step:15782	 l-p:0.19261913001537323
epoch£º789	 i:3 	 global-step:15783	 l-p:0.14914622902870178
epoch£º789	 i:4 	 global-step:15784	 l-p:0.1776473969221115
epoch£º789	 i:5 	 global-step:15785	 l-p:0.1373576670885086
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11583801358938217
epoch£º789	 i:7 	 global-step:15787	 l-p:0.11866445094347
epoch£º789	 i:8 	 global-step:15788	 l-p:0.15060104429721832
epoch£º789	 i:9 	 global-step:15789	 l-p:0.12472519278526306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1171, 5.0440, 5.0903],
        [5.1171, 5.1171, 5.1171],
        [5.1171, 5.0716, 4.7826],
        [5.1171, 4.8857, 4.8222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.11612337082624435 
model_pd.l_d.mean(): -20.930145263671875 
model_pd.lagr.mean(): -20.814022064208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4048], device='cuda:0')), ('power', tensor([-21.5724], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.11612337082624435
epoch£º790	 i:1 	 global-step:15801	 l-p:0.19202788174152374
epoch£º790	 i:2 	 global-step:15802	 l-p:0.1440589725971222
epoch£º790	 i:3 	 global-step:15803	 l-p:0.13130593299865723
epoch£º790	 i:4 	 global-step:15804	 l-p:0.12914423644542694
epoch£º790	 i:5 	 global-step:15805	 l-p:0.1279759705066681
epoch£º790	 i:6 	 global-step:15806	 l-p:0.1624625027179718
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12547074258327484
epoch£º790	 i:8 	 global-step:15808	 l-p:0.14125284552574158
epoch£º790	 i:9 	 global-step:15809	 l-p:0.08525555580854416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1314, 4.9105, 4.8717],
        [5.1314, 5.5695, 5.5350],
        [5.1314, 5.0239, 5.0759],
        [5.1314, 4.9392, 4.9476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.16047991812229156 
model_pd.l_d.mean(): -20.65229606628418 
model_pd.lagr.mean(): -20.4918155670166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4125], device='cuda:0')), ('power', tensor([-21.2994], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.16047991812229156
epoch£º791	 i:1 	 global-step:15821	 l-p:0.15150706470012665
epoch£º791	 i:2 	 global-step:15822	 l-p:0.06822114437818527
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12255395948886871
epoch£º791	 i:4 	 global-step:15824	 l-p:0.10135341435670853
epoch£º791	 i:5 	 global-step:15825	 l-p:0.13483519852161407
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09788509458303452
epoch£º791	 i:7 	 global-step:15827	 l-p:0.15237940847873688
epoch£º791	 i:8 	 global-step:15828	 l-p:0.12796030938625336
epoch£º791	 i:9 	 global-step:15829	 l-p:0.1554442048072815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1383, 4.9005, 4.7053],
        [5.1383, 5.0134, 4.7197],
        [5.1383, 5.0699, 5.1146],
        [5.1383, 5.0003, 5.0474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.1333167850971222 
model_pd.l_d.mean(): -20.79526710510254 
model_pd.lagr.mean(): -20.661951065063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4105], device='cuda:0')), ('power', tensor([-21.4419], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.1333167850971222
epoch£º792	 i:1 	 global-step:15841	 l-p:0.07264263927936554
epoch£º792	 i:2 	 global-step:15842	 l-p:0.14288754761219025
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14384885132312775
epoch£º792	 i:4 	 global-step:15844	 l-p:0.1195351704955101
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12713070213794708
epoch£º792	 i:6 	 global-step:15846	 l-p:0.09607905149459839
epoch£º792	 i:7 	 global-step:15847	 l-p:0.23736712336540222
epoch£º792	 i:8 	 global-step:15848	 l-p:0.16535182297229767
epoch£º792	 i:9 	 global-step:15849	 l-p:-1.3390088081359863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0506, 5.0505, 5.0506],
        [5.0506, 5.0478, 5.0505],
        [5.0506, 5.0505, 5.0506],
        [5.0506, 5.0034, 5.0386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.1330675482749939 
model_pd.l_d.mean(): -20.79541015625 
model_pd.lagr.mean(): -20.662342071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4187], device='cuda:0')), ('power', tensor([-21.4504], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.1330675482749939
epoch£º793	 i:1 	 global-step:15861	 l-p:0.1673249453306198
epoch£º793	 i:2 	 global-step:15862	 l-p:0.1172768771648407
epoch£º793	 i:3 	 global-step:15863	 l-p:0.11967284232378006
epoch£º793	 i:4 	 global-step:15864	 l-p:0.12143143266439438
epoch£º793	 i:5 	 global-step:15865	 l-p:0.007543716114014387
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13627971708774567
epoch£º793	 i:7 	 global-step:15867	 l-p:0.31834104657173157
epoch£º793	 i:8 	 global-step:15868	 l-p:0.1894148737192154
epoch£º793	 i:9 	 global-step:15869	 l-p:-0.6539091467857361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0267, 4.7706, 4.6221],
        [5.0267, 5.0049, 5.0236],
        [5.0267, 4.7724, 4.6480],
        [5.0267, 5.0267, 5.0267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.19675429165363312 
model_pd.l_d.mean(): -20.272106170654297 
model_pd.lagr.mean(): -20.07535171508789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-21.0229], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.19675429165363312
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11556357890367508
epoch£º794	 i:2 	 global-step:15882	 l-p:0.24819262325763702
epoch£º794	 i:3 	 global-step:15883	 l-p:0.1320352405309677
epoch£º794	 i:4 	 global-step:15884	 l-p:0.2525266110897064
epoch£º794	 i:5 	 global-step:15885	 l-p:1.1979824304580688
epoch£º794	 i:6 	 global-step:15886	 l-p:0.1129094660282135
epoch£º794	 i:7 	 global-step:15887	 l-p:0.20340068638324738
epoch£º794	 i:8 	 global-step:15888	 l-p:0.1188647523522377
epoch£º794	 i:9 	 global-step:15889	 l-p:0.1804048866033554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0861, 5.0861, 5.0861],
        [5.0861, 4.8799, 4.6119],
        [5.0861, 5.0438, 5.0762],
        [5.0861, 5.0458, 5.0770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.11213619261980057 
model_pd.l_d.mean(): -20.39234161376953 
model_pd.lagr.mean(): -20.28020477294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-21.0952], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.11213619261980057
epoch£º795	 i:1 	 global-step:15901	 l-p:0.1616421788930893
epoch£º795	 i:2 	 global-step:15902	 l-p:0.16556651890277863
epoch£º795	 i:3 	 global-step:15903	 l-p:0.14258532226085663
epoch£º795	 i:4 	 global-step:15904	 l-p:0.1571027785539627
epoch£º795	 i:5 	 global-step:15905	 l-p:0.1047707125544548
epoch£º795	 i:6 	 global-step:15906	 l-p:0.1917574107646942
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12172693014144897
epoch£º795	 i:8 	 global-step:15908	 l-p:0.11372566223144531
epoch£º795	 i:9 	 global-step:15909	 l-p:0.12102683633565903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.0952, 5.1320],
        [5.1456, 5.1035, 4.8149],
        [5.1456, 5.0158, 5.0654],
        [5.1456, 5.1455, 5.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.1374216228723526 
model_pd.l_d.mean(): -19.965421676635742 
model_pd.lagr.mean(): -19.827999114990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.6792], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.1374216228723526
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12309218943119049
epoch£º796	 i:2 	 global-step:15922	 l-p:0.09855014830827713
epoch£º796	 i:3 	 global-step:15923	 l-p:0.13492709398269653
epoch£º796	 i:4 	 global-step:15924	 l-p:0.1528104990720749
epoch£º796	 i:5 	 global-step:15925	 l-p:0.10348096489906311
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11355891823768616
epoch£º796	 i:7 	 global-step:15927	 l-p:0.14221392571926117
epoch£º796	 i:8 	 global-step:15928	 l-p:0.1353810578584671
epoch£º796	 i:9 	 global-step:15929	 l-p:0.1610787808895111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1381, 5.0496, 5.1000],
        [5.1381, 5.1218, 5.1361],
        [5.1381, 5.1374, 5.1380],
        [5.1381, 5.1381, 5.1381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.11761171370744705 
model_pd.l_d.mean(): -20.519975662231445 
model_pd.lagr.mean(): -20.40236473083496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4529], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.11761171370744705
epoch£º797	 i:1 	 global-step:15941	 l-p:0.11760047823190689
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14493608474731445
epoch£º797	 i:3 	 global-step:15943	 l-p:0.16145192086696625
epoch£º797	 i:4 	 global-step:15944	 l-p:0.18170562386512756
epoch£º797	 i:5 	 global-step:15945	 l-p:0.12011848390102386
epoch£º797	 i:6 	 global-step:15946	 l-p:0.15017838776111603
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11384641379117966
epoch£º797	 i:8 	 global-step:15948	 l-p:0.17301540076732635
epoch£º797	 i:9 	 global-step:15949	 l-p:0.18815051019191742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0694, 5.0991, 4.8310],
        [5.0694, 5.0648, 5.0692],
        [5.0694, 4.9356, 4.9862],
        [5.0694, 5.3360, 5.1906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.151435986161232 
model_pd.l_d.mean(): -19.907548904418945 
model_pd.lagr.mean(): -19.756113052368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-20.6169], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.151435986161232
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11454667896032333
epoch£º798	 i:2 	 global-step:15962	 l-p:0.2585882842540741
epoch£º798	 i:3 	 global-step:15963	 l-p:0.14419206976890564
epoch£º798	 i:4 	 global-step:15964	 l-p:-0.11253444105386734
epoch£º798	 i:5 	 global-step:15965	 l-p:0.2045460045337677
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11712725460529327
epoch£º798	 i:7 	 global-step:15967	 l-p:0.22617551684379578
epoch£º798	 i:8 	 global-step:15968	 l-p:0.27574026584625244
epoch£º798	 i:9 	 global-step:15969	 l-p:0.20600488781929016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0415, 4.9987, 5.0315],
        [5.0415, 5.0401, 5.0414],
        [5.0415, 5.0415, 5.0415],
        [5.0415, 5.0388, 5.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.11617778241634369 
model_pd.l_d.mean(): -21.021940231323242 
model_pd.lagr.mean(): -20.90576171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4049], device='cuda:0')), ('power', tensor([-21.6653], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.11617778241634369
epoch£º799	 i:1 	 global-step:15981	 l-p:-0.07777967303991318
epoch£º799	 i:2 	 global-step:15982	 l-p:0.17175008356571198
epoch£º799	 i:3 	 global-step:15983	 l-p:0.12311545014381409
epoch£º799	 i:4 	 global-step:15984	 l-p:0.13161394000053406
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10932320356369019
epoch£º799	 i:6 	 global-step:15986	 l-p:0.12142331898212433
epoch£º799	 i:7 	 global-step:15987	 l-p:0.12233669310808182
epoch£º799	 i:8 	 global-step:15988	 l-p:0.3445126414299011
epoch£º799	 i:9 	 global-step:15989	 l-p:0.1333492249250412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0507, 4.8292, 4.8065],
        [5.0507, 5.2093, 5.0020],
        [5.0507, 5.0428, 5.0501],
        [5.0507, 4.7950, 4.6426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.13506384193897247 
model_pd.l_d.mean(): -18.539836883544922 
model_pd.lagr.mean(): -18.404773712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6036], device='cuda:0')), ('power', tensor([-19.3591], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.13506384193897247
epoch£º800	 i:1 	 global-step:16001	 l-p:0.11299411207437515
epoch£º800	 i:2 	 global-step:16002	 l-p:0.16177217662334442
epoch£º800	 i:3 	 global-step:16003	 l-p:0.3190186619758606
epoch£º800	 i:4 	 global-step:16004	 l-p:0.15606635808944702
epoch£º800	 i:5 	 global-step:16005	 l-p:0.12356456369161606
epoch£º800	 i:6 	 global-step:16006	 l-p:0.9762178659439087
epoch£º800	 i:7 	 global-step:16007	 l-p:0.14293818175792694
epoch£º800	 i:8 	 global-step:16008	 l-p:0.20183859765529633
epoch£º800	 i:9 	 global-step:16009	 l-p:0.13591104745864868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[5.0613, 5.2229, 5.0169],
        [5.0613, 5.1779, 4.9486],
        [5.0613, 4.8471, 4.5785],
        [5.0613, 4.8372, 4.8083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.13954362273216248 
model_pd.l_d.mean(): -19.737245559692383 
model_pd.lagr.mean(): -19.597702026367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5078], device='cuda:0')), ('power', tensor([-20.4717], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.13954362273216248
epoch£º801	 i:1 	 global-step:16021	 l-p:0.13392631709575653
epoch£º801	 i:2 	 global-step:16022	 l-p:0.10477474331855774
epoch£º801	 i:3 	 global-step:16023	 l-p:0.17186219990253448
epoch£º801	 i:4 	 global-step:16024	 l-p:0.1560674011707306
epoch£º801	 i:5 	 global-step:16025	 l-p:0.1784484088420868
epoch£º801	 i:6 	 global-step:16026	 l-p:0.12094077467918396
epoch£º801	 i:7 	 global-step:16027	 l-p:0.1419471949338913
epoch£º801	 i:8 	 global-step:16028	 l-p:0.25187987089157104
epoch£º801	 i:9 	 global-step:16029	 l-p:0.26796260476112366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0782, 5.0748, 5.0781],
        [5.0782, 4.9586, 5.0116],
        [5.0782, 5.0782, 5.0783],
        [5.0782, 4.8281, 4.7136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.21683579683303833 
model_pd.l_d.mean(): -19.1031494140625 
model_pd.lagr.mean(): -18.886314392089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-19.8558], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.21683579683303833
epoch£º802	 i:1 	 global-step:16041	 l-p:0.1587556004524231
epoch£º802	 i:2 	 global-step:16042	 l-p:0.19668221473693848
epoch£º802	 i:3 	 global-step:16043	 l-p:0.1312093436717987
epoch£º802	 i:4 	 global-step:16044	 l-p:0.08897344022989273
epoch£º802	 i:5 	 global-step:16045	 l-p:0.1332423985004425
epoch£º802	 i:6 	 global-step:16046	 l-p:0.15150690078735352
epoch£º802	 i:7 	 global-step:16047	 l-p:0.1452845185995102
epoch£º802	 i:8 	 global-step:16048	 l-p:0.1294988989830017
epoch£º802	 i:9 	 global-step:16049	 l-p:0.13685579597949982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1091, 4.8858, 4.6375],
        [5.1091, 4.8592, 4.7077],
        [5.1091, 4.8688, 4.7889],
        [5.1091, 4.8593, 4.7155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.1445637196302414 
model_pd.l_d.mean(): -20.73007583618164 
model_pd.lagr.mean(): -20.585512161254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4403], device='cuda:0')), ('power', tensor([-21.4064], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.1445637196302414
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12270873039960861
epoch£º803	 i:2 	 global-step:16062	 l-p:0.10435159504413605
epoch£º803	 i:3 	 global-step:16063	 l-p:0.1453489512205124
epoch£º803	 i:4 	 global-step:16064	 l-p:0.18509931862354279
epoch£º803	 i:5 	 global-step:16065	 l-p:0.16706958413124084
epoch£º803	 i:6 	 global-step:16066	 l-p:0.11082073301076889
epoch£º803	 i:7 	 global-step:16067	 l-p:0.1578977406024933
epoch£º803	 i:8 	 global-step:16068	 l-p:0.16940082609653473
epoch£º803	 i:9 	 global-step:16069	 l-p:0.11388347297906876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1187, 5.1160, 5.1186],
        [5.1187, 5.0839, 5.1117],
        [5.1187, 4.8755, 4.7803],
        [5.1187, 4.9913, 4.6929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.14178572595119476 
model_pd.l_d.mean(): -19.83580207824707 
model_pd.lagr.mean(): -19.694015502929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-20.6069], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.14178572595119476
epoch£º804	 i:1 	 global-step:16081	 l-p:0.16233958303928375
epoch£º804	 i:2 	 global-step:16082	 l-p:0.10346608608961105
epoch£º804	 i:3 	 global-step:16083	 l-p:0.13663479685783386
epoch£º804	 i:4 	 global-step:16084	 l-p:0.12137475609779358
epoch£º804	 i:5 	 global-step:16085	 l-p:0.11472052335739136
epoch£º804	 i:6 	 global-step:16086	 l-p:0.1582358181476593
epoch£º804	 i:7 	 global-step:16087	 l-p:0.1288875788450241
epoch£º804	 i:8 	 global-step:16088	 l-p:0.08937033265829086
epoch£º804	 i:9 	 global-step:16089	 l-p:0.1920483410358429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 5.1237, 5.1237],
        [5.1237, 5.1237, 5.1237],
        [5.1237, 5.4262, 5.3009],
        [5.1237, 5.0085, 5.0611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.12509219348430634 
model_pd.l_d.mean(): -19.464731216430664 
model_pd.lagr.mean(): -19.33963966369629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-20.2187], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.12509219348430634
epoch£º805	 i:1 	 global-step:16101	 l-p:0.1278286725282669
epoch£º805	 i:2 	 global-step:16102	 l-p:0.16678963601589203
epoch£º805	 i:3 	 global-step:16103	 l-p:0.10401696711778641
epoch£º805	 i:4 	 global-step:16104	 l-p:0.1481328308582306
epoch£º805	 i:5 	 global-step:16105	 l-p:0.13129861652851105
epoch£º805	 i:6 	 global-step:16106	 l-p:0.16787275671958923
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13183355331420898
epoch£º805	 i:8 	 global-step:16108	 l-p:0.12295576184988022
epoch£º805	 i:9 	 global-step:16109	 l-p:0.132421612739563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1299, 5.1298, 5.1299],
        [5.1299, 5.1292, 5.1299],
        [5.1299, 5.1299, 5.1299],
        [5.1299, 5.1274, 5.1298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.07258918881416321 
model_pd.l_d.mean(): -20.363040924072266 
model_pd.lagr.mean(): -20.290451049804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.0650], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.07258918881416321
epoch£º806	 i:1 	 global-step:16121	 l-p:0.17181676626205444
epoch£º806	 i:2 	 global-step:16122	 l-p:0.09278754889965057
epoch£º806	 i:3 	 global-step:16123	 l-p:0.16023944318294525
epoch£º806	 i:4 	 global-step:16124	 l-p:0.13799189031124115
epoch£º806	 i:5 	 global-step:16125	 l-p:0.1307748705148697
epoch£º806	 i:6 	 global-step:16126	 l-p:0.12351707369089127
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12476091086864471
epoch£º806	 i:8 	 global-step:16128	 l-p:0.17708995938301086
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12899528443813324
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1374, 5.1367, 5.1374],
        [5.1374, 5.1369, 5.1374],
        [5.1374, 5.1361, 5.1373],
        [5.1374, 5.1374, 5.1374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.09292549639940262 
model_pd.l_d.mean(): -20.913654327392578 
model_pd.lagr.mean(): -20.820728302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3957], device='cuda:0')), ('power', tensor([-21.5465], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.09292549639940262
epoch£º807	 i:1 	 global-step:16141	 l-p:0.15246829390525818
epoch£º807	 i:2 	 global-step:16142	 l-p:0.13923144340515137
epoch£º807	 i:3 	 global-step:16143	 l-p:0.14899298548698425
epoch£º807	 i:4 	 global-step:16144	 l-p:0.1267770528793335
epoch£º807	 i:5 	 global-step:16145	 l-p:0.07499769330024719
epoch£º807	 i:6 	 global-step:16146	 l-p:0.1206684559583664
epoch£º807	 i:7 	 global-step:16147	 l-p:0.14835390448570251
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12119826674461365
epoch£º807	 i:9 	 global-step:16149	 l-p:0.1845984011888504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1362, 5.1308, 5.1359],
        [5.1362, 5.1232, 5.1349],
        [5.1362, 5.1362, 5.1362],
        [5.1362, 5.1321, 5.1360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.15358641743659973 
model_pd.l_d.mean(): -20.048324584960938 
model_pd.lagr.mean(): -19.894737243652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4528], device='cuda:0')), ('power', tensor([-20.7299], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.15358641743659973
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12461124360561371
epoch£º808	 i:2 	 global-step:16162	 l-p:0.1526390165090561
epoch£º808	 i:3 	 global-step:16163	 l-p:0.14389871060848236
epoch£º808	 i:4 	 global-step:16164	 l-p:0.059061747044324875
epoch£º808	 i:5 	 global-step:16165	 l-p:0.1648777425289154
epoch£º808	 i:6 	 global-step:16166	 l-p:0.14446553587913513
epoch£º808	 i:7 	 global-step:16167	 l-p:0.1239197701215744
epoch£º808	 i:8 	 global-step:16168	 l-p:0.13840718567371368
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10890024155378342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.1411, 5.1411],
        [5.1411, 5.1406, 5.1411],
        [5.1411, 5.0815, 5.1228],
        [5.1411, 5.1410, 5.1411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.11740446090698242 
model_pd.l_d.mean(): -19.2906494140625 
model_pd.lagr.mean(): -19.17324447631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5440], device='cuda:0')), ('power', tensor([-20.0573], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.11740446090698242
epoch£º809	 i:1 	 global-step:16181	 l-p:0.10934223234653473
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12749357521533966
epoch£º809	 i:3 	 global-step:16183	 l-p:0.10489484667778015
epoch£º809	 i:4 	 global-step:16184	 l-p:0.12322089821100235
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12856797873973846
epoch£º809	 i:6 	 global-step:16186	 l-p:0.1396445631980896
epoch£º809	 i:7 	 global-step:16187	 l-p:0.16072715818881989
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12435232102870941
epoch£º809	 i:9 	 global-step:16189	 l-p:0.13316991925239563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 4.9747, 4.9914],
        [5.1610, 4.9223, 4.7203],
        [5.1610, 5.6615, 5.6674],
        [5.1610, 5.1590, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.17497792840003967 
model_pd.l_d.mean(): -19.871761322021484 
model_pd.lagr.mean(): -19.6967830657959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4568], device='cuda:0')), ('power', tensor([-20.5556], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.17497792840003967
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15873922407627106
epoch£º810	 i:2 	 global-step:16202	 l-p:0.08977404981851578
epoch£º810	 i:3 	 global-step:16203	 l-p:0.10438183695077896
epoch£º810	 i:4 	 global-step:16204	 l-p:0.12353220582008362
epoch£º810	 i:5 	 global-step:16205	 l-p:0.07468409836292267
epoch£º810	 i:6 	 global-step:16206	 l-p:0.13568325340747833
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11473724246025085
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12320529669523239
epoch£º810	 i:9 	 global-step:16209	 l-p:0.13596710562705994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1578, 4.9115, 4.7671],
        [5.1578, 5.0152, 4.7209],
        [5.1578, 5.0695, 5.1200],
        [5.1578, 5.0201, 5.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.054839637130498886 
model_pd.l_d.mean(): -19.552875518798828 
model_pd.lagr.mean(): -19.498035430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-20.3173], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.054839637130498886
epoch£º811	 i:1 	 global-step:16221	 l-p:0.16039197146892548
epoch£º811	 i:2 	 global-step:16222	 l-p:0.10615553706884384
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12355760484933853
epoch£º811	 i:4 	 global-step:16224	 l-p:0.16100215911865234
epoch£º811	 i:5 	 global-step:16225	 l-p:0.15819911658763885
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12696848809719086
epoch£º811	 i:7 	 global-step:16227	 l-p:0.10905656218528748
epoch£º811	 i:8 	 global-step:16228	 l-p:0.09158607572317123
epoch£º811	 i:9 	 global-step:16229	 l-p:0.13371813297271729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1674, 5.1613, 5.1671],
        [5.1674, 5.1459, 5.1644],
        [5.1674, 5.3641, 5.1743],
        [5.1674, 5.1421, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.027799954637885094 
model_pd.l_d.mean(): -20.702421188354492 
model_pd.lagr.mean(): -20.67462158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.3699], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.027799954637885094
epoch£º812	 i:1 	 global-step:16241	 l-p:0.11730300635099411
epoch£º812	 i:2 	 global-step:16242	 l-p:0.10481541603803635
epoch£º812	 i:3 	 global-step:16243	 l-p:0.13676828145980835
epoch£º812	 i:4 	 global-step:16244	 l-p:0.15455880761146545
epoch£º812	 i:5 	 global-step:16245	 l-p:0.14699284732341766
epoch£º812	 i:6 	 global-step:16246	 l-p:0.11711563915014267
epoch£º812	 i:7 	 global-step:16247	 l-p:0.12902498245239258
epoch£º812	 i:8 	 global-step:16248	 l-p:0.13361133635044098
epoch£º812	 i:9 	 global-step:16249	 l-p:0.1238127127289772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1583, 5.0530, 5.1055],
        [5.1583, 4.9286, 4.8741],
        [5.1583, 4.9388, 4.9079],
        [5.1583, 5.1583, 5.1583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.14096267521381378 
model_pd.l_d.mean(): -19.81016731262207 
model_pd.lagr.mean(): -19.669204711914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-20.5063], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.14096267521381378
epoch£º813	 i:1 	 global-step:16261	 l-p:0.09814727306365967
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12594535946846008
epoch£º813	 i:3 	 global-step:16263	 l-p:0.12356926500797272
epoch£º813	 i:4 	 global-step:16264	 l-p:0.18758368492126465
epoch£º813	 i:5 	 global-step:16265	 l-p:0.10104783624410629
epoch£º813	 i:6 	 global-step:16266	 l-p:0.10682454705238342
epoch£º813	 i:7 	 global-step:16267	 l-p:0.14071907103061676
epoch£º813	 i:8 	 global-step:16268	 l-p:0.1720731258392334
epoch£º813	 i:9 	 global-step:16269	 l-p:0.11468782275915146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1148, 5.1148, 5.1148],
        [5.1148, 5.1148, 5.1148],
        [5.1148, 5.0708, 5.1042],
        [5.1148, 5.1148, 5.1148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13521364331245422 
model_pd.l_d.mean(): -20.718881607055664 
model_pd.lagr.mean(): -20.583667755126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4095], device='cuda:0')), ('power', tensor([-21.3636], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13521364331245422
epoch£º814	 i:1 	 global-step:16281	 l-p:0.20215031504631042
epoch£º814	 i:2 	 global-step:16282	 l-p:0.12254654616117477
epoch£º814	 i:3 	 global-step:16283	 l-p:0.10879543423652649
epoch£º814	 i:4 	 global-step:16284	 l-p:0.14831587672233582
epoch£º814	 i:5 	 global-step:16285	 l-p:0.18641555309295654
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13831380009651184
epoch£º814	 i:7 	 global-step:16287	 l-p:0.09661421924829483
epoch£º814	 i:8 	 global-step:16288	 l-p:0.13139599561691284
epoch£º814	 i:9 	 global-step:16289	 l-p:0.17204123735427856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0917, 5.0917, 5.0917],
        [5.0917, 5.2648, 5.0633],
        [5.0917, 5.0077, 5.0578],
        [5.0917, 4.9886, 5.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.11118455976247787 
model_pd.l_d.mean(): -19.046100616455078 
model_pd.lagr.mean(): -18.93491554260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5750], device='cuda:0')), ('power', tensor([-19.8416], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.11118455976247787
epoch£º815	 i:1 	 global-step:16301	 l-p:0.09486165642738342
epoch£º815	 i:2 	 global-step:16302	 l-p:0.14038170874118805
epoch£º815	 i:3 	 global-step:16303	 l-p:0.21846938133239746
epoch£º815	 i:4 	 global-step:16304	 l-p:0.12178598344326019
epoch£º815	 i:5 	 global-step:16305	 l-p:0.12927363812923431
epoch£º815	 i:6 	 global-step:16306	 l-p:0.18784406781196594
epoch£º815	 i:7 	 global-step:16307	 l-p:0.1371620148420334
epoch£º815	 i:8 	 global-step:16308	 l-p:0.17069819569587708
epoch£º815	 i:9 	 global-step:16309	 l-p:0.24194186925888062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0838, 5.0021, 5.0517],
        [5.0838, 4.9405, 4.9892],
        [5.0838, 5.0757, 5.0832],
        [5.0838, 5.0396, 5.0732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.15502415597438812 
model_pd.l_d.mean(): -20.876142501831055 
model_pd.lagr.mean(): -20.721118927001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4019], device='cuda:0')), ('power', tensor([-21.5149], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.15502415597438812
epoch£º816	 i:1 	 global-step:16321	 l-p:0.13276243209838867
epoch£º816	 i:2 	 global-step:16322	 l-p:0.13145214319229126
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07571146637201309
epoch£º816	 i:4 	 global-step:16324	 l-p:0.31872060894966125
epoch£º816	 i:5 	 global-step:16325	 l-p:0.12303575128316879
epoch£º816	 i:6 	 global-step:16326	 l-p:0.21888230741024017
epoch£º816	 i:7 	 global-step:16327	 l-p:0.2846295237541199
epoch£º816	 i:8 	 global-step:16328	 l-p:0.13193459808826447
epoch£º816	 i:9 	 global-step:16329	 l-p:0.13138838112354279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0676, 5.2457, 5.0472],
        [5.0676, 5.0675, 5.0676],
        [5.0676, 5.0665, 5.0676],
        [5.0676, 4.8147, 4.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.09694870561361313 
model_pd.l_d.mean(): -18.63080406188965 
model_pd.lagr.mean(): -18.533855438232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6069], device='cuda:0')), ('power', tensor([-19.4545], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.09694870561361313
epoch£º817	 i:1 	 global-step:16341	 l-p:0.16383910179138184
epoch£º817	 i:2 	 global-step:16342	 l-p:0.11717453598976135
epoch£º817	 i:3 	 global-step:16343	 l-p:0.17717692255973816
epoch£º817	 i:4 	 global-step:16344	 l-p:0.11829131841659546
epoch£º817	 i:5 	 global-step:16345	 l-p:0.25096333026885986
epoch£º817	 i:6 	 global-step:16346	 l-p:0.13287872076034546
epoch£º817	 i:7 	 global-step:16347	 l-p:0.28520461916923523
epoch£º817	 i:8 	 global-step:16348	 l-p:0.17547404766082764
epoch£º817	 i:9 	 global-step:16349	 l-p:0.12031229585409164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0889, 5.3559, 5.2085],
        [5.0889, 5.2477, 5.0383],
        [5.0889, 5.0889, 5.0889],
        [5.0889, 5.0169, 5.0635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.11160953342914581 
model_pd.l_d.mean(): -19.261051177978516 
model_pd.lagr.mean(): -19.14944076538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5854], device='cuda:0')), ('power', tensor([-20.0696], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.11160953342914581
epoch£º818	 i:1 	 global-step:16361	 l-p:0.14983315765857697
epoch£º818	 i:2 	 global-step:16362	 l-p:0.2530791461467743
epoch£º818	 i:3 	 global-step:16363	 l-p:0.13591723144054413
epoch£º818	 i:4 	 global-step:16364	 l-p:0.2086201012134552
epoch£º818	 i:5 	 global-step:16365	 l-p:0.10210379213094711
epoch£º818	 i:6 	 global-step:16366	 l-p:0.08703679591417313
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12865155935287476
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12614192068576813
epoch£º818	 i:9 	 global-step:16369	 l-p:0.16475746035575867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1158, 4.9994, 5.0526],
        [5.1158, 4.8765, 4.6517],
        [5.1158, 5.1110, 5.1155],
        [5.1158, 5.1738, 4.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.12571023404598236 
model_pd.l_d.mean(): -20.87384033203125 
model_pd.lagr.mean(): -20.748130798339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4245], device='cuda:0')), ('power', tensor([-21.5356], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.12571023404598236
epoch£º819	 i:1 	 global-step:16381	 l-p:0.21044360101222992
epoch£º819	 i:2 	 global-step:16382	 l-p:0.10923076421022415
epoch£º819	 i:3 	 global-step:16383	 l-p:0.11720310896635056
epoch£º819	 i:4 	 global-step:16384	 l-p:0.20201310515403748
epoch£º819	 i:5 	 global-step:16385	 l-p:0.12620992958545685
epoch£º819	 i:6 	 global-step:16386	 l-p:0.10984819382429123
epoch£º819	 i:7 	 global-step:16387	 l-p:0.17811007797718048
epoch£º819	 i:8 	 global-step:16388	 l-p:0.16936495900154114
epoch£º819	 i:9 	 global-step:16389	 l-p:0.08483928442001343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0963, 4.8697, 4.6160],
        [5.0963, 5.0950, 5.0963],
        [5.0963, 4.8436, 4.6617],
        [5.0963, 5.0963, 5.0963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.20224425196647644 
model_pd.l_d.mean(): -19.55417251586914 
model_pd.lagr.mean(): -19.3519287109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5028], device='cuda:0')), ('power', tensor([-20.2816], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.20224425196647644
epoch£º820	 i:1 	 global-step:16401	 l-p:0.1139073520898819
epoch£º820	 i:2 	 global-step:16402	 l-p:0.17641125619411469
epoch£º820	 i:3 	 global-step:16403	 l-p:0.13501855731010437
epoch£º820	 i:4 	 global-step:16404	 l-p:0.13550636172294617
epoch£º820	 i:5 	 global-step:16405	 l-p:0.10672467947006226
epoch£º820	 i:6 	 global-step:16406	 l-p:0.18799367547035217
epoch£º820	 i:7 	 global-step:16407	 l-p:0.1323522925376892
epoch£º820	 i:8 	 global-step:16408	 l-p:0.17534810304641724
epoch£º820	 i:9 	 global-step:16409	 l-p:0.10276540368795395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1114, 4.9594, 5.0042],
        [5.1114, 5.0276, 5.0776],
        [5.1114, 4.9479, 4.9867],
        [5.1114, 4.8597, 4.7318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.1244114562869072 
model_pd.l_d.mean(): -20.97489356994629 
model_pd.lagr.mean(): -20.850482940673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3966], device='cuda:0')), ('power', tensor([-21.6093], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.1244114562869072
epoch£º821	 i:1 	 global-step:16421	 l-p:0.17714659869670868
epoch£º821	 i:2 	 global-step:16422	 l-p:0.11427232623100281
epoch£º821	 i:3 	 global-step:16423	 l-p:0.145353764295578
epoch£º821	 i:4 	 global-step:16424	 l-p:0.16737176477909088
epoch£º821	 i:5 	 global-step:16425	 l-p:0.09877022355794907
epoch£º821	 i:6 	 global-step:16426	 l-p:0.18418338894844055
epoch£º821	 i:7 	 global-step:16427	 l-p:0.1318349838256836
epoch£º821	 i:8 	 global-step:16428	 l-p:0.1222154051065445
epoch£º821	 i:9 	 global-step:16429	 l-p:0.09851463884115219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1464, 5.1390, 5.1459],
        [5.1464, 5.0883, 4.7928],
        [5.1464, 5.0368, 4.7365],
        [5.1464, 5.1460, 5.1464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.13487562537193298 
model_pd.l_d.mean(): -20.060672760009766 
model_pd.lagr.mean(): -19.925796508789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.7597], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.13487562537193298
epoch£º822	 i:1 	 global-step:16441	 l-p:0.09254365414381027
epoch£º822	 i:2 	 global-step:16442	 l-p:0.15009868144989014
epoch£º822	 i:3 	 global-step:16443	 l-p:0.08815112709999084
epoch£º822	 i:4 	 global-step:16444	 l-p:0.1257944256067276
epoch£º822	 i:5 	 global-step:16445	 l-p:0.12885436415672302
epoch£º822	 i:6 	 global-step:16446	 l-p:0.11535564064979553
epoch£º822	 i:7 	 global-step:16447	 l-p:0.09938543289899826
epoch£º822	 i:8 	 global-step:16448	 l-p:0.1353747546672821
epoch£º822	 i:9 	 global-step:16449	 l-p:0.19105835258960724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 5.0718, 5.1217],
        [5.1564, 5.1564, 5.1564],
        [5.1564, 4.9356, 4.9051],
        [5.1564, 5.4751, 5.3576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.2091793715953827 
model_pd.l_d.mean(): -20.149349212646484 
model_pd.lagr.mean(): -19.940170288085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5211], device='cuda:0')), ('power', tensor([-20.9019], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.2091793715953827
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14541374146938324
epoch£º823	 i:2 	 global-step:16462	 l-p:0.11921191960573196
epoch£º823	 i:3 	 global-step:16463	 l-p:0.164010688662529
epoch£º823	 i:4 	 global-step:16464	 l-p:0.13273312151432037
epoch£º823	 i:5 	 global-step:16465	 l-p:0.09690448641777039
epoch£º823	 i:6 	 global-step:16466	 l-p:0.05006301403045654
epoch£º823	 i:7 	 global-step:16467	 l-p:0.1391402781009674
epoch£º823	 i:8 	 global-step:16468	 l-p:0.09020635485649109
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11731881648302078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 4.9029, 4.7527],
        [5.1521, 5.1266, 5.1481],
        [5.1521, 5.0489, 5.1016],
        [5.1521, 5.1521, 5.1521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.11095878481864929 
model_pd.l_d.mean(): -20.32240867614746 
model_pd.lagr.mean(): -20.211450576782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4768], device='cuda:0')), ('power', tensor([-21.0316], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.11095878481864929
epoch£º824	 i:1 	 global-step:16481	 l-p:0.11024033278226852
epoch£º824	 i:2 	 global-step:16482	 l-p:0.16233202815055847
epoch£º824	 i:3 	 global-step:16483	 l-p:0.12293210625648499
epoch£º824	 i:4 	 global-step:16484	 l-p:0.14103975892066956
epoch£º824	 i:5 	 global-step:16485	 l-p:0.05273496359586716
epoch£º824	 i:6 	 global-step:16486	 l-p:0.13418574631214142
epoch£º824	 i:7 	 global-step:16487	 l-p:0.14538730680942535
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13926586508750916
epoch£º824	 i:9 	 global-step:16489	 l-p:0.1524076759815216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.5268, 5.4530],
        [5.1411, 5.1407, 5.1411],
        [5.1411, 5.1411, 5.1411],
        [5.1411, 5.1007, 5.1320]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.08928676694631577 
model_pd.l_d.mean(): -19.813751220703125 
model_pd.lagr.mean(): -19.724464416503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5086], device='cuda:0')), ('power', tensor([-20.5499], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.08928676694631577
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11846436560153961
epoch£º825	 i:2 	 global-step:16502	 l-p:0.13382457196712494
epoch£º825	 i:3 	 global-step:16503	 l-p:0.1297677755355835
epoch£º825	 i:4 	 global-step:16504	 l-p:0.15969166159629822
epoch£º825	 i:5 	 global-step:16505	 l-p:0.12358449399471283
epoch£º825	 i:6 	 global-step:16506	 l-p:0.1273467093706131
epoch£º825	 i:7 	 global-step:16507	 l-p:0.15417782962322235
epoch£º825	 i:8 	 global-step:16508	 l-p:0.15714454650878906
epoch£º825	 i:9 	 global-step:16509	 l-p:0.1449550986289978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1286, 5.1250, 5.1285],
        [5.1286, 5.1188, 5.1278],
        [5.1286, 5.1260, 5.1285],
        [5.1286, 4.8837, 4.7925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.1264755129814148 
model_pd.l_d.mean(): -20.685256958007812 
model_pd.lagr.mean(): -20.558780670166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4235], device='cuda:0')), ('power', tensor([-21.3439], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.1264755129814148
epoch£º826	 i:1 	 global-step:16521	 l-p:0.17365355789661407
epoch£º826	 i:2 	 global-step:16522	 l-p:0.13779211044311523
epoch£º826	 i:3 	 global-step:16523	 l-p:0.08259446918964386
epoch£º826	 i:4 	 global-step:16524	 l-p:0.10235587507486343
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12528933584690094
epoch£º826	 i:6 	 global-step:16526	 l-p:0.1395270973443985
epoch£º826	 i:7 	 global-step:16527	 l-p:0.14674462378025055
epoch£º826	 i:8 	 global-step:16528	 l-p:0.14244335889816284
epoch£º826	 i:9 	 global-step:16529	 l-p:0.14265072345733643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 5.1468, 5.1475],
        [5.1475, 5.1070, 5.1384],
        [5.1475, 5.1475, 5.1475],
        [5.1475, 4.9981, 5.0431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.12500758469104767 
model_pd.l_d.mean(): -20.371646881103516 
model_pd.lagr.mean(): -20.246639251708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4487], device='cuda:0')), ('power', tensor([-21.0526], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.12500758469104767
epoch£º827	 i:1 	 global-step:16541	 l-p:0.13727331161499023
epoch£º827	 i:2 	 global-step:16542	 l-p:0.11411599069833755
epoch£º827	 i:3 	 global-step:16543	 l-p:0.09235257655382156
epoch£º827	 i:4 	 global-step:16544	 l-p:0.16424033045768738
epoch£º827	 i:5 	 global-step:16545	 l-p:0.18007980287075043
epoch£º827	 i:6 	 global-step:16546	 l-p:0.08066555857658386
epoch£º827	 i:7 	 global-step:16547	 l-p:0.13539214432239532
epoch£º827	 i:8 	 global-step:16548	 l-p:0.12821200489997864
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11561956256628036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.0506, 5.1035],
        [5.1624, 4.9713, 4.9850],
        [5.1624, 5.1177, 5.1515],
        [5.1624, 5.1237, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.1183471828699112 
model_pd.l_d.mean(): -20.460168838500977 
model_pd.lagr.mean(): -20.341821670532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.1290], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.1183471828699112
epoch£º828	 i:1 	 global-step:16561	 l-p:0.06292826682329178
epoch£º828	 i:2 	 global-step:16562	 l-p:0.06526822596788406
epoch£º828	 i:3 	 global-step:16563	 l-p:0.12683726847171783
epoch£º828	 i:4 	 global-step:16564	 l-p:0.1506788283586502
epoch£º828	 i:5 	 global-step:16565	 l-p:0.1697235107421875
epoch£º828	 i:6 	 global-step:16566	 l-p:0.14371328055858612
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13772545754909515
epoch£º828	 i:8 	 global-step:16568	 l-p:0.12404929846525192
epoch£º828	 i:9 	 global-step:16569	 l-p:0.12041859328746796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.0967, 5.1438],
        [5.1712, 5.0918, 4.7936],
        [5.1712, 5.1711, 5.1712],
        [5.1712, 5.1706, 5.1712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.07347628474235535 
model_pd.l_d.mean(): -19.601011276245117 
model_pd.lagr.mean(): -19.52753448486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4888], device='cuda:0')), ('power', tensor([-20.3146], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.07347628474235535
epoch£º829	 i:1 	 global-step:16581	 l-p:0.10763866454362869
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13309383392333984
epoch£º829	 i:3 	 global-step:16583	 l-p:0.053834881633520126
epoch£º829	 i:4 	 global-step:16584	 l-p:0.14739546179771423
epoch£º829	 i:5 	 global-step:16585	 l-p:0.10888729989528656
epoch£º829	 i:6 	 global-step:16586	 l-p:0.19376607239246368
epoch£º829	 i:7 	 global-step:16587	 l-p:0.16049377620220184
epoch£º829	 i:8 	 global-step:16588	 l-p:0.13013413548469543
epoch£º829	 i:9 	 global-step:16589	 l-p:0.08980972319841385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1641, 5.1635, 5.1641],
        [5.1641, 4.9447, 4.9173],
        [5.1641, 4.9157, 4.7801],
        [5.1641, 5.1641, 5.1641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.14363567531108856 
model_pd.l_d.mean(): -20.621135711669922 
model_pd.lagr.mean(): -20.477500915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4293], device='cuda:0')), ('power', tensor([-21.2851], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.14363567531108856
epoch£º830	 i:1 	 global-step:16601	 l-p:0.10573668032884598
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13591057062149048
epoch£º830	 i:3 	 global-step:16603	 l-p:0.08361929655075073
epoch£º830	 i:4 	 global-step:16604	 l-p:0.12887881696224213
epoch£º830	 i:5 	 global-step:16605	 l-p:0.14745844900608063
epoch£º830	 i:6 	 global-step:16606	 l-p:0.16679736971855164
epoch£º830	 i:7 	 global-step:16607	 l-p:0.0785885825753212
epoch£º830	 i:8 	 global-step:16608	 l-p:0.20357894897460938
epoch£º830	 i:9 	 global-step:16609	 l-p:0.1169523224234581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1265, 5.1265, 5.1265],
        [5.1265, 5.1238, 5.1264],
        [5.1265, 5.1120, 5.1249],
        [5.1265, 5.1265, 5.1265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13477768003940582 
model_pd.l_d.mean(): -20.04349708557129 
model_pd.lagr.mean(): -19.908720016479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-20.7436], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13477768003940582
epoch£º831	 i:1 	 global-step:16621	 l-p:0.09830745309591293
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13759660720825195
epoch£º831	 i:3 	 global-step:16623	 l-p:0.1654624491930008
epoch£º831	 i:4 	 global-step:16624	 l-p:0.0877353772521019
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15264137089252472
epoch£º831	 i:6 	 global-step:16626	 l-p:0.16215834021568298
epoch£º831	 i:7 	 global-step:16627	 l-p:0.15032954514026642
epoch£º831	 i:8 	 global-step:16628	 l-p:0.14345933496952057
epoch£º831	 i:9 	 global-step:16629	 l-p:0.15454091131687164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1139, 5.1097, 5.1137],
        [5.1139, 4.8893, 4.8598],
        [5.1139, 4.9603, 4.6602],
        [5.1139, 4.8892, 4.8595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.10130783170461655 
model_pd.l_d.mean(): -20.25948143005371 
model_pd.lagr.mean(): -20.158174514770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4729], device='cuda:0')), ('power', tensor([-20.9640], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.10130783170461655
epoch£º832	 i:1 	 global-step:16641	 l-p:0.14717251062393188
epoch£º832	 i:2 	 global-step:16642	 l-p:0.09386369585990906
epoch£º832	 i:3 	 global-step:16643	 l-p:0.08497800678014755
epoch£º832	 i:4 	 global-step:16644	 l-p:0.13114959001541138
epoch£º832	 i:5 	 global-step:16645	 l-p:0.17160826921463013
epoch£º832	 i:6 	 global-step:16646	 l-p:0.27442488074302673
epoch£º832	 i:7 	 global-step:16647	 l-p:0.15981563925743103
epoch£º832	 i:8 	 global-step:16648	 l-p:0.14663034677505493
epoch£º832	 i:9 	 global-step:16649	 l-p:0.18196484446525574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0988, 5.0988, 5.0988],
        [5.0988, 5.0918, 5.0983],
        [5.0988, 4.9879, 5.0418],
        [5.0988, 5.0950, 5.0986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.13430693745613098 
model_pd.l_d.mean(): -19.796663284301758 
model_pd.lagr.mean(): -19.662355422973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5337], device='cuda:0')), ('power', tensor([-20.5582], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.13430693745613098
epoch£º833	 i:1 	 global-step:16661	 l-p:0.14859241247177124
epoch£º833	 i:2 	 global-step:16662	 l-p:0.13549035787582397
epoch£º833	 i:3 	 global-step:16663	 l-p:0.1386180818080902
epoch£º833	 i:4 	 global-step:16664	 l-p:0.10191556811332703
epoch£º833	 i:5 	 global-step:16665	 l-p:0.11905080825090408
epoch£º833	 i:6 	 global-step:16666	 l-p:0.11147738248109818
epoch£º833	 i:7 	 global-step:16667	 l-p:0.0789833664894104
epoch£º833	 i:8 	 global-step:16668	 l-p:0.22929199039936066
epoch£º833	 i:9 	 global-step:16669	 l-p:0.34860724210739136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0884, 5.0884, 5.0884],
        [5.0884, 5.4891, 5.4262],
        [5.0884, 5.0883, 5.0884],
        [5.0884, 4.9346, 4.9802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.15873008966445923 
model_pd.l_d.mean(): -19.742708206176758 
model_pd.lagr.mean(): -19.5839786529541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5820], device='cuda:0')), ('power', tensor([-20.5531], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.15873008966445923
epoch£º834	 i:1 	 global-step:16681	 l-p:0.12501415610313416
epoch£º834	 i:2 	 global-step:16682	 l-p:0.11917565762996674
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12904949486255646
epoch£º834	 i:4 	 global-step:16684	 l-p:0.3117436468601227
epoch£º834	 i:5 	 global-step:16685	 l-p:0.10577046871185303
epoch£º834	 i:6 	 global-step:16686	 l-p:0.2547553777694702
epoch£º834	 i:7 	 global-step:16687	 l-p:0.11112529784440994
epoch£º834	 i:8 	 global-step:16688	 l-p:0.09966016560792923
epoch£º834	 i:9 	 global-step:16689	 l-p:0.14545968174934387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0904, 5.0902, 5.0904],
        [5.0904, 4.9694, 5.0232],
        [5.0904, 4.8354, 4.6445],
        [5.0904, 4.8989, 4.6091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.08098381012678146 
model_pd.l_d.mean(): -19.875167846679688 
model_pd.lagr.mean(): -19.7941837310791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5013], device='cuda:0')), ('power', tensor([-20.6045], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.08098381012678146
epoch£º835	 i:1 	 global-step:16701	 l-p:0.15031501650810242
epoch£º835	 i:2 	 global-step:16702	 l-p:0.11980143189430237
epoch£º835	 i:3 	 global-step:16703	 l-p:0.22114862501621246
epoch£º835	 i:4 	 global-step:16704	 l-p:0.23648568987846375
epoch£º835	 i:5 	 global-step:16705	 l-p:0.13336190581321716
epoch£º835	 i:6 	 global-step:16706	 l-p:0.10085763782262802
epoch£º835	 i:7 	 global-step:16707	 l-p:0.18019239604473114
epoch£º835	 i:8 	 global-step:16708	 l-p:0.13936205208301544
epoch£º835	 i:9 	 global-step:16709	 l-p:0.13288213312625885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1049, 5.0920, 5.1037],
        [5.1049, 5.1187, 4.8410],
        [5.1049, 5.1039, 5.1049],
        [5.1049, 5.0001, 5.0538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.1495869755744934 
model_pd.l_d.mean(): -20.57602310180664 
model_pd.lagr.mean(): -20.426435470581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-21.2774], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.1495869755744934
epoch£º836	 i:1 	 global-step:16721	 l-p:0.13731670379638672
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11899043619632721
epoch£º836	 i:3 	 global-step:16723	 l-p:0.10357766598463058
epoch£º836	 i:4 	 global-step:16724	 l-p:0.1705980896949768
epoch£º836	 i:5 	 global-step:16725	 l-p:0.13274410367012024
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12677057087421417
epoch£º836	 i:7 	 global-step:16727	 l-p:0.14384932816028595
epoch£º836	 i:8 	 global-step:16728	 l-p:0.13481226563453674
epoch£º836	 i:9 	 global-step:16729	 l-p:0.18784061074256897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1198, 5.0495, 5.0955],
        [5.1198, 5.1195, 5.1198],
        [5.1198, 5.0256, 4.7220],
        [5.1198, 4.9036, 4.8893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.18792296946048737 
model_pd.l_d.mean(): -20.897911071777344 
model_pd.lagr.mean(): -20.70998764038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.5586], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.18792296946048737
epoch£º837	 i:1 	 global-step:16741	 l-p:0.13245324790477753
epoch£º837	 i:2 	 global-step:16742	 l-p:0.12568922340869904
epoch£º837	 i:3 	 global-step:16743	 l-p:0.1270710825920105
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11953418701887131
epoch£º837	 i:5 	 global-step:16745	 l-p:0.13227824866771698
epoch£º837	 i:6 	 global-step:16746	 l-p:0.1862645447254181
epoch£º837	 i:7 	 global-step:16747	 l-p:0.08453343063592911
epoch£º837	 i:8 	 global-step:16748	 l-p:0.15207943320274353
epoch£º837	 i:9 	 global-step:16749	 l-p:0.12591320276260376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1273, 5.1273, 5.1273],
        [5.1273, 5.5010, 5.4186],
        [5.1273, 4.9769, 5.0228],
        [5.1273, 5.0069, 5.0602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.10761163383722305 
model_pd.l_d.mean(): -19.770111083984375 
model_pd.lagr.mean(): -19.662500381469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5867], device='cuda:0')), ('power', tensor([-20.5855], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.10761163383722305
epoch£º838	 i:1 	 global-step:16761	 l-p:0.1535091996192932
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12318013608455658
epoch£º838	 i:3 	 global-step:16763	 l-p:0.1270107477903366
epoch£º838	 i:4 	 global-step:16764	 l-p:0.11629056930541992
epoch£º838	 i:5 	 global-step:16765	 l-p:0.1516748070716858
epoch£º838	 i:6 	 global-step:16766	 l-p:0.11999399214982986
epoch£º838	 i:7 	 global-step:16767	 l-p:0.18443721532821655
epoch£º838	 i:8 	 global-step:16768	 l-p:0.1306331753730774
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11830443143844604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1467, 5.1465, 5.1467],
        [5.1467, 5.1465, 5.1467],
        [5.1467, 5.1467, 5.1467],
        [5.1467, 5.1440, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.12250455468893051 
model_pd.l_d.mean(): -20.327587127685547 
model_pd.lagr.mean(): -20.205081939697266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4813], device='cuda:0')), ('power', tensor([-21.0415], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.12250455468893051
epoch£º839	 i:1 	 global-step:16781	 l-p:0.17149938642978668
epoch£º839	 i:2 	 global-step:16782	 l-p:0.1336425095796585
epoch£º839	 i:3 	 global-step:16783	 l-p:0.1837625652551651
epoch£º839	 i:4 	 global-step:16784	 l-p:0.16862918436527252
epoch£º839	 i:5 	 global-step:16785	 l-p:0.10056789964437485
epoch£º839	 i:6 	 global-step:16786	 l-p:0.0780610516667366
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11767717450857162
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11981291323900223
epoch£º839	 i:9 	 global-step:16789	 l-p:0.11686757951974869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1420, 5.1420, 5.1420],
        [5.1420, 5.0200, 4.7170],
        [5.1420, 5.0188, 5.0716],
        [5.1420, 5.1420, 5.1420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.10888232290744781 
model_pd.l_d.mean(): -19.29936981201172 
model_pd.lagr.mean(): -19.190486907958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-20.0717], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.10888232290744781
epoch£º840	 i:1 	 global-step:16801	 l-p:0.10168714076280594
epoch£º840	 i:2 	 global-step:16802	 l-p:0.0906924158334732
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11978154629468918
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17753459513187408
epoch£º840	 i:5 	 global-step:16805	 l-p:0.16180960834026337
epoch£º840	 i:6 	 global-step:16806	 l-p:0.1281987726688385
epoch£º840	 i:7 	 global-step:16807	 l-p:0.15838679671287537
epoch£º840	 i:8 	 global-step:16808	 l-p:0.09898576885461807
epoch£º840	 i:9 	 global-step:16809	 l-p:0.14016161859035492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1501, 4.9133, 4.6829],
        [5.1501, 5.1444, 5.1498],
        [5.1501, 5.1480, 5.1500],
        [5.1501, 5.0327, 4.7299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.07926877588033676 
model_pd.l_d.mean(): -20.51227378845215 
model_pd.lagr.mean(): -20.43300437927246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.2112], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:0.07926877588033676
epoch£º841	 i:1 	 global-step:16821	 l-p:0.09770863503217697
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12318345904350281
epoch£º841	 i:3 	 global-step:16823	 l-p:0.18639524281024933
epoch£º841	 i:4 	 global-step:16824	 l-p:0.1269075870513916
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11579418927431107
epoch£º841	 i:6 	 global-step:16826	 l-p:0.15665508806705475
epoch£º841	 i:7 	 global-step:16827	 l-p:0.10677342116832733
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13037961721420288
epoch£º841	 i:9 	 global-step:16829	 l-p:0.1633892059326172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1414, 5.0584, 5.1083],
        [5.1414, 4.9432, 4.6633],
        [5.1414, 5.3231, 5.1234],
        [5.1414, 5.1414, 5.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.13143005967140198 
model_pd.l_d.mean(): -20.903459548950195 
model_pd.lagr.mean(): -20.772029876708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4007], device='cuda:0')), ('power', tensor([-21.5412], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.13143005967140198
epoch£º842	 i:1 	 global-step:16841	 l-p:0.06901282072067261
epoch£º842	 i:2 	 global-step:16842	 l-p:0.17338138818740845
epoch£º842	 i:3 	 global-step:16843	 l-p:0.1232943907380104
epoch£º842	 i:4 	 global-step:16844	 l-p:0.1365494430065155
epoch£º842	 i:5 	 global-step:16845	 l-p:0.11214865744113922
epoch£º842	 i:6 	 global-step:16846	 l-p:0.128115713596344
epoch£º842	 i:7 	 global-step:16847	 l-p:0.10750522464513779
epoch£º842	 i:8 	 global-step:16848	 l-p:0.22524413466453552
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11932601779699326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.0028, 4.6997],
        [5.1359, 5.1359, 5.1359],
        [5.1359, 5.1207, 5.1342],
        [5.1359, 5.3094, 5.1053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.10747180879116058 
model_pd.l_d.mean(): -19.966360092163086 
model_pd.lagr.mean(): -19.858888626098633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4890], device='cuda:0')), ('power', tensor([-20.6841], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.10747180879116058
epoch£º843	 i:1 	 global-step:16861	 l-p:0.11781077831983566
epoch£º843	 i:2 	 global-step:16862	 l-p:0.10157368332147598
epoch£º843	 i:3 	 global-step:16863	 l-p:0.1429443657398224
epoch£º843	 i:4 	 global-step:16864	 l-p:0.19593234360218048
epoch£º843	 i:5 	 global-step:16865	 l-p:0.0850815400481224
epoch£º843	 i:6 	 global-step:16866	 l-p:0.14896991848945618
epoch£º843	 i:7 	 global-step:16867	 l-p:0.10987906157970428
epoch£º843	 i:8 	 global-step:16868	 l-p:0.14318999648094177
epoch£º843	 i:9 	 global-step:16869	 l-p:0.1753757745027542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1421, 4.9908, 4.6908],
        [5.1421, 5.1420, 5.1421],
        [5.1421, 4.9092, 4.8602],
        [5.1421, 5.0072, 5.0581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.12147077172994614 
model_pd.l_d.mean(): -20.67091178894043 
model_pd.lagr.mean(): -20.549440383911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.3378], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.12147077172994614
epoch£º844	 i:1 	 global-step:16881	 l-p:0.12926410138607025
epoch£º844	 i:2 	 global-step:16882	 l-p:0.20349253714084625
epoch£º844	 i:3 	 global-step:16883	 l-p:0.05179109424352646
epoch£º844	 i:4 	 global-step:16884	 l-p:0.1427173912525177
epoch£º844	 i:5 	 global-step:16885	 l-p:0.13948825001716614
epoch£º844	 i:6 	 global-step:16886	 l-p:0.14030854403972626
epoch£º844	 i:7 	 global-step:16887	 l-p:0.15153929591178894
epoch£º844	 i:8 	 global-step:16888	 l-p:0.12028404325246811
epoch£º844	 i:9 	 global-step:16889	 l-p:0.13046588003635406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1346, 5.1347],
        [5.1347, 5.1172, 5.1326],
        [5.1347, 4.9579, 4.9887],
        [5.1347, 4.8849, 4.7811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.1361718326807022 
model_pd.l_d.mean(): -20.44285011291504 
model_pd.lagr.mean(): -20.306678771972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4653], device='cuda:0')), ('power', tensor([-21.1416], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.1361718326807022
epoch£º845	 i:1 	 global-step:16901	 l-p:0.12662345170974731
epoch£º845	 i:2 	 global-step:16902	 l-p:0.12821397185325623
epoch£º845	 i:3 	 global-step:16903	 l-p:0.1755063235759735
epoch£º845	 i:4 	 global-step:16904	 l-p:0.1337333768606186
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12106096744537354
epoch£º845	 i:6 	 global-step:16906	 l-p:0.11042092740535736
epoch£º845	 i:7 	 global-step:16907	 l-p:0.13467656075954437
epoch£º845	 i:8 	 global-step:16908	 l-p:0.12424562126398087
epoch£º845	 i:9 	 global-step:16909	 l-p:0.1518368273973465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1314, 5.0261, 5.0798],
        [5.1314, 5.0921, 5.1228],
        [5.1314, 5.0576, 5.1049],
        [5.1314, 5.1314, 5.1314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.13537928462028503 
model_pd.l_d.mean(): -19.531497955322266 
model_pd.lagr.mean(): -19.3961181640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5189], device='cuda:0')), ('power', tensor([-20.2750], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.13537928462028503
epoch£º846	 i:1 	 global-step:16921	 l-p:0.17057234048843384
epoch£º846	 i:2 	 global-step:16922	 l-p:0.19793646037578583
epoch£º846	 i:3 	 global-step:16923	 l-p:0.09217242896556854
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12535707652568817
epoch£º846	 i:5 	 global-step:16925	 l-p:0.06970062851905823
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11825928092002869
epoch£º846	 i:7 	 global-step:16927	 l-p:0.13989686965942383
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14413607120513916
epoch£º846	 i:9 	 global-step:16929	 l-p:0.1438589096069336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1460, 5.0980, 5.1338],
        [5.1460, 5.1460, 5.1460],
        [5.1460, 5.0946, 5.1322],
        [5.1460, 5.0387, 5.0923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12302331626415253 
model_pd.l_d.mean(): -20.743175506591797 
model_pd.lagr.mean(): -20.62015151977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.3906], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12302331626415253
epoch£º847	 i:1 	 global-step:16941	 l-p:0.15268956124782562
epoch£º847	 i:2 	 global-step:16942	 l-p:0.14514940977096558
epoch£º847	 i:3 	 global-step:16943	 l-p:0.09027842432260513
epoch£º847	 i:4 	 global-step:16944	 l-p:0.14883626997470856
epoch£º847	 i:5 	 global-step:16945	 l-p:0.17385219037532806
epoch£º847	 i:6 	 global-step:16946	 l-p:0.06336478143930435
epoch£º847	 i:7 	 global-step:16947	 l-p:0.1505550891160965
epoch£º847	 i:8 	 global-step:16948	 l-p:0.16756336390972137
epoch£º847	 i:9 	 global-step:16949	 l-p:0.11949195712804794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1394, 5.0672, 5.1139],
        [5.1394, 4.9375, 4.9437],
        [5.1394, 5.1389, 5.1394],
        [5.1394, 4.9274, 4.6561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.1737346649169922 
model_pd.l_d.mean(): -20.806575775146484 
model_pd.lagr.mean(): -20.632841110229492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4081], device='cuda:0')), ('power', tensor([-21.4509], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.1737346649169922
epoch£º848	 i:1 	 global-step:16961	 l-p:0.06400350481271744
epoch£º848	 i:2 	 global-step:16962	 l-p:0.13744115829467773
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15178778767585754
epoch£º848	 i:4 	 global-step:16964	 l-p:0.16094495356082916
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13847629725933075
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13149389624595642
epoch£º848	 i:7 	 global-step:16967	 l-p:0.15575259923934937
epoch£º848	 i:8 	 global-step:16968	 l-p:0.08058363944292068
epoch£º848	 i:9 	 global-step:16969	 l-p:0.09535340964794159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 4.9296, 4.8800],
        [5.1621, 5.1367, 4.8459],
        [5.1621, 4.9416, 4.9174],
        [5.1621, 5.1145, 5.1500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12442187964916229 
model_pd.l_d.mean(): -18.954498291015625 
model_pd.lagr.mean(): -18.830076217651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5517], device='cuda:0')), ('power', tensor([-19.7253], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12442187964916229
epoch£º849	 i:1 	 global-step:16981	 l-p:0.13684050738811493
epoch£º849	 i:2 	 global-step:16982	 l-p:0.1294049620628357
epoch£º849	 i:3 	 global-step:16983	 l-p:0.11721054464578629
epoch£º849	 i:4 	 global-step:16984	 l-p:0.08251422643661499
epoch£º849	 i:5 	 global-step:16985	 l-p:0.16321079432964325
epoch£º849	 i:6 	 global-step:16986	 l-p:0.13836036622524261
epoch£º849	 i:7 	 global-step:16987	 l-p:0.07448319345712662
epoch£º849	 i:8 	 global-step:16988	 l-p:0.12557244300842285
epoch£º849	 i:9 	 global-step:16989	 l-p:0.15965624153614044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[5.1587, 5.5036, 5.4005],
        [5.1587, 4.9075, 4.7241],
        [5.1587, 5.0060, 5.0509],
        [5.1587, 5.5203, 5.4280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.11193326860666275 
model_pd.l_d.mean(): -20.323543548583984 
model_pd.lagr.mean(): -20.211610794067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4750], device='cuda:0')), ('power', tensor([-21.0309], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.11193326860666275
epoch£º850	 i:1 	 global-step:17001	 l-p:0.14166061580181122
epoch£º850	 i:2 	 global-step:17002	 l-p:0.1154303178191185
epoch£º850	 i:3 	 global-step:17003	 l-p:0.13987256586551666
epoch£º850	 i:4 	 global-step:17004	 l-p:0.10031721740961075
epoch£º850	 i:5 	 global-step:17005	 l-p:0.1764313131570816
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11378875374794006
epoch£º850	 i:7 	 global-step:17007	 l-p:0.20253019034862518
epoch£º850	 i:8 	 global-step:17008	 l-p:0.10384948551654816
epoch£º850	 i:9 	 global-step:17009	 l-p:0.12144868820905685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1361, 5.1361],
        [5.1361, 5.1359, 5.1361],
        [5.1361, 5.0864, 5.1232],
        [5.1361, 4.9511, 4.6596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.16129574179649353 
model_pd.l_d.mean(): -19.6766357421875 
model_pd.lagr.mean(): -19.51534080505371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5594], device='cuda:0')), ('power', tensor([-20.4632], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.16129574179649353
epoch£º851	 i:1 	 global-step:17021	 l-p:0.10981811583042145
epoch£º851	 i:2 	 global-step:17022	 l-p:0.18926239013671875
epoch£º851	 i:3 	 global-step:17023	 l-p:0.11601683497428894
epoch£º851	 i:4 	 global-step:17024	 l-p:0.11732488125562668
epoch£º851	 i:5 	 global-step:17025	 l-p:0.10059361904859543
epoch£º851	 i:6 	 global-step:17026	 l-p:0.18593619763851166
epoch£º851	 i:7 	 global-step:17027	 l-p:0.08301680535078049
epoch£º851	 i:8 	 global-step:17028	 l-p:0.13338898122310638
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11894124001264572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 5.6189, 5.6046],
        [5.1429, 4.8881, 4.7512],
        [5.1429, 4.8946, 4.8015],
        [5.1429, 5.1428, 5.1429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.10106521099805832 
model_pd.l_d.mean(): -19.72441291809082 
model_pd.lagr.mean(): -19.623348236083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.4631], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.10106521099805832
epoch£º852	 i:1 	 global-step:17041	 l-p:0.09644462168216705
epoch£º852	 i:2 	 global-step:17042	 l-p:0.1502189338207245
epoch£º852	 i:3 	 global-step:17043	 l-p:0.1461412012577057
epoch£º852	 i:4 	 global-step:17044	 l-p:0.11340638995170593
epoch£º852	 i:5 	 global-step:17045	 l-p:0.12398256361484528
epoch£º852	 i:6 	 global-step:17046	 l-p:0.16323307156562805
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13905790448188782
epoch£º852	 i:8 	 global-step:17048	 l-p:0.15420500934123993
epoch£º852	 i:9 	 global-step:17049	 l-p:0.1384226232767105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1256, 5.1506, 4.8752],
        [5.1256, 5.1143, 5.1246],
        [5.1256, 4.9647, 5.0070],
        [5.1256, 4.9883, 5.0396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.1762269139289856 
model_pd.l_d.mean(): -19.552982330322266 
model_pd.lagr.mean(): -19.376754760742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5211], device='cuda:0')), ('power', tensor([-20.2990], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.1762269139289856
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12463179975748062
epoch£º853	 i:2 	 global-step:17062	 l-p:0.12537455558776855
epoch£º853	 i:3 	 global-step:17063	 l-p:0.15144146978855133
epoch£º853	 i:4 	 global-step:17064	 l-p:0.16790170967578888
epoch£º853	 i:5 	 global-step:17065	 l-p:0.13865497708320618
epoch£º853	 i:6 	 global-step:17066	 l-p:0.175961434841156
epoch£º853	 i:7 	 global-step:17067	 l-p:0.0839717760682106
epoch£º853	 i:8 	 global-step:17068	 l-p:0.15808124840259552
epoch£º853	 i:9 	 global-step:17069	 l-p:0.11248597502708435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1171, 4.9132, 4.6291],
        [5.1171, 5.1171, 5.1171],
        [5.1171, 5.4437, 5.3295],
        [5.1171, 5.1171, 5.1171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.13745540380477905 
model_pd.l_d.mean(): -20.0931396484375 
model_pd.lagr.mean(): -19.955684661865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.8580], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.13745540380477905
epoch£º854	 i:1 	 global-step:17081	 l-p:0.24529540538787842
epoch£º854	 i:2 	 global-step:17082	 l-p:0.11098390072584152
epoch£º854	 i:3 	 global-step:17083	 l-p:0.17141738533973694
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12651479244232178
epoch£º854	 i:5 	 global-step:17085	 l-p:0.0963524803519249
epoch£º854	 i:6 	 global-step:17086	 l-p:0.11912953853607178
epoch£º854	 i:7 	 global-step:17087	 l-p:0.1424577534198761
epoch£º854	 i:8 	 global-step:17088	 l-p:0.12783189117908478
epoch£º854	 i:9 	 global-step:17089	 l-p:0.13494017720222473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1148, 5.0142, 4.7066],
        [5.1148, 5.2367, 5.0046],
        [5.1148, 5.1091, 5.1144],
        [5.1148, 4.8778, 4.8293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.15928401052951813 
model_pd.l_d.mean(): -20.647201538085938 
model_pd.lagr.mean(): -20.487916946411133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.3331], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.15928401052951813
epoch£º855	 i:1 	 global-step:17101	 l-p:0.13297805190086365
epoch£º855	 i:2 	 global-step:17102	 l-p:0.12043968588113785
epoch£º855	 i:3 	 global-step:17103	 l-p:0.11758828163146973
epoch£º855	 i:4 	 global-step:17104	 l-p:0.16734792292118073
epoch£º855	 i:5 	 global-step:17105	 l-p:0.16149990260601044
epoch£º855	 i:6 	 global-step:17106	 l-p:0.142643004655838
epoch£º855	 i:7 	 global-step:17107	 l-p:0.2711605429649353
epoch£º855	 i:8 	 global-step:17108	 l-p:0.15752267837524414
epoch£º855	 i:9 	 global-step:17109	 l-p:0.12608054280281067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0891, 4.8872, 4.8990],
        [5.0891, 5.0827, 5.0887],
        [5.0891, 4.8854, 4.5961],
        [5.0891, 5.0891, 5.0891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.18785269558429718 
model_pd.l_d.mean(): -19.828798294067383 
model_pd.lagr.mean(): -19.640945434570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-20.5856], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.18785269558429718
epoch£º856	 i:1 	 global-step:17121	 l-p:0.1343768984079361
epoch£º856	 i:2 	 global-step:17122	 l-p:0.16837893426418304
epoch£º856	 i:3 	 global-step:17123	 l-p:0.11597364395856857
epoch£º856	 i:4 	 global-step:17124	 l-p:0.28336477279663086
epoch£º856	 i:5 	 global-step:17125	 l-p:0.20503385365009308
epoch£º856	 i:6 	 global-step:17126	 l-p:0.1522945761680603
epoch£º856	 i:7 	 global-step:17127	 l-p:0.1200665533542633
epoch£º856	 i:8 	 global-step:17128	 l-p:0.11370343714952469
epoch£º856	 i:9 	 global-step:17129	 l-p:0.11002977937459946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1032, 4.9996, 4.6909],
        [5.1032, 4.8974, 4.9036],
        [5.1032, 5.0842, 5.1008],
        [5.1032, 5.0369, 4.7332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.11223065853118896 
model_pd.l_d.mean(): -19.289196014404297 
model_pd.lagr.mean(): -19.176965713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6035], device='cuda:0')), ('power', tensor([-20.1165], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.11223065853118896
epoch£º857	 i:1 	 global-step:17141	 l-p:0.159169003367424
epoch£º857	 i:2 	 global-step:17142	 l-p:0.17714953422546387
epoch£º857	 i:3 	 global-step:17143	 l-p:0.2566324472427368
epoch£º857	 i:4 	 global-step:17144	 l-p:0.09872113168239594
epoch£º857	 i:5 	 global-step:17145	 l-p:0.1202792301774025
epoch£º857	 i:6 	 global-step:17146	 l-p:0.12684185802936554
epoch£º857	 i:7 	 global-step:17147	 l-p:0.16481587290763855
epoch£º857	 i:8 	 global-step:17148	 l-p:0.13045024871826172
epoch£º857	 i:9 	 global-step:17149	 l-p:0.10623542219400406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1331, 5.1327, 5.1331],
        [5.1331, 4.9671, 5.0066],
        [5.1331, 5.1331, 5.1331],
        [5.1331, 5.1256, 5.1326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.17365717887878418 
model_pd.l_d.mean(): -20.66281509399414 
model_pd.lagr.mean(): -20.489158630371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4245], device='cuda:0')), ('power', tensor([-21.3222], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.17365717887878418
epoch£º858	 i:1 	 global-step:17161	 l-p:0.14966686069965363
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12942533195018768
epoch£º858	 i:3 	 global-step:17163	 l-p:0.13955208659172058
epoch£º858	 i:4 	 global-step:17164	 l-p:0.0713658332824707
epoch£º858	 i:5 	 global-step:17165	 l-p:0.15781983733177185
epoch£º858	 i:6 	 global-step:17166	 l-p:0.1212204322218895
epoch£º858	 i:7 	 global-step:17167	 l-p:0.10701360553503036
epoch£º858	 i:8 	 global-step:17168	 l-p:0.16918693482875824
epoch£º858	 i:9 	 global-step:17169	 l-p:0.10316847264766693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1479, 4.8938, 4.7077],
        [5.1479, 5.1479, 5.1479],
        [5.1479, 5.5582, 5.4977],
        [5.1479, 5.0975, 5.1346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.12048444151878357 
model_pd.l_d.mean(): -20.554931640625 
model_pd.lagr.mean(): -20.434446334838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.2235], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.12048444151878357
epoch£º859	 i:1 	 global-step:17181	 l-p:0.09931237250566483
epoch£º859	 i:2 	 global-step:17182	 l-p:0.14740008115768433
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11414337158203125
epoch£º859	 i:4 	 global-step:17184	 l-p:0.10222214460372925
epoch£º859	 i:5 	 global-step:17185	 l-p:0.1556154191493988
epoch£º859	 i:6 	 global-step:17186	 l-p:0.13865365087985992
epoch£º859	 i:7 	 global-step:17187	 l-p:0.20703710615634918
epoch£º859	 i:8 	 global-step:17188	 l-p:0.1431310921907425
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15930141508579254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1121, 5.0657, 5.1007],
        [5.1121, 5.1038, 5.1115],
        [5.1121, 4.9569, 5.0029],
        [5.1121, 5.0973, 5.1105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.11071451008319855 
model_pd.l_d.mean(): -20.505645751953125 
model_pd.lagr.mean(): -20.39493179321289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-21.1987], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.11071451008319855
epoch£º860	 i:1 	 global-step:17201	 l-p:0.15562555193901062
epoch£º860	 i:2 	 global-step:17202	 l-p:0.14984111487865448
epoch£º860	 i:3 	 global-step:17203	 l-p:0.21562835574150085
epoch£º860	 i:4 	 global-step:17204	 l-p:0.146651953458786
epoch£º860	 i:5 	 global-step:17205	 l-p:0.09870641678571701
epoch£º860	 i:6 	 global-step:17206	 l-p:0.13065668940544128
epoch£º860	 i:7 	 global-step:17207	 l-p:0.1338021606206894
epoch£º860	 i:8 	 global-step:17208	 l-p:0.17681851983070374
epoch£º860	 i:9 	 global-step:17209	 l-p:0.16345088183879852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 4.8499, 4.7460],
        [5.1054, 5.2188, 4.9822],
        [5.1054, 5.0166, 5.0685],
        [5.1054, 5.0931, 5.1042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.17264151573181152 
model_pd.l_d.mean(): -20.63943099975586 
model_pd.lagr.mean(): -20.46678924560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.3254], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.17264151573181152
epoch£º861	 i:1 	 global-step:17221	 l-p:0.1328117996454239
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12302477657794952
epoch£º861	 i:3 	 global-step:17223	 l-p:0.17478904128074646
epoch£º861	 i:4 	 global-step:17224	 l-p:0.1356613039970398
epoch£º861	 i:5 	 global-step:17225	 l-p:0.1546478420495987
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12370147556066513
epoch£º861	 i:7 	 global-step:17227	 l-p:0.11300374567508698
epoch£º861	 i:8 	 global-step:17228	 l-p:0.1272502839565277
epoch£º861	 i:9 	 global-step:17229	 l-p:0.1865878850221634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1173, 4.8792, 4.6301],
        [5.1173, 4.9208, 4.9370],
        [5.1173, 5.1093, 5.1167],
        [5.1173, 5.1173, 5.1173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13172122836112976 
model_pd.l_d.mean(): -20.411455154418945 
model_pd.lagr.mean(): -20.279733657836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4541], device='cuda:0')), ('power', tensor([-21.0985], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13172122836112976
epoch£º862	 i:1 	 global-step:17241	 l-p:0.1389651745557785
epoch£º862	 i:2 	 global-step:17242	 l-p:0.12942036986351013
epoch£º862	 i:3 	 global-step:17243	 l-p:0.11513891816139221
epoch£º862	 i:4 	 global-step:17244	 l-p:0.18614131212234497
epoch£º862	 i:5 	 global-step:17245	 l-p:0.1942361444234848
epoch£º862	 i:6 	 global-step:17246	 l-p:0.13968589901924133
epoch£º862	 i:7 	 global-step:17247	 l-p:0.1391940414905548
epoch£º862	 i:8 	 global-step:17248	 l-p:0.16631869971752167
epoch£º862	 i:9 	 global-step:17249	 l-p:0.12619787454605103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.0744, 5.1055],
        [5.1141, 5.0935, 5.1113],
        [5.1141, 5.0324, 5.0825],
        [5.1141, 4.8807, 4.6231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.1624363362789154 
model_pd.l_d.mean(): -19.58291244506836 
model_pd.lagr.mean(): -19.42047691345215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5039], device='cuda:0')), ('power', tensor([-20.3117], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.1624363362789154
epoch£º863	 i:1 	 global-step:17261	 l-p:0.17179495096206665
epoch£º863	 i:2 	 global-step:17262	 l-p:0.1311248540878296
epoch£º863	 i:3 	 global-step:17263	 l-p:0.12122901529073715
epoch£º863	 i:4 	 global-step:17264	 l-p:0.09225637465715408
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12279033660888672
epoch£º863	 i:6 	 global-step:17266	 l-p:0.11535489559173584
epoch£º863	 i:7 	 global-step:17267	 l-p:0.2011987268924713
epoch£º863	 i:8 	 global-step:17268	 l-p:0.15533460676670074
epoch£º863	 i:9 	 global-step:17269	 l-p:0.11625171452760696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1321, 5.0351, 4.7278],
        [5.1321, 5.1321, 5.1321],
        [5.1321, 5.1321, 5.1321],
        [5.1321, 5.5301, 5.4615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.16609063744544983 
model_pd.l_d.mean(): -19.501298904418945 
model_pd.lagr.mean(): -19.335208892822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4866], device='cuda:0')), ('power', tensor([-20.2115], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.16609063744544983
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12392120063304901
epoch£º864	 i:2 	 global-step:17282	 l-p:0.10894138365983963
epoch£º864	 i:3 	 global-step:17283	 l-p:0.18265141546726227
epoch£º864	 i:4 	 global-step:17284	 l-p:0.13816379010677338
epoch£º864	 i:5 	 global-step:17285	 l-p:0.06595488637685776
epoch£º864	 i:6 	 global-step:17286	 l-p:0.12587164342403412
epoch£º864	 i:7 	 global-step:17287	 l-p:0.12382789701223373
epoch£º864	 i:8 	 global-step:17288	 l-p:0.14340397715568542
epoch£º864	 i:9 	 global-step:17289	 l-p:0.14418679475784302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1473, 5.1473],
        [5.1473, 5.1463, 5.1473],
        [5.1473, 5.1473, 5.1473],
        [5.1473, 5.1473, 5.1473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.12793029844760895 
model_pd.l_d.mean(): -20.3882999420166 
model_pd.lagr.mean(): -20.2603702545166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4397], device='cuda:0')), ('power', tensor([-21.0603], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.12793029844760895
epoch£º865	 i:1 	 global-step:17301	 l-p:0.18684878945350647
epoch£º865	 i:2 	 global-step:17302	 l-p:0.15359234809875488
epoch£º865	 i:3 	 global-step:17303	 l-p:0.07681677490472794
epoch£º865	 i:4 	 global-step:17304	 l-p:0.17389073967933655
epoch£º865	 i:5 	 global-step:17305	 l-p:0.09323346614837646
epoch£º865	 i:6 	 global-step:17306	 l-p:0.11288797110319138
epoch£º865	 i:7 	 global-step:17307	 l-p:0.09560456871986389
epoch£º865	 i:8 	 global-step:17308	 l-p:0.13083137571811676
epoch£º865	 i:9 	 global-step:17309	 l-p:0.14177219569683075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1594, 5.3269, 5.1171],
        [5.1594, 5.6382, 5.6242],
        [5.1594, 5.0983, 5.1407],
        [5.1594, 5.1566, 5.1593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.07929737865924835 
model_pd.l_d.mean(): -19.667190551757812 
model_pd.lagr.mean(): -19.587892532348633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.3948], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.07929737865924835
epoch£º866	 i:1 	 global-step:17321	 l-p:0.16486871242523193
epoch£º866	 i:2 	 global-step:17322	 l-p:0.12340982258319855
epoch£º866	 i:3 	 global-step:17323	 l-p:0.14365142583847046
epoch£º866	 i:4 	 global-step:17324	 l-p:0.11048375070095062
epoch£º866	 i:5 	 global-step:17325	 l-p:0.1651817411184311
epoch£º866	 i:6 	 global-step:17326	 l-p:0.11038139462471008
epoch£º866	 i:7 	 global-step:17327	 l-p:0.12406128644943237
epoch£º866	 i:8 	 global-step:17328	 l-p:0.12133026123046875
epoch£º866	 i:9 	 global-step:17329	 l-p:0.1138233169913292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1591, 5.1591, 5.1591],
        [5.1591, 4.9868, 5.0219],
        [5.1591, 5.2718, 5.0338],
        [5.1591, 5.1557, 5.1590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.13380321860313416 
model_pd.l_d.mean(): -19.488489151000977 
model_pd.lagr.mean(): -19.354686737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5314], device='cuda:0')), ('power', tensor([-20.2444], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.13380321860313416
epoch£º867	 i:1 	 global-step:17341	 l-p:0.10949476063251495
epoch£º867	 i:2 	 global-step:17342	 l-p:0.2131519615650177
epoch£º867	 i:3 	 global-step:17343	 l-p:0.12743455171585083
epoch£º867	 i:4 	 global-step:17344	 l-p:0.11029215902090073
epoch£º867	 i:5 	 global-step:17345	 l-p:0.1100035160779953
epoch£º867	 i:6 	 global-step:17346	 l-p:0.12269960343837738
epoch£º867	 i:7 	 global-step:17347	 l-p:0.16218672692775726
epoch£º867	 i:8 	 global-step:17348	 l-p:0.1001841127872467
epoch£º867	 i:9 	 global-step:17349	 l-p:0.14148926734924316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[5.1145, 4.8532, 4.6966],
        [5.1145, 5.1733, 4.9105],
        [5.1145, 4.9082, 4.9143],
        [5.1145, 4.8573, 4.6553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.1703830361366272 
model_pd.l_d.mean(): -20.606159210205078 
model_pd.lagr.mean(): -20.435775756835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.3053], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.1703830361366272
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11950284987688065
epoch£º868	 i:2 	 global-step:17362	 l-p:0.2066010981798172
epoch£º868	 i:3 	 global-step:17363	 l-p:0.16517440974712372
epoch£º868	 i:4 	 global-step:17364	 l-p:0.20453494787216187
epoch£º868	 i:5 	 global-step:17365	 l-p:0.10894126445055008
epoch£º868	 i:6 	 global-step:17366	 l-p:0.10791497677564621
epoch£º868	 i:7 	 global-step:17367	 l-p:0.17829258739948273
epoch£º868	 i:8 	 global-step:17368	 l-p:0.1304256021976471
epoch£º868	 i:9 	 global-step:17369	 l-p:0.09872656315565109
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0926, 5.4808, 5.4064],
        [5.0926, 5.0889, 5.0924],
        [5.0926, 4.9052, 4.6050],
        [5.0926, 4.9379, 4.9852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.1376333087682724 
model_pd.l_d.mean(): -19.539443969726562 
model_pd.lagr.mean(): -19.401811599731445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5742], device='cuda:0')), ('power', tensor([-20.3396], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.1376333087682724
epoch£º869	 i:1 	 global-step:17381	 l-p:0.09271223098039627
epoch£º869	 i:2 	 global-step:17382	 l-p:0.08808562904596329
epoch£º869	 i:3 	 global-step:17383	 l-p:0.1826736479997635
epoch£º869	 i:4 	 global-step:17384	 l-p:0.12688949704170227
epoch£º869	 i:5 	 global-step:17385	 l-p:0.29119521379470825
epoch£º869	 i:6 	 global-step:17386	 l-p:0.19667644798755646
epoch£º869	 i:7 	 global-step:17387	 l-p:1.0078282356262207
epoch£º869	 i:8 	 global-step:17388	 l-p:0.13685762882232666
epoch£º869	 i:9 	 global-step:17389	 l-p:0.16510120034217834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[5.0737, 4.8645, 4.8709],
        [5.0737, 4.8084, 4.6682],
        [5.0737, 4.8199, 4.7395],
        [5.0737, 4.8687, 4.8801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.1289859414100647 
model_pd.l_d.mean(): -20.994441986083984 
model_pd.lagr.mean(): -20.865455627441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4062], device='cuda:0')), ('power', tensor([-21.6388], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.1289859414100647
epoch£º870	 i:1 	 global-step:17401	 l-p:0.22520232200622559
epoch£º870	 i:2 	 global-step:17402	 l-p:0.10898328572511673
epoch£º870	 i:3 	 global-step:17403	 l-p:0.6021962761878967
epoch£º870	 i:4 	 global-step:17404	 l-p:0.12234971672296524
epoch£º870	 i:5 	 global-step:17405	 l-p:0.1792432814836502
epoch£º870	 i:6 	 global-step:17406	 l-p:0.1495479792356491
epoch£º870	 i:7 	 global-step:17407	 l-p:0.19000647962093353
epoch£º870	 i:8 	 global-step:17408	 l-p:0.12143503129482269
epoch£º870	 i:9 	 global-step:17409	 l-p:0.20968028903007507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0803, 4.8215, 4.7203],
        [5.0803, 5.0627, 5.0782],
        [5.0803, 4.9376, 4.6252],
        [5.0803, 4.8481, 4.8195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.17597320675849915 
model_pd.l_d.mean(): -20.265439987182617 
model_pd.lagr.mean(): -20.089466094970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-21.0162], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.17597320675849915
epoch£º871	 i:1 	 global-step:17421	 l-p:0.16990111768245697
epoch£º871	 i:2 	 global-step:17422	 l-p:0.16500653326511383
epoch£º871	 i:3 	 global-step:17423	 l-p:0.134647935628891
epoch£º871	 i:4 	 global-step:17424	 l-p:0.2234363853931427
epoch£º871	 i:5 	 global-step:17425	 l-p:0.1272127479314804
epoch£º871	 i:6 	 global-step:17426	 l-p:0.12577350437641144
epoch£º871	 i:7 	 global-step:17427	 l-p:0.17754700779914856
epoch£º871	 i:8 	 global-step:17428	 l-p:0.1062484160065651
epoch£º871	 i:9 	 global-step:17429	 l-p:0.1443871408700943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1152, 5.0735, 5.1058],
        [5.1152, 5.0482, 4.7431],
        [5.1152, 5.1152, 5.1152],
        [5.1152, 5.0831, 5.1093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.2116342931985855 
model_pd.l_d.mean(): -20.44281768798828 
model_pd.lagr.mean(): -20.231184005737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-21.1591], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.2116342931985855
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13181151449680328
epoch£º872	 i:2 	 global-step:17442	 l-p:0.11598590761423111
epoch£º872	 i:3 	 global-step:17443	 l-p:0.10473017394542694
epoch£º872	 i:4 	 global-step:17444	 l-p:0.1448247879743576
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13418039679527283
epoch£º872	 i:6 	 global-step:17446	 l-p:0.17206546664237976
epoch£º872	 i:7 	 global-step:17447	 l-p:0.08927276730537415
epoch£º872	 i:8 	 global-step:17448	 l-p:0.17158931493759155
epoch£º872	 i:9 	 global-step:17449	 l-p:0.12339149415493011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1340, 5.0066, 5.0604],
        [5.1340, 5.1264, 5.1335],
        [5.1340, 4.8817, 4.7884],
        [5.1340, 4.9050, 4.8750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.1101239025592804 
model_pd.l_d.mean(): -19.424762725830078 
model_pd.lagr.mean(): -19.314638137817383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5367], device='cuda:0')), ('power', tensor([-20.1854], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.1101239025592804
epoch£º873	 i:1 	 global-step:17461	 l-p:0.14265872538089752
epoch£º873	 i:2 	 global-step:17462	 l-p:0.16525229811668396
epoch£º873	 i:3 	 global-step:17463	 l-p:0.09767255932092667
epoch£º873	 i:4 	 global-step:17464	 l-p:0.14476852118968964
epoch£º873	 i:5 	 global-step:17465	 l-p:0.17096178233623505
epoch£º873	 i:6 	 global-step:17466	 l-p:0.15684610605239868
epoch£º873	 i:7 	 global-step:17467	 l-p:0.17029517889022827
epoch£º873	 i:8 	 global-step:17468	 l-p:0.15119630098342896
epoch£º873	 i:9 	 global-step:17469	 l-p:0.12008939683437347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1099, 5.1099, 5.1099],
        [5.1099, 5.1096, 5.1099],
        [5.1099, 5.1099, 5.1099],
        [5.1099, 5.2831, 5.0765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.11898324638605118 
model_pd.l_d.mean(): -20.687456130981445 
model_pd.lagr.mean(): -20.56847381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249], device='cuda:0')), ('power', tensor([-21.3476], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.11898324638605118
epoch£º874	 i:1 	 global-step:17481	 l-p:0.1496059000492096
epoch£º874	 i:2 	 global-step:17482	 l-p:0.1380074918270111
epoch£º874	 i:3 	 global-step:17483	 l-p:0.16681817173957825
epoch£º874	 i:4 	 global-step:17484	 l-p:0.1777924746274948
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12495328485965729
epoch£º874	 i:6 	 global-step:17486	 l-p:0.19615766406059265
epoch£º874	 i:7 	 global-step:17487	 l-p:0.13847139477729797
epoch£º874	 i:8 	 global-step:17488	 l-p:0.11326712369918823
epoch£º874	 i:9 	 global-step:17489	 l-p:0.13635393977165222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1199, 5.0762, 5.1097],
        [5.1199, 4.9589, 5.0028],
        [5.1199, 5.1199, 5.1199],
        [5.1199, 4.9330, 4.6348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.10124830156564713 
model_pd.l_d.mean(): -20.69843292236328 
model_pd.lagr.mean(): -20.597185134887695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-21.3507], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:0.10124830156564713
epoch£º875	 i:1 	 global-step:17501	 l-p:0.15635186433792114
epoch£º875	 i:2 	 global-step:17502	 l-p:0.2000972330570221
epoch£º875	 i:3 	 global-step:17503	 l-p:0.1973276287317276
epoch£º875	 i:4 	 global-step:17504	 l-p:0.11373412609100342
epoch£º875	 i:5 	 global-step:17505	 l-p:0.11174558103084564
epoch£º875	 i:6 	 global-step:17506	 l-p:0.13790075480937958
epoch£º875	 i:7 	 global-step:17507	 l-p:0.11033549904823303
epoch£º875	 i:8 	 global-step:17508	 l-p:0.0900803953409195
epoch£º875	 i:9 	 global-step:17509	 l-p:0.20031404495239258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 4.9790, 4.6708],
        [5.1282, 5.5868, 5.5587],
        [5.1282, 5.5405, 5.4806],
        [5.1282, 5.1281, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.19960014522075653 
model_pd.l_d.mean(): -19.49619483947754 
model_pd.lagr.mean(): -19.296594619750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5363], device='cuda:0')), ('power', tensor([-20.2571], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.19960014522075653
epoch£º876	 i:1 	 global-step:17521	 l-p:0.10267004370689392
epoch£º876	 i:2 	 global-step:17522	 l-p:0.10777444392442703
epoch£º876	 i:3 	 global-step:17523	 l-p:0.1316562294960022
epoch£º876	 i:4 	 global-step:17524	 l-p:0.1247931718826294
epoch£º876	 i:5 	 global-step:17525	 l-p:0.10744966566562653
epoch£º876	 i:6 	 global-step:17526	 l-p:0.1315952092409134
epoch£º876	 i:7 	 global-step:17527	 l-p:0.15971894562244415
epoch£º876	 i:8 	 global-step:17528	 l-p:0.12414728105068207
epoch£º876	 i:9 	 global-step:17529	 l-p:0.17430496215820312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1286, 5.1271, 5.1286],
        [5.1286, 5.1286, 5.1286],
        [5.1286, 4.8873, 4.6401],
        [5.1286, 5.5870, 5.5586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.11058883368968964 
model_pd.l_d.mean(): -19.18264389038086 
model_pd.lagr.mean(): -19.07205581665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-19.9154], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.11058883368968964
epoch£º877	 i:1 	 global-step:17541	 l-p:0.14892858266830444
epoch£º877	 i:2 	 global-step:17542	 l-p:0.1544928103685379
epoch£º877	 i:3 	 global-step:17543	 l-p:0.11308511346578598
epoch£º877	 i:4 	 global-step:17544	 l-p:0.1163221150636673
epoch£º877	 i:5 	 global-step:17545	 l-p:0.1405867338180542
epoch£º877	 i:6 	 global-step:17546	 l-p:0.1637873351573944
epoch£º877	 i:7 	 global-step:17547	 l-p:0.13065741956233978
epoch£º877	 i:8 	 global-step:17548	 l-p:0.17816072702407837
epoch£º877	 i:9 	 global-step:17549	 l-p:0.15323902666568756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1272, 5.1271, 5.1272],
        [5.1272, 5.4520, 5.3345],
        [5.1272, 5.4595, 5.3469],
        [5.1272, 5.1272, 5.1272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.17187844216823578 
model_pd.l_d.mean(): -20.664430618286133 
model_pd.lagr.mean(): -20.492551803588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4296], device='cuda:0')), ('power', tensor([-21.3291], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.17187844216823578
epoch£º878	 i:1 	 global-step:17561	 l-p:0.20018859207630157
epoch£º878	 i:2 	 global-step:17562	 l-p:0.09775476902723312
epoch£º878	 i:3 	 global-step:17563	 l-p:0.1439567655324936
epoch£º878	 i:4 	 global-step:17564	 l-p:0.1192692443728447
epoch£º878	 i:5 	 global-step:17565	 l-p:0.13497194647789001
epoch£º878	 i:6 	 global-step:17566	 l-p:0.13098382949829102
epoch£º878	 i:7 	 global-step:17567	 l-p:0.10183845460414886
epoch£º878	 i:8 	 global-step:17568	 l-p:0.13999781012535095
epoch£º878	 i:9 	 global-step:17569	 l-p:0.12340842932462692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1355, 5.1353, 5.1355],
        [5.1355, 5.0508, 5.1018],
        [5.1355, 4.9290, 4.6426],
        [5.1355, 5.1029, 5.1294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.14776840806007385 
model_pd.l_d.mean(): -20.597070693969727 
model_pd.lagr.mean(): -20.449302673339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4393], device='cuda:0')), ('power', tensor([-21.2710], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.14776840806007385
epoch£º879	 i:1 	 global-step:17581	 l-p:0.10023795813322067
epoch£º879	 i:2 	 global-step:17582	 l-p:0.13069912791252136
epoch£º879	 i:3 	 global-step:17583	 l-p:0.1966891884803772
epoch£º879	 i:4 	 global-step:17584	 l-p:0.06402218341827393
epoch£º879	 i:5 	 global-step:17585	 l-p:0.14018984138965607
epoch£º879	 i:6 	 global-step:17586	 l-p:0.11116155236959457
epoch£º879	 i:7 	 global-step:17587	 l-p:0.13308948278427124
epoch£º879	 i:8 	 global-step:17588	 l-p:0.21356967091560364
epoch£º879	 i:9 	 global-step:17589	 l-p:0.13957488536834717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 4.8631, 4.6720],
        [5.1237, 5.0324, 5.0850],
        [5.1237, 4.8781, 4.8143],
        [5.1237, 5.1235, 5.1237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.16675807535648346 
model_pd.l_d.mean(): -20.179903030395508 
model_pd.lagr.mean(): -20.013145446777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-20.9372], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.16675807535648346
epoch£º880	 i:1 	 global-step:17601	 l-p:0.1249670758843422
epoch£º880	 i:2 	 global-step:17602	 l-p:0.15054550766944885
epoch£º880	 i:3 	 global-step:17603	 l-p:0.11708447337150574
epoch£º880	 i:4 	 global-step:17604	 l-p:0.17480264604091644
epoch£º880	 i:5 	 global-step:17605	 l-p:0.2117953598499298
epoch£º880	 i:6 	 global-step:17606	 l-p:0.13310378789901733
epoch£º880	 i:7 	 global-step:17607	 l-p:0.13591627776622772
epoch£º880	 i:8 	 global-step:17608	 l-p:0.10116877406835556
epoch£º880	 i:9 	 global-step:17609	 l-p:0.14438550174236298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1117, 5.1117, 5.1117],
        [5.1117, 5.0046, 5.0596],
        [5.1117, 4.8650, 4.8015],
        [5.1117, 5.4117, 5.2789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.1698479801416397 
model_pd.l_d.mean(): -19.33675193786621 
model_pd.lagr.mean(): -19.16690444946289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5068], device='cuda:0')), ('power', tensor([-20.0658], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.1698479801416397
epoch£º881	 i:1 	 global-step:17621	 l-p:0.2416418343782425
epoch£º881	 i:2 	 global-step:17622	 l-p:0.13826502859592438
epoch£º881	 i:3 	 global-step:17623	 l-p:0.12683545053005219
epoch£º881	 i:4 	 global-step:17624	 l-p:0.11118476837873459
epoch£º881	 i:5 	 global-step:17625	 l-p:0.1480582058429718
epoch£º881	 i:6 	 global-step:17626	 l-p:0.12611429393291473
epoch£º881	 i:7 	 global-step:17627	 l-p:0.1309894621372223
epoch£º881	 i:8 	 global-step:17628	 l-p:0.1314186006784439
epoch£º881	 i:9 	 global-step:17629	 l-p:0.10278617590665817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1206, 4.8804, 4.8318],
        [5.1206, 4.8672, 4.6417],
        [5.1206, 5.1206, 5.1206],
        [5.1206, 5.1166, 5.1204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.09150835126638412 
model_pd.l_d.mean(): -20.425506591796875 
model_pd.lagr.mean(): -20.33399772644043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.1405], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.09150835126638412
epoch£º882	 i:1 	 global-step:17641	 l-p:0.15062078833580017
epoch£º882	 i:2 	 global-step:17642	 l-p:0.18481533229351044
epoch£º882	 i:3 	 global-step:17643	 l-p:0.09020449221134186
epoch£º882	 i:4 	 global-step:17644	 l-p:0.2674618363380432
epoch£º882	 i:5 	 global-step:17645	 l-p:0.15028247237205505
epoch£º882	 i:6 	 global-step:17646	 l-p:0.14736303687095642
epoch£º882	 i:7 	 global-step:17647	 l-p:0.14247959852218628
epoch£º882	 i:8 	 global-step:17648	 l-p:0.12074544280767441
epoch£º882	 i:9 	 global-step:17649	 l-p:0.1260349303483963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0990, 5.0963, 5.0989],
        [5.0990, 5.0985, 5.0990],
        [5.0990, 5.0863, 5.0978],
        [5.0990, 4.9365, 4.9811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.13060283660888672 
model_pd.l_d.mean(): -20.67412567138672 
model_pd.lagr.mean(): -20.543521881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-21.3578], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.13060283660888672
epoch£º883	 i:1 	 global-step:17661	 l-p:0.14441411197185516
epoch£º883	 i:2 	 global-step:17662	 l-p:0.137915700674057
epoch£º883	 i:3 	 global-step:17663	 l-p:0.279445618391037
epoch£º883	 i:4 	 global-step:17664	 l-p:0.15987282991409302
epoch£º883	 i:5 	 global-step:17665	 l-p:0.16306626796722412
epoch£º883	 i:6 	 global-step:17666	 l-p:0.19542314112186432
epoch£º883	 i:7 	 global-step:17667	 l-p:0.10392206162214279
epoch£º883	 i:8 	 global-step:17668	 l-p:0.11201883852481842
epoch£º883	 i:9 	 global-step:17669	 l-p:0.13831035792827606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1135, 5.1135, 5.1135],
        [5.1135, 4.8526, 4.6507],
        [5.1135, 5.1135, 5.1135],
        [5.1135, 4.9141, 4.6187]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.11083594709634781 
model_pd.l_d.mean(): -19.96000099182129 
model_pd.lagr.mean(): -19.849164962768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4850], device='cuda:0')), ('power', tensor([-20.6736], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.11083594709634781
epoch£º884	 i:1 	 global-step:17681	 l-p:0.09598074853420258
epoch£º884	 i:2 	 global-step:17682	 l-p:0.15677089989185333
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12874707579612732
epoch£º884	 i:4 	 global-step:17684	 l-p:0.14844341576099396
epoch£º884	 i:5 	 global-step:17685	 l-p:0.0877784863114357
epoch£º884	 i:6 	 global-step:17686	 l-p:0.1539430320262909
epoch£º884	 i:7 	 global-step:17687	 l-p:0.15735463798046112
epoch£º884	 i:8 	 global-step:17688	 l-p:0.2088455706834793
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13508659601211548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1446, 4.9837, 4.6773],
        [5.1446, 5.1446, 5.1446],
        [5.1446, 4.8948, 4.8151],
        [5.1446, 5.1706, 4.8927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.1388883739709854 
model_pd.l_d.mean(): -20.378801345825195 
model_pd.lagr.mean(): -20.239912033081055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4779], device='cuda:0')), ('power', tensor([-21.0897], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.1388883739709854
epoch£º885	 i:1 	 global-step:17701	 l-p:0.19872523844242096
epoch£º885	 i:2 	 global-step:17702	 l-p:0.07968604564666748
epoch£º885	 i:3 	 global-step:17703	 l-p:0.13757212460041046
epoch£º885	 i:4 	 global-step:17704	 l-p:0.1366022527217865
epoch£º885	 i:5 	 global-step:17705	 l-p:0.12125056982040405
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12081531435251236
epoch£º885	 i:7 	 global-step:17707	 l-p:0.12506256997585297
epoch£º885	 i:8 	 global-step:17708	 l-p:0.12406482547521591
epoch£º885	 i:9 	 global-step:17709	 l-p:0.1403827965259552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1384, 5.1367, 5.1384],
        [5.1384, 4.9839, 5.0310],
        [5.1384, 5.1308, 5.1379],
        [5.1384, 5.0455, 5.0985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14820793271064758 
model_pd.l_d.mean(): -19.381107330322266 
model_pd.lagr.mean(): -19.232898712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5724], device='cuda:0')), ('power', tensor([-20.1777], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14820793271064758
epoch£º886	 i:1 	 global-step:17721	 l-p:0.10103943943977356
epoch£º886	 i:2 	 global-step:17722	 l-p:0.17979519069194794
epoch£º886	 i:3 	 global-step:17723	 l-p:0.14834371209144592
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11799943447113037
epoch£º886	 i:5 	 global-step:17725	 l-p:0.2372361719608307
epoch£º886	 i:6 	 global-step:17726	 l-p:0.1609117090702057
epoch£º886	 i:7 	 global-step:17727	 l-p:0.09812410175800323
epoch£º886	 i:8 	 global-step:17728	 l-p:0.1443621963262558
epoch£º886	 i:9 	 global-step:17729	 l-p:0.08491901308298111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1124, 5.0899, 5.1092],
        [5.1124, 4.8494, 4.7226],
        [5.1124, 4.9960, 5.0515],
        [5.1124, 5.0193, 5.0725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.12101950496435165 
model_pd.l_d.mean(): -20.988842010498047 
model_pd.lagr.mean(): -20.867822647094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3905], device='cuda:0')), ('power', tensor([-21.6171], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.12101950496435165
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11673833429813385
epoch£º887	 i:2 	 global-step:17742	 l-p:0.15096543729305267
epoch£º887	 i:3 	 global-step:17743	 l-p:0.12130601704120636
epoch£º887	 i:4 	 global-step:17744	 l-p:0.2289268970489502
epoch£º887	 i:5 	 global-step:17745	 l-p:0.12197244167327881
epoch£º887	 i:6 	 global-step:17746	 l-p:0.2222018837928772
epoch£º887	 i:7 	 global-step:17747	 l-p:0.12192117422819138
epoch£º887	 i:8 	 global-step:17748	 l-p:0.2158687561750412
epoch£º887	 i:9 	 global-step:17749	 l-p:0.08642051368951797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1081, 5.4060, 5.2712],
        [5.1081, 4.8637, 4.8101],
        [5.1081, 5.3833, 5.2345],
        [5.1081, 4.8657, 4.8170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.07693971693515778 
model_pd.l_d.mean(): -19.44655990600586 
model_pd.lagr.mean(): -19.369619369506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5878], device='cuda:0')), ('power', tensor([-20.2596], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.07693971693515778
epoch£º888	 i:1 	 global-step:17761	 l-p:0.26766955852508545
epoch£º888	 i:2 	 global-step:17762	 l-p:0.13543087244033813
epoch£º888	 i:3 	 global-step:17763	 l-p:0.11728645861148834
epoch£º888	 i:4 	 global-step:17764	 l-p:0.17838138341903687
epoch£º888	 i:5 	 global-step:17765	 l-p:0.12425652891397476
epoch£º888	 i:6 	 global-step:17766	 l-p:0.10713183879852295
epoch£º888	 i:7 	 global-step:17767	 l-p:0.11985524743795395
epoch£º888	 i:8 	 global-step:17768	 l-p:0.19778212904930115
epoch£º888	 i:9 	 global-step:17769	 l-p:0.1977364867925644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0994, 5.0994, 5.0994],
        [5.0994, 5.0994, 5.0994],
        [5.0994, 4.9297, 4.9711],
        [5.0994, 5.0370, 5.0803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.16517890989780426 
model_pd.l_d.mean(): -20.904926300048828 
model_pd.lagr.mean(): -20.739748001098633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4270], device='cuda:0')), ('power', tensor([-21.5696], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.16517890989780426
epoch£º889	 i:1 	 global-step:17781	 l-p:0.1858552098274231
epoch£º889	 i:2 	 global-step:17782	 l-p:0.1303737461566925
epoch£º889	 i:3 	 global-step:17783	 l-p:0.15101011097431183
epoch£º889	 i:4 	 global-step:17784	 l-p:0.12231374531984329
epoch£º889	 i:5 	 global-step:17785	 l-p:0.15159893035888672
epoch£º889	 i:6 	 global-step:17786	 l-p:0.12836502492427826
epoch£º889	 i:7 	 global-step:17787	 l-p:0.16713987290859222
epoch£º889	 i:8 	 global-step:17788	 l-p:0.10790422558784485
epoch£º889	 i:9 	 global-step:17789	 l-p:0.1724167913198471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 5.1280, 5.1281],
        [5.1281, 5.1281, 5.1281],
        [5.1281, 4.9193, 4.9252],
        [5.1281, 4.9112, 4.9064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.11508193612098694 
model_pd.l_d.mean(): -20.463754653930664 
model_pd.lagr.mean(): -20.34867286682129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-21.1665], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.11508193612098694
epoch£º890	 i:1 	 global-step:17801	 l-p:0.1299402117729187
epoch£º890	 i:2 	 global-step:17802	 l-p:0.09700381755828857
epoch£º890	 i:3 	 global-step:17803	 l-p:0.13020338118076324
epoch£º890	 i:4 	 global-step:17804	 l-p:0.20046202838420868
epoch£º890	 i:5 	 global-step:17805	 l-p:0.139922097325325
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13927637040615082
epoch£º890	 i:7 	 global-step:17807	 l-p:0.14501447975635529
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10775475203990936
epoch£º890	 i:9 	 global-step:17809	 l-p:0.13021230697631836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.1390, 5.1569],
        [5.1597, 5.2658, 5.0222],
        [5.1597, 5.1012, 5.1426],
        [5.1597, 5.1573, 5.1597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.1285971850156784 
model_pd.l_d.mean(): -19.118885040283203 
model_pd.lagr.mean(): -18.99028778076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5519], device='cuda:0')), ('power', tensor([-19.8916], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.1285971850156784
epoch£º891	 i:1 	 global-step:17821	 l-p:0.10641736537218094
epoch£º891	 i:2 	 global-step:17822	 l-p:0.1365996152162552
epoch£º891	 i:3 	 global-step:17823	 l-p:0.15819378197193146
epoch£º891	 i:4 	 global-step:17824	 l-p:0.10989417880773544
epoch£º891	 i:5 	 global-step:17825	 l-p:0.12109887599945068
epoch£º891	 i:6 	 global-step:17826	 l-p:0.10359761118888855
epoch£º891	 i:7 	 global-step:17827	 l-p:0.14533492922782898
epoch£º891	 i:8 	 global-step:17828	 l-p:0.16050463914871216
epoch£º891	 i:9 	 global-step:17829	 l-p:0.10554345697164536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.9320, 4.8924],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 5.0422, 4.7316],
        [5.1664, 5.1620, 5.1662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.13477353751659393 
model_pd.l_d.mean(): -20.001667022705078 
model_pd.lagr.mean(): -19.866893768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5266], device='cuda:0')), ('power', tensor([-20.7582], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.13477353751659393
epoch£º892	 i:1 	 global-step:17841	 l-p:0.13790865242481232
epoch£º892	 i:2 	 global-step:17842	 l-p:0.17985159158706665
epoch£º892	 i:3 	 global-step:17843	 l-p:0.04304781183600426
epoch£º892	 i:4 	 global-step:17844	 l-p:0.16292817890644073
epoch£º892	 i:5 	 global-step:17845	 l-p:0.09401645511388779
epoch£º892	 i:6 	 global-step:17846	 l-p:0.09914401173591614
epoch£º892	 i:7 	 global-step:17847	 l-p:0.1164589449763298
epoch£º892	 i:8 	 global-step:17848	 l-p:0.14524774253368378
epoch£º892	 i:9 	 global-step:17849	 l-p:0.14112061262130737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.2305, 4.9711],
        [5.1580, 5.5572, 5.4860],
        [5.1580, 5.1448, 5.1567],
        [5.1580, 4.9555, 4.9669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.07140188664197922 
model_pd.l_d.mean(): -19.39385986328125 
model_pd.lagr.mean(): -19.322458267211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5874], device='cuda:0')), ('power', tensor([-20.2059], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.07140188664197922
epoch£º893	 i:1 	 global-step:17861	 l-p:0.16946662962436676
epoch£º893	 i:2 	 global-step:17862	 l-p:0.13635750114917755
epoch£º893	 i:3 	 global-step:17863	 l-p:0.124300017952919
epoch£º893	 i:4 	 global-step:17864	 l-p:0.20567357540130615
epoch£º893	 i:5 	 global-step:17865	 l-p:0.12006379663944244
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11660502851009369
epoch£º893	 i:7 	 global-step:17867	 l-p:0.1575261801481247
epoch£º893	 i:8 	 global-step:17868	 l-p:0.10976362228393555
epoch£º893	 i:9 	 global-step:17869	 l-p:0.13249775767326355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.1248, 5.1253],
        [5.1253, 5.1194, 5.1249],
        [5.1253, 4.9133, 4.6244],
        [5.1253, 4.8606, 4.7192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.125083327293396 
model_pd.l_d.mean(): -20.052427291870117 
model_pd.lagr.mean(): -19.927343368530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4813], device='cuda:0')), ('power', tensor([-20.7632], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.125083327293396
epoch£º894	 i:1 	 global-step:17881	 l-p:0.07441047579050064
epoch£º894	 i:2 	 global-step:17882	 l-p:0.13252361118793488
epoch£º894	 i:3 	 global-step:17883	 l-p:0.12429305911064148
epoch£º894	 i:4 	 global-step:17884	 l-p:0.09488348662853241
epoch£º894	 i:5 	 global-step:17885	 l-p:0.12337899953126907
epoch£º894	 i:6 	 global-step:17886	 l-p:0.11013375967741013
epoch£º894	 i:7 	 global-step:17887	 l-p:0.24207927286624908
epoch£º894	 i:8 	 global-step:17888	 l-p:0.35805341601371765
epoch£º894	 i:9 	 global-step:17889	 l-p:0.24079611897468567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0883, 5.0883, 5.0883],
        [5.0883, 5.0754, 5.0871],
        [5.0883, 5.3775, 5.2372],
        [5.0883, 4.8196, 4.6784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.1295793354511261 
model_pd.l_d.mean(): -18.937429428100586 
model_pd.lagr.mean(): -18.807849884033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6032], device='cuda:0')), ('power', tensor([-19.7606], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.1295793354511261
epoch£º895	 i:1 	 global-step:17901	 l-p:0.1809757500886917
epoch£º895	 i:2 	 global-step:17902	 l-p:0.16252070665359497
epoch£º895	 i:3 	 global-step:17903	 l-p:0.1646992415189743
epoch£º895	 i:4 	 global-step:17904	 l-p:0.14003823697566986
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11944524198770523
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13696478307247162
epoch£º895	 i:7 	 global-step:17907	 l-p:0.1874908059835434
epoch£º895	 i:8 	 global-step:17908	 l-p:0.25146475434303284
epoch£º895	 i:9 	 global-step:17909	 l-p:0.09434673935174942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1094, 5.4962, 5.4179],
        [5.1094, 5.0600, 5.0969],
        [5.1094, 5.1094, 5.1094],
        [5.1094, 4.8443, 4.7160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.13516154885292053 
model_pd.l_d.mean(): -20.268667221069336 
model_pd.lagr.mean(): -20.13350486755371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4892], device='cuda:0')), ('power', tensor([-20.9899], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.13516154885292053
epoch£º896	 i:1 	 global-step:17921	 l-p:0.19337725639343262
epoch£º896	 i:2 	 global-step:17922	 l-p:0.11672654747962952
epoch£º896	 i:3 	 global-step:17923	 l-p:0.18206346035003662
epoch£º896	 i:4 	 global-step:17924	 l-p:0.17440104484558105
epoch£º896	 i:5 	 global-step:17925	 l-p:0.1288169026374817
epoch£º896	 i:6 	 global-step:17926	 l-p:0.11958740651607513
epoch£º896	 i:7 	 global-step:17927	 l-p:0.15294595062732697
epoch£º896	 i:8 	 global-step:17928	 l-p:0.11358123272657394
epoch£º896	 i:9 	 global-step:17929	 l-p:0.13187824189662933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1400, 5.0960, 5.1297],
        [5.1400, 5.1356, 5.1398],
        [5.1400, 5.0945, 5.1291],
        [5.1400, 4.8967, 4.8424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.14091454446315765 
model_pd.l_d.mean(): -20.552927017211914 
model_pd.lagr.mean(): -20.412012100219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4274], device='cuda:0')), ('power', tensor([-21.2141], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.14091454446315765
epoch£º897	 i:1 	 global-step:17941	 l-p:0.07718544453382492
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11885303258895874
epoch£º897	 i:3 	 global-step:17943	 l-p:0.13813947141170502
epoch£º897	 i:4 	 global-step:17944	 l-p:0.1589745730161667
epoch£º897	 i:5 	 global-step:17945	 l-p:0.14049239456653595
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14840936660766602
epoch£º897	 i:7 	 global-step:17947	 l-p:0.07171351462602615
epoch£º897	 i:8 	 global-step:17948	 l-p:0.2178502082824707
epoch£º897	 i:9 	 global-step:17949	 l-p:0.14312338829040527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1452, 4.9043, 4.8550],
        [5.1452, 5.1446, 5.1452],
        [5.1452, 5.1012, 5.1350],
        [5.1452, 5.6104, 5.5844]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.15639930963516235 
model_pd.l_d.mean(): -19.659038543701172 
model_pd.lagr.mean(): -19.502639770507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.3996], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.15639930963516235
epoch£º898	 i:1 	 global-step:17961	 l-p:0.16142962872982025
epoch£º898	 i:2 	 global-step:17962	 l-p:0.1549081802368164
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14220473170280457
epoch£º898	 i:4 	 global-step:17964	 l-p:0.06388096511363983
epoch£º898	 i:5 	 global-step:17965	 l-p:0.08627671003341675
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10655972361564636
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14457784593105316
epoch£º898	 i:8 	 global-step:17968	 l-p:0.1444309651851654
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10837620496749878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1868, 5.0927, 4.7836],
        [5.1868, 4.9403, 4.7101],
        [5.1868, 4.9489, 4.8992],
        [5.1868, 5.1857, 5.1867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.15417897701263428 
model_pd.l_d.mean(): -20.10102081298828 
model_pd.lagr.mean(): -19.946842193603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-20.7782], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.15417897701263428
epoch£º899	 i:1 	 global-step:17981	 l-p:0.07350342720746994
epoch£º899	 i:2 	 global-step:17982	 l-p:0.09411747008562088
epoch£º899	 i:3 	 global-step:17983	 l-p:0.11984515935182571
epoch£º899	 i:4 	 global-step:17984	 l-p:0.13053058087825775
epoch£º899	 i:5 	 global-step:17985	 l-p:0.1844378113746643
epoch£º899	 i:6 	 global-step:17986	 l-p:0.149894118309021
epoch£º899	 i:7 	 global-step:17987	 l-p:0.027039773762226105
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12490354478359222
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12325947731733322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1856, 4.9273, 4.7965],
        [5.1856, 5.0786, 5.1331],
        [5.1856, 5.1856, 5.1856],
        [5.1856, 5.0854, 5.1393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.14657017588615417 
model_pd.l_d.mean(): -20.645252227783203 
model_pd.lagr.mean(): -20.498682022094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4185], device='cuda:0')), ('power', tensor([-21.2984], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.14657017588615417
epoch£º900	 i:1 	 global-step:18001	 l-p:0.05278458446264267
epoch£º900	 i:2 	 global-step:18002	 l-p:0.15163220465183258
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15679976344108582
epoch£º900	 i:4 	 global-step:18004	 l-p:0.12840960919857025
epoch£º900	 i:5 	 global-step:18005	 l-p:0.13303428888320923
epoch£º900	 i:6 	 global-step:18006	 l-p:0.10042678564786911
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12021884322166443
epoch£º900	 i:8 	 global-step:18008	 l-p:0.1287844032049179
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11694993823766708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1472, 5.1052, 5.1378],
        [5.1472, 5.1429, 5.1470],
        [5.1472, 5.2668, 5.0294],
        [5.1472, 4.9188, 4.8955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.18287688493728638 
model_pd.l_d.mean(): -20.459259033203125 
model_pd.lagr.mean(): -20.276382446289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-21.1691], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.18287688493728638
epoch£º901	 i:1 	 global-step:18021	 l-p:0.14242225885391235
epoch£º901	 i:2 	 global-step:18022	 l-p:0.123956598341465
epoch£º901	 i:3 	 global-step:18023	 l-p:0.09502998739480972
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12643125653266907
epoch£º901	 i:5 	 global-step:18025	 l-p:0.1506960242986679
epoch£º901	 i:6 	 global-step:18026	 l-p:0.18365246057510376
epoch£º901	 i:7 	 global-step:18027	 l-p:0.10225602984428406
epoch£º901	 i:8 	 global-step:18028	 l-p:0.21658627688884735
epoch£º901	 i:9 	 global-step:18029	 l-p:0.11877553910017014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1128, 5.1097, 5.1127],
        [5.1128, 4.8787, 4.8498],
        [5.1128, 4.9763, 4.6608],
        [5.1128, 5.0402, 5.0878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.13306991755962372 
model_pd.l_d.mean(): -19.580581665039062 
model_pd.lagr.mean(): -19.447511672973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.2963], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.13306991755962372
epoch£º902	 i:1 	 global-step:18041	 l-p:0.16334746778011322
epoch£º902	 i:2 	 global-step:18042	 l-p:0.1968119740486145
epoch£º902	 i:3 	 global-step:18043	 l-p:0.19367189705371857
epoch£º902	 i:4 	 global-step:18044	 l-p:0.11177154630422592
epoch£º902	 i:5 	 global-step:18045	 l-p:0.17463748157024384
epoch£º902	 i:6 	 global-step:18046	 l-p:0.11623850464820862
epoch£º902	 i:7 	 global-step:18047	 l-p:0.10832858830690384
epoch£º902	 i:8 	 global-step:18048	 l-p:0.1379508376121521
epoch£º902	 i:9 	 global-step:18049	 l-p:0.11053042858839035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1334, 5.0864, 5.1219],
        [5.1334, 5.4366, 5.3034],
        [5.1334, 4.9509, 4.9834],
        [5.1334, 4.9884, 5.0398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.14424459636211395 
model_pd.l_d.mean(): -20.270099639892578 
model_pd.lagr.mean(): -20.1258544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5132], device='cuda:0')), ('power', tensor([-21.0159], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.14424459636211395
epoch£º903	 i:1 	 global-step:18061	 l-p:0.1387472003698349
epoch£º903	 i:2 	 global-step:18062	 l-p:0.14974437654018402
epoch£º903	 i:3 	 global-step:18063	 l-p:0.13426809012889862
epoch£º903	 i:4 	 global-step:18064	 l-p:0.09962943196296692
epoch£º903	 i:5 	 global-step:18065	 l-p:0.155779629945755
epoch£º903	 i:6 	 global-step:18066	 l-p:0.1326388567686081
epoch£º903	 i:7 	 global-step:18067	 l-p:0.1274837851524353
epoch£º903	 i:8 	 global-step:18068	 l-p:0.1266913115978241
epoch£º903	 i:9 	 global-step:18069	 l-p:0.14389412105083466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1436, 5.1185, 5.1397],
        [5.1436, 4.9645, 4.9989],
        [5.1436, 5.1436, 5.1436],
        [5.1436, 5.1394, 5.1434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11699848622083664 
model_pd.l_d.mean(): -19.359338760375977 
model_pd.lagr.mean(): -19.242340087890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5364], device='cuda:0')), ('power', tensor([-20.1189], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11699848622083664
epoch£º904	 i:1 	 global-step:18081	 l-p:0.15273341536521912
epoch£º904	 i:2 	 global-step:18082	 l-p:0.15226541459560394
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14653192460536957
epoch£º904	 i:4 	 global-step:18084	 l-p:0.1271655559539795
epoch£º904	 i:5 	 global-step:18085	 l-p:0.1914215087890625
epoch£º904	 i:6 	 global-step:18086	 l-p:0.12272237241268158
epoch£º904	 i:7 	 global-step:18087	 l-p:0.1926555186510086
epoch£º904	 i:8 	 global-step:18088	 l-p:0.08018440008163452
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12821970880031586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 5.1142, 5.1150],
        [5.1151, 5.0063, 5.0620],
        [5.1151, 4.8505, 4.6473],
        [5.1151, 4.8651, 4.8011]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.1386032998561859 
model_pd.l_d.mean(): -20.57806968688965 
model_pd.lagr.mean(): -20.43946647644043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4224], device='cuda:0')), ('power', tensor([-21.2345], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.1386032998561859
epoch£º905	 i:1 	 global-step:18101	 l-p:0.12456604838371277
epoch£º905	 i:2 	 global-step:18102	 l-p:0.13224412500858307
epoch£º905	 i:3 	 global-step:18103	 l-p:0.12364588677883148
epoch£º905	 i:4 	 global-step:18104	 l-p:0.09174684435129166
epoch£º905	 i:5 	 global-step:18105	 l-p:0.20034009218215942
epoch£º905	 i:6 	 global-step:18106	 l-p:0.19946883618831635
epoch£º905	 i:7 	 global-step:18107	 l-p:0.2445392608642578
epoch£º905	 i:8 	 global-step:18108	 l-p:0.13685379922389984
epoch£º905	 i:9 	 global-step:18109	 l-p:0.17789340019226074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1077, 4.8775, 4.5988],
        [5.1077, 4.8889, 4.5998],
        [5.1077, 5.1962, 4.9436],
        [5.1077, 5.1073, 5.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.1263420283794403 
model_pd.l_d.mean(): -20.23847770690918 
model_pd.lagr.mean(): -20.11213493347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.1263420283794403
epoch£º906	 i:1 	 global-step:18121	 l-p:0.0995769128203392
epoch£º906	 i:2 	 global-step:18122	 l-p:0.1327180415391922
epoch£º906	 i:3 	 global-step:18123	 l-p:0.2377563863992691
epoch£º906	 i:4 	 global-step:18124	 l-p:0.14775224030017853
epoch£º906	 i:5 	 global-step:18125	 l-p:0.18858428299427032
epoch£º906	 i:6 	 global-step:18126	 l-p:0.15059971809387207
epoch£º906	 i:7 	 global-step:18127	 l-p:0.12176023423671722
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11685744673013687
epoch£º906	 i:9 	 global-step:18129	 l-p:0.15316662192344666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 5.1061, 5.1289],
        [5.1333, 5.0090, 5.0644],
        [5.1333, 5.0553, 5.1047],
        [5.1333, 4.9016, 4.6296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.21962092816829681 
model_pd.l_d.mean(): -19.623605728149414 
model_pd.lagr.mean(): -19.40398406982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4792], device='cuda:0')), ('power', tensor([-20.3276], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.21962092816829681
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12865737080574036
epoch£º907	 i:2 	 global-step:18142	 l-p:0.058843936771154404
epoch£º907	 i:3 	 global-step:18143	 l-p:0.1592089831829071
epoch£º907	 i:4 	 global-step:18144	 l-p:0.13410550355911255
epoch£º907	 i:5 	 global-step:18145	 l-p:0.11322849988937378
epoch£º907	 i:6 	 global-step:18146	 l-p:0.13412907719612122
epoch£º907	 i:7 	 global-step:18147	 l-p:0.11824405193328857
epoch£º907	 i:8 	 global-step:18148	 l-p:0.1782337725162506
epoch£º907	 i:9 	 global-step:18149	 l-p:0.12531204521656036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[5.1440, 5.4269, 5.2807],
        [5.1440, 4.9576, 4.9867],
        [5.1440, 4.9477, 4.6494],
        [5.1440, 4.9386, 4.6456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.11703904718160629 
model_pd.l_d.mean(): -19.340829849243164 
model_pd.lagr.mean(): -19.223791122436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5880], device='cuda:0')), ('power', tensor([-20.1529], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.11703904718160629
epoch£º908	 i:1 	 global-step:18161	 l-p:0.1355094164609909
epoch£º908	 i:2 	 global-step:18162	 l-p:0.11833716928958893
epoch£º908	 i:3 	 global-step:18163	 l-p:0.12901373207569122
epoch£º908	 i:4 	 global-step:18164	 l-p:0.09383104741573334
epoch£º908	 i:5 	 global-step:18165	 l-p:0.13509954512119293
epoch£º908	 i:6 	 global-step:18166	 l-p:0.184626504778862
epoch£º908	 i:7 	 global-step:18167	 l-p:0.13012221455574036
epoch£º908	 i:8 	 global-step:18168	 l-p:0.13576802611351013
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15082059800624847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1540, 5.1401, 5.1526],
        [5.1540, 5.1540, 5.1540],
        [5.1540, 5.0494, 4.7364],
        [5.1540, 5.1540, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.13766761124134064 
model_pd.l_d.mean(): -20.40734100341797 
model_pd.lagr.mean(): -20.26967430114746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4472], device='cuda:0')), ('power', tensor([-21.0872], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.13766761124134064
epoch£º909	 i:1 	 global-step:18181	 l-p:0.13298973441123962
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11825423687696457
epoch£º909	 i:3 	 global-step:18183	 l-p:0.10740770399570465
epoch£º909	 i:4 	 global-step:18184	 l-p:0.13199956715106964
epoch£º909	 i:5 	 global-step:18185	 l-p:0.12263939529657364
epoch£º909	 i:6 	 global-step:18186	 l-p:0.21708278357982635
epoch£º909	 i:7 	 global-step:18187	 l-p:0.09389030188322067
epoch£º909	 i:8 	 global-step:18188	 l-p:0.10074466466903687
epoch£º909	 i:9 	 global-step:18189	 l-p:0.15014305710792542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1438, 5.1438, 5.1438],
        [5.1438, 5.1435, 5.1438],
        [5.1438, 5.1438, 5.1438],
        [5.1438, 4.9205, 4.6412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.179280623793602 
model_pd.l_d.mean(): -19.31942367553711 
model_pd.lagr.mean(): -19.1401424407959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5377], device='cuda:0')), ('power', tensor([-20.0798], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.179280623793602
epoch£º910	 i:1 	 global-step:18201	 l-p:0.15349087119102478
epoch£º910	 i:2 	 global-step:18202	 l-p:0.060792405158281326
epoch£º910	 i:3 	 global-step:18203	 l-p:0.15046118199825287
epoch£º910	 i:4 	 global-step:18204	 l-p:0.16843131184577942
epoch£º910	 i:5 	 global-step:18205	 l-p:0.12479257583618164
epoch£º910	 i:6 	 global-step:18206	 l-p:0.1139373779296875
epoch£º910	 i:7 	 global-step:18207	 l-p:0.1885143220424652
epoch£º910	 i:8 	 global-step:18208	 l-p:0.10514255613088608
epoch£º910	 i:9 	 global-step:18209	 l-p:0.14367827773094177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1179, 4.9191, 4.9389],
        [5.1179, 5.1179, 5.1179],
        [5.1179, 4.9795, 4.6634],
        [5.1179, 4.8924, 4.8791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.15958647429943085 
model_pd.l_d.mean(): -20.740873336791992 
model_pd.lagr.mean(): -20.581287384033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-21.4141], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.15958647429943085
epoch£º911	 i:1 	 global-step:18221	 l-p:0.26900210976600647
epoch£º911	 i:2 	 global-step:18222	 l-p:0.133900448679924
epoch£º911	 i:3 	 global-step:18223	 l-p:0.18454596400260925
epoch£º911	 i:4 	 global-step:18224	 l-p:0.1332288682460785
epoch£º911	 i:5 	 global-step:18225	 l-p:0.15183080732822418
epoch£º911	 i:6 	 global-step:18226	 l-p:0.09198568761348724
epoch£º911	 i:7 	 global-step:18227	 l-p:0.1315024048089981
epoch£º911	 i:8 	 global-step:18228	 l-p:0.11365178972482681
epoch£º911	 i:9 	 global-step:18229	 l-p:0.09507318586111069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.2244, 4.9820],
        [5.1141, 5.0393, 5.0878],
        [5.1141, 4.8575, 4.6207],
        [5.1141, 5.1089, 5.1138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.15697121620178223 
model_pd.l_d.mean(): -19.41851043701172 
model_pd.lagr.mean(): -19.261539459228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5875], device='cuda:0')), ('power', tensor([-20.2309], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.15697121620178223
epoch£º912	 i:1 	 global-step:18241	 l-p:0.1134149506688118
epoch£º912	 i:2 	 global-step:18242	 l-p:0.17274516820907593
epoch£º912	 i:3 	 global-step:18243	 l-p:0.17398549616336823
epoch£º912	 i:4 	 global-step:18244	 l-p:0.10851336270570755
epoch£º912	 i:5 	 global-step:18245	 l-p:0.16940365731716156
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13981957733631134
epoch£º912	 i:7 	 global-step:18247	 l-p:0.13918143510818481
epoch£º912	 i:8 	 global-step:18248	 l-p:0.16331012547016144
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12793460488319397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1185, 5.1185, 5.1185],
        [5.1185, 4.8624, 4.6256],
        [5.1185, 5.1181, 5.1185],
        [5.1185, 4.9078, 4.6132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.11029721796512604 
model_pd.l_d.mean(): -20.14947509765625 
model_pd.lagr.mean(): -20.03917694091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5139], device='cuda:0')), ('power', tensor([-20.8946], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.11029721796512604
epoch£º913	 i:1 	 global-step:18261	 l-p:0.1157231479883194
epoch£º913	 i:2 	 global-step:18262	 l-p:0.12696413695812225
epoch£º913	 i:3 	 global-step:18263	 l-p:0.14153985679149628
epoch£º913	 i:4 	 global-step:18264	 l-p:0.1411585658788681
epoch£º913	 i:5 	 global-step:18265	 l-p:0.1816861629486084
epoch£º913	 i:6 	 global-step:18266	 l-p:0.17741036415100098
epoch£º913	 i:7 	 global-step:18267	 l-p:0.1844940185546875
epoch£º913	 i:8 	 global-step:18268	 l-p:0.2303524911403656
epoch£º913	 i:9 	 global-step:18269	 l-p:0.12267841398715973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1150, 4.8549, 4.7613],
        [5.1150, 5.1149, 5.1150],
        [5.1150, 5.0653, 5.1024],
        [5.1150, 4.8459, 4.6875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.1283811330795288 
model_pd.l_d.mean(): -19.95448875427246 
model_pd.lagr.mean(): -19.826107025146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.6826], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.1283811330795288
epoch£º914	 i:1 	 global-step:18281	 l-p:0.10410255938768387
epoch£º914	 i:2 	 global-step:18282	 l-p:0.09282051026821136
epoch£º914	 i:3 	 global-step:18283	 l-p:0.1627701222896576
epoch£º914	 i:4 	 global-step:18284	 l-p:0.14032435417175293
epoch£º914	 i:5 	 global-step:18285	 l-p:0.16273543238639832
epoch£º914	 i:6 	 global-step:18286	 l-p:0.1910654753446579
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12420269101858139
epoch£º914	 i:8 	 global-step:18288	 l-p:0.10416713356971741
epoch£º914	 i:9 	 global-step:18289	 l-p:0.16411662101745605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 5.0868, 5.1347],
        [5.1605, 5.1583, 5.1604],
        [5.1605, 5.1605, 5.1605],
        [5.1605, 5.1560, 5.1603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.11179664731025696 
model_pd.l_d.mean(): -20.65573501586914 
model_pd.lagr.mean(): -20.54393768310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-21.3373], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.11179664731025696
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12358323484659195
epoch£º915	 i:2 	 global-step:18302	 l-p:0.09129089117050171
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13429003953933716
epoch£º915	 i:4 	 global-step:18304	 l-p:0.18861068785190582
epoch£º915	 i:5 	 global-step:18305	 l-p:0.11109434813261032
epoch£º915	 i:6 	 global-step:18306	 l-p:0.1868164837360382
epoch£º915	 i:7 	 global-step:18307	 l-p:0.10578983277082443
epoch£º915	 i:8 	 global-step:18308	 l-p:0.11618546396493912
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12274415791034698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1689, 5.6464, 5.6272],
        [5.1689, 5.1689, 5.1689],
        [5.1689, 5.1437, 5.1650],
        [5.1689, 4.9439, 4.6692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13471609354019165 
model_pd.l_d.mean(): -19.935728073120117 
model_pd.lagr.mean(): -19.80101203918457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-20.6092], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13471609354019165
epoch£º916	 i:1 	 global-step:18321	 l-p:0.10078391432762146
epoch£º916	 i:2 	 global-step:18322	 l-p:0.19536754488945007
epoch£º916	 i:3 	 global-step:18323	 l-p:0.10316848754882812
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12304972112178802
epoch£º916	 i:5 	 global-step:18325	 l-p:0.10409858822822571
epoch£º916	 i:6 	 global-step:18326	 l-p:0.12872940301895142
epoch£º916	 i:7 	 global-step:18327	 l-p:0.15287013351917267
epoch£º916	 i:8 	 global-step:18328	 l-p:0.128822460770607
epoch£º916	 i:9 	 global-step:18329	 l-p:0.12582111358642578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1459, 5.1037, 5.1364],
        [5.1459, 5.1459, 5.1459],
        [5.1459, 5.4236, 5.2734],
        [5.1459, 5.0774, 5.1234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12889525294303894 
model_pd.l_d.mean(): -20.91860580444336 
model_pd.lagr.mean(): -20.789710998535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3878], device='cuda:0')), ('power', tensor([-21.5433], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12889525294303894
epoch£º917	 i:1 	 global-step:18341	 l-p:0.13061386346817017
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12369120121002197
epoch£º917	 i:3 	 global-step:18343	 l-p:0.15953674912452698
epoch£º917	 i:4 	 global-step:18344	 l-p:0.1601446270942688
epoch£º917	 i:5 	 global-step:18345	 l-p:0.12228237837553024
epoch£º917	 i:6 	 global-step:18346	 l-p:0.16991187632083893
epoch£º917	 i:7 	 global-step:18347	 l-p:0.19931752979755402
epoch£º917	 i:8 	 global-step:18348	 l-p:0.1476014256477356
epoch£º917	 i:9 	 global-step:18349	 l-p:0.09516707062721252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1149, 5.1149, 5.1149],
        [5.1149, 5.1012, 5.1135],
        [5.1149, 5.1149, 5.1149],
        [5.1149, 4.9684, 4.6514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.16873881220817566 
model_pd.l_d.mean(): -18.372100830078125 
model_pd.lagr.mean(): -18.20336151123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6174], device='cuda:0')), ('power', tensor([-19.2036], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.16873881220817566
epoch£º918	 i:1 	 global-step:18361	 l-p:0.1630256623029709
epoch£º918	 i:2 	 global-step:18362	 l-p:0.07828600704669952
epoch£º918	 i:3 	 global-step:18363	 l-p:0.1641211211681366
epoch£º918	 i:4 	 global-step:18364	 l-p:0.16718639433383942
epoch£º918	 i:5 	 global-step:18365	 l-p:0.13328731060028076
epoch£º918	 i:6 	 global-step:18366	 l-p:0.132882758975029
epoch£º918	 i:7 	 global-step:18367	 l-p:0.2119983434677124
epoch£º918	 i:8 	 global-step:18368	 l-p:0.11472680419683456
epoch£º918	 i:9 	 global-step:18369	 l-p:0.12200405448675156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[5.1212, 5.5022, 5.4182],
        [5.1212, 5.0688, 4.7617],
        [5.1212, 5.1785, 4.9105],
        [5.1212, 4.9192, 4.9366]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.14196839928627014 
model_pd.l_d.mean(): -19.464094161987305 
model_pd.lagr.mean(): -19.322126388549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.2061], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.14196839928627014
epoch£º919	 i:1 	 global-step:18381	 l-p:0.16337557137012482
epoch£º919	 i:2 	 global-step:18382	 l-p:0.16825778782367706
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11412189155817032
epoch£º919	 i:4 	 global-step:18384	 l-p:0.18277215957641602
epoch£º919	 i:5 	 global-step:18385	 l-p:0.151377335190773
epoch£º919	 i:6 	 global-step:18386	 l-p:0.1309933066368103
epoch£º919	 i:7 	 global-step:18387	 l-p:0.13874180614948273
epoch£º919	 i:8 	 global-step:18388	 l-p:0.11964420229196548
epoch£º919	 i:9 	 global-step:18389	 l-p:0.13183513283729553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1322, 4.9222, 4.6273],
        [5.1322, 5.1321, 5.1322],
        [5.1322, 4.9260, 4.9383],
        [5.1322, 4.8878, 4.8387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.15034861862659454 
model_pd.l_d.mean(): -19.876888275146484 
model_pd.lagr.mean(): -19.726539611816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5103], device='cuda:0')), ('power', tensor([-20.6154], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.15034861862659454
epoch£º920	 i:1 	 global-step:18401	 l-p:0.11207769066095352
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14570234715938568
epoch£º920	 i:3 	 global-step:18403	 l-p:0.11446493864059448
epoch£º920	 i:4 	 global-step:18404	 l-p:0.11824016273021698
epoch£º920	 i:5 	 global-step:18405	 l-p:0.16116805374622345
epoch£º920	 i:6 	 global-step:18406	 l-p:0.14428700506687164
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10903908312320709
epoch£º920	 i:8 	 global-step:18408	 l-p:0.20204415917396545
epoch£º920	 i:9 	 global-step:18409	 l-p:0.15117040276527405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1222, 5.1222, 5.1222],
        [5.1222, 5.0045, 4.6870],
        [5.1222, 5.0470, 5.0957],
        [5.1222, 5.1222, 5.1222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.15877628326416016 
model_pd.l_d.mean(): -19.29349136352539 
model_pd.lagr.mean(): -19.134716033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5742], device='cuda:0')), ('power', tensor([-20.0909], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.15877628326416016
epoch£º921	 i:1 	 global-step:18421	 l-p:0.13452069461345673
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15337446331977844
epoch£º921	 i:3 	 global-step:18423	 l-p:0.1558508574962616
epoch£º921	 i:4 	 global-step:18424	 l-p:0.10321932286024094
epoch£º921	 i:5 	 global-step:18425	 l-p:0.10722054541110992
epoch£º921	 i:6 	 global-step:18426	 l-p:0.1269139051437378
epoch£º921	 i:7 	 global-step:18427	 l-p:0.14141933619976044
epoch£º921	 i:8 	 global-step:18428	 l-p:0.15523208677768707
epoch£º921	 i:9 	 global-step:18429	 l-p:0.15145324170589447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 5.1547, 5.1547],
        [5.1547, 4.9675, 4.9967],
        [5.1547, 5.4609, 5.3278],
        [5.1547, 5.2676, 5.0252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.16941528022289276 
model_pd.l_d.mean(): -20.525094985961914 
model_pd.lagr.mean(): -20.355680465698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.2170], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.16941528022289276
epoch£º922	 i:1 	 global-step:18441	 l-p:0.11423257738351822
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13493496179580688
epoch£º922	 i:3 	 global-step:18443	 l-p:0.12024686485528946
epoch£º922	 i:4 	 global-step:18444	 l-p:0.11499691754579544
epoch£º922	 i:5 	 global-step:18445	 l-p:0.09741061180830002
epoch£º922	 i:6 	 global-step:18446	 l-p:0.08879886567592621
epoch£º922	 i:7 	 global-step:18447	 l-p:0.16775982081890106
epoch£º922	 i:8 	 global-step:18448	 l-p:0.19030456244945526
epoch£º922	 i:9 	 global-step:18449	 l-p:0.098616823554039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1683, 4.9503, 4.6667],
        [5.1683, 4.9388, 4.9151],
        [5.1683, 5.0209, 5.0717],
        [5.1683, 5.1659, 5.1682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.1201462596654892 
model_pd.l_d.mean(): -20.479705810546875 
model_pd.lagr.mean(): -20.359560012817383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4399], device='cuda:0')), ('power', tensor([-21.1529], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.1201462596654892
epoch£º923	 i:1 	 global-step:18461	 l-p:0.17768484354019165
epoch£º923	 i:2 	 global-step:18462	 l-p:0.11412737518548965
epoch£º923	 i:3 	 global-step:18463	 l-p:0.11008583754301071
epoch£º923	 i:4 	 global-step:18464	 l-p:0.12577730417251587
epoch£º923	 i:5 	 global-step:18465	 l-p:0.12577566504478455
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10658542066812515
epoch£º923	 i:7 	 global-step:18467	 l-p:0.1563054770231247
epoch£º923	 i:8 	 global-step:18468	 l-p:0.13458776473999023
epoch£º923	 i:9 	 global-step:18469	 l-p:0.0960930809378624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1604, 4.9591, 4.9751],
        [5.1604, 4.9488, 4.6585],
        [5.1604, 5.1014, 5.1432],
        [5.1604, 5.0118, 5.0625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.08379165828227997 
model_pd.l_d.mean(): -20.248371124267578 
model_pd.lagr.mean(): -20.164579391479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4909], device='cuda:0')), ('power', tensor([-20.9711], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.08379165828227997
epoch£º924	 i:1 	 global-step:18481	 l-p:0.18103520572185516
epoch£º924	 i:2 	 global-step:18482	 l-p:0.14235927164554596
epoch£º924	 i:3 	 global-step:18483	 l-p:0.13335631787776947
epoch£º924	 i:4 	 global-step:18484	 l-p:0.1246429905295372
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12482984364032745
epoch£º924	 i:6 	 global-step:18486	 l-p:0.20527172088623047
epoch£º924	 i:7 	 global-step:18487	 l-p:0.10747577250003815
epoch£º924	 i:8 	 global-step:18488	 l-p:0.09005717188119888
epoch£º924	 i:9 	 global-step:18489	 l-p:0.11327098309993744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.1411, 5.1411],
        [5.1411, 5.1163, 4.8168],
        [5.1411, 4.9834, 4.6690],
        [5.1411, 4.9182, 4.6334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.15736055374145508 
model_pd.l_d.mean(): -20.450912475585938 
model_pd.lagr.mean(): -20.29355239868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4532], device='cuda:0')), ('power', tensor([-21.1374], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.15736055374145508
epoch£º925	 i:1 	 global-step:18501	 l-p:0.08538410812616348
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12508870661258698
epoch£º925	 i:3 	 global-step:18503	 l-p:0.18127663433551788
epoch£º925	 i:4 	 global-step:18504	 l-p:0.11453349888324738
epoch£º925	 i:5 	 global-step:18505	 l-p:0.13515494763851166
epoch£º925	 i:6 	 global-step:18506	 l-p:0.1539536714553833
epoch£º925	 i:7 	 global-step:18507	 l-p:0.16254447400569916
epoch£º925	 i:8 	 global-step:18508	 l-p:0.1301153600215912
epoch£º925	 i:9 	 global-step:18509	 l-p:0.20665374398231506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 5.1184, 5.1184],
        [5.1184, 4.8500, 4.7210],
        [5.1184, 4.8484, 4.7052],
        [5.1184, 5.0262, 4.7102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.13168129324913025 
model_pd.l_d.mean(): -19.9381160736084 
model_pd.lagr.mean(): -19.806434631347656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4585], device='cuda:0')), ('power', tensor([-20.6244], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.13168129324913025
epoch£º926	 i:1 	 global-step:18521	 l-p:0.20754031836986542
epoch£º926	 i:2 	 global-step:18522	 l-p:0.11523634940385818
epoch£º926	 i:3 	 global-step:18523	 l-p:0.1239902451634407
epoch£º926	 i:4 	 global-step:18524	 l-p:0.10118687152862549
epoch£º926	 i:5 	 global-step:18525	 l-p:0.1625290960073471
epoch£º926	 i:6 	 global-step:18526	 l-p:0.17723003029823303
epoch£º926	 i:7 	 global-step:18527	 l-p:0.11267117410898209
epoch£º926	 i:8 	 global-step:18528	 l-p:0.23170329630374908
epoch£º926	 i:9 	 global-step:18529	 l-p:0.1266879439353943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1097, 5.1090, 5.1097],
        [5.1097, 5.0349, 5.0836],
        [5.1097, 5.1056, 5.1095],
        [5.1097, 5.0228, 5.0752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.13516657054424286 
model_pd.l_d.mean(): -18.62023162841797 
model_pd.lagr.mean(): -18.485065460205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5514], device='cuda:0')), ('power', tensor([-19.3870], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.13516657054424286
epoch£º927	 i:1 	 global-step:18541	 l-p:0.16111604869365692
epoch£º927	 i:2 	 global-step:18542	 l-p:0.12945228815078735
epoch£º927	 i:3 	 global-step:18543	 l-p:0.1349330097436905
epoch£º927	 i:4 	 global-step:18544	 l-p:0.16979390382766724
epoch£º927	 i:5 	 global-step:18545	 l-p:0.11615969985723495
epoch£º927	 i:6 	 global-step:18546	 l-p:0.11892657727003098
epoch£º927	 i:7 	 global-step:18547	 l-p:0.27468961477279663
epoch£º927	 i:8 	 global-step:18548	 l-p:0.12212113291025162
epoch£º927	 i:9 	 global-step:18549	 l-p:0.16087020933628082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 5.1089, 5.1089],
        [5.1089, 5.0006, 5.0568],
        [5.1089, 5.1089, 5.1089],
        [5.1089, 5.1071, 5.1088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.11098743975162506 
model_pd.l_d.mean(): -20.421701431274414 
model_pd.lagr.mean(): -20.310714721679688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-21.1042], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.11098743975162506
epoch£º928	 i:1 	 global-step:18561	 l-p:0.1179562658071518
epoch£º928	 i:2 	 global-step:18562	 l-p:0.15754428505897522
epoch£º928	 i:3 	 global-step:18563	 l-p:0.1383877694606781
epoch£º928	 i:4 	 global-step:18564	 l-p:0.17853859066963196
epoch£º928	 i:5 	 global-step:18565	 l-p:0.13174229860305786
epoch£º928	 i:6 	 global-step:18566	 l-p:0.17856347560882568
epoch£º928	 i:7 	 global-step:18567	 l-p:0.12059438228607178
epoch£º928	 i:8 	 global-step:18568	 l-p:0.17427077889442444
epoch£º928	 i:9 	 global-step:18569	 l-p:0.5103906989097595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0890, 5.0890, 5.0890],
        [5.0890, 5.0402, 5.0769],
        [5.0890, 4.8142, 4.6430],
        [5.0890, 5.0890, 5.0890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.0969935804605484 
model_pd.l_d.mean(): -20.64544105529785 
model_pd.lagr.mean(): -20.548446655273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4481], device='cuda:0')), ('power', tensor([-21.3288], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.0969935804605484
epoch£º929	 i:1 	 global-step:18581	 l-p:0.5466869473457336
epoch£º929	 i:2 	 global-step:18582	 l-p:0.21876579523086548
epoch£º929	 i:3 	 global-step:18583	 l-p:0.13378649950027466
epoch£º929	 i:4 	 global-step:18584	 l-p:0.0982842966914177
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13898669183254242
epoch£º929	 i:6 	 global-step:18586	 l-p:0.15888066589832306
epoch£º929	 i:7 	 global-step:18587	 l-p:0.1345827430486679
epoch£º929	 i:8 	 global-step:18588	 l-p:0.19250427186489105
epoch£º929	 i:9 	 global-step:18589	 l-p:0.13637031614780426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0902, 5.0820, 5.0896],
        [5.0902, 4.9415, 4.6205],
        [5.0902, 5.0702, 5.0876],
        [5.0902, 5.0890, 5.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.10742061585187912 
model_pd.l_d.mean(): -20.563982009887695 
model_pd.lagr.mean(): -20.456562042236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.10742061585187912
epoch£º930	 i:1 	 global-step:18601	 l-p:0.14014410972595215
epoch£º930	 i:2 	 global-step:18602	 l-p:0.1803310364484787
epoch£º930	 i:3 	 global-step:18603	 l-p:0.7221728563308716
epoch£º930	 i:4 	 global-step:18604	 l-p:0.21622711420059204
epoch£º930	 i:5 	 global-step:18605	 l-p:0.1377943903207779
epoch£º930	 i:6 	 global-step:18606	 l-p:0.18487697839736938
epoch£º930	 i:7 	 global-step:18607	 l-p:0.2524968683719635
epoch£º930	 i:8 	 global-step:18608	 l-p:0.0969778448343277
epoch£º930	 i:9 	 global-step:18609	 l-p:0.08603454381227493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0981, 5.0959, 5.0980],
        [5.0981, 4.8255, 4.6869],
        [5.0981, 5.0981, 5.0981],
        [5.0981, 4.9878, 5.0443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.16603606939315796 
model_pd.l_d.mean(): -20.295249938964844 
model_pd.lagr.mean(): -20.129213333129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-21.0048], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.16603606939315796
epoch£º931	 i:1 	 global-step:18621	 l-p:0.14378200471401215
epoch£º931	 i:2 	 global-step:18622	 l-p:0.16295140981674194
epoch£º931	 i:3 	 global-step:18623	 l-p:0.20320330560207367
epoch£º931	 i:4 	 global-step:18624	 l-p:0.1209276095032692
epoch£º931	 i:5 	 global-step:18625	 l-p:0.13266737759113312
epoch£º931	 i:6 	 global-step:18626	 l-p:0.10926744341850281
epoch£º931	 i:7 	 global-step:18627	 l-p:0.14482416212558746
epoch£º931	 i:8 	 global-step:18628	 l-p:0.21774466335773468
epoch£º931	 i:9 	 global-step:18629	 l-p:0.118599534034729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1216, 4.9667, 4.6486],
        [5.1216, 5.0374, 5.0891],
        [5.1216, 4.8983, 4.8919],
        [5.1216, 5.1079, 5.1203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.20236413180828094 
model_pd.l_d.mean(): -19.041839599609375 
model_pd.lagr.mean(): -18.839475631713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5720], device='cuda:0')), ('power', tensor([-19.8342], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.20236413180828094
epoch£º932	 i:1 	 global-step:18641	 l-p:0.16558393836021423
epoch£º932	 i:2 	 global-step:18642	 l-p:0.12276643514633179
epoch£º932	 i:3 	 global-step:18643	 l-p:0.20247061550617218
epoch£º932	 i:4 	 global-step:18644	 l-p:0.09627120941877365
epoch£º932	 i:5 	 global-step:18645	 l-p:0.10571835935115814
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12362754344940186
epoch£º932	 i:7 	 global-step:18647	 l-p:0.08286046981811523
epoch£º932	 i:8 	 global-step:18648	 l-p:0.13189131021499634
epoch£º932	 i:9 	 global-step:18649	 l-p:0.2029518485069275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1290, 4.9882, 4.6693],
        [5.1290, 4.8822, 4.8325],
        [5.1290, 4.8585, 4.6740],
        [5.1290, 5.1290, 5.1290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.13015630841255188 
model_pd.l_d.mean(): -20.613004684448242 
model_pd.lagr.mean(): -20.48284912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.3050], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.13015630841255188
epoch£º933	 i:1 	 global-step:18661	 l-p:0.09837951511144638
epoch£º933	 i:2 	 global-step:18662	 l-p:0.15751539170742035
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12589693069458008
epoch£º933	 i:4 	 global-step:18664	 l-p:0.15665118396282196
epoch£º933	 i:5 	 global-step:18665	 l-p:0.12242432683706284
epoch£º933	 i:6 	 global-step:18666	 l-p:0.20527485013008118
epoch£º933	 i:7 	 global-step:18667	 l-p:0.1763720065355301
epoch£º933	 i:8 	 global-step:18668	 l-p:0.10415329039096832
epoch£º933	 i:9 	 global-step:18669	 l-p:0.1546887904405594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1361, 5.1361],
        [5.1361, 5.1361, 5.1361],
        [5.1361, 4.9822, 5.0323],
        [5.1361, 5.0206, 5.0770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.18865074217319489 
model_pd.l_d.mean(): -20.052658081054688 
model_pd.lagr.mean(): -19.8640079498291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.7744], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.18865074217319489
epoch£º934	 i:1 	 global-step:18681	 l-p:0.13122281432151794
epoch£º934	 i:2 	 global-step:18682	 l-p:0.09447072446346283
epoch£º934	 i:3 	 global-step:18683	 l-p:0.1202547699213028
epoch£º934	 i:4 	 global-step:18684	 l-p:0.1533556431531906
epoch£º934	 i:5 	 global-step:18685	 l-p:0.1578337401151657
epoch£º934	 i:6 	 global-step:18686	 l-p:0.14370492100715637
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11768044531345367
epoch£º934	 i:8 	 global-step:18688	 l-p:0.12817256152629852
epoch£º934	 i:9 	 global-step:18689	 l-p:0.1327846497297287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 4.9245, 4.6321],
        [5.1419, 5.1275, 5.1404],
        [5.1419, 5.1341, 5.1414],
        [5.1419, 4.9919, 5.0432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.15943510830402374 
model_pd.l_d.mean(): -20.479969024658203 
model_pd.lagr.mean(): -20.320533752441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4710], device='cuda:0')), ('power', tensor([-21.1849], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.15943510830402374
epoch£º935	 i:1 	 global-step:18701	 l-p:0.12584824860095978
epoch£º935	 i:2 	 global-step:18702	 l-p:0.14193624258041382
epoch£º935	 i:3 	 global-step:18703	 l-p:0.15070614218711853
epoch£º935	 i:4 	 global-step:18704	 l-p:0.11080504208803177
epoch£º935	 i:5 	 global-step:18705	 l-p:0.13004694879055023
epoch£º935	 i:6 	 global-step:18706	 l-p:0.1556650996208191
epoch£º935	 i:7 	 global-step:18707	 l-p:0.13603267073631287
epoch£º935	 i:8 	 global-step:18708	 l-p:0.17339083552360535
epoch£º935	 i:9 	 global-step:18709	 l-p:0.13106301426887512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1266, 5.0165, 5.0729],
        [5.1266, 5.0936, 5.1205],
        [5.1266, 4.8617, 4.7589],
        [5.1266, 5.2669, 5.0375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16553935408592224 
model_pd.l_d.mean(): -19.759685516357422 
model_pd.lagr.mean(): -19.594146728515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4885], device='cuda:0')), ('power', tensor([-20.4746], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16553935408592224
epoch£º936	 i:1 	 global-step:18721	 l-p:0.13500958681106567
epoch£º936	 i:2 	 global-step:18722	 l-p:0.1969749480485916
epoch£º936	 i:3 	 global-step:18723	 l-p:0.1373671293258667
epoch£º936	 i:4 	 global-step:18724	 l-p:0.13557937741279602
epoch£º936	 i:5 	 global-step:18725	 l-p:0.13839919865131378
epoch£º936	 i:6 	 global-step:18726	 l-p:0.11845541000366211
epoch£º936	 i:7 	 global-step:18727	 l-p:0.12251441925764084
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12177196890115738
epoch£º936	 i:9 	 global-step:18729	 l-p:0.11485756933689117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1476, 4.8973, 4.8372],
        [5.1476, 5.1696, 4.8853],
        [5.1476, 4.9113, 4.8819],
        [5.1476, 5.1472, 5.1476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.0865422785282135 
model_pd.l_d.mean(): -20.73978042602539 
model_pd.lagr.mean(): -20.65323829650879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4271], device='cuda:0')), ('power', tensor([-21.4028], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.0865422785282135
epoch£º937	 i:1 	 global-step:18741	 l-p:0.12109363079071045
epoch£º937	 i:2 	 global-step:18742	 l-p:0.13594010472297668
epoch£º937	 i:3 	 global-step:18743	 l-p:0.13438723981380463
epoch£º937	 i:4 	 global-step:18744	 l-p:0.12749306857585907
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12519963085651398
epoch£º937	 i:6 	 global-step:18746	 l-p:0.15712399780750275
epoch£º937	 i:7 	 global-step:18747	 l-p:0.21979744732379913
epoch£º937	 i:8 	 global-step:18748	 l-p:0.12062117457389832
epoch£º937	 i:9 	 global-step:18749	 l-p:0.15919511020183563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1388, 5.1388, 5.1388],
        [5.1388, 5.1087, 5.1336],
        [5.1388, 4.8678, 4.7106],
        [5.1388, 5.0095, 5.0654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.13183827698230743 
model_pd.l_d.mean(): -19.43499755859375 
model_pd.lagr.mean(): -19.303159713745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-20.1422], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.13183827698230743
epoch£º938	 i:1 	 global-step:18761	 l-p:0.16723032295703888
epoch£º938	 i:2 	 global-step:18762	 l-p:0.16990767419338226
epoch£º938	 i:3 	 global-step:18763	 l-p:0.09998136013746262
epoch£º938	 i:4 	 global-step:18764	 l-p:0.15620949864387512
epoch£º938	 i:5 	 global-step:18765	 l-p:0.1645064651966095
epoch£º938	 i:6 	 global-step:18766	 l-p:0.11677224934101105
epoch£º938	 i:7 	 global-step:18767	 l-p:0.05340622365474701
epoch£º938	 i:8 	 global-step:18768	 l-p:0.12080223113298416
epoch£º938	 i:9 	 global-step:18769	 l-p:0.1889294534921646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[5.1425, 5.3967, 5.2307],
        [5.1425, 4.9406, 4.6373],
        [5.1425, 4.8737, 4.7421],
        [5.1425, 5.0522, 4.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.08899908512830734 
model_pd.l_d.mean(): -20.113496780395508 
model_pd.lagr.mean(): -20.024497985839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4914], device='cuda:0')), ('power', tensor([-20.8353], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.08899908512830734
epoch£º939	 i:1 	 global-step:18781	 l-p:0.11631985753774643
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12367194145917892
epoch£º939	 i:3 	 global-step:18783	 l-p:0.16804340481758118
epoch£º939	 i:4 	 global-step:18784	 l-p:0.08752370625734329
epoch£º939	 i:5 	 global-step:18785	 l-p:0.1879570186138153
epoch£º939	 i:6 	 global-step:18786	 l-p:0.11464117467403412
epoch£º939	 i:7 	 global-step:18787	 l-p:0.16321921348571777
epoch£º939	 i:8 	 global-step:18788	 l-p:0.1789875626564026
epoch£º939	 i:9 	 global-step:18789	 l-p:0.13124574720859528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1485, 5.1474, 5.1485],
        [5.1485, 4.9474, 4.6443],
        [5.1485, 5.1341, 5.1470],
        [5.1485, 5.0733, 5.1220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.13387584686279297 
model_pd.l_d.mean(): -19.64612579345703 
model_pd.lagr.mean(): -19.512248992919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5401], device='cuda:0')), ('power', tensor([-20.4126], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.13387584686279297
epoch£º940	 i:1 	 global-step:18801	 l-p:0.08663442730903625
epoch£º940	 i:2 	 global-step:18802	 l-p:0.14856202900409698
epoch£º940	 i:3 	 global-step:18803	 l-p:0.11227487027645111
epoch£º940	 i:4 	 global-step:18804	 l-p:0.17295578122138977
epoch£º940	 i:5 	 global-step:18805	 l-p:0.1090528592467308
epoch£º940	 i:6 	 global-step:18806	 l-p:0.16284425556659698
epoch£º940	 i:7 	 global-step:18807	 l-p:0.1710321009159088
epoch£º940	 i:8 	 global-step:18808	 l-p:0.08372736722230911
epoch£º940	 i:9 	 global-step:18809	 l-p:0.16575664281845093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 5.0242, 4.7062],
        [5.1605, 5.1603, 5.1605],
        [5.1605, 4.9038, 4.8234],
        [5.1605, 4.9176, 4.6537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.14377205073833466 
model_pd.l_d.mean(): -19.54328155517578 
model_pd.lagr.mean(): -19.39950942993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4975], device='cuda:0')), ('power', tensor([-20.2651], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.14377205073833466
epoch£º941	 i:1 	 global-step:18821	 l-p:0.12996606528759003
epoch£º941	 i:2 	 global-step:18822	 l-p:0.15006865561008453
epoch£º941	 i:3 	 global-step:18823	 l-p:0.09552714973688126
epoch£º941	 i:4 	 global-step:18824	 l-p:0.15134836733341217
epoch£º941	 i:5 	 global-step:18825	 l-p:0.16356369853019714
epoch£º941	 i:6 	 global-step:18826	 l-p:0.11003024876117706
epoch£º941	 i:7 	 global-step:18827	 l-p:0.128750279545784
epoch£º941	 i:8 	 global-step:18828	 l-p:0.08168588578701019
epoch£º941	 i:9 	 global-step:18829	 l-p:0.12234964221715927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 4.9714, 4.9974],
        [5.1646, 5.5748, 5.5073],
        [5.1646, 4.9494, 4.6569],
        [5.1646, 5.1594, 5.1643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12138765305280685 
model_pd.l_d.mean(): -20.325639724731445 
model_pd.lagr.mean(): -20.204252243041992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-20.9985], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12138765305280685
epoch£º942	 i:1 	 global-step:18841	 l-p:0.15489435195922852
epoch£º942	 i:2 	 global-step:18842	 l-p:0.12466178834438324
epoch£º942	 i:3 	 global-step:18843	 l-p:0.1724880039691925
epoch£º942	 i:4 	 global-step:18844	 l-p:0.13117925822734833
epoch£º942	 i:5 	 global-step:18845	 l-p:0.1164749339222908
epoch£º942	 i:6 	 global-step:18846	 l-p:0.1264183074235916
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12406846135854721
epoch£º942	 i:8 	 global-step:18848	 l-p:0.17922751605510712
epoch£º942	 i:9 	 global-step:18849	 l-p:0.05758393183350563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 5.1666, 5.1666],
        [5.1666, 4.9099, 4.8292],
        [5.1666, 5.1665, 5.1666],
        [5.1666, 5.1665, 5.1666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12129766494035721 
model_pd.l_d.mean(): -20.382219314575195 
model_pd.lagr.mean(): -20.260921478271484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4186], device='cuda:0')), ('power', tensor([-21.0326], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12129766494035721
epoch£º943	 i:1 	 global-step:18861	 l-p:0.1371518224477768
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13371089100837708
epoch£º943	 i:3 	 global-step:18863	 l-p:0.09481967985630035
epoch£º943	 i:4 	 global-step:18864	 l-p:0.14253193140029907
epoch£º943	 i:5 	 global-step:18865	 l-p:0.08781072497367859
epoch£º943	 i:6 	 global-step:18866	 l-p:0.15580323338508606
epoch£º943	 i:7 	 global-step:18867	 l-p:0.1300673484802246
epoch£º943	 i:8 	 global-step:18868	 l-p:0.11711717396974564
epoch£º943	 i:9 	 global-step:18869	 l-p:0.16154013574123383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[5.1779, 4.9297, 4.6777],
        [5.1779, 5.1056, 4.7930],
        [5.1779, 4.9378, 4.8982],
        [5.1779, 4.9952, 5.0291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.11378743499517441 
model_pd.l_d.mean(): -20.981449127197266 
model_pd.lagr.mean(): -20.86766242980957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3762], device='cuda:0')), ('power', tensor([-21.5951], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.11378743499517441
epoch£º944	 i:1 	 global-step:18881	 l-p:0.1043851226568222
epoch£º944	 i:2 	 global-step:18882	 l-p:0.11990712583065033
epoch£º944	 i:3 	 global-step:18883	 l-p:0.14296109974384308
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11536496877670288
epoch£º944	 i:5 	 global-step:18885	 l-p:0.11362140625715256
epoch£º944	 i:6 	 global-step:18886	 l-p:0.14330676198005676
epoch£º944	 i:7 	 global-step:18887	 l-p:0.15898431837558746
epoch£º944	 i:8 	 global-step:18888	 l-p:0.11757196485996246
epoch£º944	 i:9 	 global-step:18889	 l-p:0.13039568066596985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.0451, 4.7272],
        [5.1740, 5.1699, 5.1738],
        [5.1740, 5.0793, 5.1333],
        [5.1740, 5.1740, 5.1740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.0985753983259201 
model_pd.l_d.mean(): -19.63430404663086 
model_pd.lagr.mean(): -19.535728454589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-20.3094], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.0985753983259201
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12363670021295547
epoch£º945	 i:2 	 global-step:18902	 l-p:0.12612147629261017
epoch£º945	 i:3 	 global-step:18903	 l-p:0.06439679861068726
epoch£º945	 i:4 	 global-step:18904	 l-p:0.11311504244804382
epoch£º945	 i:5 	 global-step:18905	 l-p:0.15076178312301636
epoch£º945	 i:6 	 global-step:18906	 l-p:0.10366767644882202
epoch£º945	 i:7 	 global-step:18907	 l-p:0.13954727351665497
epoch£º945	 i:8 	 global-step:18908	 l-p:0.15513040125370026
epoch£º945	 i:9 	 global-step:18909	 l-p:0.27280697226524353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1430, 5.0071, 4.6866],
        [5.1430, 5.1328, 4.8355],
        [5.1430, 5.1430, 5.1430],
        [5.1430, 5.5898, 5.5471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.20646166801452637 
model_pd.l_d.mean(): -19.689777374267578 
model_pd.lagr.mean(): -19.48331642150879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5310], device='cuda:0')), ('power', tensor([-20.4474], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.20646166801452637
epoch£º946	 i:1 	 global-step:18921	 l-p:0.12257476896047592
epoch£º946	 i:2 	 global-step:18922	 l-p:0.12576714158058167
epoch£º946	 i:3 	 global-step:18923	 l-p:0.09958387166261673
epoch£º946	 i:4 	 global-step:18924	 l-p:0.12855558097362518
epoch£º946	 i:5 	 global-step:18925	 l-p:0.1296525001525879
epoch£º946	 i:6 	 global-step:18926	 l-p:0.194535493850708
epoch£º946	 i:7 	 global-step:18927	 l-p:0.09767879545688629
epoch£º946	 i:8 	 global-step:18928	 l-p:0.14026309549808502
epoch£º946	 i:9 	 global-step:18929	 l-p:0.12029233574867249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1460, 5.1325, 5.1446],
        [5.1460, 5.1439, 5.1459],
        [5.1460, 5.1460, 5.1460],
        [5.1460, 5.1460, 5.1460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.19657965004444122 
model_pd.l_d.mean(): -20.84156608581543 
model_pd.lagr.mean(): -20.644987106323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4185], device='cuda:0')), ('power', tensor([-21.4969], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.19657965004444122
epoch£º947	 i:1 	 global-step:18941	 l-p:0.15505926311016083
epoch£º947	 i:2 	 global-step:18942	 l-p:0.12738072872161865
epoch£º947	 i:3 	 global-step:18943	 l-p:0.12059570103883743
epoch£º947	 i:4 	 global-step:18944	 l-p:0.09932330995798111
epoch£º947	 i:5 	 global-step:18945	 l-p:0.14791373908519745
epoch£º947	 i:6 	 global-step:18946	 l-p:0.140823632478714
epoch£º947	 i:7 	 global-step:18947	 l-p:0.09760978817939758
epoch£º947	 i:8 	 global-step:18948	 l-p:0.14441774785518646
epoch£º947	 i:9 	 global-step:18949	 l-p:0.16865716874599457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1345, 5.1646, 4.8822],
        [5.1345, 5.0606, 5.1090],
        [5.1345, 5.1344, 5.1345],
        [5.1345, 5.0007, 5.0566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11700502783060074 
model_pd.l_d.mean(): -19.34575080871582 
model_pd.lagr.mean(): -19.22874641418457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5814], device='cuda:0')), ('power', tensor([-20.1511], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11700502783060074
epoch£º948	 i:1 	 global-step:18961	 l-p:0.11663981527090073
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11791253089904785
epoch£º948	 i:3 	 global-step:18963	 l-p:0.14888562262058258
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14604401588439941
epoch£º948	 i:5 	 global-step:18965	 l-p:0.17146281898021698
epoch£º948	 i:6 	 global-step:18966	 l-p:0.16016735136508942
epoch£º948	 i:7 	 global-step:18967	 l-p:0.06897818297147751
epoch£º948	 i:8 	 global-step:18968	 l-p:0.14189334213733673
epoch£º948	 i:9 	 global-step:18969	 l-p:0.19789133965969086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1398, 5.1386, 5.1397],
        [5.1398, 5.0224, 5.0793],
        [5.1398, 5.0637, 5.1129],
        [5.1398, 5.0859, 5.1254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.12593889236450195 
model_pd.l_d.mean(): -20.784427642822266 
model_pd.lagr.mean(): -20.658489227294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4065], device='cuda:0')), ('power', tensor([-21.4269], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.12593889236450195
epoch£º949	 i:1 	 global-step:18981	 l-p:0.10053011775016785
epoch£º949	 i:2 	 global-step:18982	 l-p:0.15254248678684235
epoch£º949	 i:3 	 global-step:18983	 l-p:0.1480935513973236
epoch£º949	 i:4 	 global-step:18984	 l-p:0.19743451476097107
epoch£º949	 i:5 	 global-step:18985	 l-p:0.177383154630661
epoch£º949	 i:6 	 global-step:18986	 l-p:0.1269250214099884
epoch£º949	 i:7 	 global-step:18987	 l-p:0.08263398706912994
epoch£º949	 i:8 	 global-step:18988	 l-p:0.15009088814258575
epoch£º949	 i:9 	 global-step:18989	 l-p:0.12303051352500916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1531, 5.1453, 5.1526],
        [5.1531, 4.8810, 4.7232],
        [5.1531, 4.8917, 4.6570],
        [5.1531, 5.1211, 5.1473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.11576566100120544 
model_pd.l_d.mean(): -19.728193283081055 
model_pd.lagr.mean(): -19.6124267578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.4542], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.11576566100120544
epoch£º950	 i:1 	 global-step:19001	 l-p:0.1904800683259964
epoch£º950	 i:2 	 global-step:19002	 l-p:0.1541510671377182
epoch£º950	 i:3 	 global-step:19003	 l-p:0.13381262123584747
epoch£º950	 i:4 	 global-step:19004	 l-p:0.11277258396148682
epoch£º950	 i:5 	 global-step:19005	 l-p:0.14862731099128723
epoch£º950	 i:6 	 global-step:19006	 l-p:0.16765782237052917
epoch£º950	 i:7 	 global-step:19007	 l-p:0.11308227479457855
epoch£º950	 i:8 	 global-step:19008	 l-p:0.1337873637676239
epoch£º950	 i:9 	 global-step:19009	 l-p:0.09217420220375061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1363, 4.9624, 5.0046],
        [5.1363, 5.1363, 5.1363],
        [5.1363, 5.1363, 5.1363],
        [5.1363, 4.8623, 4.7044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.21702449023723602 
model_pd.l_d.mean(): -19.0607967376709 
model_pd.lagr.mean(): -18.843772888183594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5336], device='cuda:0')), ('power', tensor([-19.8143], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.21702449023723602
epoch£º951	 i:1 	 global-step:19021	 l-p:0.13895486295223236
epoch£º951	 i:2 	 global-step:19022	 l-p:0.1326163113117218
epoch£º951	 i:3 	 global-step:19023	 l-p:0.09004481136798859
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12870873510837555
epoch£º951	 i:5 	 global-step:19025	 l-p:0.16073137521743774
epoch£º951	 i:6 	 global-step:19026	 l-p:0.1653263419866562
epoch£º951	 i:7 	 global-step:19027	 l-p:0.1392592042684555
epoch£º951	 i:8 	 global-step:19028	 l-p:0.11323602497577667
epoch£º951	 i:9 	 global-step:19029	 l-p:0.12798185646533966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1285, 5.1224, 5.1281],
        [5.1285, 4.9815, 4.6592],
        [5.1285, 5.0872, 5.1195],
        [5.1285, 5.1054, 5.1252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.11949458718299866 
model_pd.l_d.mean(): -20.34933090209961 
model_pd.lagr.mean(): -20.229835510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4645], device='cuda:0')), ('power', tensor([-21.0462], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.11949458718299866
epoch£º952	 i:1 	 global-step:19041	 l-p:0.16069960594177246
epoch£º952	 i:2 	 global-step:19042	 l-p:0.11604929715394974
epoch£º952	 i:3 	 global-step:19043	 l-p:0.1642177700996399
epoch£º952	 i:4 	 global-step:19044	 l-p:0.1961112916469574
epoch£º952	 i:5 	 global-step:19045	 l-p:0.14052152633666992
epoch£º952	 i:6 	 global-step:19046	 l-p:0.11840146780014038
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12763893604278564
epoch£º952	 i:8 	 global-step:19048	 l-p:0.13388265669345856
epoch£º952	 i:9 	 global-step:19049	 l-p:0.1959819495677948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1113, 4.8345, 4.6452],
        [5.1113, 5.1111, 5.1113],
        [5.1113, 5.1112, 5.1113],
        [5.1113, 5.1074, 5.1111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.10301903635263443 
model_pd.l_d.mean(): -20.452674865722656 
model_pd.lagr.mean(): -20.349655151367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-21.1683], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.10301903635263443
epoch£º953	 i:1 	 global-step:19061	 l-p:0.2581040859222412
epoch£º953	 i:2 	 global-step:19062	 l-p:0.1759166717529297
epoch£º953	 i:3 	 global-step:19063	 l-p:0.18568114936351776
epoch£º953	 i:4 	 global-step:19064	 l-p:0.16036708652973175
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13454310595989227
epoch£º953	 i:6 	 global-step:19066	 l-p:0.09953806549310684
epoch£º953	 i:7 	 global-step:19067	 l-p:0.11047185957431793
epoch£º953	 i:8 	 global-step:19068	 l-p:0.22892779111862183
epoch£º953	 i:9 	 global-step:19069	 l-p:0.17085078358650208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0948, 5.0275, 5.0735],
        [5.0948, 5.4673, 5.3751],
        [5.0948, 5.0919, 5.0947],
        [5.0948, 4.9075, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.15179218351840973 
model_pd.l_d.mean(): -19.842693328857422 
model_pd.lagr.mean(): -19.690900802612305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5652], device='cuda:0')), ('power', tensor([-20.6370], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.15179218351840973
epoch£º954	 i:1 	 global-step:19081	 l-p:0.11126828193664551
epoch£º954	 i:2 	 global-step:19082	 l-p:0.4173925817012787
epoch£º954	 i:3 	 global-step:19083	 l-p:0.22381913661956787
epoch£º954	 i:4 	 global-step:19084	 l-p:0.09997446089982986
epoch£º954	 i:5 	 global-step:19085	 l-p:0.12391732633113861
epoch£º954	 i:6 	 global-step:19086	 l-p:0.2657467722892761
epoch£º954	 i:7 	 global-step:19087	 l-p:0.10792697966098785
epoch£º954	 i:8 	 global-step:19088	 l-p:0.16427218914031982
epoch£º954	 i:9 	 global-step:19089	 l-p:0.13026082515716553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0926, 5.0926, 5.0926],
        [5.0926, 5.4641, 5.3713],
        [5.0926, 5.0144, 5.0647],
        [5.0926, 5.0513, 5.0837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.09825265407562256 
model_pd.l_d.mean(): -20.199535369873047 
model_pd.lagr.mean(): -20.101282119750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5194], device='cuda:0')), ('power', tensor([-20.9509], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.09825265407562256
epoch£º955	 i:1 	 global-step:19101	 l-p:0.11945147067308426
epoch£º955	 i:2 	 global-step:19102	 l-p:0.09574977308511734
epoch£º955	 i:3 	 global-step:19103	 l-p:-0.20673465728759766
epoch£º955	 i:4 	 global-step:19104	 l-p:0.22887971997261047
epoch£º955	 i:5 	 global-step:19105	 l-p:0.1345224529504776
epoch£º955	 i:6 	 global-step:19106	 l-p:0.19637897610664368
epoch£º955	 i:7 	 global-step:19107	 l-p:0.22122107446193695
epoch£º955	 i:8 	 global-step:19108	 l-p:0.15404094755649567
epoch£º955	 i:9 	 global-step:19109	 l-p:3.2667272090911865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0599, 5.0260, 4.7184],
        [5.0599, 5.0592, 5.0599],
        [5.0599, 5.0588, 5.0599],
        [5.0599, 4.7768, 4.6191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.1497654914855957 
model_pd.l_d.mean(): -21.001148223876953 
model_pd.lagr.mean(): -20.851383209228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-21.6615], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.1497654914855957
epoch£º956	 i:1 	 global-step:19121	 l-p:0.163166806101799
epoch£º956	 i:2 	 global-step:19122	 l-p:0.12406252324581146
epoch£º956	 i:3 	 global-step:19123	 l-p:0.19611027836799622
epoch£º956	 i:4 	 global-step:19124	 l-p:0.12774677574634552
epoch£º956	 i:5 	 global-step:19125	 l-p:0.17595542967319489
epoch£º956	 i:6 	 global-step:19126	 l-p:-1.3943557739257812
epoch£º956	 i:7 	 global-step:19127	 l-p:0.12986940145492554
epoch£º956	 i:8 	 global-step:19128	 l-p:0.05842839926481247
epoch£º956	 i:9 	 global-step:19129	 l-p:0.2613622844219208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0724, 5.0720, 5.0724],
        [5.0724, 4.8955, 4.9391],
        [5.0724, 5.0507, 5.0695],
        [5.0724, 5.0491, 5.0691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.16734173893928528 
model_pd.l_d.mean(): -20.7320556640625 
model_pd.lagr.mean(): -20.564714431762695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4586], device='cuda:0')), ('power', tensor([-21.4271], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.16734173893928528
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13593684136867523
epoch£º957	 i:2 	 global-step:19142	 l-p:0.21669165790081024
epoch£º957	 i:3 	 global-step:19143	 l-p:0.11217189580202103
epoch£º957	 i:4 	 global-step:19144	 l-p:0.09724853932857513
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11978024244308472
epoch£º957	 i:6 	 global-step:19146	 l-p:0.15412351489067078
epoch£º957	 i:7 	 global-step:19147	 l-p:0.2156037539243698
epoch£º957	 i:8 	 global-step:19148	 l-p:0.32450512051582336
epoch£º957	 i:9 	 global-step:19149	 l-p:0.1969241499900818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1079, 5.0847, 5.1046],
        [5.1079, 5.0375, 5.0848],
        [5.1079, 5.0298, 5.0800],
        [5.1079, 5.0069, 4.6843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.13954634964466095 
model_pd.l_d.mean(): -20.845951080322266 
model_pd.lagr.mean(): -20.706405639648438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-21.5009], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.13954634964466095
epoch£º958	 i:1 	 global-step:19161	 l-p:0.24153439700603485
epoch£º958	 i:2 	 global-step:19162	 l-p:0.12298144400119781
epoch£º958	 i:3 	 global-step:19163	 l-p:0.17112265527248383
epoch£º958	 i:4 	 global-step:19164	 l-p:0.1580645591020584
epoch£º958	 i:5 	 global-step:19165	 l-p:0.10205727070569992
epoch£º958	 i:6 	 global-step:19166	 l-p:0.10902410000562668
epoch£º958	 i:7 	 global-step:19167	 l-p:0.16486169397830963
epoch£º958	 i:8 	 global-step:19168	 l-p:0.12734615802764893
epoch£º958	 i:9 	 global-step:19169	 l-p:0.13846296072006226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 4.9926, 4.6770],
        [5.1666, 5.1666, 5.1666],
        [5.1666, 4.9355, 4.6514],
        [5.1666, 5.1638, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.0555993914604187 
model_pd.l_d.mean(): -20.572660446166992 
model_pd.lagr.mean(): -20.517061233520508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4381], device='cuda:0')), ('power', tensor([-21.2450], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.0555993914604187
epoch£º959	 i:1 	 global-step:19181	 l-p:0.15063410997390747
epoch£º959	 i:2 	 global-step:19182	 l-p:0.16599181294441223
epoch£º959	 i:3 	 global-step:19183	 l-p:0.1370256096124649
epoch£º959	 i:4 	 global-step:19184	 l-p:0.12236842513084412
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12602059543132782
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13861365616321564
epoch£º959	 i:7 	 global-step:19187	 l-p:0.1390644609928131
epoch£º959	 i:8 	 global-step:19188	 l-p:0.11626570671796799
epoch£º959	 i:9 	 global-step:19189	 l-p:0.10009565949440002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1942, 5.1942, 5.1942],
        [5.1942, 5.1907, 5.1941],
        [5.1942, 5.5021, 5.3658],
        [5.1942, 5.4907, 5.3473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.10678274184465408 
model_pd.l_d.mean(): -19.57832145690918 
model_pd.lagr.mean(): -19.471538543701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.3057], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.10678274184465408
epoch£º960	 i:1 	 global-step:19201	 l-p:0.16469444334506989
epoch£º960	 i:2 	 global-step:19202	 l-p:0.1299530416727066
epoch£º960	 i:3 	 global-step:19203	 l-p:0.12095857411623001
epoch£º960	 i:4 	 global-step:19204	 l-p:0.13693572580814362
epoch£º960	 i:5 	 global-step:19205	 l-p:0.11048482358455658
epoch£º960	 i:6 	 global-step:19206	 l-p:0.11787925660610199
epoch£º960	 i:7 	 global-step:19207	 l-p:0.10543569177389145
epoch£º960	 i:8 	 global-step:19208	 l-p:0.12540549039840698
epoch£º960	 i:9 	 global-step:19209	 l-p:0.11693945527076721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.0356, 4.7152],
        [5.1750, 5.1021, 5.1501],
        [5.1750, 5.0145, 4.6963],
        [5.1750, 5.1701, 5.1747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.12787893414497375 
model_pd.l_d.mean(): -19.577621459960938 
model_pd.lagr.mean(): -19.449743270874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5761], device='cuda:0')), ('power', tensor([-20.3802], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.12787893414497375
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12177935987710953
epoch£º961	 i:2 	 global-step:19222	 l-p:0.16934655606746674
epoch£º961	 i:3 	 global-step:19223	 l-p:0.1331193596124649
epoch£º961	 i:4 	 global-step:19224	 l-p:0.1673460751771927
epoch£º961	 i:5 	 global-step:19225	 l-p:0.1242889016866684
epoch£º961	 i:6 	 global-step:19226	 l-p:0.1075965017080307
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13564160466194153
epoch£º961	 i:8 	 global-step:19228	 l-p:0.12020112574100494
epoch£º961	 i:9 	 global-step:19229	 l-p:0.12139619886875153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1510, 5.1028, 5.1393],
        [5.1510, 5.1510, 5.1510],
        [5.1510, 5.1496, 5.1510],
        [5.1510, 5.0513, 5.1068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.11412845551967621 
model_pd.l_d.mean(): -20.83792495727539 
model_pd.lagr.mean(): -20.723796844482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4027], device='cuda:0')), ('power', tensor([-21.4770], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.11412845551967621
epoch£º962	 i:1 	 global-step:19241	 l-p:0.0962907075881958
epoch£º962	 i:2 	 global-step:19242	 l-p:0.12832699716091156
epoch£º962	 i:3 	 global-step:19243	 l-p:0.14531616866588593
epoch£º962	 i:4 	 global-step:19244	 l-p:0.10706674307584763
epoch£º962	 i:5 	 global-step:19245	 l-p:0.23391930758953094
epoch£º962	 i:6 	 global-step:19246	 l-p:0.11001165211200714
epoch£º962	 i:7 	 global-step:19247	 l-p:0.2991597652435303
epoch£º962	 i:8 	 global-step:19248	 l-p:0.15762601792812347
epoch£º962	 i:9 	 global-step:19249	 l-p:0.1755218803882599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1025, 5.1001, 5.1025],
        [5.1025, 5.1010, 5.1025],
        [5.1025, 5.0477, 5.0878],
        [5.1025, 5.0591, 5.0928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.12280254065990448 
model_pd.l_d.mean(): -20.07113265991211 
model_pd.lagr.mean(): -19.94832992553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.7988], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.12280254065990448
epoch£º963	 i:1 	 global-step:19261	 l-p:0.13961398601531982
epoch£º963	 i:2 	 global-step:19262	 l-p:0.13059084117412567
epoch£º963	 i:3 	 global-step:19263	 l-p:0.08806611597537994
epoch£º963	 i:4 	 global-step:19264	 l-p:0.1898581087589264
epoch£º963	 i:5 	 global-step:19265	 l-p:0.2796805202960968
epoch£º963	 i:6 	 global-step:19266	 l-p:-1.0915077924728394
epoch£º963	 i:7 	 global-step:19267	 l-p:0.15725745260715485
epoch£º963	 i:8 	 global-step:19268	 l-p:0.25429877638816833
epoch£º963	 i:9 	 global-step:19269	 l-p:0.11807676404714584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0824, 5.0807, 5.0823],
        [5.0824, 5.0820, 5.0824],
        [5.0824, 5.5081, 5.4510],
        [5.0824, 4.8706, 4.5571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.2117765247821808 
model_pd.l_d.mean(): -18.607532501220703 
model_pd.lagr.mean(): -18.395755767822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6019], device='cuda:0')), ('power', tensor([-19.4257], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.2117765247821808
epoch£º964	 i:1 	 global-step:19281	 l-p:0.20983995497226715
epoch£º964	 i:2 	 global-step:19282	 l-p:0.1572137176990509
epoch£º964	 i:3 	 global-step:19283	 l-p:0.2703663110733032
epoch£º964	 i:4 	 global-step:19284	 l-p:0.15038654208183289
epoch£º964	 i:5 	 global-step:19285	 l-p:1.3463916778564453
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13457176089286804
epoch£º964	 i:7 	 global-step:19287	 l-p:0.16119176149368286
epoch£º964	 i:8 	 global-step:19288	 l-p:0.10691271722316742
epoch£º964	 i:9 	 global-step:19289	 l-p:0.07384774833917618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.0959, 4.8614, 4.5648],
        [5.0959, 4.8255, 4.7312],
        [5.0959, 4.8633, 4.5651],
        [5.0959, 4.8420, 4.7936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.1886998564004898 
model_pd.l_d.mean(): -20.53180503845215 
model_pd.lagr.mean(): -20.34310531616211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4570], device='cuda:0')), ('power', tensor([-21.2231], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.1886998564004898
epoch£º965	 i:1 	 global-step:19301	 l-p:0.1456805020570755
epoch£º965	 i:2 	 global-step:19302	 l-p:0.14413349330425262
epoch£º965	 i:3 	 global-step:19303	 l-p:0.09675829857587814
epoch£º965	 i:4 	 global-step:19304	 l-p:0.13153910636901855
epoch£º965	 i:5 	 global-step:19305	 l-p:0.23221059143543243
epoch£º965	 i:6 	 global-step:19306	 l-p:0.2354060560464859
epoch£º965	 i:7 	 global-step:19307	 l-p:0.16896609961986542
epoch£º965	 i:8 	 global-step:19308	 l-p:0.13033433258533478
epoch£º965	 i:9 	 global-step:19309	 l-p:0.07800716161727905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 5.2449, 4.9971],
        [5.1351, 5.1346, 5.1351],
        [5.1351, 5.2150, 4.9528],
        [5.1351, 4.8888, 4.8500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.22117270529270172 
model_pd.l_d.mean(): -19.513681411743164 
model_pd.lagr.mean(): -19.292509078979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5477], device='cuda:0')), ('power', tensor([-20.2864], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.22117270529270172
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11329153180122375
epoch£º966	 i:2 	 global-step:19322	 l-p:0.07496568560600281
epoch£º966	 i:3 	 global-step:19323	 l-p:0.12456279247999191
epoch£º966	 i:4 	 global-step:19324	 l-p:0.13308557868003845
epoch£º966	 i:5 	 global-step:19325	 l-p:0.1463414877653122
epoch£º966	 i:6 	 global-step:19326	 l-p:0.14072942733764648
epoch£º966	 i:7 	 global-step:19327	 l-p:0.12016450613737106
epoch£º966	 i:8 	 global-step:19328	 l-p:0.14094321429729462
epoch£º966	 i:9 	 global-step:19329	 l-p:0.1801004558801651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 5.1427, 5.1429],
        [5.1429, 5.1429, 5.1429],
        [5.1429, 5.1429, 5.1429],
        [5.1429, 4.8767, 4.6406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.16334788501262665 
model_pd.l_d.mean(): -20.839262008666992 
model_pd.lagr.mean(): -20.675914764404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4363], device='cuda:0')), ('power', tensor([-21.5127], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.16334788501262665
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14190800487995148
epoch£º967	 i:2 	 global-step:19342	 l-p:0.15893232822418213
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12306240946054459
epoch£º967	 i:4 	 global-step:19344	 l-p:0.11284518241882324
epoch£º967	 i:5 	 global-step:19345	 l-p:0.13051794469356537
epoch£º967	 i:6 	 global-step:19346	 l-p:0.16464604437351227
epoch£º967	 i:7 	 global-step:19347	 l-p:0.16445070505142212
epoch£º967	 i:8 	 global-step:19348	 l-p:0.16515526175498962
epoch£º967	 i:9 	 global-step:19349	 l-p:0.12731455266475677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1306, 5.0676, 4.7519],
        [5.1306, 4.9804, 4.6558],
        [5.1306, 5.0451, 5.0976],
        [5.1306, 4.8643, 4.6229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.09049466997385025 
model_pd.l_d.mean(): -20.287050247192383 
model_pd.lagr.mean(): -20.196556091308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5098], device='cuda:0')), ('power', tensor([-21.0296], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:0.09049466997385025
epoch£º968	 i:1 	 global-step:19361	 l-p:0.18990500271320343
epoch£º968	 i:2 	 global-step:19362	 l-p:0.11253110319375992
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15384142100811005
epoch£º968	 i:4 	 global-step:19364	 l-p:0.09543478488922119
epoch£º968	 i:5 	 global-step:19365	 l-p:0.165310800075531
epoch£º968	 i:6 	 global-step:19366	 l-p:0.10622836649417877
epoch£º968	 i:7 	 global-step:19367	 l-p:0.1500873863697052
epoch£º968	 i:8 	 global-step:19368	 l-p:0.2033196985721588
epoch£º968	 i:9 	 global-step:19369	 l-p:0.14205054938793182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 4.8893, 4.6220],
        [5.1417, 5.2118, 4.9446],
        [5.1417, 4.9907, 4.6668],
        [5.1417, 5.0194, 5.0768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.15538153052330017 
model_pd.l_d.mean(): -20.950124740600586 
model_pd.lagr.mean(): -20.794742584228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3922], device='cuda:0')), ('power', tensor([-21.5798], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.15538153052330017
epoch£º969	 i:1 	 global-step:19381	 l-p:0.13261853158473969
epoch£º969	 i:2 	 global-step:19382	 l-p:0.11743777990341187
epoch£º969	 i:3 	 global-step:19383	 l-p:0.1202545315027237
epoch£º969	 i:4 	 global-step:19384	 l-p:0.1437259316444397
epoch£º969	 i:5 	 global-step:19385	 l-p:0.20281219482421875
epoch£º969	 i:6 	 global-step:19386	 l-p:0.12424997240304947
epoch£º969	 i:7 	 global-step:19387	 l-p:0.13578036427497864
epoch£º969	 i:8 	 global-step:19388	 l-p:0.1992873102426529
epoch£º969	 i:9 	 global-step:19389	 l-p:0.09844981878995895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1402, 5.1401, 5.1402],
        [5.1402, 4.9124, 4.9054],
        [5.1402, 4.9882, 5.0410],
        [5.1402, 4.8700, 4.7635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.12199201434850693 
model_pd.l_d.mean(): -20.658039093017578 
model_pd.lagr.mean(): -20.536046981811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.3346], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.12199201434850693
epoch£º970	 i:1 	 global-step:19401	 l-p:0.11620579659938812
epoch£º970	 i:2 	 global-step:19402	 l-p:0.18634968996047974
epoch£º970	 i:3 	 global-step:19403	 l-p:0.1902771294116974
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13627366721630096
epoch£º970	 i:5 	 global-step:19405	 l-p:0.10929140448570251
epoch£º970	 i:6 	 global-step:19406	 l-p:0.12028355151414871
epoch£º970	 i:7 	 global-step:19407	 l-p:0.1267824023962021
epoch£º970	 i:8 	 global-step:19408	 l-p:0.13726437091827393
epoch£º970	 i:9 	 global-step:19409	 l-p:0.11138825863599777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.0697, 5.1238],
        [5.1621, 5.0741, 5.1272],
        [5.1621, 5.1613, 5.1621],
        [5.1621, 4.8902, 4.6818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.10280677676200867 
model_pd.l_d.mean(): -20.756484985351562 
model_pd.lagr.mean(): -20.65367889404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-21.4074], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.10280677676200867
epoch£º971	 i:1 	 global-step:19421	 l-p:0.13751891255378723
epoch£º971	 i:2 	 global-step:19422	 l-p:0.09121347218751907
epoch£º971	 i:3 	 global-step:19423	 l-p:0.14088772237300873
epoch£º971	 i:4 	 global-step:19424	 l-p:0.1464608907699585
epoch£º971	 i:5 	 global-step:19425	 l-p:0.1605144888162613
epoch£º971	 i:6 	 global-step:19426	 l-p:0.1828227937221527
epoch£º971	 i:7 	 global-step:19427	 l-p:0.10860660672187805
epoch£º971	 i:8 	 global-step:19428	 l-p:0.150831937789917
epoch£º971	 i:9 	 global-step:19429	 l-p:0.14021316170692444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1495, 5.0822, 5.1282],
        [5.1495, 5.1479, 5.1495],
        [5.1495, 4.8984, 4.8491],
        [5.1495, 5.1467, 5.1494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.15695542097091675 
model_pd.l_d.mean(): -19.964229583740234 
model_pd.lagr.mean(): -19.807273864746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.7198], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.15695542097091675
epoch£º972	 i:1 	 global-step:19441	 l-p:0.11898104101419449
epoch£º972	 i:2 	 global-step:19442	 l-p:0.14035509526729584
epoch£º972	 i:3 	 global-step:19443	 l-p:0.14517703652381897
epoch£º972	 i:4 	 global-step:19444	 l-p:0.1613004505634308
epoch£º972	 i:5 	 global-step:19445	 l-p:0.146432027220726
epoch£º972	 i:6 	 global-step:19446	 l-p:0.19299353659152985
epoch£º972	 i:7 	 global-step:19447	 l-p:0.08324144780635834
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10892549902200699
epoch£º972	 i:9 	 global-step:19449	 l-p:0.10784167796373367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 5.1411, 5.1534],
        [5.1547, 4.9425, 4.9547],
        [5.1547, 5.0438, 5.1009],
        [5.1547, 5.1199, 4.8118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.18167580664157867 
model_pd.l_d.mean(): -19.989194869995117 
model_pd.lagr.mean(): -19.807519912719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4465], device='cuda:0')), ('power', tensor([-20.6638], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.18167580664157867
epoch£º973	 i:1 	 global-step:19461	 l-p:0.1308709979057312
epoch£º973	 i:2 	 global-step:19462	 l-p:0.0706896111369133
epoch£º973	 i:3 	 global-step:19463	 l-p:0.1344704031944275
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16253243386745453
epoch£º973	 i:5 	 global-step:19465	 l-p:0.12304014712572098
epoch£º973	 i:6 	 global-step:19466	 l-p:0.15263885259628296
epoch£º973	 i:7 	 global-step:19467	 l-p:0.16921868920326233
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11401556432247162
epoch£º973	 i:9 	 global-step:19469	 l-p:0.12101331353187561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1389, 4.8743, 4.7919],
        [5.1389, 5.0857, 5.1250],
        [5.1389, 4.9213, 4.9287],
        [5.1389, 5.1786, 4.8975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.1418028473854065 
model_pd.l_d.mean(): -19.239591598510742 
model_pd.lagr.mean(): -19.097787857055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5091], device='cuda:0')), ('power', tensor([-19.9699], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.1418028473854065
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12157332897186279
epoch£º974	 i:2 	 global-step:19482	 l-p:0.13464300334453583
epoch£º974	 i:3 	 global-step:19483	 l-p:0.14229348301887512
epoch£º974	 i:4 	 global-step:19484	 l-p:0.23332753777503967
epoch£º974	 i:5 	 global-step:19485	 l-p:0.1375667303800583
epoch£º974	 i:6 	 global-step:19486	 l-p:0.13210849463939667
epoch£º974	 i:7 	 global-step:19487	 l-p:0.11343322694301605
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13753046095371246
epoch£º974	 i:9 	 global-step:19489	 l-p:0.17797687649726868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1255, 5.1255, 5.1255],
        [5.1255, 5.1255, 5.1255],
        [5.1255, 4.8462, 4.6533],
        [5.1255, 4.9933, 5.0506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.10941128432750702 
model_pd.l_d.mean(): -19.540800094604492 
model_pd.lagr.mean(): -19.43138885498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.2497], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.10941128432750702
epoch£º975	 i:1 	 global-step:19501	 l-p:0.1987570971250534
epoch£º975	 i:2 	 global-step:19502	 l-p:0.16389548778533936
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13359595835208893
epoch£º975	 i:4 	 global-step:19504	 l-p:0.10295026749372482
epoch£º975	 i:5 	 global-step:19505	 l-p:0.15022751688957214
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10740408301353455
epoch£º975	 i:7 	 global-step:19507	 l-p:0.3267999589443207
epoch£º975	 i:8 	 global-step:19508	 l-p:0.11214280873537064
epoch£º975	 i:9 	 global-step:19509	 l-p:0.14779722690582275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1045, 4.8410, 4.7717],
        [5.1045, 5.0784, 5.1005],
        [5.1045, 5.1416, 4.8591],
        [5.1045, 5.1045, 5.1045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.24896886944770813 
model_pd.l_d.mean(): -20.543546676635742 
model_pd.lagr.mean(): -20.294578552246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.2541], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.24896886944770813
epoch£º976	 i:1 	 global-step:19521	 l-p:0.3752155303955078
epoch£º976	 i:2 	 global-step:19522	 l-p:0.1542736291885376
epoch£º976	 i:3 	 global-step:19523	 l-p:0.15767359733581543
epoch£º976	 i:4 	 global-step:19524	 l-p:0.16312646865844727
epoch£º976	 i:5 	 global-step:19525	 l-p:0.12763942778110504
epoch£º976	 i:6 	 global-step:19526	 l-p:0.11119338870048523
epoch£º976	 i:7 	 global-step:19527	 l-p:0.12827569246292114
epoch£º976	 i:8 	 global-step:19528	 l-p:0.1225329339504242
epoch£º976	 i:9 	 global-step:19529	 l-p:0.1448458433151245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1132, 5.1132, 5.1132],
        [5.1132, 5.1132, 5.1132],
        [5.1132, 5.0174, 5.0728],
        [5.1132, 4.8588, 4.8104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.1704987734556198 
model_pd.l_d.mean(): -20.669801712036133 
model_pd.lagr.mean(): -20.499303817749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.3673], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.1704987734556198
epoch£º977	 i:1 	 global-step:19541	 l-p:0.1799587458372116
epoch£º977	 i:2 	 global-step:19542	 l-p:0.11498626321554184
epoch£º977	 i:3 	 global-step:19543	 l-p:0.1240818127989769
epoch£º977	 i:4 	 global-step:19544	 l-p:0.167526513338089
epoch£º977	 i:5 	 global-step:19545	 l-p:0.08939986675977707
epoch£º977	 i:6 	 global-step:19546	 l-p:0.11338622123003006
epoch£º977	 i:7 	 global-step:19547	 l-p:0.21485568583011627
epoch£º977	 i:8 	 global-step:19548	 l-p:0.41788339614868164
epoch£º977	 i:9 	 global-step:19549	 l-p:0.20052501559257507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1064, 5.1057, 5.1064],
        [5.1064, 5.0250, 5.0766],
        [5.1064, 5.1064, 5.1064],
        [5.1064, 4.8234, 4.6509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.1335127204656601 
model_pd.l_d.mean(): -20.315521240234375 
model_pd.lagr.mean(): -20.182008743286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-21.0387], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.1335127204656601
epoch£º978	 i:1 	 global-step:19561	 l-p:0.1669427901506424
epoch£º978	 i:2 	 global-step:19562	 l-p:0.25110960006713867
epoch£º978	 i:3 	 global-step:19563	 l-p:0.11930156499147415
epoch£º978	 i:4 	 global-step:19564	 l-p:0.1491575837135315
epoch£º978	 i:5 	 global-step:19565	 l-p:0.1326633244752884
epoch£º978	 i:6 	 global-step:19566	 l-p:0.19204826653003693
epoch£º978	 i:7 	 global-step:19567	 l-p:0.25801974534988403
epoch£º978	 i:8 	 global-step:19568	 l-p:0.0863480344414711
epoch£º978	 i:9 	 global-step:19569	 l-p:0.121551014482975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[5.1193, 5.5267, 5.4549],
        [5.1193, 4.8429, 4.6193],
        [5.1193, 4.8686, 4.5882],
        [5.1193, 4.8461, 4.7424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.16912250220775604 
model_pd.l_d.mean(): -19.832000732421875 
model_pd.lagr.mean(): -19.662878036499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5455], device='cuda:0')), ('power', tensor([-20.6060], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.16912250220775604
epoch£º979	 i:1 	 global-step:19581	 l-p:0.11819904297590256
epoch£º979	 i:2 	 global-step:19582	 l-p:0.18309594690799713
epoch£º979	 i:3 	 global-step:19583	 l-p:0.14686401188373566
epoch£º979	 i:4 	 global-step:19584	 l-p:0.11543510109186172
epoch£º979	 i:5 	 global-step:19585	 l-p:0.12256848812103271
epoch£º979	 i:6 	 global-step:19586	 l-p:0.12268329411745071
epoch£º979	 i:7 	 global-step:19587	 l-p:0.16960109770298004
epoch£º979	 i:8 	 global-step:19588	 l-p:0.16237914562225342
epoch£º979	 i:9 	 global-step:19589	 l-p:0.12375721335411072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1502, 5.1502, 5.1502],
        [5.1502, 5.1472, 5.1501],
        [5.1502, 5.1045, 5.1396],
        [5.1502, 5.1498, 5.1502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.18873263895511627 
model_pd.l_d.mean(): -20.675676345825195 
model_pd.lagr.mean(): -20.4869441986084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.3433], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.18873263895511627
epoch£º980	 i:1 	 global-step:19601	 l-p:0.14763329923152924
epoch£º980	 i:2 	 global-step:19602	 l-p:0.13946832716464996
epoch£º980	 i:3 	 global-step:19603	 l-p:0.07857462763786316
epoch£º980	 i:4 	 global-step:19604	 l-p:0.1054975688457489
epoch£º980	 i:5 	 global-step:19605	 l-p:0.19115164875984192
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11627248674631119
epoch£º980	 i:7 	 global-step:19607	 l-p:0.14787961542606354
epoch£º980	 i:8 	 global-step:19608	 l-p:0.1260143369436264
epoch£º980	 i:9 	 global-step:19609	 l-p:0.1101377010345459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.0438, 5.1013],
        [5.1662, 5.1662, 5.1662],
        [5.1662, 5.1640, 5.1662],
        [5.1662, 4.9213, 4.6439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13957944512367249 
model_pd.l_d.mean(): -19.752498626708984 
model_pd.lagr.mean(): -19.612918853759766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-20.4629], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13957944512367249
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13140448927879333
epoch£º981	 i:2 	 global-step:19622	 l-p:0.13229036331176758
epoch£º981	 i:3 	 global-step:19623	 l-p:0.13932369649410248
epoch£º981	 i:4 	 global-step:19624	 l-p:0.12992249429225922
epoch£º981	 i:5 	 global-step:19625	 l-p:0.16071002185344696
epoch£º981	 i:6 	 global-step:19626	 l-p:0.15588460862636566
epoch£º981	 i:7 	 global-step:19627	 l-p:0.1655157208442688
epoch£º981	 i:8 	 global-step:19628	 l-p:0.09042354673147202
epoch£º981	 i:9 	 global-step:19629	 l-p:0.19081225991249084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1259, 4.8765, 4.8379],
        [5.1259, 5.2788, 5.0518],
        [5.1259, 5.1259, 5.1259],
        [5.1259, 5.0393, 5.0924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.15099944174289703 
model_pd.l_d.mean(): -20.515443801879883 
model_pd.lagr.mean(): -20.364444732666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4716], device='cuda:0')), ('power', tensor([-21.2214], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.15099944174289703
epoch£º982	 i:1 	 global-step:19641	 l-p:0.16245773434638977
epoch£º982	 i:2 	 global-step:19642	 l-p:0.17782866954803467
epoch£º982	 i:3 	 global-step:19643	 l-p:0.15115229785442352
epoch£º982	 i:4 	 global-step:19644	 l-p:0.15521416068077087
epoch£º982	 i:5 	 global-step:19645	 l-p:0.07148106396198273
epoch£º982	 i:6 	 global-step:19646	 l-p:0.1487503945827484
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12430087476968765
epoch£º982	 i:8 	 global-step:19648	 l-p:0.1964578628540039
epoch£º982	 i:9 	 global-step:19649	 l-p:0.17004089057445526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1277, 4.9578, 5.0049],
        [5.1277, 4.9950, 5.0525],
        [5.1277, 5.0410, 5.0941],
        [5.1277, 5.1247, 5.1275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.07498860359191895 
model_pd.l_d.mean(): -20.68788719177246 
model_pd.lagr.mean(): -20.612897872924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4547], device='cuda:0')), ('power', tensor([-21.3785], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.07498860359191895
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11910738050937653
epoch£º983	 i:2 	 global-step:19662	 l-p:0.14679700136184692
epoch£º983	 i:3 	 global-step:19663	 l-p:0.13407379388809204
epoch£º983	 i:4 	 global-step:19664	 l-p:0.130607470870018
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11814003437757492
epoch£º983	 i:6 	 global-step:19666	 l-p:0.14919045567512512
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14509999752044678
epoch£º983	 i:8 	 global-step:19668	 l-p:0.2418529987335205
epoch£º983	 i:9 	 global-step:19669	 l-p:0.14790816605091095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1495, 4.9653, 4.6446],
        [5.1495, 5.1434, 5.1491],
        [5.1495, 5.1494, 5.1495],
        [5.1495, 5.1444, 5.1492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.07836303859949112 
model_pd.l_d.mean(): -20.199260711669922 
model_pd.lagr.mean(): -20.12089729309082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4985], device='cuda:0')), ('power', tensor([-20.9292], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.07836303859949112
epoch£º984	 i:1 	 global-step:19681	 l-p:0.1832258552312851
epoch£º984	 i:2 	 global-step:19682	 l-p:0.11835059523582458
epoch£º984	 i:3 	 global-step:19683	 l-p:0.1381712257862091
epoch£º984	 i:4 	 global-step:19684	 l-p:0.12054523080587387
epoch£º984	 i:5 	 global-step:19685	 l-p:0.11504341661930084
epoch£º984	 i:6 	 global-step:19686	 l-p:0.1138647049665451
epoch£º984	 i:7 	 global-step:19687	 l-p:0.1781979352235794
epoch£º984	 i:8 	 global-step:19688	 l-p:0.200881689786911
epoch£º984	 i:9 	 global-step:19689	 l-p:0.14301197230815887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1394, 5.5286, 5.4435],
        [5.1394, 4.9697, 5.0167],
        [5.1394, 5.1803, 4.8985],
        [5.1394, 5.1394, 5.1394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.18297192454338074 
model_pd.l_d.mean(): -20.446958541870117 
model_pd.lagr.mean(): -20.263986587524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4662], device='cuda:0')), ('power', tensor([-21.1466], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.18297192454338074
epoch£º985	 i:1 	 global-step:19701	 l-p:0.12783627212047577
epoch£º985	 i:2 	 global-step:19702	 l-p:0.1451837122440338
epoch£º985	 i:3 	 global-step:19703	 l-p:0.15710538625717163
epoch£º985	 i:4 	 global-step:19704	 l-p:0.16033874452114105
epoch£º985	 i:5 	 global-step:19705	 l-p:0.11350426077842712
epoch£º985	 i:6 	 global-step:19706	 l-p:0.13366152346134186
epoch£º985	 i:7 	 global-step:19707	 l-p:0.1289852112531662
epoch£º985	 i:8 	 global-step:19708	 l-p:0.18539349734783173
epoch£º985	 i:9 	 global-step:19709	 l-p:0.08764411509037018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1463, 5.1463, 5.1463],
        [5.1463, 5.0545, 5.1089],
        [5.1463, 5.1419, 5.1461],
        [5.1463, 5.0170, 5.0747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.09658236056566238 
model_pd.l_d.mean(): -20.516183853149414 
model_pd.lagr.mean(): -20.419601440429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.2105], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.09658236056566238
epoch£º986	 i:1 	 global-step:19721	 l-p:0.16214697062969208
epoch£º986	 i:2 	 global-step:19722	 l-p:0.1178533062338829
epoch£º986	 i:3 	 global-step:19723	 l-p:0.0917612835764885
epoch£º986	 i:4 	 global-step:19724	 l-p:0.15708579123020172
epoch£º986	 i:5 	 global-step:19725	 l-p:0.21067768335342407
epoch£º986	 i:6 	 global-step:19726	 l-p:0.17901916801929474
epoch£º986	 i:7 	 global-step:19727	 l-p:0.1363377869129181
epoch£º986	 i:8 	 global-step:19728	 l-p:0.10334032028913498
epoch£º986	 i:9 	 global-step:19729	 l-p:0.10757948458194733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1688, 5.1688, 5.1689],
        [5.1688, 5.1688, 5.1689],
        [5.1688, 5.1202, 5.1570],
        [5.1688, 5.0826, 5.1354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11555266380310059 
model_pd.l_d.mean(): -19.37690544128418 
model_pd.lagr.mean(): -19.2613525390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4857], device='cuda:0')), ('power', tensor([-20.0848], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11555266380310059
epoch£º987	 i:1 	 global-step:19741	 l-p:0.15640880167484283
epoch£º987	 i:2 	 global-step:19742	 l-p:0.040644802153110504
epoch£º987	 i:3 	 global-step:19743	 l-p:0.09453511238098145
epoch£º987	 i:4 	 global-step:19744	 l-p:0.1529477834701538
epoch£º987	 i:5 	 global-step:19745	 l-p:0.14513175189495087
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13278837502002716
epoch£º987	 i:7 	 global-step:19747	 l-p:0.1752929389476776
epoch£º987	 i:8 	 global-step:19748	 l-p:0.162555992603302
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14550647139549255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 4.8847, 4.6704],
        [5.1601, 4.8835, 4.7501],
        [5.1601, 5.1597, 5.1601],
        [5.1601, 5.1601, 5.1601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.16795030236244202 
model_pd.l_d.mean(): -20.752288818359375 
model_pd.lagr.mean(): -20.584339141845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4134], device='cuda:0')), ('power', tensor([-21.4014], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.16795030236244202
epoch£º988	 i:1 	 global-step:19761	 l-p:0.1118386909365654
epoch£º988	 i:2 	 global-step:19762	 l-p:0.1310834139585495
epoch£º988	 i:3 	 global-step:19763	 l-p:0.15123669803142548
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1727830022573471
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11686601489782333
epoch£º988	 i:6 	 global-step:19766	 l-p:0.09587779641151428
epoch£º988	 i:7 	 global-step:19767	 l-p:0.18415842950344086
epoch£º988	 i:8 	 global-step:19768	 l-p:0.1583472639322281
epoch£º988	 i:9 	 global-step:19769	 l-p:0.11073534935712814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1424, 4.9324, 4.9504],
        [5.1424, 5.1392, 5.1423],
        [5.1424, 5.0128, 5.0707],
        [5.1424, 5.1424, 5.1424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.13186368346214294 
model_pd.l_d.mean(): -20.773548126220703 
model_pd.lagr.mean(): -20.64168357849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4123], device='cuda:0')), ('power', tensor([-21.4217], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.13186368346214294
epoch£º989	 i:1 	 global-step:19781	 l-p:0.19771404564380646
epoch£º989	 i:2 	 global-step:19782	 l-p:0.1444302201271057
epoch£º989	 i:3 	 global-step:19783	 l-p:0.07473332434892654
epoch£º989	 i:4 	 global-step:19784	 l-p:0.17166703939437866
epoch£º989	 i:5 	 global-step:19785	 l-p:0.17095467448234558
epoch£º989	 i:6 	 global-step:19786	 l-p:0.13030306994915009
epoch£º989	 i:7 	 global-step:19787	 l-p:0.1607433259487152
epoch£º989	 i:8 	 global-step:19788	 l-p:0.07598324865102768
epoch£º989	 i:9 	 global-step:19789	 l-p:0.13462354242801666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[5.1680, 5.4598, 5.3112],
        [5.1680, 4.8994, 4.8038],
        [5.1680, 5.0068, 4.6819],
        [5.1680, 5.4853, 5.3526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.10044711828231812 
model_pd.l_d.mean(): -19.171926498413086 
model_pd.lagr.mean(): -19.07147979736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5896], device='cuda:0')), ('power', tensor([-19.9838], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:0.10044711828231812
epoch£º990	 i:1 	 global-step:19801	 l-p:0.13655127584934235
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12086477875709534
epoch£º990	 i:3 	 global-step:19803	 l-p:0.1298077404499054
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11115194857120514
epoch£º990	 i:5 	 global-step:19805	 l-p:0.15133298933506012
epoch£º990	 i:6 	 global-step:19806	 l-p:0.14436021447181702
epoch£º990	 i:7 	 global-step:19807	 l-p:0.14066849648952484
epoch£º990	 i:8 	 global-step:19808	 l-p:0.14219045639038086
epoch£º990	 i:9 	 global-step:19809	 l-p:0.06909050792455673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1968, 5.1469, 5.1843],
        [5.1968, 4.9221, 4.7768],
        [5.1968, 5.1171, 5.1678],
        [5.1968, 5.1927, 5.1966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.13163171708583832 
model_pd.l_d.mean(): -20.903207778930664 
model_pd.lagr.mean(): -20.771575927734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3820], device='cuda:0')), ('power', tensor([-21.5218], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.13163171708583832
epoch£º991	 i:1 	 global-step:19821	 l-p:0.04907253012061119
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13310636579990387
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15576550364494324
epoch£º991	 i:4 	 global-step:19824	 l-p:0.12737306952476501
epoch£º991	 i:5 	 global-step:19825	 l-p:0.11816976219415665
epoch£º991	 i:6 	 global-step:19826	 l-p:0.1408853828907013
epoch£º991	 i:7 	 global-step:19827	 l-p:0.1656159907579422
epoch£º991	 i:8 	 global-step:19828	 l-p:0.10601092875003815
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10901867598295212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1771, 5.1769, 5.1771],
        [5.1771, 5.1623, 5.1755],
        [5.1771, 5.2291, 4.9514],
        [5.1771, 5.0294, 4.7035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.11591081321239471 
model_pd.l_d.mean(): -20.667531967163086 
model_pd.lagr.mean(): -20.551620483398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.3214], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.11591081321239471
epoch£º992	 i:1 	 global-step:19841	 l-p:0.15316417813301086
epoch£º992	 i:2 	 global-step:19842	 l-p:0.13270822167396545
epoch£º992	 i:3 	 global-step:19843	 l-p:0.1075601652264595
epoch£º992	 i:4 	 global-step:19844	 l-p:0.16427960991859436
epoch£º992	 i:5 	 global-step:19845	 l-p:0.10411945730447769
epoch£º992	 i:6 	 global-step:19846	 l-p:0.1617821604013443
epoch£º992	 i:7 	 global-step:19847	 l-p:0.12971869111061096
epoch£º992	 i:8 	 global-step:19848	 l-p:0.11000896990299225
epoch£º992	 i:9 	 global-step:19849	 l-p:0.14086750149726868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.2640, 5.0122],
        [5.1563, 4.8764, 4.6849],
        [5.1563, 5.1563, 5.1563],
        [5.1563, 5.1453, 5.1554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.2535081207752228 
model_pd.l_d.mean(): -20.501449584960938 
model_pd.lagr.mean(): -20.247941970825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-21.2054], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.2535081207752228
epoch£º993	 i:1 	 global-step:19861	 l-p:0.1509755402803421
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12219611555337906
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15201234817504883
epoch£º993	 i:4 	 global-step:19864	 l-p:0.09708309173583984
epoch£º993	 i:5 	 global-step:19865	 l-p:0.09871882945299149
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12217866629362106
epoch£º993	 i:7 	 global-step:19867	 l-p:0.13128499686717987
epoch£º993	 i:8 	 global-step:19868	 l-p:0.13792742788791656
epoch£º993	 i:9 	 global-step:19869	 l-p:0.13111840188503265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.1204, 5.1251],
        [5.1253, 5.1537, 4.8657],
        [5.1253, 4.8882, 4.5888],
        [5.1253, 5.1252, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13312970101833344 
model_pd.l_d.mean(): -20.369905471801758 
model_pd.lagr.mean(): -20.23677635192871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4761], device='cuda:0')), ('power', tensor([-21.0789], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13312970101833344
epoch£º994	 i:1 	 global-step:19881	 l-p:0.11249137669801712
epoch£º994	 i:2 	 global-step:19882	 l-p:0.1169758215546608
epoch£º994	 i:3 	 global-step:19883	 l-p:0.1472531110048294
epoch£º994	 i:4 	 global-step:19884	 l-p:0.12516480684280396
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13192713260650635
epoch£º994	 i:6 	 global-step:19886	 l-p:0.6859281063079834
epoch£º994	 i:7 	 global-step:19887	 l-p:0.20219925045967102
epoch£º994	 i:8 	 global-step:19888	 l-p:0.18839068710803986
epoch£º994	 i:9 	 global-step:19889	 l-p:0.17593331634998322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1030, 4.9830, 5.0419],
        [5.1030, 4.9048, 4.5803],
        [5.1030, 4.9057, 4.9381],
        [5.1030, 5.1009, 5.1030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.17684249579906464 
model_pd.l_d.mean(): -20.72767448425293 
model_pd.lagr.mean(): -20.550832748413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4511], device='cuda:0')), ('power', tensor([-21.4150], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.17684249579906464
epoch£º995	 i:1 	 global-step:19901	 l-p:0.1131112203001976
epoch£º995	 i:2 	 global-step:19902	 l-p:0.1279730349779129
epoch£º995	 i:3 	 global-step:19903	 l-p:0.2717559337615967
epoch£º995	 i:4 	 global-step:19904	 l-p:0.1568690836429596
epoch£º995	 i:5 	 global-step:19905	 l-p:0.30853271484375
epoch£º995	 i:6 	 global-step:19906	 l-p:0.11969860643148422
epoch£º995	 i:7 	 global-step:19907	 l-p:0.0868445411324501
epoch£º995	 i:8 	 global-step:19908	 l-p:0.15488643944263458
epoch£º995	 i:9 	 global-step:19909	 l-p:0.12351037561893463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 4.9004, 4.9091],
        [5.1212, 5.4361, 5.3024],
        [5.1212, 4.8365, 4.6482],
        [5.1212, 5.1205, 5.1212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.1140020340681076 
model_pd.l_d.mean(): -18.665882110595703 
model_pd.lagr.mean(): -18.5518798828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5988], device='cuda:0')), ('power', tensor([-19.4816], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.1140020340681076
epoch£º996	 i:1 	 global-step:19921	 l-p:0.2557225823402405
epoch£º996	 i:2 	 global-step:19922	 l-p:0.14163251221179962
epoch£º996	 i:3 	 global-step:19923	 l-p:0.1502133309841156
epoch£º996	 i:4 	 global-step:19924	 l-p:0.17533397674560547
epoch£º996	 i:5 	 global-step:19925	 l-p:0.14366082847118378
epoch£º996	 i:6 	 global-step:19926	 l-p:0.12901394069194794
epoch£º996	 i:7 	 global-step:19927	 l-p:0.1399831622838974
epoch£º996	 i:8 	 global-step:19928	 l-p:0.12813399732112885
epoch£º996	 i:9 	 global-step:19929	 l-p:0.1192803755402565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1444, 5.0666, 5.1170],
        [5.1444, 5.1444, 5.1444],
        [5.1444, 4.8647, 4.7312],
        [5.1444, 5.2177, 4.9492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.15237392485141754 
model_pd.l_d.mean(): -19.93825340270996 
model_pd.lagr.mean(): -19.785879135131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5411], device='cuda:0')), ('power', tensor([-20.7089], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.15237392485141754
epoch£º997	 i:1 	 global-step:19941	 l-p:0.14017774164676666
epoch£º997	 i:2 	 global-step:19942	 l-p:0.11597058922052383
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11022702604532242
epoch£º997	 i:4 	 global-step:19944	 l-p:0.1699572652578354
epoch£º997	 i:5 	 global-step:19945	 l-p:0.19075579941272736
epoch£º997	 i:6 	 global-step:19946	 l-p:0.14224494993686676
epoch£º997	 i:7 	 global-step:19947	 l-p:0.13105756044387817
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10948705673217773
epoch£º997	 i:9 	 global-step:19949	 l-p:0.19566883146762848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 4.9580, 4.6309],
        [5.1351, 5.1335, 5.1351],
        [5.1351, 5.1351, 5.1351],
        [5.1351, 5.0972, 5.1275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.13573670387268066 
model_pd.l_d.mean(): -21.03047752380371 
model_pd.lagr.mean(): -20.89474105834961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3795], device='cuda:0')), ('power', tensor([-21.6480], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.13573670387268066
epoch£º998	 i:1 	 global-step:19961	 l-p:0.0819120779633522
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15664568543434143
epoch£º998	 i:3 	 global-step:19963	 l-p:0.20977218449115753
epoch£º998	 i:4 	 global-step:19964	 l-p:0.12771902978420258
epoch£º998	 i:5 	 global-step:19965	 l-p:0.14680400490760803
epoch£º998	 i:6 	 global-step:19966	 l-p:0.16149795055389404
epoch£º998	 i:7 	 global-step:19967	 l-p:0.10397982597351074
epoch£º998	 i:8 	 global-step:19968	 l-p:0.16561561822891235
epoch£º998	 i:9 	 global-step:19969	 l-p:0.14833445847034454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1484, 5.1482, 5.1484],
        [5.1484, 4.9497, 4.9789],
        [5.1484, 5.1455, 5.1483],
        [5.1484, 5.1439, 5.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.1512310951948166 
model_pd.l_d.mean(): -18.050752639770508 
model_pd.lagr.mean(): -17.899520874023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6051], device='cuda:0')), ('power', tensor([-18.8662], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.1512310951948166
epoch£º999	 i:1 	 global-step:19981	 l-p:0.09968877583742142
epoch£º999	 i:2 	 global-step:19982	 l-p:0.17846927046775818
epoch£º999	 i:3 	 global-step:19983	 l-p:0.12597741186618805
epoch£º999	 i:4 	 global-step:19984	 l-p:0.12182164192199707
epoch£º999	 i:5 	 global-step:19985	 l-p:0.14677858352661133
epoch£º999	 i:6 	 global-step:19986	 l-p:0.12999659776687622
epoch£º999	 i:7 	 global-step:19987	 l-p:0.12697310745716095
epoch£º999	 i:8 	 global-step:19988	 l-p:0.17863072454929352
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11471360176801682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.0696, 5.1229],
        [5.1565, 4.9306, 4.6258],
        [5.1565, 5.0643, 5.1190],
        [5.1565, 5.1935, 4.9084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.12321574985980988 
model_pd.l_d.mean(): -20.52121925354004 
model_pd.lagr.mean(): -20.39800262451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415], device='cuda:0')), ('power', tensor([-21.1965], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.12321574985980988
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12431547045707703
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.13589416444301605
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.16422483325004578
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.13586963713169098
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.13137546181678772
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.1389438658952713
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.09409870952367783
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.21606868505477905
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.14371506869792938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1422, 5.1411, 5.1422],
        [5.1422, 4.8915, 4.8538],
        [5.1422, 4.9791, 5.0300],
        [5.1422, 5.1422, 5.1422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.10549488663673401 
model_pd.l_d.mean(): -19.465757369995117 
model_pd.lagr.mean(): -19.360261917114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-20.1714], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.10549488663673401
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.19533191621303558
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.1423352062702179
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.21553321182727814
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.08859546482563019
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.0851692259311676
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.13134057819843292
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.17301613092422485
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.19084326922893524
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.1084425300359726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1371, 5.1155, 5.1342],
        [5.1371, 4.9160, 4.9240],
        [5.1371, 5.5070, 5.4078],
        [5.1371, 5.0034, 5.0614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.10591167211532593 
model_pd.l_d.mean(): -20.509906768798828 
model_pd.lagr.mean(): -20.403995513916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4563], device='cuda:0')), ('power', tensor([-21.2002], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.10591167211532593
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.22131477296352386
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.13155865669250488
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.11204911023378372
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.10753092169761658
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.20946669578552246
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.14947034418582916
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.13314981758594513
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.14082810282707214
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.13002578914165497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1427, 4.9741, 4.6456],
        [5.1427, 5.1427, 5.1427],
        [5.1427, 5.1427, 5.1427],
        [5.1427, 5.1006, 5.1336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.1271778792142868 
model_pd.l_d.mean(): -19.74799156188965 
model_pd.lagr.mean(): -19.620813369750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5527], device='cuda:0')), ('power', tensor([-20.5284], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.1271778792142868
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.1454763263463974
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.15960603952407837
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.20860092341899872
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.12842608988285065
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.14404010772705078
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.11575164645910263
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.10939788818359375
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.14014866948127747
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.15451428294181824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1323, 4.8486, 4.6475],
        [5.1323, 5.1244, 4.8215],
        [5.1323, 4.8876, 4.8625],
        [5.1323, 5.1320, 5.1323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.17747622728347778 
model_pd.l_d.mean(): -20.308225631713867 
model_pd.lagr.mean(): -20.130748748779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5053], device='cuda:0')), ('power', tensor([-21.0464], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.17747622728347778
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.17440547049045563
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.13585910201072693
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.13681523501873016
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.13090118765830994
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.12150876969099045
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.10640760511159897
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.1423092931509018
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.16432251036167145
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.23657473921775818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 5.0105, 5.0689],
        [5.1237, 4.8766, 4.8484],
        [5.1237, 5.1223, 5.1237],
        [5.1237, 4.9520, 4.9998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.12901173532009125 
model_pd.l_d.mean(): -20.522001266479492 
model_pd.lagr.mean(): -20.392990112304688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4375], device='cuda:0')), ('power', tensor([-21.1932], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.12901173532009125
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11534307152032852
epoch£º1005	 i:2 	 global-step:20102	 l-p:0.1453929841518402
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.18651993572711945
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.09918449074029922
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.20571467280387878
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.18683940172195435
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.15211883187294006
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.1095268726348877
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.22749042510986328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[5.1226, 5.4135, 5.2642],
        [5.1226, 4.9497, 4.6197],
        [5.1226, 5.2655, 5.0312],
        [5.1226, 5.2082, 4.9451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.16328772902488708 
model_pd.l_d.mean(): -19.1328182220459 
model_pd.lagr.mean(): -18.96953010559082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6092], device='cuda:0')), ('power', tensor([-19.9643], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.16328772902488708
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.10713671147823334
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.16805998980998993
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.10651963949203491
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.1691897213459015
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.17291660606861115
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.12179944664239883
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.1477692723274231
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.14923295378684998
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.19522295892238617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1339, 5.0785, 5.1191],
        [5.1339, 5.1332, 5.1339],
        [5.1339, 5.1339, 5.1339],
        [5.1339, 5.1120, 5.1309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.14844577014446259 
model_pd.l_d.mean(): -20.030569076538086 
model_pd.lagr.mean(): -19.882123947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5355], device='cuda:0')), ('power', tensor([-20.7966], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.14844577014446259
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.23183345794677734
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.1300809681415558
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11793222278356552
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.14366063475608826
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.1161288395524025
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.1650714874267578
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.11856231093406677
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13310380280017853
epoch£º1007	 i:9 	 global-step:20149	 l-p:0.15134812891483307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1390, 4.9425, 4.9745],
        [5.1390, 5.1051, 5.1328],
        [5.1390, 4.9479, 4.6236],
        [5.1390, 4.8910, 4.6020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.09245053678750992 
model_pd.l_d.mean(): -20.674137115478516 
model_pd.lagr.mean(): -20.58168601989746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-21.3693], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.09245053678750992
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.1776939332485199
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.10606900602579117
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.18347467482089996
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.1538086235523224
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.1771388202905655
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.10573026537895203
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.1406743973493576
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13976985216140747
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.1429966539144516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1512, 5.1541],
        [5.1542, 5.1541, 5.1542],
        [5.1542, 4.9658, 5.0030],
        [5.1542, 5.0339, 5.0923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.17798934876918793 
model_pd.l_d.mean(): -20.76319122314453 
model_pd.lagr.mean(): -20.585201263427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.4312], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.17798934876918793
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.11765706539154053
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.15235787630081177
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.10642197728157043
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.18119676411151886
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11428359895944595
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13793021440505981
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12307488918304443
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.08674220740795135
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.16167694330215454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.0097, 4.6808],
        [5.1625, 5.1624, 5.1625],
        [5.1625, 5.1625, 5.1625],
        [5.1625, 5.1407, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12488200515508652 
model_pd.l_d.mean(): -20.133825302124023 
model_pd.lagr.mean(): -20.008943557739258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4873], device='cuda:0')), ('power', tensor([-20.8516], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12488200515508652
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.15219947695732117
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.06897880136966705
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.18467652797698975
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.18977920711040497
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12482728809118271
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.11391183733940125
epoch£º1010	 i:7 	 global-step:20207	 l-p:0.11387515068054199
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.0975194126367569
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.18567030131816864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.2314, 4.9629],
        [5.1565, 5.2614, 5.0073],
        [5.1565, 5.2514, 4.9923],
        [5.1565, 5.1560, 5.1565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.21872223913669586 
model_pd.l_d.mean(): -18.41876983642578 
model_pd.lagr.mean(): -18.200048446655273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6154], device='cuda:0')), ('power', tensor([-19.2488], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.21872223913669586
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.09231775254011154
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.14577911794185638
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.1172119677066803
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.1041453555226326
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.14739929139614105
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.13368192315101624
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.14110398292541504
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12448278069496155
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.13991379737854004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1571, 5.0029, 5.0566],
        [5.1571, 5.1525, 5.1569],
        [5.1571, 4.8781, 4.6627],
        [5.1571, 5.0415, 5.0997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.13138800859451294 
model_pd.l_d.mean(): -20.75768280029297 
model_pd.lagr.mean(): -20.62629508972168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4223], device='cuda:0')), ('power', tensor([-21.4159], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.13138800859451294
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.14343877136707306
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.18472261726856232
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.18713045120239258
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.1246144101023674
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.11319282650947571
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.1369478702545166
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14737962186336517
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.1317310780286789
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.08508123457431793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1483, 5.0379, 4.7089],
        [5.1483, 5.1483, 5.1483],
        [5.1483, 4.8650, 4.7048],
        [5.1483, 4.8658, 4.6672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.14830398559570312 
model_pd.l_d.mean(): -20.617340087890625 
model_pd.lagr.mean(): -20.469036102294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4574], device='cuda:0')), ('power', tensor([-21.3099], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.14830398559570312
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.1328331083059311
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12554393708705902
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.22758212685585022
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.1084136962890625
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.11816060543060303
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.10369377583265305
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.15341058373451233
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.16823041439056396
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.11695212125778198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1422, 5.5790, 5.5240],
        [5.1422, 5.1418, 5.1422],
        [5.1422, 5.1421, 5.1422],
        [5.1422, 4.8584, 4.6661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.14290055632591248 
model_pd.l_d.mean(): -19.903711318969727 
model_pd.lagr.mean(): -19.76081085205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4859], device='cuda:0')), ('power', tensor([-20.6176], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.14290055632591248
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.20290446281433105
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.11741559952497482
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.09877916425466537
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.1272994428873062
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.2010287046432495
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12373717129230499
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.2024100422859192
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.0984048917889595
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.15888488292694092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1251, 4.9697, 4.6376],
        [5.1251, 5.1191, 5.1248],
        [5.1251, 5.1232, 5.1251],
        [5.1251, 5.4947, 5.3952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.15484696626663208 
model_pd.l_d.mean(): -20.197904586791992 
model_pd.lagr.mean(): -20.043058395385742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5143], device='cuda:0')), ('power', tensor([-20.9440], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.15484696626663208
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.1395554095506668
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.12187172472476959
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.1574101746082306
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.10351100564002991
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.23598304390907288
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.16276271641254425
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.15283775329589844
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.30882492661476135
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.1233113631606102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1119, 5.1115, 5.1119],
        [5.1119, 5.1119, 5.1119],
        [5.1119, 5.0560, 5.0970],
        [5.1119, 5.1103, 5.1119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.14532095193862915 
model_pd.l_d.mean(): -19.086458206176758 
model_pd.lagr.mean(): -18.941137313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4964], device='cuda:0')), ('power', tensor([-19.8021], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.14532095193862915
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.25371024012565613
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.10904859006404877
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.14518211781978607
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.10480257123708725
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.10660131275653839
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.3039407730102539
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.15746235847473145
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.2127530574798584
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.10497675836086273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1157, 5.1157, 5.1157],
        [5.1157, 5.0242, 5.0791],
        [5.1157, 4.8822, 4.8773],
        [5.1157, 5.1157, 5.1157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.1164347231388092 
model_pd.l_d.mean(): -20.147632598876953 
model_pd.lagr.mean(): -20.031198501586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5054], device='cuda:0')), ('power', tensor([-20.8841], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.1164347231388092
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.1303948163986206
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.1307438164949417
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.35075193643569946
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.1474810391664505
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.11782298982143402
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.13072055578231812
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.17645791172981262
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.21010734140872955
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.16285490989685059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1100, 5.1053, 5.1097],
        [5.1100, 5.0279, 5.0801],
        [5.1100, 5.0801, 4.7686],
        [5.1100, 4.8340, 4.7403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.12161177396774292 
model_pd.l_d.mean(): -19.654644012451172 
model_pd.lagr.mean(): -19.533031463623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5113], device='cuda:0')), ('power', tensor([-20.3918], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.12161177396774292
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.44960275292396545
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.14514243602752686
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.17055276036262512
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.13426880538463593
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.09755248576402664
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.19517846405506134
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.13866935670375824
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.12442869693040848
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.12901100516319275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1157, 5.1050, 5.1148],
        [5.1157, 4.8503, 4.7861],
        [5.1157, 5.1383, 4.8466],
        [5.1157, 5.5146, 5.4345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13999152183532715 
model_pd.l_d.mean(): -20.547119140625 
model_pd.lagr.mean(): -20.407127380371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-21.2183], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13999152183532715
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.20076049864292145
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.20086577534675598
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.14176808297634125
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.13300715386867523
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.2649405598640442
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.12248819321393967
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.17996720969676971
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.09063740819692612
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.11880847811698914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1229, 4.8585, 4.5865],
        [5.1229, 5.0327, 5.0872],
        [5.1229, 4.8364, 4.6439],
        [5.1229, 4.9857, 4.6528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.13758805394172668 
model_pd.l_d.mean(): -20.48261833190918 
model_pd.lagr.mean(): -20.345029830932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-21.1847], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.13758805394172668
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.23540234565734863
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.1682262122631073
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.257843017578125
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.10099058598279953
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.07388724386692047
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.1281379908323288
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.16915030777454376
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.12746845185756683
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.13142189383506775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1290, 4.8443, 4.6979],
        [5.1290, 5.1290, 5.1290],
        [5.1290, 4.8552, 4.6045],
        [5.1290, 4.8710, 4.8221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.129514679312706 
model_pd.l_d.mean(): -20.74632453918457 
model_pd.lagr.mean(): -20.616809844970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-21.4224], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.129514679312706
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.1239042654633522
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.10956740379333496
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.23021170496940613
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12903068959712982
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.14630191028118134
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.21472430229187012
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.2355017364025116
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.10484185814857483
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.1452779769897461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1169, 5.1168, 5.1169],
        [5.1169, 4.8514, 4.5792],
        [5.1169, 4.9151, 4.9444],
        [5.1169, 5.1169, 5.1169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.14236821234226227 
model_pd.l_d.mean(): -20.695064544677734 
model_pd.lagr.mean(): -20.552696228027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4378], device='cuda:0')), ('power', tensor([-21.3685], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.14236821234226227
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.1646694391965866
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.17382147908210754
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.13510142266750336
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.11359100043773651
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.11071470379829407
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.20499272644519806
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.15017014741897583
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.32974544167518616
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.11894694715738297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1162, 5.1155, 5.1162],
        [5.1162, 4.8367, 4.5965],
        [5.1162, 5.1162, 5.1162],
        [5.1162, 4.9491, 4.9997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.14592985808849335 
model_pd.l_d.mean(): -21.06719970703125 
model_pd.lagr.mean(): -20.9212703704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3754], device='cuda:0')), ('power', tensor([-21.6809], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.14592985808849335
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.2626703083515167
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11927983909845352
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.30927881598472595
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.1791212111711502
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.13510257005691528
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12238267809152603
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.0888240709900856
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.12316368520259857
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.13690507411956787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1203, 5.1203, 5.1203],
        [5.1203, 5.0923, 4.7814],
        [5.1203, 5.0671, 4.7484],
        [5.1203, 5.1203, 5.1203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.21164366602897644 
model_pd.l_d.mean(): -19.719871520996094 
model_pd.lagr.mean(): -19.508228302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5443], device='cuda:0')), ('power', tensor([-20.4914], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.21164366602897644
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.13828808069229126
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.11677419394254684
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.26129433512687683
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12937723100185394
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.09521423280239105
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.18006721138954163
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.12532874941825867
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.12173575162887573
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.17650873959064484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1263, 5.1249, 5.1263],
        [5.1263, 5.1210, 5.1260],
        [5.1263, 5.1263, 5.1263],
        [5.1263, 5.1255, 5.1263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.21121878921985626 
model_pd.l_d.mean(): -20.743450164794922 
model_pd.lagr.mean(): -20.5322322845459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.3971], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.21121878921985626
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.1514669507741928
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13456498086452484
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.1297081559896469
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.17435777187347412
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.1282651275396347
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.12703363597393036
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.12029487639665604
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.174141988158226
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.16258785128593445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[5.1318, 5.5357, 5.4583],
        [5.1318, 4.8477, 4.6363],
        [5.1318, 5.1640, 4.8760],
        [5.1318, 4.8744, 4.5928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.0794445276260376 
model_pd.l_d.mean(): -19.943531036376953 
model_pd.lagr.mean(): -19.864086151123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-20.7123], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.0794445276260376
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.13043305277824402
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.16537593305110931
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.2125113606452942
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.15848758816719055
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.1430833339691162
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.14398618042469025
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.1569650024175644
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.14051617681980133
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.15676423907279968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1343, 4.8481, 4.6734],
        [5.1343, 4.9741, 4.6422],
        [5.1343, 5.0451, 5.0993],
        [5.1343, 4.8760, 4.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.13818302750587463 
model_pd.l_d.mean(): -19.98057746887207 
model_pd.lagr.mean(): -19.84239387512207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-20.7573], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.13818302750587463
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.1357557624578476
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.15395289659500122
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.13945476710796356
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.09840679168701172
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.24226932227611542
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.1618945598602295
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.12666618824005127
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.13236604630947113
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.1439054012298584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.1352, 5.1359],
        [5.1359, 5.1359, 5.1359],
        [5.1359, 5.1314, 5.1357],
        [5.1359, 5.1142, 5.1330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.1814398169517517 
model_pd.l_d.mean(): -20.980527877807617 
model_pd.lagr.mean(): -20.799087524414062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3793], device='cuda:0')), ('power', tensor([-21.5973], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.1814398169517517
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.18908320367336273
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.11270608007907867
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.14976130425930023
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.20277664065361023
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.15733394026756287
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.10584835708141327
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.1292164921760559
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.10715092718601227
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.11594562977552414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1460, 4.8865, 4.6111],
        [5.1460, 5.1460, 5.1460],
        [5.1460, 5.1460, 5.1460],
        [5.1460, 4.9374, 4.9593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1301124393939972 
model_pd.l_d.mean(): -20.836631774902344 
model_pd.lagr.mean(): -20.706520080566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4027], device='cuda:0')), ('power', tensor([-21.4758], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1301124393939972
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.1284637302160263
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.08739759027957916
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.1654471606016159
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.25134679675102234
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.1069968119263649
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.12407263368368149
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.13219104707241058
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.10612291842699051
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.20913852751255035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1387, 5.1259, 5.1375],
        [5.1387, 5.1015, 5.1314],
        [5.1387, 5.2237, 4.9593],
        [5.1387, 4.9116, 4.9136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.15642061829566956 
model_pd.l_d.mean(): -20.254182815551758 
model_pd.lagr.mean(): -20.097763061523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4901], device='cuda:0')), ('power', tensor([-20.9762], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.15642061829566956
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.13003146648406982
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.168319433927536
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.09992562979459763
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.11060187965631485
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.16689515113830566
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12750662863254547
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.25224894285202026
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.08383891731500626
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.1679261326789856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1369, 5.4001, 5.2331],
        [5.1369, 4.9977, 4.6651],
        [5.1369, 4.8664, 4.7855],
        [5.1369, 5.1149, 5.1339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.1307883858680725 
model_pd.l_d.mean(): -19.92674446105957 
model_pd.lagr.mean(): -19.795955657958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.6540], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.1307883858680725
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.1651904582977295
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.14481917023658752
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.17882220447063446
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12522682547569275
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.1309053748846054
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.13201436400413513
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.08849319070577621
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.10411901026964188
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.2455107420682907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1420, 4.9047, 4.8925],
        [5.1420, 5.0660, 5.1159],
        [5.1420, 5.1416, 5.1420],
        [5.1420, 5.0005, 5.0578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.18450875580310822 
model_pd.l_d.mean(): -20.243181228637695 
model_pd.lagr.mean(): -20.058671951293945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-20.9501], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.18450875580310822
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.13140252232551575
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17557361721992493
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.1489221304655075
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.1292140632867813
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.14667381346225739
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.1052795797586441
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.17123925685882568
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.08021944016218185
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.13533683121204376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 5.1549, 5.1550],
        [5.1550, 4.8711, 4.7106],
        [5.1550, 4.9391, 4.9532],
        [5.1550, 5.1538, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.07792464643716812 
model_pd.l_d.mean(): -20.55477523803711 
model_pd.lagr.mean(): -20.476850509643555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-21.2505], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.07792464643716812
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.14027507603168488
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.20479191839694977
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.16167998313903809
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.13165320456027985
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.15297934412956238
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12979042530059814
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12389855831861496
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.15461958944797516
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.10153040289878845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1551, 4.9130, 4.6177],
        [5.1551, 5.1441, 5.1542],
        [5.1551, 5.0997, 5.1403],
        [5.1551, 5.1522, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.22804860770702362 
model_pd.l_d.mean(): -20.912508010864258 
model_pd.lagr.mean(): -20.684459686279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3878], device='cuda:0')), ('power', tensor([-21.5372], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.22804860770702362
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.13764294981956482
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.11517444252967834
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.13981139659881592
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.20179328322410583
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.1799243688583374
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.06222626566886902
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.10492251813411713
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.08465909957885742
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.11494361609220505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.1502, 5.1620],
        [5.1632, 5.1544, 5.1626],
        [5.1632, 4.9036, 4.6320],
        [5.1632, 5.1447, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.08638855069875717 
model_pd.l_d.mean(): -19.692001342773438 
model_pd.lagr.mean(): -19.605613708496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4853], device='cuda:0')), ('power', tensor([-20.4030], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.08638855069875717
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.14961190521717072
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.1737174540758133
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.11662259697914124
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.10016608983278275
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.13944567739963531
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.13994836807250977
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.12576177716255188
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.1968170553445816
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11665438115596771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.1373, 5.1576],
        [5.1609, 5.6026, 5.5497],
        [5.1609, 4.9705, 5.0068],
        [5.1609, 4.9877, 5.0343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.1435518115758896 
model_pd.l_d.mean(): -19.426000595092773 
model_pd.lagr.mean(): -19.28244972229004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5493], device='cuda:0')), ('power', tensor([-20.1994], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.1435518115758896
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.15370923280715942
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.1692422777414322
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.10113644599914551
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.1149856373667717
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.08824001252651215
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.16302832961082458
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.1526796817779541
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.13047650456428528
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.15570178627967834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.1363, 5.1542],
        [5.1568, 4.9784, 5.0225],
        [5.1568, 4.8729, 4.7147],
        [5.1568, 5.1527, 5.1566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.12428174912929535 
model_pd.l_d.mean(): -18.977333068847656 
model_pd.lagr.mean(): -18.853052139282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5501], device='cuda:0')), ('power', tensor([-19.7467], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.12428174912929535
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.16522258520126343
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.15621519088745117
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.07117653638124466
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.14888080954551697
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.11921470612287521
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.13907265663146973
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.17428500950336456
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12787079811096191
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.1320873647928238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 4.9154, 4.8811],
        [5.1644, 5.0611, 4.7324],
        [5.1644, 4.9202, 4.8947],
        [5.1644, 5.1644, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.13945111632347107 
model_pd.l_d.mean(): -19.81002426147461 
model_pd.lagr.mean(): -19.67057228088379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4405], device='cuda:0')), ('power', tensor([-20.4765], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.13945111632347107
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.09152102470397949
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.12372424453496933
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.19842632114887238
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.08164218068122864
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.17549514770507812
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.15694096684455872
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.10959281027317047
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.16799315810203552
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.12007971853017807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1588, 5.0688, 5.1231],
        [5.1588, 4.9941, 5.0447],
        [5.1588, 5.1047, 5.1446],
        [5.1588, 4.9704, 4.6451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.146812304854393 
model_pd.l_d.mean(): -20.596126556396484 
model_pd.lagr.mean(): -20.44931411743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4641], device='cuda:0')), ('power', tensor([-21.2953], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.146812304854393
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.12066568434238434
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.12052597105503082
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11572835594415665
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.13033269345760345
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12691988050937653
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.15538358688354492
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12312919646501541
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.2856639325618744
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.09884382784366608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1400, 5.1400, 5.1400],
        [5.1400, 5.1053, 5.1335],
        [5.1400, 4.8662, 4.6150],
        [5.1400, 5.1369, 5.1398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.09280816465616226 
model_pd.l_d.mean(): -20.112743377685547 
model_pd.lagr.mean(): -20.019935607910156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5292], device='cuda:0')), ('power', tensor([-20.8731], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.09280816465616226
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13302689790725708
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.15297643840312958
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.14350475370883942
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.11314110457897186
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.18457116186618805
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.17422863841056824
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.18397893011569977
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.13858729600906372
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.14945444464683533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1397, 5.1396, 5.1397],
        [5.1397, 4.8864, 4.8478],
        [5.1397, 5.1354, 5.1395],
        [5.1397, 5.1160, 5.1363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11877567321062088 
model_pd.l_d.mean(): -18.894227981567383 
model_pd.lagr.mean(): -18.77545166015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5571], device='cuda:0')), ('power', tensor([-19.6699], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11877567321062088
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.13480383157730103
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.11269346624612808
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.11992412060499191
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.15667547285556793
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.20921725034713745
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.13143832981586456
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.20731112360954285
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.16284236311912537
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.06917475163936615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1511, 5.1511, 5.1511],
        [5.1511, 4.9793, 5.0270],
        [5.1511, 5.1510, 5.1511],
        [5.1511, 4.9572, 4.6324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.1508147269487381 
model_pd.l_d.mean(): -20.42583465576172 
model_pd.lagr.mean(): -20.275020599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4583], device='cuda:0')), ('power', tensor([-21.1173], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.1508147269487381
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.12181154638528824
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.13387712836265564
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11782006174325943
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.15207204222679138
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.09832849353551865
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.11462366580963135
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.15816304087638855
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.17960429191589355
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13486739993095398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 5.1687, 5.1687],
        [5.1687, 5.0824, 5.1357],
        [5.1687, 5.5104, 5.3909],
        [5.1687, 4.9389, 4.9355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.1696932017803192 
model_pd.l_d.mean(): -20.596439361572266 
model_pd.lagr.mean(): -20.426746368408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.2797], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.1696932017803192
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11322663724422455
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.16434289515018463
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.13530363142490387
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13495808839797974
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.11231552064418793
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.06747745722532272
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.16010141372680664
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11433073878288269
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.14753806591033936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.1715, 5.1726],
        [5.1726, 5.1724, 5.1726],
        [5.1726, 5.1169, 5.1577],
        [5.1726, 5.0877, 4.7622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.19742052257061005 
model_pd.l_d.mean(): -20.31035804748535 
model_pd.lagr.mean(): -20.112937927246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4594], device='cuda:0')), ('power', tensor([-21.0016], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.19742052257061005
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14597728848457336
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.04035725072026253
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.13435332477092743
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12403380870819092
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.1332695037126541
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.17388935387134552
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.11308259516954422
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.1066204234957695
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.1487201750278473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1689, 5.4739, 5.3313],
        [5.1689, 5.0782, 5.1327],
        [5.1689, 5.1629, 5.1685],
        [5.1689, 4.8908, 4.6650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.15123121440410614 
model_pd.l_d.mean(): -19.025930404663086 
model_pd.lagr.mean(): -18.874698638916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5331], device='cuda:0')), ('power', tensor([-19.7785], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.15123121440410614
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.17086859047412872
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.11971001327037811
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.19985945522785187
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11351163685321808
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.14524193108081818
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.08441758900880814
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.1383107602596283
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.09140107780694962
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.11270814388990402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1708, 4.8950, 4.7872],
        [5.1708, 5.1701, 5.1708],
        [5.1708, 5.0033, 4.6743],
        [5.1708, 5.1708, 5.1708]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.10093724727630615 
model_pd.l_d.mean(): -20.341745376586914 
model_pd.lagr.mean(): -20.240808486938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4657], device='cuda:0')), ('power', tensor([-21.0398], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.10093724727630615
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.11591079086065292
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.09098942577838898
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.15976868569850922
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.12427756190299988
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.1154189184308052
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.1403559148311615
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1542803794145584
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.21045154333114624
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.1293787956237793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.0144, 5.0706],
        [5.1611, 5.1348, 5.1571],
        [5.1611, 4.8770, 4.7187],
        [5.1611, 5.0925, 5.1394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.10847131162881851 
model_pd.l_d.mean(): -19.974748611450195 
model_pd.lagr.mean(): -19.86627769470215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4656], device='cuda:0')), ('power', tensor([-20.6687], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.10847131162881851
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.20075076818466187
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.1300421804189682
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.14206035435199738
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.11302138864994049
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.20307707786560059
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.11673431098461151
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.14489132165908813
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.09887713193893433
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.1139858216047287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 5.3091, 5.0779],
        [5.1567, 4.9774, 5.0213],
        [5.1567, 5.1567, 5.1567],
        [5.1567, 4.8899, 4.6269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.08614704757928848 
model_pd.l_d.mean(): -19.88274383544922 
model_pd.lagr.mean(): -19.79659652709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5216], device='cuda:0')), ('power', tensor([-20.6329], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.08614704757928848
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.08264964818954468
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14694496989250183
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.13152682781219482
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.12034808844327927
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.23168782889842987
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11358310282230377
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.19948986172676086
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.1260678470134735
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.15816478431224823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.0738, 5.1235],
        [5.1490, 4.9742, 4.6440],
        [5.1490, 5.5320, 5.4395],
        [5.1490, 4.9576, 4.9943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.1286342442035675 
model_pd.l_d.mean(): -19.475404739379883 
model_pd.lagr.mean(): -19.346771240234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-20.2209], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.1286342442035675
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.11869347095489502
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.12867583334445953
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.1577492356300354
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.18408195674419403
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.108124740421772
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.15067246556282043
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.1558200567960739
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.13316144049167633
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.14171932637691498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1505, 4.8799, 4.7986],
        [5.1505, 5.1052, 5.1402],
        [5.1505, 5.1491, 5.1505],
        [5.1505, 5.1132, 5.1431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.13264308869838715 
model_pd.l_d.mean(): -20.877822875976562 
model_pd.lagr.mean(): -20.745180130004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4152], device='cuda:0')), ('power', tensor([-21.5302], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.13264308869838715
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.11087684333324432
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11337103694677353
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.1474205106496811
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.1491701304912567
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.11573104560375214
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.22748756408691406
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.1577829122543335
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.12087561935186386
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.17746181786060333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1375, 4.8499, 4.6752],
        [5.1375, 5.1364, 5.1375],
        [5.1375, 4.8501, 4.6606],
        [5.1375, 4.8535, 4.7213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.15717342495918274 
model_pd.l_d.mean(): -19.059314727783203 
model_pd.lagr.mean(): -18.902141571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5575], device='cuda:0')), ('power', tensor([-19.8371], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.15717342495918274
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.15769624710083008
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.2095373123884201
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.10730260610580444
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.14835688471794128
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.09131745994091034
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.12664714455604553
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.12515752017498016
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.1451493501663208
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.18024882674217224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1463, 5.1463, 5.1463],
        [5.1463, 5.1463, 5.1463],
        [5.1463, 5.0079, 5.0660],
        [5.1463, 4.9001, 4.8750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.0998968854546547 
model_pd.l_d.mean(): -20.677202224731445 
model_pd.lagr.mean(): -20.57730484008789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4312], device='cuda:0')), ('power', tensor([-21.3437], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.0998968854546547
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.14102165400981903
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.1398056596517563
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13440227508544922
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.19322602450847626
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.09950868040323257
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.08429817110300064
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.23262400925159454
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.13061730563640594
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.1373744010925293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[5.1586, 4.9372, 4.6242],
        [5.1586, 5.4425, 5.2867],
        [5.1586, 4.9006, 4.8506],
        [5.1586, 4.9574, 4.9864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.15863890945911407 
model_pd.l_d.mean(): -20.53818702697754 
model_pd.lagr.mean(): -20.379549026489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4610], device='cuda:0')), ('power', tensor([-21.2336], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.15863890945911407
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.11776304244995117
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.17490564286708832
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.14110200107097626
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.10622706264257431
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.11145858466625214
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.11540471762418747
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.150802880525589
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.16851435601711273
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.1198057159781456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1631, 5.1601, 5.1630],
        [5.1631, 5.1631, 5.1631],
        [5.1631, 5.1627, 5.1631],
        [5.1631, 5.1589, 5.1629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.12008102238178253 
model_pd.l_d.mean(): -17.693796157836914 
model_pd.lagr.mean(): -17.573715209960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6184], device='cuda:0')), ('power', tensor([-18.5189], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.12008102238178253
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12041747570037842
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.1446397751569748
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.12099842727184296
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.1915096938610077
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.09366575628519058
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.14080026745796204
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.1406106948852539
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.171934574842453
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.09646522998809814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.1639, 5.1694],
        [5.1697, 5.1230, 5.1588],
        [5.1697, 4.8908, 4.6646],
        [5.1697, 5.0035, 5.0535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.17534026503562927 
model_pd.l_d.mean(): -19.480792999267578 
model_pd.lagr.mean(): -19.305452346801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5829], device='cuda:0')), ('power', tensor([-20.2892], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.17534026503562927
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.06294120848178864
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.13917317986488342
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.1623918116092682
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.14805640280246735
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13715825974941254
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.11272159218788147
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.08173637092113495
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.15995733439922333
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13213764131069183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1830, 5.1830, 5.1830],
        [5.1830, 5.1825, 5.1830],
        [5.1830, 5.1755, 5.1825],
        [5.1830, 5.1830, 5.1830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.18137668073177338 
model_pd.l_d.mean(): -21.0006160736084 
model_pd.lagr.mean(): -20.819238662719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3677], device='cuda:0')), ('power', tensor([-21.6058], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.18137668073177338
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12304051965475082
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.1625089943408966
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.09655231982469559
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12194344401359558
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.08388856798410416
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.10781039297580719
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.1624341607093811
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.09583806991577148
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.15736623108386993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1776, 5.1160, 5.1597],
        [5.1776, 5.2544, 4.9849],
        [5.1776, 5.1776, 5.1776],
        [5.1776, 5.0877, 5.1420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.1679994910955429 
model_pd.l_d.mean(): -20.510385513305664 
model_pd.lagr.mean(): -20.34238624572754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4457], device='cuda:0')), ('power', tensor([-21.1899], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.1679994910955429
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.0991591215133667
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.1147402748465538
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.09999947249889374
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.13307923078536987
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.1291436105966568
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.11412999778985977
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.16228099167346954
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.15241894125938416
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.1294434666633606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.1571, 5.1735],
        [5.1757, 4.9454, 4.9419],
        [5.1757, 4.9152, 4.8573],
        [5.1757, 5.1199, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13448533415794373 
model_pd.l_d.mean(): -20.7691707611084 
model_pd.lagr.mean(): -20.634685516357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4083], device='cuda:0')), ('power', tensor([-21.4133], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13448533415794373
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10891780257225037
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.11277196556329727
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.12770892679691315
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.1664350926876068
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.17821836471557617
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.15630081295967102
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.07420419156551361
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.12606973946094513
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.1924344003200531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1485, 5.1448, 5.1483],
        [5.1485, 4.9580, 4.9958],
        [5.1485, 5.1298, 5.1462],
        [5.1485, 5.1264, 5.1455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12432309240102768 
model_pd.l_d.mean(): -20.58962631225586 
model_pd.lagr.mean(): -20.465303421020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4448], device='cuda:0')), ('power', tensor([-21.2690], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12432309240102768
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.11187449842691422
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.0977039784193039
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.18333202600479126
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.2805934250354767
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.14318545162677765
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.1455131471157074
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.11721645295619965
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.13671496510505676
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.1295354813337326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1299, 5.1269, 5.1298],
        [5.1299, 4.8527, 4.6005],
        [5.1299, 4.9315, 4.6037],
        [5.1299, 5.1294, 5.1299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.1339508444070816 
model_pd.l_d.mean(): -19.928476333618164 
model_pd.lagr.mean(): -19.794525146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4548], device='cuda:0')), ('power', tensor([-20.6109], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.1339508444070816
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.1794491708278656
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.12901899218559265
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.16657596826553345
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.11495902389287949
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.13987503945827484
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.1451191008090973
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.19545528292655945
epoch£º1060	 i:8 	 global-step:21208	 l-p:0.10894775390625
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.25986015796661377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1238, 5.1238, 5.1238],
        [5.1238, 4.9308, 4.9681],
        [5.1238, 5.4881, 5.3835],
        [5.1238, 4.8503, 4.7694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.1279985010623932 
model_pd.l_d.mean(): -19.711551666259766 
model_pd.lagr.mean(): -19.583553314208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-20.4682], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.1279985010623932
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.15917856991291046
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.2113599181175232
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.2116004079580307
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.09500554949045181
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.13852199912071228
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.13427454233169556
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12540961802005768
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.18158642947673798
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.16948817670345306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1322, 5.1318, 5.1322],
        [5.1322, 5.0867, 5.1219],
        [5.1322, 4.9570, 5.0044],
        [5.1322, 4.9896, 5.0475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.09907592833042145 
model_pd.l_d.mean(): -20.226606369018555 
model_pd.lagr.mean(): -20.127531051635742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4932], device='cuda:0')), ('power', tensor([-20.9515], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.09907592833042145
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.13619811832904816
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.13272027671337128
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.21165351569652557
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.11395744234323502
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.11683908104896545
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.13967519998550415
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12329541891813278
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.12179593741893768
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.31020838022232056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1318, 5.1315, 5.1318],
        [5.1318, 4.8766, 4.8382],
        [5.1318, 5.1318, 5.1318],
        [5.1318, 4.8547, 4.7605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.1409875601530075 
model_pd.l_d.mean(): -20.652631759643555 
model_pd.lagr.mean(): -20.51164436340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-21.3185], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.1409875601530075
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.061920296400785446
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.1675376147031784
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.14592115581035614
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.13046063482761383
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.20654433965682983
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.16737845540046692
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.2431885302066803
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.1182887926697731
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.16612300276756287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1272, 5.1270, 5.1272],
        [5.1272, 4.8377, 4.6741],
        [5.1272, 4.9074, 4.9213],
        [5.1272, 5.0505, 5.1009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.30796438455581665 
model_pd.l_d.mean(): -20.60907745361328 
model_pd.lagr.mean(): -20.30111312866211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4338], device='cuda:0')), ('power', tensor([-21.2775], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.30796438455581665
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.1613249033689499
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.16719070076942444
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.13046714663505554
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.13424181938171387
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.12324485182762146
epoch£º1064	 i:6 	 global-step:21286	 l-p:0.09433510899543762
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.11900093406438828
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.16339093446731567
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1308334469795227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1335, 5.1335, 5.1335],
        [5.1335, 5.1335, 5.1335],
        [5.1335, 5.0138, 4.6799],
        [5.1335, 5.1328, 5.1335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.13583631813526154 
model_pd.l_d.mean(): -19.868627548217773 
model_pd.lagr.mean(): -19.732791900634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.5953], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.13583631813526154
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.2541690468788147
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.14841467142105103
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.14195333421230316
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.23401248455047607
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.11684242635965347
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.12597797811031342
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.06270606070756912
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12307429313659668
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.1594393253326416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 5.4993, 5.3961],
        [5.1324, 4.9025, 4.5892],
        [5.1324, 5.0192, 5.0781],
        [5.1324, 4.8891, 4.5861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.22158107161521912 
model_pd.l_d.mean(): -20.528074264526367 
model_pd.lagr.mean(): -20.306493759155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-21.2104], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.22158107161521912
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11609712243080139
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.12695318460464478
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.20402544736862183
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.17244140803813934
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.13485245406627655
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.09691563993692398
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12864401936531067
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.1616365760564804
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.1374623030424118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1308, 5.1308, 5.1308],
        [5.1308, 5.1296, 5.1308],
        [5.1308, 5.1256, 5.1305],
        [5.1308, 5.4567, 5.3271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.18132714927196503 
model_pd.l_d.mean(): -20.486562728881836 
model_pd.lagr.mean(): -20.305234909057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.1899], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.18132714927196503
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.13527195155620575
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.172472283244133
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.0858326330780983
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.14993353188037872
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.16890056431293488
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.16230414807796478
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.1692412793636322
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11188646405935287
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.16802258789539337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1374, 5.1374, 5.1374],
        [5.1374, 5.1369, 5.1374],
        [5.1374, 5.1374, 5.1374],
        [5.1374, 5.1374, 5.1374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.2140035778284073 
model_pd.l_d.mean(): -19.52696418762207 
model_pd.lagr.mean(): -19.312959671020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4947], device='cuda:0')), ('power', tensor([-20.2458], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.2140035778284073
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.17356283962726593
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12727141380310059
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.13982465863227844
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.07288482785224915
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.17200329899787903
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.11644647270441055
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.1444852352142334
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.17700916528701782
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.10546715557575226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 4.9080, 4.8835],
        [5.1542, 5.1407, 5.1529],
        [5.1542, 5.1335, 5.1515],
        [5.1542, 5.0173, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.11206302046775818 
model_pd.l_d.mean(): -20.73908805847168 
model_pd.lagr.mean(): -20.627025604248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4092], device='cuda:0')), ('power', tensor([-21.3838], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.11206302046775818
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.09393703937530518
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.16454806923866272
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.1476006805896759
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.1295204758644104
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.15096087753772736
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.11905694752931595
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.13057184219360352
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.17247945070266724
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.1438014805316925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.3175, 5.0802],
        [5.1750, 5.1735, 5.1749],
        [5.1750, 5.1376, 5.1676],
        [5.1750, 5.2489, 4.9776]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.09922143816947937 
model_pd.l_d.mean(): -19.489667892456055 
model_pd.lagr.mean(): -19.390445709228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-20.2669], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.09922143816947937
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.1432356834411621
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.14098134636878967
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.13654182851314545
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.13709619641304016
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.12899503111839294
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.07842139899730682
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.15975321829319
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.1568983793258667
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11192240566015244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1892, 5.1892, 5.1892],
        [5.1892, 5.1713, 5.1871],
        [5.1892, 5.0365, 5.0911],
        [5.1892, 4.9466, 4.9239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.1086871474981308 
model_pd.l_d.mean(): -20.028322219848633 
model_pd.lagr.mean(): -19.919635772705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.7353], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.1086871474981308
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.11185187846422195
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.140104278922081
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.14653749763965607
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.15040594339370728
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.08691240102052689
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.09562505781650543
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.12346331030130386
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.17031778395175934
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.13917870819568634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1881, 5.1881, 5.1881],
        [5.1881, 5.5827, 5.4962],
        [5.1881, 5.1873, 5.1881],
        [5.1881, 4.9061, 4.7036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.1491040736436844 
model_pd.l_d.mean(): -19.533050537109375 
model_pd.lagr.mean(): -19.383947372436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5474], device='cuda:0')), ('power', tensor([-20.3058], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.1491040736436844
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.09882763028144836
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.10055321455001831
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.10456640273332596
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.11954993009567261
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15868377685546875
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.14920984208583832
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.12165805697441101
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12112240493297577
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.14459067583084106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1873, 5.1656, 5.1844],
        [5.1873, 5.1806, 5.1869],
        [5.1873, 5.1201, 4.7972],
        [5.1873, 5.1716, 5.1856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.09719831496477127 
model_pd.l_d.mean(): -19.677358627319336 
model_pd.lagr.mean(): -19.58016014099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.4377], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.09719831496477127
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.10614845156669617
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.12312051653862
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.13147364556789398
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11107233911752701
epoch£º1073	 i:5 	 global-step:21465	 l-p:0.12211786210536957
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.17452819645404816
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.14568375051021576
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.13684803247451782
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.15223988890647888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1708, 5.1708, 5.1708],
        [5.1708, 4.8885, 4.7571],
        [5.1708, 5.0792, 5.1341],
        [5.1708, 4.8880, 4.7532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.09719785302877426 
model_pd.l_d.mean(): -19.830703735351562 
model_pd.lagr.mean(): -19.733505249023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-20.5541], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.09719785302877426
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12352444976568222
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.13422881066799164
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.15239647030830383
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.16586868464946747
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.16685186326503754
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.12208888679742813
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.15059588849544525
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.16789142787456512
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.09530406445264816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1536, 5.0561, 5.1125],
        [5.1536, 5.1498, 5.1535],
        [5.1536, 5.1533, 5.1536],
        [5.1536, 4.9212, 4.9183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.09359133243560791 
model_pd.l_d.mean(): -20.808616638183594 
model_pd.lagr.mean(): -20.715024948120117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050], device='cuda:0')), ('power', tensor([-21.4497], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.09359133243560791
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.1617044359445572
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.13407550752162933
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.13350439071655273
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.16545730829238892
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12875109910964966
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.1319027543067932
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.139093816280365
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.19064010679721832
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12643516063690186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 5.1288, 5.1455],
        [5.1478, 5.3956, 5.2175],
        [5.1478, 5.4256, 5.2655],
        [5.1478, 5.1130, 5.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.11138535290956497 
model_pd.l_d.mean(): -20.370450973510742 
model_pd.lagr.mean(): -20.259065628051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4903], device='cuda:0')), ('power', tensor([-21.0940], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.11138535290956497
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.1576179713010788
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.17609761655330658
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.1335267722606659
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.13190168142318726
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10650825500488281
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.1910221427679062
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.08222775161266327
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.20422549545764923
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13160482048988342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1468, 4.8687, 4.7692],
        [5.1468, 4.8844, 4.8302],
        [5.1468, 5.1417, 5.1465],
        [5.1468, 5.1461, 5.1468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.1673690378665924 
model_pd.l_d.mean(): -20.058595657348633 
model_pd.lagr.mean(): -19.891225814819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4314], device='cuda:0')), ('power', tensor([-20.7185], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.1673690378665924
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.10717066377401352
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.1266769915819168
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.19580936431884766
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.19636885821819305
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.11897414922714233
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.12686462700366974
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.1571308970451355
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.10242298245429993
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12242257595062256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1501, 4.8653, 4.7327],
        [5.1501, 4.9621, 5.0022],
        [5.1501, 4.9007, 4.8721],
        [5.1501, 5.1501, 5.1501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.15957561135292053 
model_pd.l_d.mean(): -19.360689163208008 
model_pd.lagr.mean(): -19.201112747192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5403], device='cuda:0')), ('power', tensor([-20.1243], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.15957561135292053
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13709841668605804
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.1262487769126892
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.1527601182460785
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12478256970643997
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.10015485435724258
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.12020354717969894
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.15016761422157288
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.22025664150714874
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.14367879927158356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1446, 5.1055, 5.1367],
        [5.1446, 5.0240, 4.6896],
        [5.1446, 5.1399, 5.1444],
        [5.1446, 4.8558, 4.6919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.2748875617980957 
model_pd.l_d.mean(): -20.172182083129883 
model_pd.lagr.mean(): -19.897294998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5065], device='cuda:0')), ('power', tensor([-20.9100], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.2748875617980957
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.11820437759160995
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.06458824127912521
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13511435687541962
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.12190791219472885
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.14043162763118744
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.13279198110103607
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.14044049382209778
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.16964022815227509
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.1374729722738266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1471, 5.1190, 5.1426],
        [5.1471, 5.0477, 5.1046],
        [5.1471, 5.4084, 5.2381],
        [5.1471, 4.9444, 4.6182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11422336101531982 
model_pd.l_d.mean(): -20.25153923034668 
model_pd.lagr.mean(): -20.13731575012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.9746], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11422336101531982
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.11494368314743042
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.11716771870851517
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.13666844367980957
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.22174255549907684
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.10742378234863281
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13456764817237854
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.19222818315029144
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.1603606939315796
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.1581953912973404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1414, 5.1412, 5.1414],
        [5.1414, 5.1414, 5.1414],
        [5.1414, 5.1399, 5.1413],
        [5.1414, 5.1328, 5.1408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11344560235738754 
model_pd.l_d.mean(): -19.343544006347656 
model_pd.lagr.mean(): -19.230098724365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-20.0976], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11344560235738754
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.21848861873149872
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.18852834403514862
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.1299218386411667
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.22607558965682983
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.12578684091567993
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.10354016721248627
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.08822547644376755
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.10909587889909744
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.1473090946674347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1460, 5.1460, 5.1460],
        [5.1460, 5.0239, 5.0834],
        [5.1460, 4.8923, 4.6001],
        [5.1460, 4.9832, 4.6484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.11211822181940079 
model_pd.l_d.mean(): -19.369937896728516 
model_pd.lagr.mean(): -19.25782012939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5467], device='cuda:0')), ('power', tensor([-20.1402], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.11211822181940079
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.10919847339391708
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.1346680223941803
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13823287189006805
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.11785684525966644
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.1883939951658249
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.2716830372810364
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.14092320203781128
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.14665943384170532
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.1085638776421547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1337, 5.1308, 5.1336],
        [5.1337, 4.8829, 4.8546],
        [5.1337, 5.4204, 5.2657],
        [5.1337, 4.9922, 4.6556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.11938037723302841 
model_pd.l_d.mean(): -20.181577682495117 
model_pd.lagr.mean(): -20.062196731567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5196], device='cuda:0')), ('power', tensor([-20.9329], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.11938037723302841
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.2284276932477951
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.15182016789913177
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.1284586787223816
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.09918887913227081
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.11495809257030487
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.23820053040981293
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.1602545529603958
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.10129588097333908
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.20494529604911804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1232, 5.4250, 5.2797],
        [5.1232, 5.1091, 5.1218],
        [5.1232, 4.8332, 4.6202],
        [5.1232, 4.9793, 4.6418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.12862803041934967 
model_pd.l_d.mean(): -20.22730255126953 
model_pd.lagr.mean(): -20.098674774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5195], device='cuda:0')), ('power', tensor([-20.9790], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.12862803041934967
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.17621192336082458
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.1261574774980545
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.154940664768219
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.10308495908975601
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.12197202444076538
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.1394239068031311
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.1116260215640068
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.24272823333740234
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.44147980213165283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1134, 4.9951, 5.0549],
        [5.1134, 4.8392, 4.5683],
        [5.1134, 4.8763, 4.8718],
        [5.1134, 5.1106, 5.1133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.395501971244812 
model_pd.l_d.mean(): -19.669702529907227 
model_pd.lagr.mean(): -19.274200439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5036], device='cuda:0')), ('power', tensor([-20.3992], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.395501971244812
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.1380654126405716
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.14830619096755981
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13261151313781738
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.08718357980251312
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.19913840293884277
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.13686922192573547
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.11211401969194412
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.10946788638830185
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.34013891220092773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1084, 5.0177, 5.0729],
        [5.1084, 5.1871, 4.9177],
        [5.1084, 5.1084, 5.1084],
        [5.1084, 5.1083, 5.1084]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.22643297910690308 
model_pd.l_d.mean(): -19.874969482421875 
model_pd.lagr.mean(): -19.648536682128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350], device='cuda:0')), ('power', tensor([-20.5366], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.22643297910690308
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.1345176100730896
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.07243763655424118
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.13691739737987518
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.11346933245658875
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.17741981148719788
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.26889941096305847
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.11338783800601959
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.14616580307483673
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.4333060383796692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1139, 5.1138, 5.1139],
        [5.1139, 5.0805, 5.1079],
        [5.1139, 5.0947, 5.1116],
        [5.1139, 4.9397, 4.6028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.18856623768806458 
model_pd.l_d.mean(): -19.563552856445312 
model_pd.lagr.mean(): -19.37498664855957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5366], device='cuda:0')), ('power', tensor([-20.3255], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.18856623768806458
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.13300864398479462
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.18194147944450378
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.15006965398788452
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.12088552117347717
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.3368091583251953
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.12350782006978989
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.13433204591274261
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.10459653288125992
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.17965373396873474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1234, 5.1126, 5.1225],
        [5.1234, 5.1044, 4.7937],
        [5.1234, 5.3870, 5.2182],
        [5.1234, 5.0604, 4.7357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.1433805674314499 
model_pd.l_d.mean(): -20.13794708251953 
model_pd.lagr.mean(): -19.994565963745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5109], device='cuda:0')), ('power', tensor([-20.8799], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.1433805674314499
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.1337302178144455
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.18338313698768616
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.14795446395874023
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.07482545077800751
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.20177127420902252
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.10895917564630508
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.2989875078201294
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.13385909795761108
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.14987704157829285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1328, 5.0415, 5.0967],
        [5.1328, 5.0369, 5.0932],
        [5.1328, 4.8611, 4.5905],
        [5.1328, 5.1328, 5.1328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.15361586213111877 
model_pd.l_d.mean(): -20.709209442138672 
model_pd.lagr.mean(): -20.555593490600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4441], device='cuda:0')), ('power', tensor([-21.3892], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.15361586213111877
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.20940054953098297
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.16629810631275177
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14319463074207306
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.11845028400421143
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11860515177249908
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.125135138630867
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.10332020372152328
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.16025689244270325
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.18094249069690704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 4.8597, 4.7164],
        [5.1473, 4.8693, 4.6177],
        [5.1473, 5.2112, 4.9345],
        [5.1473, 5.1459, 5.1473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.20463289320468903 
model_pd.l_d.mean(): -20.68304443359375 
model_pd.lagr.mean(): -20.478410720825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.3508], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.20463289320468903
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.1533169448375702
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.10859797894954681
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11976434290409088
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.1428217887878418
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.1569206416606903
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.1688607782125473
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.12875531613826752
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.12010902911424637
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11932094395160675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1527, 5.0432, 4.7098],
        [5.1527, 4.8738, 4.6266],
        [5.1527, 4.9773, 4.6438],
        [5.1527, 5.1525, 5.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.15400075912475586 
model_pd.l_d.mean(): -20.311620712280273 
model_pd.lagr.mean(): -20.15761947631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4364], device='cuda:0')), ('power', tensor([-20.9794], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.15400075912475586
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.14080572128295898
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.18696673214435577
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.14306244254112244
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.16087542474269867
epoch£º1091	 i:5 	 global-step:21825	 l-p:0.16002103686332703
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.1251494288444519
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.09157121181488037
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.1400946080684662
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.1242225170135498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.0005, 5.0579],
        [5.1473, 5.1285, 5.1451],
        [5.1473, 5.0897, 4.7669],
        [5.1473, 4.8969, 4.8686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.1319618970155716 
model_pd.l_d.mean(): -20.514074325561523 
model_pd.lagr.mean(): -20.382112503051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-21.2229], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.1319618970155716
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.12381692975759506
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.13982808589935303
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.13228000700473785
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.26995378732681274
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.1423831284046173
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.08133334666490555
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.1761595904827118
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.1302153617143631
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12645111978054047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1379, 5.0778, 4.7540],
        [5.1379, 5.1378, 5.1379],
        [5.1379, 5.1379, 5.1379],
        [5.1379, 5.1378, 5.1379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.18069866299629211 
model_pd.l_d.mean(): -20.493032455444336 
model_pd.lagr.mean(): -20.312334060668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4636], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.18069866299629211
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.12334506958723068
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.11123466491699219
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.11388489603996277
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.15094509720802307
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.10632846504449844
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.16634555160999298
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.10767897218465805
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.21459433436393738
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.23452197015285492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1339, 5.0530, 5.1050],
        [5.1339, 5.0300, 5.0880],
        [5.1339, 5.1339, 5.1339],
        [5.1339, 4.8861, 4.8643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.16172319650650024 
model_pd.l_d.mean(): -20.84988784790039 
model_pd.lagr.mean(): -20.68816375732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116], device='cuda:0')), ('power', tensor([-21.4982], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.16172319650650024
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.12815003097057343
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.11457207053899765
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.13137118518352509
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.17360906302928925
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12722724676132202
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.1114235669374466
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.18112751841545105
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.2400013655424118
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.17580169439315796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1311, 5.1289, 5.1311],
        [5.1311, 4.9738, 5.0292],
        [5.1311, 4.8794, 4.8512],
        [5.1311, 5.1311, 5.1311]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.1404404640197754 
model_pd.l_d.mean(): -20.37916374206543 
model_pd.lagr.mean(): -20.238723754882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.0855], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.1404404640197754
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.10237076878547668
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.2175007164478302
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.12867064774036407
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.15601269900798798
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.10194088518619537
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.10945221781730652
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.2593879997730255
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.1913367658853531
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13502858579158783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1288, 4.8806, 4.8589],
        [5.1288, 4.8767, 4.8482],
        [5.1288, 5.1287, 5.1288],
        [5.1288, 4.9135, 4.9338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.1236104965209961 
model_pd.l_d.mean(): -17.801416397094727 
model_pd.lagr.mean(): -17.677806854248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6464], device='cuda:0')), ('power', tensor([-18.6563], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.1236104965209961
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.13900774717330933
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.25281020998954773
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.09854582697153091
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.18925295770168304
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.1254316121339798
epoch£º1096	 i:6 	 global-step:21926	 l-p:0.12917661666870117
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.1491227149963379
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.17004401981830597
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.15235106647014618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1427, 5.1428, 5.1428],
        [5.1427, 5.1411, 5.1427],
        [5.1427, 5.1428, 5.1427],
        [5.1427, 5.2660, 5.0180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.10438770055770874 
model_pd.l_d.mean(): -19.590721130371094 
model_pd.lagr.mean(): -19.4863338470459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-20.3000], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.10438770055770874
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.12974947690963745
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.1379147469997406
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12710297107696533
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.13304124772548676
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.13211074471473694
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.20637492835521698
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.1340981125831604
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.16073253750801086
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.18590214848518372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1450, 5.1403, 5.1448],
        [5.1450, 5.1322, 5.1439],
        [5.1450, 4.8547, 4.6904],
        [5.1450, 4.9270, 4.9435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.1862521767616272 
model_pd.l_d.mean(): -20.845928192138672 
model_pd.lagr.mean(): -20.65967559814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4070], device='cuda:0')), ('power', tensor([-21.4895], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.1862521767616272
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.203700989484787
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.17577508091926575
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.16673775017261505
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.16572721302509308
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11566013097763062
epoch£º1098	 i:6 	 global-step:21966	 l-p:0.07359160482883453
epoch£º1098	 i:7 	 global-step:21967	 l-p:0.1107751652598381
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11701415479183197
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.10188611596822739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.1548, 4.8475],
        [5.1644, 5.1642, 5.1644],
        [5.1644, 5.0876, 5.1380],
        [5.1644, 5.1644, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.1315116137266159 
model_pd.l_d.mean(): -20.574899673461914 
model_pd.lagr.mean(): -20.443387985229492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4519], device='cuda:0')), ('power', tensor([-21.2614], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.1315116137266159
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.20873139798641205
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.11650209128856659
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.1090025007724762
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.09378736466169357
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.19892944395542145
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.08446437865495682
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.1796744465827942
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.11731687933206558
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11463005095720291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 5.1730, 5.1730],
        [5.1730, 5.5142, 5.3922],
        [5.1730, 5.2630, 4.9981],
        [5.1730, 4.9208, 4.8866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.10989060997962952 
model_pd.l_d.mean(): -20.407241821289062 
model_pd.lagr.mean(): -20.297351837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4721], device='cuda:0')), ('power', tensor([-21.1125], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:0.10989060997962952
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.17208977043628693
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10552844405174255
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.1519104242324829
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.12889452278614044
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.10160708427429199
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.10057423263788223
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.17442534863948822
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.14027422666549683
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.1344546228647232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 4.9327, 4.6348],
        [5.1778, 5.1512, 5.1737],
        [5.1778, 5.1727, 5.1775],
        [5.1778, 4.9548, 4.6399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.1167265772819519 
model_pd.l_d.mean(): -20.34701156616211 
model_pd.lagr.mean(): -20.23028564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-21.0580], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.1167265772819519
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.12287171930074692
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.15976741909980774
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.07914961129426956
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.13876351714134216
epoch£º1101	 i:5 	 global-step:22025	 l-p:0.12257484346628189
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.11600490659475327
epoch£º1101	 i:7 	 global-step:22027	 l-p:0.11263927072286606
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.1159684881567955
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.22043834626674652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1817, 4.9226, 4.8729],
        [5.1817, 5.0078, 4.6760],
        [5.1817, 5.1732, 5.1811],
        [5.1817, 4.8951, 4.7319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12937144935131073 
model_pd.l_d.mean(): -19.668310165405273 
model_pd.lagr.mean(): -19.538938522338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-20.3619], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12937144935131073
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.15719428658485413
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.15022513270378113
epoch£º1102	 i:3 	 global-step:22043	 l-p:0.15436390042304993
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.14259810745716095
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.10016906261444092
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.12999069690704346
epoch£º1102	 i:7 	 global-step:22047	 l-p:0.11394574493169785
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.16819484531879425
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.07185712456703186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1703, 5.1485, 5.1674],
        [5.1703, 5.0087, 5.0618],
        [5.1703, 5.1697, 4.8655],
        [5.1703, 5.1703, 5.1704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.14002959430217743 
model_pd.l_d.mean(): -20.479644775390625 
model_pd.lagr.mean(): -20.339614868164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.1750], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.14002959430217743
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.14720779657363892
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.19709275662899017
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.1367625594139099
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.14603550732135773
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.057201385498046875
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.1151796504855156
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.13134419918060303
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.10900767892599106
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.16704301536083221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.4662, 5.3160],
        [5.1697, 5.0802, 5.1347],
        [5.1697, 5.1697, 5.1697],
        [5.1697, 5.1654, 5.1695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.11703554540872574 
model_pd.l_d.mean(): -20.529809951782227 
model_pd.lagr.mean(): -20.41277503967285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4592], device='cuda:0')), ('power', tensor([-21.2233], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.11703554540872574
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.14877815544605255
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.07093535363674164
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.13246165215969086
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.12236329913139343
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.1458289921283722
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.16422052681446075
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11512736976146698
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.20719702541828156
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12698674201965332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 4.9721, 4.6428],
        [5.1661, 5.1585, 5.1656],
        [5.1661, 5.0729, 4.7418],
        [5.1661, 4.9924, 5.0408]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.16195692121982574 
model_pd.l_d.mean(): -20.41853141784668 
model_pd.lagr.mean(): -20.256574630737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4687], device='cuda:0')), ('power', tensor([-21.1204], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.16195692121982574
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.13031840324401855
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.13312150537967682
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.17328228056430817
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.15318094193935394
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.1620391458272934
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.09357558935880661
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.11735357344150543
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.09376316517591476
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.1505558341741562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.2439, 4.9774],
        [5.1568, 5.1538, 5.1567],
        [5.1568, 5.1329, 5.1534],
        [5.1568, 5.1234, 5.1508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.1357969045639038 
model_pd.l_d.mean(): -20.30034828186035 
model_pd.lagr.mean(): -20.16455078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4621], device='cuda:0')), ('power', tensor([-20.9942], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.1357969045639038
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.10587833821773529
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.1254516988992691
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.14507946372032166
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.11279195547103882
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.21813294291496277
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.13437949120998383
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.16770248115062714
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.12340797483921051
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.1481727659702301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 5.2890, 5.0493],
        [5.1489, 5.1489, 5.1489],
        [5.1489, 5.0501, 5.1071],
        [5.1489, 5.1459, 5.1488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.10870742797851562 
model_pd.l_d.mean(): -20.5032958984375 
model_pd.lagr.mean(): -20.394588470458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4721], device='cuda:0')), ('power', tensor([-21.2097], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.10870742797851562
epoch£º1107	 i:1 	 global-step:22141	 l-p:0.13531263172626495
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.16889366507530212
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.12013828009366989
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.1914639174938202
epoch£º1107	 i:5 	 global-step:22145	 l-p:0.16157619655132294
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12669864296913147
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.13130773603916168
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.11454039067029953
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.1718098372220993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1504, 4.8654, 4.7441],
        [5.1504, 5.1504, 5.1505],
        [5.1504, 4.9143, 4.9093],
        [5.1504, 4.8667, 4.6287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13901005685329437 
model_pd.l_d.mean(): -20.23748779296875 
model_pd.lagr.mean(): -20.098478317260742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-20.9525], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13901005685329437
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.1927809864282608
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.17553488910198212
epoch£º1108	 i:3 	 global-step:22163	 l-p:0.11983420699834824
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.1755681335926056
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.12267900258302689
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.09534230828285217
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.09972337633371353
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.1651591956615448
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.13261868059635162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1561, 5.2459, 4.9807],
        [5.1561, 5.1560, 5.1561],
        [5.1561, 4.9624, 4.9997],
        [5.1561, 4.9353, 4.9488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.15943543612957 
model_pd.l_d.mean(): -20.331750869750977 
model_pd.lagr.mean(): -20.17231559753418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4723], device='cuda:0')), ('power', tensor([-21.0364], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.15943543612957
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.19107595086097717
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.13826921582221985
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.14596401154994965
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.15716850757598877
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.126207172870636
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.10420326143503189
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.08691823482513428
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.14032745361328125
epoch£º1109	 i:9 	 global-step:22189	 l-p:0.12867867946624756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[5.1710, 4.9904, 5.0351],
        [5.1710, 5.0096, 4.6746],
        [5.1710, 5.2076, 4.9181],
        [5.1710, 5.2563, 4.9887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.17230257391929626 
model_pd.l_d.mean(): -20.214378356933594 
model_pd.lagr.mean(): -20.042076110839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5057], device='cuda:0')), ('power', tensor([-20.9519], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.17230257391929626
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.1208423376083374
epoch£º1110	 i:2 	 global-step:22202	 l-p:0.10720553249120712
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.17435993254184723
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.15778647363185883
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.13512596487998962
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12691925466060638
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.08887260407209396
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11751390248537064
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.1326112002134323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1781, 5.1555, 4.8432],
        [5.1781, 5.1397, 5.1705],
        [5.1781, 5.1782, 5.1782],
        [5.1781, 4.8904, 4.7258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.1325984001159668 
model_pd.l_d.mean(): -18.86240577697754 
model_pd.lagr.mean(): -18.729806900024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-19.6175], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.1325984001159668
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.09412974119186401
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.148370161652565
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.1582242101430893
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.1331007480621338
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.187066450715065
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.12937061488628387
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.10839100927114487
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.06220492348074913
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.20424777269363403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1603, 4.9189, 4.6129],
        [5.1603, 4.9424, 4.6210],
        [5.1603, 4.8987, 4.8487],
        [5.1603, 4.9190, 4.6129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.13364766538143158 
model_pd.l_d.mean(): -19.845088958740234 
model_pd.lagr.mean(): -19.711441040039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.5661], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.13364766538143158
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.198451966047287
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.07692540436983109
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.18834470212459564
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.1677486151456833
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.11747830361127853
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.09540742635726929
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.13478314876556396
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.13232801854610443
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.13155776262283325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 5.1638, 5.1650],
        [5.1651, 5.1645, 5.1651],
        [5.1651, 5.1651, 5.1651],
        [5.1651, 4.9172, 4.8930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.16588035225868225 
model_pd.l_d.mean(): -19.419279098510742 
model_pd.lagr.mean(): -19.253398895263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5308], device='cuda:0')), ('power', tensor([-20.1737], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.16588035225868225
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.16752716898918152
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12631677091121674
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.1323835551738739
epoch£º1113	 i:4 	 global-step:22264	 l-p:0.10819797217845917
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.157550647854805
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.12819108366966248
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.11184272170066833
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.16525427997112274
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.10997865349054337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.0778, 5.1302],
        [5.1600, 4.8698, 4.6997],
        [5.1600, 4.8873, 4.6216],
        [5.1600, 5.1570, 5.1599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.13113152980804443 
model_pd.l_d.mean(): -18.5034236907959 
model_pd.lagr.mean(): -18.372291564941406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5417], device='cuda:0')), ('power', tensor([-19.2590], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.13113152980804443
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.15385960042476654
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.15548190474510193
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.1518424153327942
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12455423176288605
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.1581258475780487
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.1712314337491989
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13890524208545685
epoch£º1114	 i:8 	 global-step:22288	 l-p:0.10404796153306961
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.09450583904981613
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 4.9801, 4.6460],
        [5.1599, 5.1531, 5.1595],
        [5.1599, 5.2414, 4.9719],
        [5.1599, 5.1455, 5.1585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.12192124873399734 
model_pd.l_d.mean(): -20.14405632019043 
model_pd.lagr.mean(): -20.02213478088379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.8691], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.12192124873399734
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.13927307724952698
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.12993106245994568
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.12570588290691376
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.22044971585273743
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.1531079262495041
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.14212168753147125
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.11990678310394287
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12492959946393967
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.10831920057535172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 4.9332, 4.9418],
        [5.1586, 4.9112, 4.8891],
        [5.1586, 4.9088, 4.6095],
        [5.1586, 5.1586, 5.1586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.1394006311893463 
model_pd.l_d.mean(): -20.51166534423828 
model_pd.lagr.mean(): -20.372264862060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4442], device='cuda:0')), ('power', tensor([-21.1896], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.1394006311893463
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.18027594685554504
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12338534742593765
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.18039818108081818
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.1517638862133026
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.12813912332057953
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.11081945896148682
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.10655713081359863
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13971014320850372
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.14084652066230774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 5.0933, 5.1378],
        [5.1559, 5.1477, 5.1553],
        [5.1559, 4.8996, 4.8619],
        [5.1559, 5.1558, 5.1559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.13488955795764923 
model_pd.l_d.mean(): -20.714736938476562 
model_pd.lagr.mean(): -20.57984733581543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194], device='cuda:0')), ('power', tensor([-21.3695], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.13488955795764923
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.12561744451522827
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.2175629734992981
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.053433530032634735
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.1771412193775177
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.17167671024799347
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.10851247608661652
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.10487224161624908
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.18256570398807526
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.12994897365570068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 5.1509, 5.1561],
        [5.1564, 4.9819, 5.0306],
        [5.1564, 5.0664, 5.1213],
        [5.1564, 5.1536, 5.1563]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.12023486196994781 
model_pd.l_d.mean(): -19.261327743530273 
model_pd.lagr.mean(): -19.14109230041504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-19.9895], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.12023486196994781
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.1284063458442688
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.10153254866600037
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12589934468269348
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.1466425359249115
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.1738675981760025
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.20387040078639984
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.18637895584106445
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.123873271048069
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.11260976642370224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1514, 4.9099, 4.8980],
        [5.1514, 5.2812, 5.0356],
        [5.1514, 5.1461, 5.1511],
        [5.1514, 5.5142, 5.4059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.1087450385093689 
model_pd.l_d.mean(): -19.273942947387695 
model_pd.lagr.mean(): -19.165197372436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5679], device='cuda:0')), ('power', tensor([-20.0648], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.1087450385093689
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.1253509223461151
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.16106544435024261
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.16583295166492462
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.15480676293373108
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.11191583424806595
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.11889384686946869
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.1466389298439026
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.15029017627239227
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.1591612994670868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1639, 4.8845, 4.6349],
        [5.1639, 5.0394, 5.0991],
        [5.1639, 5.1144, 5.1519],
        [5.1639, 5.2592, 4.9962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.14701057970523834 
model_pd.l_d.mean(): -19.437902450561523 
model_pd.lagr.mean(): -19.290891647338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5481], device='cuda:0')), ('power', tensor([-20.2103], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.14701057970523834
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.18233360350131989
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.07666933536529541
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12146047502756119
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.14989008009433746
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.1265779286623001
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.14197924733161926
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.14059235155582428
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.13244999945163727
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.13297493755817413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1794, 5.0639, 4.7294],
        [5.1794, 5.2097, 4.9171],
        [5.1794, 5.1686, 5.1785],
        [5.1794, 5.1791, 5.1794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.12190864235162735 
model_pd.l_d.mean(): -20.90913963317871 
model_pd.lagr.mean(): -20.7872314453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3962], device='cuda:0')), ('power', tensor([-21.5424], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.12190864235162735
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.15033242106437683
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.11331719160079956
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.13761067390441895
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.13036108016967773
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.09947233647108078
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.170347660779953
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.10483777523040771
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.12580884993076324
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.18026939034461975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.1269, 5.1605],
        [5.1697, 5.1365, 5.1638],
        [5.1697, 5.0814, 5.1357],
        [5.1697, 5.4339, 5.2633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.16045863926410675 
model_pd.l_d.mean(): -20.01679039001465 
model_pd.lagr.mean(): -19.85633087158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.7747], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.16045863926410675
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.16327476501464844
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.10695096105337143
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.10776518285274506
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13059483468532562
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.14050348103046417
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.1871190220117569
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.13329869508743286
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.11729967594146729
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.09592194110155106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 4.8933, 4.6452],
        [5.1723, 4.9741, 4.6452],
        [5.1723, 5.0840, 5.1383],
        [5.1723, 5.1669, 5.1720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.11992980539798737 
model_pd.l_d.mean(): -20.37470054626465 
model_pd.lagr.mean(): -20.254770278930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-21.0896], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.11992980539798737
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.11063134670257568
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.16599184274673462
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.11126872897148132
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11944500356912613
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.15790724754333496
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.10192414373159409
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14931011199951172
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.17807132005691528
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12821531295776367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.0001, 4.6647],
        [5.1696, 5.0517, 5.1112],
        [5.1696, 4.9033, 4.8426],
        [5.1696, 5.0659, 5.1238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.1618116945028305 
model_pd.l_d.mean(): -20.68052864074707 
model_pd.lagr.mean(): -20.51871681213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4235], device='cuda:0')), ('power', tensor([-21.3391], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.1618116945028305
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.10891570895910263
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13054633140563965
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.11503352969884872
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13738015294075012
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.08965325355529785
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.1514471471309662
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.12769323587417603
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.17258243262767792
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.185480535030365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.0670, 5.1226],
        [5.1597, 4.8688, 4.6740],
        [5.1597, 4.8689, 4.7053],
        [5.1597, 5.1508, 5.1590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15846803784370422 
model_pd.l_d.mean(): -20.892759323120117 
model_pd.lagr.mean(): -20.734291076660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.5467], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15846803784370422
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.15514595806598663
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.1288173794746399
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.11116620153188705
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.13992784917354584
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.1437327116727829
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.15527214109897614
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.08732527494430542
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14204998314380646
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.1600661426782608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1648, 5.1648],
        [5.1648, 5.0759, 5.1304],
        [5.1648, 4.9984, 5.0505],
        [5.1648, 4.8798, 4.7581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.10677341371774673 
model_pd.l_d.mean(): -20.743099212646484 
model_pd.lagr.mean(): -20.63632583618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4343], device='cuda:0')), ('power', tensor([-21.4135], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.10677341371774673
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.21048232913017273
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.1743614226579666
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.10964640974998474
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11833979934453964
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12965402007102966
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.16094142198562622
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.11847425252199173
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.12972718477249146
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.09292040020227432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1743, 4.8861, 4.7378],
        [5.1743, 5.1738, 5.1743],
        [5.1743, 5.1741, 5.1743],
        [5.1743, 4.9962, 4.6622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.17758403718471527 
model_pd.l_d.mean(): -20.701251983642578 
model_pd.lagr.mean(): -20.52366828918457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4530], device='cuda:0')), ('power', tensor([-21.3903], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.17758403718471527
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.07981698960065842
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.1338672637939453
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11209570616483688
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.11409952491521835
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.15373675525188446
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.12960603833198547
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.14825138449668884
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.11087333410978317
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.18106938898563385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.1697, 5.1697],
        [5.1697, 5.1697, 5.1697],
        [5.1697, 5.1148, 5.1553],
        [5.1697, 4.8793, 4.7025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.18683913350105286 
model_pd.l_d.mean(): -18.802106857299805 
model_pd.lagr.mean(): -18.615266799926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5549], device='cuda:0')), ('power', tensor([-19.5745], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.18683913350105286
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.14864438772201538
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.11866341531276703
epoch£º1128	 i:3 	 global-step:22563	 l-p:0.1103866919875145
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12312459945678711
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.16023699939250946
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.10048840939998627
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.12074360251426697
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.16079814732074738
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12949365377426147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1627, 5.1627, 5.1627],
        [5.1627, 5.0712, 5.1265],
        [5.1627, 5.1504, 5.1616],
        [5.1627, 5.1467, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.11557120084762573 
model_pd.l_d.mean(): -19.713823318481445 
model_pd.lagr.mean(): -19.598251342773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-20.4005], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.11557120084762573
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.18593356013298035
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.1135258674621582
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12425407767295837
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.13790404796600342
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.15137137472629547
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.18562735617160797
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.19757495820522308
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.0870695486664772
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.0902683213353157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.4042, 5.2224],
        [5.1589, 5.1588, 5.1589],
        [5.1589, 5.4343, 5.2705],
        [5.1589, 4.9109, 4.8891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.1735651195049286 
model_pd.l_d.mean(): -19.2181396484375 
model_pd.lagr.mean(): -19.044574737548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5615], device='cuda:0')), ('power', tensor([-20.0018], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.1735651195049286
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.14229749143123627
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.1149788424372673
epoch£º1130	 i:3 	 global-step:22603	 l-p:0.11591575294733047
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.1673925220966339
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.18166093528270721
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.11811968684196472
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.09588555991649628
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12435545772314072
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.14722876250743866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1640, 5.1640, 5.1640],
        [5.1640, 5.1211, 5.1547],
        [5.1640, 5.2288, 4.9509],
        [5.1640, 5.0697, 5.1257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.16235145926475525 
model_pd.l_d.mean(): -19.977088928222656 
model_pd.lagr.mean(): -19.81473731994629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.7005], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.16235145926475525
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.1430383324623108
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.1329026073217392
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.10812535881996155
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.12941356003284454
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.1908736228942871
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.10059669613838196
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.09653358906507492
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.1897011548280716
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12330128997564316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[5.1649, 5.4263, 5.2537],
        [5.1649, 4.9133, 4.8847],
        [5.1649, 4.9092, 4.6153],
        [5.1649, 4.8894, 4.8058]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12955661118030548 
model_pd.l_d.mean(): -20.587602615356445 
model_pd.lagr.mean(): -20.458045959472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4438], device='cuda:0')), ('power', tensor([-21.2660], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12955661118030548
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.1750441938638687
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.15999019145965576
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.16840875148773193
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.0702657625079155
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.11518507450819016
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13087275624275208
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.1378275603055954
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.11452355980873108
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.15763303637504578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1732, 5.1732, 5.1732],
        [5.1732, 4.9555, 4.6334],
        [5.1732, 4.9584, 4.9783],
        [5.1732, 5.1246, 5.1617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.08344043791294098 
model_pd.l_d.mean(): -20.023122787475586 
model_pd.lagr.mean(): -19.939682006835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4948], device='cuda:0')), ('power', tensor([-20.7474], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:0.08344043791294098
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.13195857405662537
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.14461369812488556
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.12465620040893555
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.15209998190402985
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.12636946141719818
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.11234704405069351
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.1692042052745819
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11611877381801605
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.1654675304889679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1814, 4.9345, 4.9120],
        [5.1814, 4.9569, 4.9657],
        [5.1814, 5.0916, 5.1463],
        [5.1814, 5.0053, 5.0529]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.16055096685886383 
model_pd.l_d.mean(): -20.30689239501953 
model_pd.lagr.mean(): -20.14634132385254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4868], device='cuda:0')), ('power', tensor([-21.0262], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.16055096685886383
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.1526007354259491
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13693509995937347
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.10653404146432877
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.17091791331768036
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.14386752247810364
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.10744290798902512
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.12089437991380692
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.123166024684906
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.09107563644647598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1811, 5.4634, 5.3030],
        [5.1811, 4.9080, 4.6452],
        [5.1811, 5.1302, 5.1686],
        [5.1811, 4.8917, 4.7267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.13980071246623993 
model_pd.l_d.mean(): -20.239154815673828 
model_pd.lagr.mean(): -20.099353790283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5025], device='cuda:0')), ('power', tensor([-20.9737], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.13980071246623993
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.13117152452468872
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.12683473527431488
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.12125631421804428
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.14344020187854767
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.0654417872428894
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.13149932026863098
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.11341280490159988
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.17888368666172028
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.1469636708498001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1898, 5.1616, 5.1853],
        [5.1898, 5.1878, 5.1897],
        [5.1898, 5.0214, 5.0721],
        [5.1898, 4.9013, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.10739833861589432 
model_pd.l_d.mean(): -20.81935691833496 
model_pd.lagr.mean(): -20.711957931518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3849], device='cuda:0')), ('power', tensor([-21.4400], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.10739833861589432
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.09722154587507248
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.15012407302856445
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.13538594543933868
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.1284620612859726
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.1006917729973793
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.1421336680650711
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.13183365762233734
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.10966548323631287
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.1693039983510971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1957, 5.1953, 5.1957],
        [5.1957, 5.1957, 5.1957],
        [5.1957, 5.1906, 5.1955],
        [5.1957, 4.9088, 4.7046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.09334203600883484 
model_pd.l_d.mean(): -20.30703353881836 
model_pd.lagr.mean(): -20.21369171142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-21.0067], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.09334203600883484
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.10444676876068115
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.08739959448575974
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.1368967741727829
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.15477782487869263
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.17273077368736267
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.15082640945911407
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11639295518398285
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.16824598610401154
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.08802472800016403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1851, 5.1809, 5.1849],
        [5.1851, 5.1851, 5.1851],
        [5.1851, 5.1309, 5.1711],
        [5.1851, 5.1354, 4.8135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.11318504810333252 
model_pd.l_d.mean(): -19.465011596679688 
model_pd.lagr.mean(): -19.351825714111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4885], device='cuda:0')), ('power', tensor([-20.1768], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.11318504810333252
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.14977379143238068
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.1495361179113388
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12923738360404968
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.18185867369174957
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.08525190502405167
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.11420933157205582
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.17355434596538544
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.13648495078086853
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.102521151304245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.1702, 5.1702],
        [5.1702, 5.1566, 5.1689],
        [5.1702, 4.9161, 4.6203],
        [5.1702, 5.1542, 5.1685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.13394801318645477 
model_pd.l_d.mean(): -20.177703857421875 
model_pd.lagr.mean(): -20.04375648498535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.8974], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.13394801318645477
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.12053611129522324
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14548148214817047
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.13449285924434662
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.15197834372520447
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.1641826629638672
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.265855997800827
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.0903928354382515
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.11352825164794922
epoch£º1139	 i:9 	 global-step:22789	 l-p:0.08152616024017334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1504, 5.1504, 5.1504],
        [5.1504, 5.1001, 5.1381],
        [5.1504, 5.1493, 5.1504],
        [5.1504, 5.1504, 5.1504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.1704416275024414 
model_pd.l_d.mean(): -20.951513290405273 
model_pd.lagr.mean(): -20.781070709228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3885], device='cuda:0')), ('power', tensor([-21.5773], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.1704416275024414
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.1429218053817749
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.06563525646924973
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.2290744036436081
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1309920996427536
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.1427314579486847
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.14279623329639435
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.13125407695770264
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.18917609751224518
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.1287412792444229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1412, 5.0109, 5.0712],
        [5.1412, 5.1406, 5.1412],
        [5.1412, 5.0301, 4.6927],
        [5.1412, 4.9137, 4.9228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.12363694608211517 
model_pd.l_d.mean(): -19.085460662841797 
model_pd.lagr.mean(): -18.961824417114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6101], device='cuda:0')), ('power', tensor([-19.9173], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:0.12363694608211517
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.21656814217567444
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.1477617472410202
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.1114925667643547
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12656369805335999
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.18866178393363953
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.20666296780109406
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.09373031556606293
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.14315293729305267
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.11572812497615814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 5.1498, 5.1503],
        [5.1503, 5.5244, 5.4225],
        [5.1503, 5.1502, 5.1503],
        [5.1503, 5.0879, 4.7614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.12669342756271362 
model_pd.l_d.mean(): -19.79263687133789 
model_pd.lagr.mean(): -19.665943145751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.4731], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.12669342756271362
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.2553614675998688
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.13773849606513977
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.1339293271303177
epoch£º1142	 i:4 	 global-step:22844	 l-p:0.15034011006355286
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.1310160756111145
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.1330663114786148
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.13334639370441437
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.11389804631471634
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11279968917369843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.4264, 5.2588],
        [5.1565, 5.1229, 5.1504],
        [5.1565, 4.9614, 4.9992],
        [5.1565, 5.1552, 5.1564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.10604535788297653 
model_pd.l_d.mean(): -20.13387680053711 
model_pd.lagr.mean(): -20.02783203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4330], device='cuda:0')), ('power', tensor([-20.7962], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.10604535788297653
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.13194215297698975
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.17995251715183258
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.14331471920013428
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.1250801384449005
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.1318426877260208
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.10552895069122314
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.13244089484214783
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.22201628983020782
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.14809732139110565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1534, 5.1534, 5.1534],
        [5.1534, 5.1534, 5.1534],
        [5.1534, 5.1532, 5.1534],
        [5.1534, 4.8601, 4.6895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.09813562035560608 
model_pd.l_d.mean(): -20.435266494750977 
model_pd.lagr.mean(): -20.33713150024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-21.1288], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.09813562035560608
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15445563197135925
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.11515587568283081
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.15428434312343597
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.1097785085439682
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.0950842872262001
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.18286354839801788
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.12302420288324356
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.25468096137046814
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13689513504505157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.0652, 5.1201],
        [5.1546, 5.0137, 5.0730],
        [5.1546, 5.1483, 5.1542],
        [5.1546, 5.1437, 5.1537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.11861717700958252 
model_pd.l_d.mean(): -19.94564437866211 
model_pd.lagr.mean(): -19.8270263671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5155], device='cuda:0')), ('power', tensor([-20.6902], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.11861717700958252
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.1307574212551117
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.12164817750453949
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.17314735054969788
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.13427436351776123
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.1459655612707138
epoch£º1145	 i:6 	 global-step:22906	 l-p:0.06874208897352219
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.17898300290107727
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.1575683355331421
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.18993455171585083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.1566, 5.1566],
        [5.1566, 5.5567, 5.4717],
        [5.1566, 4.8641, 4.7039],
        [5.1566, 4.8657, 4.7215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.17928831279277802 
model_pd.l_d.mean(): -19.879098892211914 
model_pd.lagr.mean(): -19.699810028076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5512], device='cuda:0')), ('power', tensor([-20.6594], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.17928831279277802
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.15805713832378387
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.108528271317482
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.1101367250084877
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.1495523452758789
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.11069631576538086
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.1715807318687439
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.1476815640926361
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.12786468863487244
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.13259083032608032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.2910, 5.0442],
        [5.1613, 5.1608, 5.1613],
        [5.1613, 5.1602, 5.1613],
        [5.1613, 4.9785, 4.6425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.08828862756490707 
model_pd.l_d.mean(): -20.251728057861328 
model_pd.lagr.mean(): -20.16343879699707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4998], device='cuda:0')), ('power', tensor([-20.9836], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.08828862756490707
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.15072427690029144
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.13288111984729767
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.1493678241968155
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.1304536610841751
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.1523122489452362
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.14355674386024475
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.13645213842391968
epoch£º1147	 i:8 	 global-step:22948	 l-p:0.12649357318878174
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.1840151995420456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 5.4155, 5.2401],
        [5.1581, 5.0768, 5.1291],
        [5.1581, 5.0424, 5.1022],
        [5.1581, 5.1538, 5.1579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11975862830877304 
model_pd.l_d.mean(): -19.760053634643555 
model_pd.lagr.mean(): -19.640295028686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5194], device='cuda:0')), ('power', tensor([-20.5066], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11975862830877304
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.16256672143936157
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.15395070612430573
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.15101946890354156
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.14006303250789642
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.14094369113445282
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.13086040318012238
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11676646769046783
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.13125638663768768
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.17774994671344757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 5.0029, 4.6629],
        [5.1516, 4.8978, 4.5964],
        [5.1516, 5.0181, 5.0781],
        [5.1516, 5.0352, 5.0951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.18159151077270508 
model_pd.l_d.mean(): -19.876514434814453 
model_pd.lagr.mean(): -19.694923400878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5281], device='cuda:0')), ('power', tensor([-20.6332], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.18159151077270508
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.07329370081424713
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.18835662305355072
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.15452976524829865
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.09999129176139832
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12320846319198608
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.138514444231987
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.17490939795970917
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.19153229892253876
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08207317441701889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1625, 5.1625],
        [5.1625, 5.1622, 5.1625],
        [5.1625, 5.1623, 5.1625],
        [5.1625, 5.1621, 5.1625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11519850790500641 
model_pd.l_d.mean(): -20.519105911254883 
model_pd.lagr.mean(): -20.403907775878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.1711], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11519850790500641
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.1172424927353859
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.0910772755742073
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14107899367809296
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.2191944420337677
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.14548225700855255
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.10661964863538742
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.1539650559425354
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.15566247701644897
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.14522607624530792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1648, 5.1649],
        [5.1648, 5.1648, 5.1649],
        [5.1648, 5.0759, 5.1306],
        [5.1648, 4.8991, 4.8443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.18590590357780457 
model_pd.l_d.mean(): -20.68315315246582 
model_pd.lagr.mean(): -20.49724769592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-21.3456], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.18590590357780457
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.10290026664733887
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.12576930224895477
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.1409294605255127
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12132144719362259
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.15492402017116547
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.10585131496191025
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.1295214295387268
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.12917868793010712
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.15610447525978088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1783, 5.1790],
        [5.1790, 4.8876, 4.7097],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 4.9892, 5.0297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.1435716450214386 
model_pd.l_d.mean(): -20.90114402770996 
model_pd.lagr.mean(): -20.757572174072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3769], device='cuda:0')), ('power', tensor([-21.5145], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.1435716450214386
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.09874378144741058
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.13050365447998047
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.0538603775203228
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.16356885433197021
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.13466475903987885
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11402573436498642
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.14312514662742615
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.17067503929138184
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.1698085367679596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1824, 5.1813, 5.1823],
        [5.1824, 5.1300, 5.1692],
        [5.1824, 5.0603, 5.1201],
        [5.1824, 5.1683, 5.1810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.1641160547733307 
model_pd.l_d.mean(): -20.46880531311035 
model_pd.lagr.mean(): -20.304689407348633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4651], device='cuda:0')), ('power', tensor([-21.1677], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.1641160547733307
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.15118727087974548
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.12026531249284744
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.11909700185060501
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.1199873685836792
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.08098429441452026
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12824572622776031
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.10793226957321167
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.15974576771259308
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.17450667917728424
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.1325, 5.1639],
        [5.1719, 5.0446, 5.1047],
        [5.1719, 5.1719, 5.1719],
        [5.1719, 5.1383, 5.1659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13786581158638 
model_pd.l_d.mean(): -20.678998947143555 
model_pd.lagr.mean(): -20.541133880615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-21.3524], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13786581158638
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.15827533602714539
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.1218930184841156
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.14031052589416504
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.1713327318429947
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.13697223365306854
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.06448905915021896
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.1592591255903244
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.135996475815773
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.1312200129032135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1713, 5.1666, 5.1711],
        [5.1713, 5.1713, 5.1713],
        [5.1713, 5.1713, 5.1713],
        [5.1713, 5.4859, 5.3451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.09474393725395203 
model_pd.l_d.mean(): -19.340553283691406 
model_pd.lagr.mean(): -19.24580955505371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5409], device='cuda:0')), ('power', tensor([-20.1045], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.09474393725395203
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12464746832847595
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.10045599937438965
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.21469610929489136
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.1754845529794693
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.1489957571029663
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.1500304788351059
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.11295068264007568
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.11345468461513519
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12779420614242554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1675, 5.1226, 5.1574],
        [5.1675, 5.1675, 5.1675],
        [5.1675, 5.1280, 5.1595],
        [5.1675, 4.8970, 4.6204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.16000790894031525 
model_pd.l_d.mean(): -21.007061004638672 
model_pd.lagr.mean(): -20.84705352783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3804], device='cuda:0')), ('power', tensor([-21.6252], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.16000790894031525
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.13292230665683746
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.10962506383657455
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.19405868649482727
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.1740788221359253
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14139509201049805
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.1559416502714157
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.11644762009382248
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.09691470116376877
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.12282957136631012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1542, 5.1542],
        [5.1542, 4.8975, 4.8638],
        [5.1542, 5.1513, 5.1541],
        [5.1542, 5.0539, 5.1115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.12820760905742645 
model_pd.l_d.mean(): -20.228803634643555 
model_pd.lagr.mean(): -20.100595474243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.9583], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.12820760905742645
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.17946948111057281
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.14231929183006287
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.1735648363828659
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.12941673398017883
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12877358496189117
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.1925404667854309
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.08790036290884018
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12400224059820175
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.11401607096195221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.0510, 5.1108],
        [5.1667, 5.1667, 5.1667],
        [5.1667, 5.1667, 5.1667],
        [5.1667, 4.8981, 4.8374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.15304109454154968 
model_pd.l_d.mean(): -20.05744171142578 
model_pd.lagr.mean(): -19.904399871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-20.7854], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.15304109454154968
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12319228798151016
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.14090342819690704
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.15339314937591553
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11879392713308334
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.13100945949554443
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.13169553875923157
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.19130733609199524
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.11349047720432281
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.1320541352033615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1540, 4.8614, 4.6465],
        [5.1540, 5.1539, 5.1540],
        [5.1540, 5.1537, 5.1540],
        [5.1540, 5.0973, 5.1389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.19487877190113068 
model_pd.l_d.mean(): -20.553237915039062 
model_pd.lagr.mean(): -20.35835838317871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4476], device='cuda:0')), ('power', tensor([-21.2351], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.19487877190113068
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.1513831615447998
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.12083113193511963
epoch£º1159	 i:3 	 global-step:23183	 l-p:0.12561459839344025
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.13282322883605957
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10997988283634186
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.147747203707695
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14769501984119415
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.1428382396697998
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.16360041499137878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1502, 4.8789, 4.5979],
        [5.1502, 5.1447, 5.1499],
        [5.1502, 4.9138, 4.5960],
        [5.1502, 5.1501, 5.1502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.1299915313720703 
model_pd.l_d.mean(): -20.11208724975586 
model_pd.lagr.mean(): -19.98209571838379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-20.7975], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.1299915313720703
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.1762818545103073
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.15397964417934418
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.12496449798345566
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.16229072213172913
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.09597223252058029
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.13065963983535767
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.10741834342479706
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.24879500269889832
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.1394272744655609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 4.9513, 4.6145],
        [5.1449, 5.1448, 5.1449],
        [5.1449, 5.0439, 5.1017],
        [5.1449, 4.9239, 4.9410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.08755519986152649 
model_pd.l_d.mean(): -18.627939224243164 
model_pd.lagr.mean(): -18.54038429260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6388], device='cuda:0')), ('power', tensor([-19.4842], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.08755519986152649
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.13698570430278778
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.11305098980665207
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.13592754304409027
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.10112704336643219
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.2205389142036438
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.18963241577148438
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.19321618974208832
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.11908204108476639
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.14880692958831787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[5.1576, 5.0985, 4.7721],
        [5.1576, 5.0182, 5.0778],
        [5.1576, 4.9036, 4.8750],
        [5.1576, 4.8952, 4.6028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.2196652889251709 
model_pd.l_d.mean(): -20.2174072265625 
model_pd.lagr.mean(): -19.99774169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5204], device='cuda:0')), ('power', tensor([-20.9700], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.2196652889251709
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.09512319415807724
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12473360449075699
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.15106900036334991
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12200529128313065
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.08230516314506531
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.19298185408115387
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.11588571965694427
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.13805615901947021
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.14303918182849884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1742, 5.0344, 4.6950],
        [5.1742, 5.1742, 5.1742],
        [5.1742, 5.1742, 5.1742],
        [5.1742, 5.5053, 5.3744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.11665954440832138 
model_pd.l_d.mean(): -19.854021072387695 
model_pd.lagr.mean(): -19.737361907958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-20.5697], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.11665954440832138
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.10212508589029312
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.19222800433635712
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11223112791776657
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.1608959138393402
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14522458612918854
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.0975421667098999
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.08365835249423981
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.16057957708835602
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.1604251265525818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[5.1780, 5.0450, 5.1048],
        [5.1780, 5.0146, 5.0684],
        [5.1780, 4.9149, 4.8656],
        [5.1780, 5.5587, 5.4596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.15676067769527435 
model_pd.l_d.mean(): -20.090850830078125 
model_pd.lagr.mean(): -19.93408966064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.8208], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.15676067769527435
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.10590417683124542
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.1919936090707779
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.046477049589157104
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13197675347328186
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.15732958912849426
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.1311926245689392
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.1421947479248047
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14987625181674957
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.11539818346500397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1774, 4.9334, 4.6253],
        [5.1774, 5.0545, 4.7162],
        [5.1774, 4.9578, 4.9744],
        [5.1774, 5.1774, 5.1774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11749694496393204 
model_pd.l_d.mean(): -19.95801544189453 
model_pd.lagr.mean(): -19.840518951416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5165], device='cuda:0')), ('power', tensor([-20.7038], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11749694496393204
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.07908342778682709
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.1521444320678711
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.17553390562534332
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.10569152235984802
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.10644850134849548
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.09123273938894272
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.2218247503042221
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.1973094344139099
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.11429020017385483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1660, 5.1660, 5.1660],
        [5.1660, 5.0249, 5.0842],
        [5.1660, 5.1517, 5.1646],
        [5.1660, 5.0326, 5.0927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.18027926981449127 
model_pd.l_d.mean(): -20.52507972717285 
model_pd.lagr.mean(): -20.34480094909668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.2115], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.18027926981449127
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.1568102091550827
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.151943176984787
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.10871285200119019
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.14242671430110931
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.10807467997074127
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.12290611118078232
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.11913394927978516
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.1467342972755432
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.1418817639350891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.0538, 5.1124],
        [5.1586, 5.1365, 5.1557],
        [5.1586, 4.8939, 4.8449],
        [5.1586, 4.9365, 4.9518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.14478065073490143 
model_pd.l_d.mean(): -20.458984375 
model_pd.lagr.mean(): -20.3142032623291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-21.1912], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.14478065073490143
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.08323471248149872
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.18388673663139343
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.2085934430360794
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.08758024126291275
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12738870084285736
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.1565917581319809
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.13164734840393066
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.12709790468215942
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.18753716349601746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1469, 5.1423, 5.1466],
        [5.1469, 5.1629, 4.8622],
        [5.1469, 5.1408, 5.1465],
        [5.1469, 5.5812, 5.5185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.1775546669960022 
model_pd.l_d.mean(): -17.399126052856445 
model_pd.lagr.mean(): -17.22157096862793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6644], device='cuda:0')), ('power', tensor([-18.2680], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.1775546669960022
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11940799653530121
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.13183248043060303
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.27459123730659485
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.12690050899982452
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.15910856425762177
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.11680029332637787
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.12832650542259216
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.1307409256696701
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.11256270855665207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1487, 5.0009, 4.6591],
        [5.1487, 5.1464, 5.1487],
        [5.1487, 5.0828, 4.7538],
        [5.1487, 5.0437, 5.1024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.15150147676467896 
model_pd.l_d.mean(): -20.666698455810547 
model_pd.lagr.mean(): -20.51519775390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4279], device='cuda:0')), ('power', tensor([-21.3297], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.15150147676467896
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.14954836666584015
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.1415233463048935
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.20335762202739716
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13515757024288177
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.11669643223285675
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.10183911770582199
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.1668323129415512
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.17215301096439362
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11953897029161453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1538, 5.1538, 5.1538],
        [5.1538, 4.8814, 4.8161],
        [5.1538, 4.9145, 4.9099],
        [5.1538, 4.9053, 4.5959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.1434004157781601 
model_pd.l_d.mean(): -20.06157684326172 
model_pd.lagr.mean(): -19.918176651000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-20.8045], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.1434004157781601
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.12870018184185028
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12623845040798187
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.09284981340169907
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12415247410535812
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.1709812730550766
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.18093419075012207
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.2030719518661499
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.15529322624206543
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.09650503098964691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1590, 5.1583, 5.1590],
        [5.1590, 5.0421, 4.7030],
        [5.1590, 5.0313, 4.6910],
        [5.1590, 5.0330, 5.0935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.13368727266788483 
model_pd.l_d.mean(): -20.418880462646484 
model_pd.lagr.mean(): -20.285192489624023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.1026], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.13368727266788483
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.19426396489143372
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.11179990321397781
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.13684163987636566
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.16797283291816711
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.08660252392292023
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.19923292100429535
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.12065016478300095
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.11818423867225647
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.14583532512187958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1553, 5.1523, 5.1552],
        [5.1553, 5.1522, 5.1552],
        [5.1553, 5.1513, 5.1551],
        [5.1553, 5.0137, 5.0733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.13142870366573334 
model_pd.l_d.mean(): -20.80585289001465 
model_pd.lagr.mean(): -20.67442512512207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-21.4572], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.13142870366573334
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.17945067584514618
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.14676985144615173
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.09074569493532181
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.14641839265823364
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.14624501764774323
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.13267451524734497
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.21254099905490875
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.1365460306406021
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.11630313843488693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 4.8560, 4.6636],
        [5.1521, 5.1521, 5.1521],
        [5.1521, 4.8727, 4.7893],
        [5.1521, 5.1521, 5.1521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.13280664384365082 
model_pd.l_d.mean(): -20.105661392211914 
model_pd.lagr.mean(): -19.972854614257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-20.8009], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.13280664384365082
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.10359849780797958
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.1611085832118988
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12852957844734192
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.11593689024448395
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.11491364985704422
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.1583007276058197
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.15018370747566223
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.20103225111961365
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.16313286125659943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1590, 4.9897, 5.0421],
        [5.1590, 5.0309, 5.0914],
        [5.1590, 4.8734, 4.6233],
        [5.1590, 5.2166, 4.9335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.16430304944515228 
model_pd.l_d.mean(): -19.681659698486328 
model_pd.lagr.mean(): -19.517356872558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-20.4177], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.16430304944515228
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.18338076770305634
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.1297219842672348
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.13295505940914154
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.1577320098876953
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11911661177873611
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.11329471319913864
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.10806302726268768
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.13320660591125488
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.15792380273342133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1616, 5.5361, 5.4330],
        [5.1616, 4.9993, 5.0542],
        [5.1616, 4.8684, 4.7204],
        [5.1616, 5.1615, 5.1616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.11790075153112411 
model_pd.l_d.mean(): -19.02130699157715 
model_pd.lagr.mean(): -18.903406143188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5916], device='cuda:0')), ('power', tensor([-19.8336], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.11790075153112411
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.21411316096782684
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.13264918327331543
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.1282542645931244
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.08308826386928558
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.11064647138118744
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.12924621999263763
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.153555229306221
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.19663077592849731
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.13638316094875336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 4.9053, 4.8766],
        [5.1600, 4.8645, 4.6860],
        [5.1600, 5.1600, 5.1600],
        [5.1600, 4.9075, 4.8826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12355601787567139 
model_pd.l_d.mean(): -19.95315170288086 
model_pd.lagr.mean(): -19.8295955657959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-20.6177], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12355601787567139
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.12394792586565018
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.18326038122177124
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.1110575869679451
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.09180226922035217
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.12235302478075027
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.18140289187431335
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.14468133449554443
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.16371746361255646
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14782069623470306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.1362, 5.1606],
        [5.1653, 5.1084, 5.1501],
        [5.1653, 5.1627, 5.1652],
        [5.1653, 5.2444, 4.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.18784494698047638 
model_pd.l_d.mean(): -20.390426635742188 
model_pd.lagr.mean(): -20.20258140563965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4691], device='cuda:0')), ('power', tensor([-21.0924], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.18784494698047638
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.08779396116733551
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.15045081079006195
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13642992079257965
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.10741301625967026
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.12421039491891861
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.14526911079883575
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.1721174269914627
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.13815313577651978
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13708841800689697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1646, 5.1646],
        [5.1646, 5.1375, 5.1604],
        [5.1646, 4.8731, 4.7370],
        [5.1646, 5.1646, 5.1646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.19347569346427917 
model_pd.l_d.mean(): -20.94114875793457 
model_pd.lagr.mean(): -20.74767303466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3918], device='cuda:0')), ('power', tensor([-21.5703], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.19347569346427917
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.1013103649020195
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.12133911997079849
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.14896820485591888
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.11101463437080383
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.11154910922050476
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.16889694333076477
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.12105165421962738
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.16313225030899048
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13864320516586304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1716, 4.9954, 5.0446],
        [5.1716, 5.1522, 5.1692],
        [5.1716, 5.1103, 4.7827],
        [5.1716, 5.1378, 5.1655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.13571041822433472 
model_pd.l_d.mean(): -20.695934295654297 
model_pd.lagr.mean(): -20.560224533081055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4252], device='cuda:0')), ('power', tensor([-21.3565], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.13571041822433472
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11214649677276611
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.15593314170837402
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.12622100114822388
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.12786728143692017
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.13006485998630524
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.1455254852771759
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.1821969598531723
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.12809690833091736
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.11320989578962326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1746, 5.1656, 5.1739],
        [5.1746, 5.1178, 5.1595],
        [5.1746, 5.1011, 5.1505],
        [5.1746, 5.1551, 5.1722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.14098702371120453 
model_pd.l_d.mean(): -19.7297306060791 
model_pd.lagr.mean(): -19.588743209838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4691], device='cuda:0')), ('power', tensor([-20.4246], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.14098702371120453
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.124127097427845
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.13228864967823029
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.14889656007289886
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13508571684360504
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.15747949481010437
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.09029807895421982
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.11644238978624344
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.15534892678260803
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.20070064067840576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 5.4580, 5.3087],
        [5.1559, 5.1473, 5.1553],
        [5.1559, 5.1336, 5.1529],
        [5.1559, 5.4087, 5.2293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.12522006034851074 
model_pd.l_d.mean(): -20.022159576416016 
model_pd.lagr.mean(): -19.896940231323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-20.7865], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.12522006034851074
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.18835990130901337
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.12630672752857208
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.2010195553302765
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.1680576056241989
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.0944938138127327
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11649814248085022
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.08631478250026703
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.18053759634494781
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.15299460291862488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1497, 5.0247, 4.6834],
        [5.1497, 4.9726, 5.0222],
        [5.1497, 4.9463, 4.9801],
        [5.1497, 5.1497, 5.1497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.127507284283638 
model_pd.l_d.mean(): -20.636032104492188 
model_pd.lagr.mean(): -20.50852394104004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4306], device='cuda:0')), ('power', tensor([-21.3014], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.127507284283638
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.15057286620140076
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.20182105898857117
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12820851802825928
epoch£º1182	 i:4 	 global-step:23644	 l-p:0.10823183506727219
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.22352834045886993
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.08154640346765518
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.16903790831565857
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13033200800418854
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.17006079852581024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1463, 5.4070, 5.2324],
        [5.1463, 5.3042, 5.0710],
        [5.1463, 5.1463, 5.1463],
        [5.1463, 4.8580, 4.6073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.10195663571357727 
model_pd.l_d.mean(): -20.255701065063477 
model_pd.lagr.mean(): -20.153743743896484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5098], device='cuda:0')), ('power', tensor([-20.9979], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.10195663571357727
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.14224229753017426
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.09309208393096924
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.1467178910970688
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.20090685784816742
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.22067305445671082
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.14102406799793243
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.13614234328269958
epoch£º1183	 i:8 	 global-step:23668	 l-p:0.11879182606935501
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.1615002453327179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1511, 5.0878, 4.7588],
        [5.1511, 5.1428, 5.1505],
        [5.1511, 5.1946, 4.9048],
        [5.1511, 4.9109, 4.9064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.17260591685771942 
model_pd.l_d.mean(): -20.637643814086914 
model_pd.lagr.mean(): -20.465038299560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4416], device='cuda:0')), ('power', tensor([-21.3143], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.17260591685771942
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.17109893262386322
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.1777298003435135
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.11700797080993652
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.10239497572183609
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.1495383381843567
epoch£º1184	 i:6 	 global-step:23686	 l-p:0.14134259521961212
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.07579360157251358
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.20019470155239105
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.11520212143659592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 4.8741, 4.7400],
        [5.1657, 4.9420, 4.9563],
        [5.1657, 4.9386, 4.9491],
        [5.1657, 5.0424, 5.1029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.15359151363372803 
model_pd.l_d.mean(): -20.508268356323242 
model_pd.lagr.mean(): -20.354677200317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4503], device='cuda:0')), ('power', tensor([-21.1924], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.15359151363372803
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12696875631809235
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.15894964337348938
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12169559299945831
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.12513718008995056
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.18878327310085297
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.13499955832958221
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.14035449922084808
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.1111435741186142
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.10875095427036285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1732, 5.1298, 5.1638],
        [5.1732, 5.1211, 5.1602],
        [5.1732, 4.9058, 4.8512],
        [5.1732, 5.1353, 4.8143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.08873913437128067 
model_pd.l_d.mean(): -20.49275779724121 
model_pd.lagr.mean(): -20.40401840209961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4557], device='cuda:0')), ('power', tensor([-21.1822], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.08873913437128067
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.11993331462144852
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.1378335803747177
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14784172177314758
epoch£º1186	 i:4 	 global-step:23724	 l-p:0.1375514566898346
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.18954741954803467
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.11073245853185654
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13797122240066528
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.1179710328578949
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.1574699580669403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1770, 5.0252, 5.0828],
        [5.1770, 5.3068, 5.0582],
        [5.1770, 5.0954, 5.1479],
        [5.1770, 5.1755, 5.1770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.18084460496902466 
model_pd.l_d.mean(): -20.482770919799805 
model_pd.lagr.mean(): -20.301925659179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.1702], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.18084460496902466
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.14713534712791443
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11550092697143555
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.15287581086158752
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12188230454921722
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.1446002721786499
epoch£º1187	 i:6 	 global-step:23746	 l-p:0.09608235955238342
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.15615977346897125
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.07035277038812637
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15658412873744965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1801, 5.0878, 5.1436],
        [5.1801, 5.1801, 5.1801],
        [5.1801, 5.6060, 5.5360],
        [5.1801, 5.1350, 5.1700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12494703382253647 
model_pd.l_d.mean(): -20.43475341796875 
model_pd.lagr.mean(): -20.30980682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4007], device='cuda:0')), ('power', tensor([-21.0674], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12494703382253647
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.1633829027414322
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11231717467308044
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.12312985211610794
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.11032157391309738
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.18936336040496826
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.06394924223423004
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.18278749287128448
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.13636218011379242
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12060132622718811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1851, 5.0455, 5.1051],
        [5.1851, 4.9647, 4.9814],
        [5.1851, 5.1851, 5.1851],
        [5.1851, 5.1832, 5.1850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.1126943975687027 
model_pd.l_d.mean(): -20.998939514160156 
model_pd.lagr.mean(): -20.886245727539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3640], device='cuda:0')), ('power', tensor([-21.6003], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.1126943975687027
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.19906236231327057
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.16912661492824554
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.05541767552495003
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.13955169916152954
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.1278361678123474
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.19316647946834564
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.09213639795780182
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.1216370016336441
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.1014418974518776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1854, 5.1854, 5.1854],
        [5.1854, 5.1664, 5.1831],
        [5.1854, 5.1854, 5.1854],
        [5.1854, 5.1693, 5.1837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.08671209216117859 
model_pd.l_d.mean(): -19.6950626373291 
model_pd.lagr.mean(): -19.60835075378418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-20.4144], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.08671209216117859
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.16602115333080292
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.07245180010795593
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.16207793354988098
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.15792961418628693
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.19368389248847961
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.13378427922725677
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.12912678718566895
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.08917924016714096
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.12025247514247894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1876, 5.5554, 5.4466],
        [5.1876, 5.1876, 5.1876],
        [5.1876, 5.1443, 5.1783],
        [5.1876, 4.9658, 4.9808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.07372651994228363 
model_pd.l_d.mean(): -19.26557159423828 
model_pd.lagr.mean(): -19.191844940185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5710], device='cuda:0')), ('power', tensor([-20.0594], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.07372651994228363
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.1238403171300888
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.10646827518939972
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.15048587322235107
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.09028592705726624
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.15597203373908997
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.15128612518310547
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.127530038356781
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.14112097024917603
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.19141480326652527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1852, 5.1073, 5.1585],
        [5.1852, 5.0944, 5.1498],
        [5.1852, 5.0252, 5.0806],
        [5.1852, 5.1802, 5.1850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.14256414771080017 
model_pd.l_d.mean(): -19.75885772705078 
model_pd.lagr.mean(): -19.61629295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5177], device='cuda:0')), ('power', tensor([-20.5036], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.14256414771080017
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.1863887757062912
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.135110005736351
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.08667923510074615
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.13345922529697418
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.15428870916366577
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.13287240266799927
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.10900968313217163
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.14701266586780548
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.09361730515956879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1812, 4.8951, 4.6499],
        [5.1812, 5.1257, 5.1667],
        [5.1812, 5.1764, 5.1809],
        [5.1812, 5.1587, 5.1782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.14113318920135498 
model_pd.l_d.mean(): -20.166894912719727 
model_pd.lagr.mean(): -20.0257625579834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5161], device='cuda:0')), ('power', tensor([-20.9146], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.14113318920135498
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.10032238066196442
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.10970135778188705
epoch£º1193	 i:3 	 global-step:23863	 l-p:0.14662478864192963
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.1425098329782486
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14853422343730927
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.1322082132101059
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.15250560641288757
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.12737485766410828
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.13291028141975403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1774, 4.9305, 4.6207],
        [5.1774, 5.1763, 5.1774],
        [5.1774, 5.0331, 4.6916],
        [5.1774, 4.8843, 4.6697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.11992555111646652 
model_pd.l_d.mean(): -20.500642776489258 
model_pd.lagr.mean(): -20.38071632385254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4482], device='cuda:0')), ('power', tensor([-21.1825], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.11992555111646652
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.1365015208721161
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.13004523515701294
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.16062070429325104
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.1414874941110611
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.16263331472873688
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.10958884656429291
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.12400675565004349
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.14993400871753693
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.1716170608997345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.1517, 5.1566],
        [5.1568, 5.1564, 5.1568],
        [5.1568, 5.0224, 5.0830],
        [5.1568, 4.8849, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.14174450933933258 
model_pd.l_d.mean(): -20.50950050354004 
model_pd.lagr.mean(): -20.367755889892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.1968], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.14174450933933258
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.11873509734869003
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.17175410687923431
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.16879253089427948
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12437968701124191
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.08799833059310913
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.13509228825569153
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.13980020582675934
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.15645572543144226
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.17928443849086761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.0777, 5.1305],
        [5.1597, 5.1285, 5.1544],
        [5.1597, 5.3810, 5.1825],
        [5.1597, 5.2374, 4.9627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.13531267642974854 
model_pd.l_d.mean(): -20.300048828125 
model_pd.lagr.mean(): -20.164735794067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4864], device='cuda:0')), ('power', tensor([-21.0188], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.13531267642974854
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.14686797559261322
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.18921120464801788
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.1498643308877945
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.10163160413503647
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.1447206437587738
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.14015361666679382
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12308398634195328
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.13528946042060852
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.1372077763080597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1620, 4.8817, 4.7979],
        [5.1620, 5.0785, 5.1318],
        [5.1620, 5.4850, 5.3479],
        [5.1620, 4.9557, 4.6209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.1480010747909546 
model_pd.l_d.mean(): -20.15043830871582 
model_pd.lagr.mean(): -20.002437591552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-20.8912], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.1480010747909546
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.14057014882564545
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.1165100634098053
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.1438291072845459
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.1500677913427353
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.20919187366962433
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13868923485279083
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.16307148337364197
epoch£º1197	 i:8 	 global-step:23948	 l-p:0.12842316925525665
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.0736064538359642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1631, 5.1421, 4.8260],
        [5.1631, 5.1631, 5.1631],
        [5.1631, 5.1587, 5.1629],
        [5.1631, 5.1463, 5.1613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.10654469579458237 
model_pd.l_d.mean(): -20.048870086669922 
model_pd.lagr.mean(): -19.942325592041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4973], device='cuda:0')), ('power', tensor([-20.7760], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:0.10654469579458237
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.12893010675907135
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.09615600109100342
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.16722525656223297
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.1425250768661499
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12049407511949539
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.13661445677280426
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.14173032343387604
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.2154836803674698
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.14629100263118744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1638, 5.1623, 5.1637],
        [5.1638, 5.1637, 5.1638],
        [5.1638, 5.1629, 5.1638],
        [5.1638, 5.1638, 5.1638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.10657352954149246 
model_pd.l_d.mean(): -19.271812438964844 
model_pd.lagr.mean(): -19.165239334106445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.0485], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.10657352954149246
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.18936054408550262
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.09353738278150558
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14322511851787567
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.13188065588474274
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.15064555406570435
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.09289305657148361
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.18859317898750305
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.134058877825737
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.1633863002061844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.0835, 5.1355],
        [5.1632, 5.1632, 5.1632],
        [5.1632, 4.9339, 4.9431],
        [5.1632, 4.9287, 4.9318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05070827901363373 
model_pd.l_d.mean(): -20.295827865600586 
model_pd.lagr.mean(): -20.245119094848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.9954], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.05070827901363373
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.12026048451662064
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.11255799978971481
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.1360207498073578
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.16929560899734497
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.13018852472305298
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.16709460318088531
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.21945247054100037
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.11340206116437912
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.18439404666423798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1622, 5.1622, 5.1622],
        [5.1622, 5.1379, 5.1587],
        [5.1622, 4.8729, 4.6271],
        [5.1622, 4.8675, 4.6466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.13752350211143494 
model_pd.l_d.mean(): -19.023666381835938 
model_pd.lagr.mean(): -18.88614273071289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5397], device='cuda:0')), ('power', tensor([-19.7829], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.13752350211143494
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.14020489156246185
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.1884477734565735
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10715242475271225
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.13272227346897125
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.1224718913435936
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.1343833953142166
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.14505958557128906
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.18901003897190094
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.11269127577543259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.0764, 5.1298],
        [5.1600, 4.8757, 4.7802],
        [5.1600, 5.1600, 5.1600],
        [5.1600, 5.4425, 5.2802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.08788085728883743 
model_pd.l_d.mean(): -19.82332992553711 
model_pd.lagr.mean(): -19.735448837280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-20.5645], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.08788085728883743
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.14911837875843048
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14254136383533478
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.14338722825050354
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.09408596158027649
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10305450111627579
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.15066173672676086
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.27466022968292236
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.15244552493095398
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.14020828902721405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 5.1292, 5.1497],
        [5.1530, 5.1527, 5.1530],
        [5.1530, 4.9773, 4.6347],
        [5.1530, 4.9012, 4.5902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.22924692928791046 
model_pd.l_d.mean(): -20.436874389648438 
model_pd.lagr.mean(): -20.20762825012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-21.1655], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.22924692928791046
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.15809521079063416
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.11436210572719574
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.17242896556854248
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12216231971979141
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.1539888083934784
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.10627064108848572
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.1359320878982544
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.13239866495132446
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.12703733146190643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1515, 5.1424, 5.1508],
        [5.1515, 5.1222, 5.1467],
        [5.1515, 4.8634, 4.7573],
        [5.1515, 4.9914, 5.0479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.16741149127483368 
model_pd.l_d.mean(): -20.729984283447266 
model_pd.lagr.mean(): -20.562572479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4295], device='cuda:0')), ('power', tensor([-21.3953], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.16741149127483368
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.1420925259590149
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.12900851666927338
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.12412847578525543
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12986432015895844
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.26748374104499817
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.08251433819532394
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.1445191204547882
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.11295071989297867
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.16085216403007507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 4.8973, 4.5878],
        [5.1513, 5.0981, 5.1379],
        [5.1513, 5.2716, 5.0179],
        [5.1513, 4.9223, 4.5951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.11662498861551285 
model_pd.l_d.mean(): -20.545190811157227 
model_pd.lagr.mean(): -20.428565979003906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.2483], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.11662498861551285
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.15050718188285828
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12730267643928528
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.2232189178466797
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.10350874066352844
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.14745904505252838
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.165247842669487
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12989506125450134
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.19420528411865234
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.11714377999305725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1476, 5.1476, 5.1476],
        [5.1476, 4.8487, 4.6518],
        [5.1476, 5.0967, 5.1353],
        [5.1476, 4.8728, 4.8079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.16962504386901855 
model_pd.l_d.mean(): -20.764610290527344 
model_pd.lagr.mean(): -20.594985961914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4193], device='cuda:0')), ('power', tensor([-21.4199], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.16962504386901855
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12707681953907013
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.07405585795640945
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.14215700328350067
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.1633225977420807
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.18482743203639984
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.17505915462970734
epoch£º1206	 i:7 	 global-step:24127	 l-p:0.0927479937672615
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.1275130659341812
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.21486487984657288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1519, 4.9722, 5.0212],
        [5.1519, 4.8767, 4.5937],
        [5.1519, 5.1507, 5.1519],
        [5.1519, 4.8540, 4.6905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.1723891645669937 
model_pd.l_d.mean(): -20.204641342163086 
model_pd.lagr.mean(): -20.032251358032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.9447], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.1723891645669937
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.13330727815628052
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.1184721365571022
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.17060275375843048
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.17336593568325043
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.12885631620883942
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.1471918672323227
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.15346449613571167
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.12458100914955139
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11887887865304947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 4.9837, 4.6412],
        [5.1579, 5.0264, 5.0873],
        [5.1579, 5.1579, 5.1579],
        [5.1579, 5.0081, 5.0669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.12961922585964203 
model_pd.l_d.mean(): -20.088775634765625 
model_pd.lagr.mean(): -19.959156036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5136], device='cuda:0')), ('power', tensor([-20.8330], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.12961922585964203
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.20788384974002838
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.17721717059612274
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.12408190220594406
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.13629592955112457
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.18548209965229034
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.1386633962392807
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.08503124862909317
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.132198765873909
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10896052420139313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1571, 5.0629, 5.1194],
        [5.1571, 5.1446, 5.1560],
        [5.1571, 5.1571, 5.1571],
        [5.1571, 5.4374, 5.2738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.1662958860397339 
model_pd.l_d.mean(): -19.65941619873047 
model_pd.lagr.mean(): -19.493120193481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4750], device='cuda:0')), ('power', tensor([-20.3595], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.1662958860397339
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.17279063165187836
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.16175830364227295
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.1048702597618103
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.1404278725385666
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.114902064204216
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.11670569330453873
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.214043989777565
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.12195076048374176
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.12426026165485382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1533, 5.1516, 5.1532],
        [5.1533, 5.3717, 5.1713],
        [5.1533, 5.1479, 5.1530],
        [5.1533, 5.1014, 5.1405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.1540452539920807 
model_pd.l_d.mean(): -20.05960464477539 
model_pd.lagr.mean(): -19.905559539794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5290], device='cuda:0')), ('power', tensor([-20.8192], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.1540452539920807
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.12642431259155273
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12307054549455643
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.1718236654996872
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.1026453748345375
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.10267908126115799
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.1419435292482376
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.1627994328737259
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.2557728588581085
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.12109793722629547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 5.1503, 5.1503],
        [5.1503, 5.1360, 5.1489],
        [5.1503, 5.1503, 5.1503],
        [5.1503, 5.1502, 5.1503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.19413785636425018 
model_pd.l_d.mean(): -20.592533111572266 
model_pd.lagr.mean(): -20.398395538330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4608], device='cuda:0')), ('power', tensor([-21.2883], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.19413785636425018
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.11876066029071808
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.15175071358680725
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.1190507784485817
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.2425379902124405
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13720189034938812
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12786348164081573
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.07680570334196091
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.15786805748939514
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.14034990966320038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 5.1445, 5.1504],
        [5.1507, 5.1489, 5.1507],
        [5.1507, 4.9806, 5.0338],
        [5.1507, 5.1507, 5.1507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.18939661979675293 
model_pd.l_d.mean(): -19.30147361755371 
model_pd.lagr.mean(): -19.112077713012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5566], device='cuda:0')), ('power', tensor([-20.0811], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.18939661979675293
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.09390348196029663
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13941600918769836
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.08445022255182266
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.16075673699378967
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.21645678579807281
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.1968366652727127
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.12424638122320175
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.10994800925254822
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.14026781916618347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1556, 5.1555, 5.1556],
        [5.1556, 5.1555, 5.1556],
        [5.1556, 5.0713, 5.1249],
        [5.1556, 5.0266, 5.0876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.15527652204036713 
model_pd.l_d.mean(): -20.39889907836914 
model_pd.lagr.mean(): -20.243621826171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-21.1050], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.15527652204036713
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.1377595067024231
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.1171143427491188
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.10055690258741379
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.1020626649260521
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.13082663714885712
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.15200507640838623
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.13255806267261505
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.1956292688846588
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.20887824892997742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.0055, 5.0640],
        [5.1572, 4.9492, 4.6136],
        [5.1572, 5.0933, 5.1386],
        [5.1572, 5.0798, 5.1309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.13833144307136536 
model_pd.l_d.mean(): -20.473344802856445 
model_pd.lagr.mean(): -20.335012435913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.1652], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.13833144307136536
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.12098422646522522
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1837429702281952
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.1430349498987198
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.15022480487823486
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.1292281299829483
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.11598445475101471
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.16188974678516388
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11590798944234848
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.17275096476078033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1573, 4.9628, 4.6235],
        [5.1573, 4.9820, 4.6394],
        [5.1573, 4.9182, 4.9161],
        [5.1573, 5.1573, 5.1573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.13139890134334564 
model_pd.l_d.mean(): -20.672534942626953 
model_pd.lagr.mean(): -20.541135787963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.3492], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.13139890134334564
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.15087661147117615
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.14285336434841156
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.15901435911655426
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.17063413560390472
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.11370143294334412
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14974603056907654
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.16658152639865875
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.1044885590672493
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.1373431533575058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1593, 4.9845, 4.6420],
        [5.1593, 5.1158, 4.7918],
        [5.1593, 5.0651, 5.1216],
        [5.1593, 5.1589, 5.1593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12214145064353943 
model_pd.l_d.mean(): -19.46059226989746 
model_pd.lagr.mean(): -19.338451385498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4776], device='cuda:0')), ('power', tensor([-20.1612], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12214145064353943
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.1448790282011032
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.1475447416305542
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.13340362906455994
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.11118249595165253
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.0874074324965477
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12400337308645248
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.15112680196762085
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.244585320353508
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.1678275316953659
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.1535, 5.1565],
        [5.1566, 4.8626, 4.7283],
        [5.1566, 5.0072, 4.6633],
        [5.1566, 4.9712, 5.0172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.15523825585842133 
model_pd.l_d.mean(): -19.48944664001465 
model_pd.lagr.mean(): -19.33420753479004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5411], device='cuda:0')), ('power', tensor([-20.2552], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.15523825585842133
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.1306968778371811
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.1420087367296219
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.17928995192050934
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.1130932867527008
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.21310147643089294
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.1497226059436798
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12917694449424744
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.05632704496383667
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.15587592124938965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 4.9666, 4.6273],
        [5.1605, 4.8625, 4.6841],
        [5.1605, 5.1604, 5.1605],
        [5.1605, 5.0447, 5.1050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.16954749822616577 
model_pd.l_d.mean(): -19.8984317779541 
model_pd.lagr.mean(): -19.728883743286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-20.6277], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.16954749822616577
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.16099074482917786
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.17239080369472504
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.1423521637916565
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.17483393847942352
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.07352457195520401
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.15015017986297607
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.102665476500988
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.1307380646467209
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.13130596280097961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 4.8662, 4.6600],
        [5.1630, 5.0794, 5.1328],
        [5.1630, 5.1238, 5.1552],
        [5.1630, 5.1631, 5.1631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.13196785748004913 
model_pd.l_d.mean(): -19.3372802734375 
model_pd.lagr.mean(): -19.205312728881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5074], device='cuda:0')), ('power', tensor([-20.0670], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.13196785748004913
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13259267807006836
epoch£º1219	 i:2 	 global-step:24382	 l-p:0.07096973061561584
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.17428618669509888
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.11247605830430984
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.17695671319961548
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.16697688400745392
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.12125135213136673
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.20740611851215363
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.11041256040334702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 5.1635, 5.1636],
        [5.1636, 5.3849, 5.1860],
        [5.1636, 5.0767, 4.7409],
        [5.1636, 4.9908, 4.6484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.17786987125873566 
model_pd.l_d.mean(): -20.754383087158203 
model_pd.lagr.mean(): -20.576513290405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4277], device='cuda:0')), ('power', tensor([-21.4181], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.17786987125873566
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13716304302215576
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.12884075939655304
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.12296067923307419
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.18781732022762299
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12112835794687271
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.07553616911172867
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.14263121783733368
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.13296766579151154
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.17893309891223907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.1602, 5.1613],
        [5.1613, 5.1613, 5.1613],
        [5.1613, 5.1388, 4.8218],
        [5.1613, 4.8638, 4.6643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.11040619760751724 
model_pd.l_d.mean(): -20.42271614074707 
model_pd.lagr.mean(): -20.31230926513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-21.1420], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:0.11040619760751724
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.10978201776742935
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.15428702533245087
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.175829216837883
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.16534030437469482
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.1341804414987564
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.15341299772262573
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.18179908394813538
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.11007150262594223
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.12400931864976883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1587, 5.1587, 5.1587],
        [5.1587, 5.1028, 5.1441],
        [5.1587, 5.1419, 5.1568],
        [5.1587, 5.1582, 5.1587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.13507556915283203 
model_pd.l_d.mean(): -20.79903793334961 
model_pd.lagr.mean(): -20.663963317871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4131], device='cuda:0')), ('power', tensor([-21.4483], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.13507556915283203
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13253983855247498
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.1390419602394104
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.10830249637365341
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.11529311537742615
epoch£º1222	 i:5 	 global-step:24445	 l-p:0.15139934420585632
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.1402801275253296
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.16407553851604462
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.23979251086711884
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.13002173602581024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 5.1007, 5.1390],
        [5.1512, 5.1374, 5.1499],
        [5.1512, 5.0712, 5.1234],
        [5.1512, 4.9309, 4.9511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11756186932325363 
model_pd.l_d.mean(): -19.444416046142578 
model_pd.lagr.mean(): -19.326854705810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5049], device='cuda:0')), ('power', tensor([-20.1727], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11756186932325363
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19611862301826477
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.13953500986099243
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.14834758639335632
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.20187216997146606
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.16536816954612732
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.12067028135061264
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10809365659952164
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.1396244466304779
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.13123415410518646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.0688, 5.1214],
        [5.1498, 5.1498, 5.1498],
        [5.1498, 5.5154, 5.4055],
        [5.1498, 4.8522, 4.6358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.21405257284641266 
model_pd.l_d.mean(): -19.177661895751953 
model_pd.lagr.mean(): -18.96360969543457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-19.9233], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.21405257284641266
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.1687479019165039
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13262788951396942
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.10794667899608612
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.20323346555233002
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.13027013838291168
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.135316863656044
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.1499292552471161
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.09402868896722794
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.1340019702911377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1497, 5.1270, 5.1466],
        [5.1497, 5.1495, 5.1497],
        [5.1497, 5.1312, 5.1475],
        [5.1497, 5.1497, 5.1497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.11013828217983246 
model_pd.l_d.mean(): -20.46674919128418 
model_pd.lagr.mean(): -20.356611251831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4422], device='cuda:0')), ('power', tensor([-21.1422], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.11013828217983246
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.2259906679391861
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.13614150881767273
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12554508447647095
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.12337276339530945
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12959147989749908
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.16924750804901123
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.20344457030296326
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.11405850946903229
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.15100182592868805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1461, 5.0142, 5.0752],
        [5.1461, 4.8772, 4.5822],
        [5.1461, 5.1461, 5.1461],
        [5.1461, 5.1461, 5.1461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.12702450156211853 
model_pd.l_d.mean(): -19.129575729370117 
model_pd.lagr.mean(): -19.002552032470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5736], device='cuda:0')), ('power', tensor([-19.9246], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.12702450156211853
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.1672087013721466
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.1169857457280159
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.17846667766571045
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.20602981746196747
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.16601867973804474
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.11768891662359238
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.09787802398204803
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.16000734269618988
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.1569475680589676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 5.1411, 5.1463],
        [5.1466, 5.1454, 5.1465],
        [5.1466, 5.0560, 5.1116],
        [5.1466, 5.1466, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.10043859481811523 
model_pd.l_d.mean(): -20.19098663330078 
model_pd.lagr.mean(): -20.090547561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5037], device='cuda:0')), ('power', tensor([-20.9262], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.10043859481811523
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.1844041794538498
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.12250591814517975
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.14552532136440277
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.09480079263448715
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.14316841959953308
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.16924327611923218
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.2101495862007141
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.14117595553398132
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.17250990867614746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1495, 4.9908, 4.6461],
        [5.1495, 5.1385, 5.1486],
        [5.1495, 5.1464, 5.1494],
        [5.1495, 4.9722, 4.6288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.13169299066066742 
model_pd.l_d.mean(): -20.511810302734375 
model_pd.lagr.mean(): -20.380117416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4513], device='cuda:0')), ('power', tensor([-21.1970], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.13169299066066742
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.1465396285057068
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.15007159113883972
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12413270771503448
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.13108208775520325
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.16992701590061188
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.09585864096879959
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.22358126938343048
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.1329660266637802
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.15013523399829865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1561, 5.1042, 5.1433],
        [5.1561, 5.1483, 5.1556],
        [5.1561, 4.8574, 4.6794],
        [5.1561, 4.8617, 4.7273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.09385858476161957 
model_pd.l_d.mean(): -19.383684158325195 
model_pd.lagr.mean(): -19.289825439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4933], device='cuda:0')), ('power', tensor([-20.0995], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.09385858476161957
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.20006220042705536
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12560611963272095
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.13858924806118011
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.1918000429868698
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11639434844255447
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.13045425713062286
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.16091354191303253
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13481299579143524
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.12867851555347443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1616, 5.1065, 5.1473],
        [5.1616, 5.1529, 5.1610],
        [5.1616, 5.1611, 5.1616],
        [5.1616, 5.0362, 4.6937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.12688703835010529 
model_pd.l_d.mean(): -20.92618751525879 
model_pd.lagr.mean(): -20.799301147460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3969], device='cuda:0')), ('power', tensor([-21.5604], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.12688703835010529
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14346793293952942
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.20187391340732574
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12356466054916382
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.12355867028236389
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.10225829482078552
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13336001336574554
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.19049155712127686
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.11805444210767746
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.14528287947177887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 4.9151, 4.5994],
        [5.1611, 5.1610, 5.1611],
        [5.1611, 5.1611, 5.1611],
        [5.1611, 5.0315, 4.6885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.09226952493190765 
model_pd.l_d.mean(): -20.65179443359375 
model_pd.lagr.mean(): -20.559524536132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-21.3220], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.09226952493190765
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13635477423667908
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.2050991803407669
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.15094095468521118
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.2584606409072876
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14808034896850586
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12220781296491623
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.1178894117474556
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.12704163789749146
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.06871771812438965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1525, 4.8925, 4.5879],
        [5.1525, 5.1363, 5.1508],
        [5.1525, 4.8721, 4.5968],
        [5.1525, 4.8819, 4.8274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.1740458458662033 
model_pd.l_d.mean(): -20.79420280456543 
model_pd.lagr.mean(): -20.62015724182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4265], device='cuda:0')), ('power', tensor([-21.4571], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.1740458458662033
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.07127472013235092
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13170059025287628
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.15686771273612976
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.15097536146640778
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13717085123062134
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.2505260705947876
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.140144482254982
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.1420002430677414
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.12464050948619843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 4.8627, 4.7792],
        [5.1453, 5.0267, 4.6842],
        [5.1453, 5.1453, 5.1453],
        [5.1453, 5.1362, 5.1446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.10905998200178146 
model_pd.l_d.mean(): -20.61123275756836 
model_pd.lagr.mean(): -20.502172470092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-21.2793], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.10905998200178146
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11758951097726822
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.09759712219238281
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.1292726844549179
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.1949891448020935
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.2616861164569855
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.2525259256362915
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11698709428310394
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.1569805145263672
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.10346829891204834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1393, 5.1377, 5.1393],
        [5.1393, 5.1393, 5.1393],
        [5.1393, 5.1393, 5.1393],
        [5.1393, 4.8704, 4.8218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.12713970243930817 
model_pd.l_d.mean(): -20.769941329956055 
model_pd.lagr.mean(): -20.64280128479004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4052], device='cuda:0')), ('power', tensor([-21.4108], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.12713970243930817
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.11802619695663452
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11980292946100235
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.1570458859205246
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10908792912960052
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.2575021982192993
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.3458962142467499
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.10997027903795242
epoch£º1234	 i:8 	 global-step:24688	 l-p:0.10572770982980728
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.12253759801387787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.0911, 5.1253],
        [5.1347, 5.1316, 5.1346],
        [5.1347, 5.0994, 5.1282],
        [5.1347, 5.1263, 5.1341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13977697491645813 
model_pd.l_d.mean(): -20.714326858520508 
model_pd.lagr.mean(): -20.57455062866211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.3685], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13977697491645813
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.1922318935394287
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.21182750165462494
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.28301817178726196
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.10897615551948547
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.11913739889860153
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.09322364628314972
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.09677869826555252
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.1324814409017563
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.23057495057582855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1322, 4.8601, 4.8058],
        [5.1322, 4.9617, 5.0155],
        [5.1322, 4.9678, 5.0237],
        [5.1322, 5.1322, 5.1322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.09319953620433807 
model_pd.l_d.mean(): -20.31886863708496 
model_pd.lagr.mean(): -20.225669860839844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4989], device='cuda:0')), ('power', tensor([-21.0506], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.09319953620433807
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.1351584643125534
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.20644015073776245
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.1231270357966423
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.17267680168151855
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.3058638572692871
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.14015738666057587
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.1385258138179779
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.1967422068119049
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.10635648667812347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.1360, 5.1360],
        [5.1360, 5.1360, 5.1360],
        [5.1360, 5.1355, 5.1360],
        [5.1360, 5.0904, 5.1259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.12246745079755783 
model_pd.l_d.mean(): -20.042299270629883 
model_pd.lagr.mean(): -19.919832229614258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-20.7342], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.12246745079755783
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.24181559681892395
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13484643399715424
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.1021495908498764
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.14408043026924133
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.16835474967956543
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.14946439862251282
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.09627456218004227
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.10484819114208221
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.3011000454425812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1388, 4.8378, 4.6585],
        [5.1388, 4.8698, 4.8213],
        [5.1388, 5.0746, 5.1202],
        [5.1388, 5.0930, 5.1286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.12946878373622894 
model_pd.l_d.mean(): -21.1092472076416 
model_pd.lagr.mean(): -20.979778289794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3617], device='cuda:0')), ('power', tensor([-21.7094], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.12946878373622894
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.10896892100572586
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.15087410807609558
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.1558963507413864
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.1573495864868164
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.12986241281032562
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.3022894859313965
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.14006179571151733
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.13985054194927216
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.11657360941171646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 5.1444, 5.1449],
        [5.1449, 5.1371, 5.1444],
        [5.1449, 5.2556, 4.9966],
        [5.1449, 5.1317, 5.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.11995721608400345 
model_pd.l_d.mean(): -20.97136688232422 
model_pd.lagr.mean(): -20.851409912109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3916], device='cuda:0')), ('power', tensor([-21.6006], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.11995721608400345
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13789808750152588
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.12483073770999908
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.13021491467952728
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10789810121059418
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.33381175994873047
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.11860436946153641
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.18646474182605743
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.12663982808589935
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.12833081185817719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 5.5305, 5.4348],
        [5.1429, 5.4258, 5.2635],
        [5.1429, 5.0223, 5.0833],
        [5.1429, 4.9252, 4.5904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12159872055053711 
model_pd.l_d.mean(): -20.133338928222656 
model_pd.lagr.mean(): -20.01173973083496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.8634], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12159872055053711
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.22972369194030762
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.18244235217571259
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.13233047723770142
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.13807058334350586
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.11602957546710968
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.12973150610923767
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.16757886111736298
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.15882515907287598
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13975642621517181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 5.0175, 5.0788],
        [5.1449, 4.8583, 4.7629],
        [5.1449, 4.9379, 4.9705],
        [5.1449, 5.0771, 5.1244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.14025910198688507 
model_pd.l_d.mean(): -18.597862243652344 
model_pd.lagr.mean(): -18.457603454589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6012], device='cuda:0')), ('power', tensor([-19.4153], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.14025910198688507
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.16201213002204895
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.26012495160102844
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12878045439720154
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.11863493919372559
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.18763460218906403
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12370236217975616
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.09463907033205032
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.10390637814998627
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.18253742158412933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1461, 5.0279, 4.6852],
        [5.1461, 5.1413, 4.8303],
        [5.1461, 5.1461, 5.1461],
        [5.1461, 5.1418, 5.1459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.18152347207069397 
model_pd.l_d.mean(): -20.151336669921875 
model_pd.lagr.mean(): -19.969812393188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5113], device='cuda:0')), ('power', tensor([-20.8939], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.18152347207069397
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.11211959272623062
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.14793619513511658
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.09531520307064056
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.1423346996307373
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.17988824844360352
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.21252819895744324
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.1542191207408905
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.1156093031167984
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.14622820913791656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 5.1503, 5.1507],
        [5.1507, 5.0697, 5.1222],
        [5.1507, 5.0213, 5.0824],
        [5.1507, 4.8590, 4.7426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.18172699213027954 
model_pd.l_d.mean(): -20.34942626953125 
model_pd.lagr.mean(): -20.167699813842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4776], device='cuda:0')), ('power', tensor([-21.0597], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.18172699213027954
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.1403314769268036
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.12751571834087372
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.12304765731096268
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.1716632843017578
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.1272980123758316
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.1346583366394043
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.15756233036518097
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.13276927173137665
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.16439387202262878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1553, 5.0843, 5.1329],
        [5.1553, 5.0050, 5.0640],
        [5.1553, 4.9253, 4.9353],
        [5.1553, 5.1553, 5.1553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.1781935840845108 
model_pd.l_d.mean(): -20.311782836914062 
model_pd.lagr.mean(): -20.133588790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-20.9739], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.1781935840845108
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.13924458622932434
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.24449588358402252
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.13414348661899567
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.12113698571920395
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.11602025479078293
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.1366574764251709
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.14400549232959747
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.07814386487007141
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13966093957424164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1612, 5.0400, 4.6976],
        [5.1612, 5.0127, 4.6684],
        [5.1612, 5.2148, 4.9282],
        [5.1612, 4.9095, 4.5978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.163828507065773 
model_pd.l_d.mean(): -20.588394165039062 
model_pd.lagr.mean(): -20.4245662689209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4621], device='cuda:0')), ('power', tensor([-21.2854], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.163828507065773
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.22833558917045593
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.11421845853328705
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.06267533451318741
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.1639028936624527
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.16493144631385803
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.12877149879932404
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.10304364562034607
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.15834102034568787
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12388476729393005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 4.9089, 4.5973],
        [5.1609, 5.1609, 5.1609],
        [5.1609, 5.1172, 5.1514],
        [5.1609, 5.1609, 5.1609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.2161419689655304 
model_pd.l_d.mean(): -20.137990951538086 
model_pd.lagr.mean(): -19.92184829711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-20.8993], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.2161419689655304
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.11801311373710632
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.14525163173675537
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.13439764082431793
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.13243021070957184
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.1154770627617836
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11759914457798004
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.11898200958967209
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.15382252633571625
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.17531369626522064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1555, 4.8584, 4.7098],
        [5.1555, 5.1555, 5.1555],
        [5.1555, 4.9831, 5.0355],
        [5.1555, 5.2691, 5.0113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.15512284636497498 
model_pd.l_d.mean(): -20.065996170043945 
model_pd.lagr.mean(): -19.910873413085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4868], device='cuda:0')), ('power', tensor([-20.7826], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.15512284636497498
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.11898645013570786
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.1078144833445549
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.15226072072982788
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.1585065722465515
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12161760032176971
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.16952289640903473
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.1383790224790573
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.1440107375383377
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.17722105979919434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 5.0651, 5.1208],
        [5.1557, 5.1558, 5.1558],
        [5.1557, 4.8742, 4.7922],
        [5.1557, 4.8569, 4.6904]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.13427406549453735 
model_pd.l_d.mean(): -21.03467559814453 
model_pd.lagr.mean(): -20.900402069091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3809], device='cuda:0')), ('power', tensor([-21.6537], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.13427406549453735
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.1708042472600937
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12797203660011292
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.1341249793767929
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.2144652009010315
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.17376254498958588
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.13272710144519806
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.1555725634098053
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.07888344675302505
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.12086393684148788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 4.8588, 4.6434],
        [5.1565, 5.1565, 5.1565],
        [5.1565, 4.8806, 4.5971],
        [5.1565, 5.1565, 5.1565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.17711210250854492 
model_pd.l_d.mean(): -20.32125473022461 
model_pd.lagr.mean(): -20.144142150878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4859], device='cuda:0')), ('power', tensor([-21.0397], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.17711210250854492
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.08920875936746597
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.18565161526203156
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.16435372829437256
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.10121659934520721
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.1752108484506607
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.12411092221736908
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.1644807755947113
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.10965394973754883
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.1454237848520279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[5.1584, 4.9519, 4.9843],
        [5.1584, 4.9280, 4.9375],
        [5.1584, 5.0051, 5.0635],
        [5.1584, 5.0119, 5.0715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.13335226476192474 
model_pd.l_d.mean(): -20.393123626708984 
model_pd.lagr.mean(): -20.2597713470459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.0780], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.13335226476192474
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.17343685030937195
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13610804080963135
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.12859763205051422
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.12861478328704834
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.2251746952533722
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.07633812725543976
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.12255122512578964
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.11103122681379318
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.20343276858329773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 4.8746, 4.7924],
        [5.1562, 5.1554, 5.1562],
        [5.1562, 5.1562, 5.1562],
        [5.1562, 4.9496, 4.9820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.09588175266981125 
model_pd.l_d.mean(): -20.091413497924805 
model_pd.lagr.mean(): -19.99553108215332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.09588175266981125
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.13296842575073242
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.18986088037490845
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.13256940245628357
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.17817993462085724
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.113946832716465
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.11358564347028732
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.23371164500713348
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.12210959941148758
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.12634769082069397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.0106, 5.0703],
        [5.1572, 4.9812, 4.6376],
        [5.1572, 5.1572, 5.1572],
        [5.1572, 4.8852, 4.5955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.16025719046592712 
model_pd.l_d.mean(): -20.098636627197266 
model_pd.lagr.mean(): -19.938379287719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-20.8381], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.16025719046592712
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.21123681962490082
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.13877302408218384
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.12411438673734665
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.1108952984213829
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.15074282884597778
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.10031041502952576
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.16617648303508759
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.144273579120636
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.12221981585025787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1590, 5.1590],
        [5.1589, 5.1584, 5.1589],
        [5.1589, 5.1590, 5.1590],
        [5.1589, 5.1590, 5.1590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.1251816749572754 
model_pd.l_d.mean(): -20.215312957763672 
model_pd.lagr.mean(): -20.090131759643555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4320], device='cuda:0')), ('power', tensor([-20.8775], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.1251816749572754
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.13570110499858856
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.15228645503520966
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.1647823452949524
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.1629822552204132
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.133898064494133
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.09827267378568649
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.09281009435653687
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.1136065199971199
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.26174992322921753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1552, 5.4148, 5.2382],
        [5.1552, 4.9826, 5.0351],
        [5.1552, 5.1521, 5.1550],
        [5.1552, 4.8563, 4.6925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.12362231314182281 
model_pd.l_d.mean(): -19.30161476135254 
model_pd.lagr.mean(): -19.17799186706543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5425], device='cuda:0')), ('power', tensor([-20.0667], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.12362231314182281
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.1091630756855011
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.11171714216470718
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.11623601615428925
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.1420123279094696
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.12270021438598633
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.2633567154407501
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.12306258827447891
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.21526381373405457
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.131363183259964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 4.9717, 5.0209],
        [5.1521, 4.8566, 4.7221],
        [5.1521, 5.0200, 5.0812],
        [5.1521, 5.1521, 5.1521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.1158374696969986 
model_pd.l_d.mean(): -19.215665817260742 
model_pd.lagr.mean(): -19.099828720092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5672], device='cuda:0')), ('power', tensor([-20.0052], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.1158374696969986
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.184351846575737
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.12004345655441284
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.1834510713815689
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.11739640682935715
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.20704196393489838
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.16167940199375153
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.1203797236084938
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.14388515055179596
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.10653632879257202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1536, 5.0519, 5.1103],
        [5.1536, 4.8781, 4.8130],
        [5.1536, 5.4309, 5.2648],
        [5.1536, 5.1449, 5.1530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.0875476598739624 
model_pd.l_d.mean(): -20.557910919189453 
model_pd.lagr.mean(): -20.47036361694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-21.2359], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.0875476598739624
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.1497131735086441
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.16763515770435333
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.18542835116386414
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.17383244633674622
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.15248142182826996
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.10233256965875626
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.132303848862648
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.1634683609008789
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.1443004459142685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 4.8604, 4.7439],
        [5.1523, 5.0370, 4.6947],
        [5.1523, 5.1519, 5.1523],
        [5.1523, 5.1523, 5.1523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.11621604859828949 
model_pd.l_d.mean(): -20.428495407104492 
model_pd.lagr.mean(): -20.312278747558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4739], device='cuda:0')), ('power', tensor([-21.1359], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.11621604859828949
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.2185237854719162
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.11920645087957382
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.23243853449821472
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.10328634083271027
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.11183200776576996
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.14957305788993835
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.10654905438423157
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.15096290409564972
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.15652495622634888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1512, 5.1508, 5.1512],
        [5.1512, 5.1512, 5.1512],
        [5.1512, 4.8935, 4.8654],
        [5.1512, 4.8671, 4.5968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.13555501401424408 
model_pd.l_d.mean(): -20.837501525878906 
model_pd.lagr.mean(): -20.701946258544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-21.4924], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.13555501401424408
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.19207963347434998
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.20063215494155884
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.1290598213672638
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.12148119509220123
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.23000432550907135
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.06657718867063522
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.12234441190958023
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.14514003694057465
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.1227516233921051
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1553, 5.1509, 5.1551],
        [5.1553, 4.8797, 4.8143],
        [5.1553, 5.1511, 5.1551],
        [5.1553, 4.9793, 4.6354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.14533944427967072 
model_pd.l_d.mean(): -20.00531578063965 
model_pd.lagr.mean(): -19.859975814819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5160], device='cuda:0')), ('power', tensor([-20.7511], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.14533944427967072
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.1642458140850067
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.13474392890930176
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.12108256667852402
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.07905592024326324
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.1771964281797409
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.13102684915065765
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13803395628929138
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.20091485977172852
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.1453266739845276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.1542, 5.1584],
        [5.1586, 4.8962, 4.8587],
        [5.1586, 5.1586, 5.1586],
        [5.1586, 5.0728, 5.1270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.16483649611473083 
model_pd.l_d.mean(): -20.498762130737305 
model_pd.lagr.mean(): -20.333925247192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-21.1933], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.16483649611473083
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.19315481185913086
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.15988542139530182
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.11764795333147049
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.166738361120224
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.1290789693593979
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.10237056761980057
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.1883961260318756
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.06360393017530441
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.13528098165988922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 5.1626, 5.1626],
        [5.1626, 5.1625, 5.1626],
        [5.1626, 5.1457, 5.1608],
        [5.1626, 5.1578, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.17232950031757355 
model_pd.l_d.mean(): -20.82097053527832 
model_pd.lagr.mean(): -20.64864158630371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4155], device='cuda:0')), ('power', tensor([-21.4730], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.17232950031757355
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.16071102023124695
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.21625801920890808
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.11578888446092606
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.104110486805439
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.09677378088235855
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.11491944640874863
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.12598265707492828
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.18402521312236786
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.11454307287931442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.1790, 4.8756],
        [5.1642, 5.1398, 5.1607],
        [5.1642, 5.1772, 4.8731],
        [5.1642, 5.0489, 4.7072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.1827266961336136 
model_pd.l_d.mean(): -19.626312255859375 
model_pd.lagr.mean(): -19.443586349487305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4924], device='cuda:0')), ('power', tensor([-20.3439], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.1827266961336136
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.16380707919597626
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.1631852239370346
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.18347671627998352
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.11629869043827057
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.11552369594573975
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.14966559410095215
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.0825403705239296
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.101785808801651
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.1449947953224182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1645, 5.2983, 5.0506],
        [5.1645, 5.1162, 5.1532],
        [5.1645, 5.0685, 5.1255],
        [5.1645, 5.0029, 5.0590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.08169618248939514 
model_pd.l_d.mean(): -18.968082427978516 
model_pd.lagr.mean(): -18.88638687133789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5345], device='cuda:0')), ('power', tensor([-19.7214], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.08169618248939514
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.14124427735805511
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.2139335423707962
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.1173066794872284
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.11632852256298065
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.1982582062482834
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.1386762261390686
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.10516010969877243
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.13085059821605682
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.15770451724529266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 4.8693, 4.6478],
        [5.1654, 4.9489, 4.9725],
        [5.1654, 5.1654, 5.1654],
        [5.1654, 5.1654, 5.1654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.1053166463971138 
model_pd.l_d.mean(): -19.193138122558594 
model_pd.lagr.mean(): -19.08782196044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5229], device='cuda:0')), ('power', tensor([-19.9371], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.1053166463971138
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.21940112113952637
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.14410175383090973
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.16384455561637878
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12464585155248642
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.10423007607460022
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.19388459622859955
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.10269986093044281
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.1334221363067627
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.10873372852802277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.1583, 5.1632],
        [5.1635, 4.9543, 4.9844],
        [5.1635, 5.1099, 5.1500],
        [5.1635, 5.1635, 5.1635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10759726911783218 
model_pd.l_d.mean(): -19.402118682861328 
model_pd.lagr.mean(): -19.29452133178711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5302], device='cuda:0')), ('power', tensor([-20.1558], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10759726911783218
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.13901816308498383
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.144142284989357
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.15769323706626892
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.12633325159549713
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.11700886487960815
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.2164415717124939
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.1953584849834442
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.12001386284828186
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.10193805396556854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[5.1589, 4.9340, 4.9499],
        [5.1589, 5.0054, 5.0639],
        [5.1589, 4.9502, 4.9810],
        [5.1589, 4.9041, 4.8801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.20703135430812836 
model_pd.l_d.mean(): -19.86231803894043 
model_pd.lagr.mean(): -19.65528678894043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-20.5657], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.20703135430812836
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.1589781641960144
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.13137337565422058
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.1393195539712906
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.10715675354003906
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.1953655630350113
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.1045265719294548
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.09008647501468658
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.13048341870307922
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.16483262181282043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1585, 4.8878, 4.8334],
        [5.1585, 5.1580, 5.1585],
        [5.1585, 5.1521, 5.1582],
        [5.1585, 5.1585, 5.1585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.1364019513130188 
model_pd.l_d.mean(): -20.80643653869629 
model_pd.lagr.mean(): -20.670034408569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4247], device='cuda:0')), ('power', tensor([-21.4677], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.1364019513130188
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.12327052652835846
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12560343742370605
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.10101968795061111
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.16888706386089325
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.1799568086862564
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.15632964670658112
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.13232900202274323
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.18403205275535583
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.13395823538303375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.1546, 5.1562],
        [5.1563, 4.9594, 4.9991],
        [5.1563, 5.1563, 5.1563],
        [5.1563, 4.9028, 4.8814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.13406230509281158 
model_pd.l_d.mean(): -19.552440643310547 
model_pd.lagr.mean(): -19.418378829956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4715], device='cuda:0')), ('power', tensor([-20.2478], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.13406230509281158
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12684164941310883
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.15555520355701447
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.16959810256958008
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.13059385120868683
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.12828278541564941
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.20154230296611786
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.09098163992166519
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.13307176530361176
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.16840381920337677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 4.9804, 5.0304],
        [5.1586, 5.1508, 5.1581],
        [5.1586, 5.1586, 5.1586],
        [5.1586, 5.1586, 5.1586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.07053294777870178 
model_pd.l_d.mean(): -20.28650665283203 
model_pd.lagr.mean(): -20.215972900390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-20.9882], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.07053294777870178
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.17168666422367096
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.12868691980838776
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.17824989557266235
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.11688534170389175
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.1656608134508133
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.15796445310115814
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.14379377663135529
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.1286047101020813
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.1536131650209427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 5.2843, 5.0276],
        [5.1678, 5.1667, 5.1678],
        [5.1678, 4.8701, 4.6634],
        [5.1678, 4.8772, 4.6307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.13062787055969238 
model_pd.l_d.mean(): -20.495685577392578 
model_pd.lagr.mean(): -20.36505699157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4536], device='cuda:0')), ('power', tensor([-21.1830], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.13062787055969238
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13838127255439758
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.04874371364712715
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.18958765268325806
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.16203391551971436
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.1472378373146057
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.14976029098033905
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.14175567030906677
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.1454046368598938
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12417294830083847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1744, 5.1761, 4.8674],
        [5.1744, 4.9891, 4.6481],
        [5.1744, 5.1220, 5.1614],
        [5.1744, 4.8808, 4.7461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.13182373344898224 
model_pd.l_d.mean(): -19.428329467773438 
model_pd.lagr.mean(): -19.296504974365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.1637], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.13182373344898224
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.15705235302448273
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.14991557598114014
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.12044412642717361
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.13838182389736176
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.14832936227321625
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.18823127448558807
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.12355343252420425
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.12453951686620712
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.08903194218873978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[5.1719, 4.9783, 4.6386],
        [5.1719, 4.8804, 4.6388],
        [5.1719, 4.9030, 4.6115],
        [5.1719, 5.3940, 5.1947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.13610932230949402 
model_pd.l_d.mean(): -20.067440032958984 
model_pd.lagr.mean(): -19.931331634521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.8258], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.13610932230949402
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.1318153589963913
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12200871855020523
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.13425035774707794
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.10051414370536804
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.14335690438747406
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.14498309791088104
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.14164532721042633
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.13277669250965118
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.17747800052165985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1764, 4.8793, 4.6754],
        [5.1764, 5.1212, 5.1621],
        [5.1764, 5.1760, 5.1764],
        [5.1764, 5.1717, 5.1762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.15736791491508484 
model_pd.l_d.mean(): -19.882150650024414 
model_pd.lagr.mean(): -19.724782943725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5382], device='cuda:0')), ('power', tensor([-20.6493], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.15736791491508484
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.1921243965625763
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.08514074981212616
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14719808101654053
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.11365295946598053
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.09982013702392578
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17803174257278442
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12572239339351654
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.10864496231079102
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.14354829490184784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1774, 5.1774, 5.1774],
        [5.1774, 4.9998, 5.0495],
        [5.1774, 4.9687, 4.9986],
        [5.1774, 5.1752, 5.1774]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.14927959442138672 
model_pd.l_d.mean(): -20.053325653076172 
model_pd.lagr.mean(): -19.90404510498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4626], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.14927959442138672
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.1274653524160385
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.14401012659072876
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.10880298167467117
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.17572446167469025
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12568345665931702
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.12128148972988129
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.08047425746917725
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.19937515258789062
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.1395079493522644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 5.0416, 5.1026],
        [5.1687, 5.1667, 5.1686],
        [5.1687, 5.1680, 5.1687],
        [5.1687, 5.5259, 5.4094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.09627711772918701 
model_pd.l_d.mean(): -20.242403030395508 
model_pd.lagr.mean(): -20.14612579345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-20.9647], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.09627711772918701
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.1471584290266037
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.10186289250850677
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.13688774406909943
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.12084934115409851
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.1369103044271469
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.17116624116897583
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.15588107705116272
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.13126498460769653
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.20037443935871124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1642, 5.1646],
        [5.1646, 4.9739, 4.6327],
        [5.1646, 5.0158, 4.6709],
        [5.1646, 4.8695, 4.7328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.12086771428585052 
model_pd.l_d.mean(): -20.51904296875 
model_pd.lagr.mean(): -20.398176193237305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4702], device='cuda:0')), ('power', tensor([-21.2236], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.12086771428585052
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.14237317442893982
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.14802096784114838
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.17681938409805298
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.1317802220582962
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.12405660003423691
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.1216982826590538
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.11849572509527206
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.12591376900672913
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.1920793354511261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 4.9275, 4.6049],
        [5.1656, 5.1641, 5.1656],
        [5.1656, 5.4324, 5.2596],
        [5.1656, 5.1651, 5.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.18473908305168152 
model_pd.l_d.mean(): -20.398942947387695 
model_pd.lagr.mean(): -20.214204788208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5086], device='cuda:0')), ('power', tensor([-21.1415], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.18473908305168152
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11908157914876938
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.1444372981786728
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.12087246030569077
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.15315929055213928
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.11134186387062073
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.1243467703461647
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.11168035119771957
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.1264960616827011
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.19556403160095215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1692, 5.1692, 5.1692],
        [5.1692, 5.1829, 4.8789],
        [5.1692, 5.0803, 4.7432],
        [5.1692, 5.1545, 5.1677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.14398248493671417 
model_pd.l_d.mean(): -20.561382293701172 
model_pd.lagr.mean(): -20.417400360107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4385], device='cuda:0')), ('power', tensor([-21.2341], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.14398248493671417
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.16107028722763062
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.11377724260091782
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.16791677474975586
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13325347006320953
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.1594756841659546
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.12022077292203903
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.09474169462919235
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.15130046010017395
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.13340149819850922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.0785, 5.1347],
        [5.1712, 5.1710, 5.1712],
        [5.1712, 4.9843, 4.6429],
        [5.1712, 5.1712, 5.1712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.13516788184642792 
model_pd.l_d.mean(): -18.076940536499023 
model_pd.lagr.mean(): -17.9417724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6625], device='cuda:0')), ('power', tensor([-18.9513], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.13516788184642792
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.1308807134628296
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.15350307524204254
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.12052251398563385
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.17524440586566925
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.1071576401591301
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.15726134181022644
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.13026151061058044
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.14209599792957306
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.1281580924987793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1685, 4.8714, 4.6558],
        [5.1685, 5.0152, 5.0735],
        [5.1685, 5.1657, 5.1684],
        [5.1685, 5.1668, 5.1684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.14721651375293732 
model_pd.l_d.mean(): -19.764333724975586 
model_pd.lagr.mean(): -19.617116928100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-20.4923], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.14721651375293732
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.1444040983915329
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.13386332988739014
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.14216502010822296
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.1301402896642685
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.12155630439519882
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.15591181814670563
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.1292106956243515
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.149908646941185
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.1399262696504593
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.1441, 5.1626],
        [5.1653, 4.8948, 4.8402],
        [5.1653, 5.0297, 5.0906],
        [5.1653, 4.8838, 4.8015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.09004201740026474 
model_pd.l_d.mean(): -20.580663681030273 
model_pd.lagr.mean(): -20.49062156677246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-21.2508], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.09004201740026474
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.1768311709165573
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.14308972656726837
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.1613529622554779
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.16861659288406372
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.12766824662685394
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.17336903512477875
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.1237734854221344
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.09299598634243011
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.13270322978496552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.1711, 5.1712],
        [5.1712, 5.1690, 5.1712],
        [5.1712, 4.9293, 4.9239],
        [5.1712, 5.0427, 4.6993]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.10572737455368042 
model_pd.l_d.mean(): -20.38988494873047 
model_pd.lagr.mean(): -20.284156799316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4710], device='cuda:0')), ('power', tensor([-21.0939], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.10572737455368042
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.0806053876876831
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.15091536939144135
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.16091060638427734
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.11061666905879974
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.13616244494915009
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.09952819347381592
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.192550390958786
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.20902416110038757
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.14055244624614716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 4.8733, 4.7364],
        [5.1682, 5.1675, 5.1682],
        [5.1682, 5.1659, 5.1681],
        [5.1682, 5.1682, 5.1682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.08129621297121048 
model_pd.l_d.mean(): -20.232282638549805 
model_pd.lagr.mean(): -20.150985717773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4764], device='cuda:0')), ('power', tensor([-20.9401], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.08129621297121048
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.13860099017620087
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.16443267464637756
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.14717698097229004
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.1858748197555542
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.08474823087453842
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12308239936828613
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.1548493653535843
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.15134526789188385
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.15044783055782318
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.2449, 4.9664],
        [5.1726, 5.1696, 5.1725],
        [5.1726, 5.0887, 5.1422],
        [5.1726, 5.1725, 5.1726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.12903961539268494 
model_pd.l_d.mean(): -20.82110023498535 
model_pd.lagr.mean(): -20.692060470581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.4753], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.12903961539268494
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.15211057662963867
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12547902762889862
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12559930980205536
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.14952999353408813
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.19434085488319397
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.13812676072120667
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.14137178659439087
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.0953511893749237
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.13028055429458618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 4.9597, 4.6233],
        [5.1684, 5.1629, 5.1681],
        [5.1684, 4.9006, 4.8515],
        [5.1684, 5.0533, 4.7113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.21215195953845978 
model_pd.l_d.mean(): -20.68499183654785 
model_pd.lagr.mean(): -20.47283935546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4426], device='cuda:0')), ('power', tensor([-21.3632], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.21215195953845978
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.16288547217845917
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.11272110044956207
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.0985853374004364
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.12557600438594818
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.13477563858032227
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.10431405156850815
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.16627860069274902
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.1414080709218979
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.1358923316001892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1643, 4.8826, 4.8003],
        [5.1643, 5.1205, 5.1548],
        [5.1643, 5.4882, 5.3504],
        [5.1643, 5.2972, 5.0487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.12201514095067978 
model_pd.l_d.mean(): -19.56854248046875 
model_pd.lagr.mean(): -19.4465274810791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5160], device='cuda:0')), ('power', tensor([-20.3096], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.12201514095067978
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.1300792396068573
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.12074185162782669
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.19553905725479126
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.16212911903858185
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.1891075074672699
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.14439460635185242
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.1186458021402359
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.12348243594169617
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.09852445125579834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1660, 5.1013, 4.7700],
        [5.1660, 4.8675, 4.7032],
        [5.1660, 5.1659, 5.1660],
        [5.1660, 5.1660, 5.1660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.12645873427391052 
model_pd.l_d.mean(): -20.435270309448242 
model_pd.lagr.mean(): -20.30881118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4941], device='cuda:0')), ('power', tensor([-21.1634], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.12645873427391052
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11693954467773438
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.1521034687757492
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14642976224422455
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.13548842072486877
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.12426747381687164
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.11234496533870697
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12290891259908676
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.18487393856048584
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.18527868390083313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.1621, 5.1621],
        [5.1621, 5.1620, 5.1621],
        [5.1621, 4.8797, 4.7959],
        [5.1621, 5.0395, 4.6962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.17478998005390167 
model_pd.l_d.mean(): -19.948871612548828 
model_pd.lagr.mean(): -19.77408218383789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4817], device='cuda:0')), ('power', tensor([-20.6590], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.17478998005390167
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.1608165204524994
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.1327306181192398
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.19501681625843048
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.12081008404493332
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.08213140070438385
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.13243421912193298
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.13057926297187805
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.10522765666246414
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.18048660457134247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.2940, 5.0443],
        [5.1635, 5.0706, 5.1269],
        [5.1635, 5.1635, 5.1635],
        [5.1635, 5.1199, 5.1541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.10730104148387909 
model_pd.l_d.mean(): -20.320859909057617 
model_pd.lagr.mean(): -20.213558197021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-20.9882], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.10730104148387909
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.10689569264650345
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.11710498481988907
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.2261144518852234
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.13071651756763458
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.13284087181091309
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12296666949987411
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.21263554692268372
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13771553337574005
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.11811113357543945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1625, 5.1625],
        [5.1625, 4.9390, 4.9565],
        [5.1625, 4.9622, 4.9996],
        [5.1625, 5.0728, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.10391876846551895 
model_pd.l_d.mean(): -19.642396926879883 
model_pd.lagr.mean(): -19.53847885131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5245], device='cuda:0')), ('power', tensor([-20.3929], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.10391876846551895
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.23207052052021027
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.1495000272989273
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12466584891080856
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.2297852337360382
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.13550981879234314
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.08201101422309875
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.09655414521694183
epoch£º1290	 i:8 	 global-step:25808	 l-p:0.09594029933214188
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.1673588752746582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1607, 4.9343, 4.9488],
        [5.1607, 4.9245, 4.9277],
        [5.1607, 4.8631, 4.7142],
        [5.1607, 5.1607, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.1299203485250473 
model_pd.l_d.mean(): -20.897930145263672 
model_pd.lagr.mean(): -20.768009185791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4024], device='cuda:0')), ('power', tensor([-21.5373], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.1299203485250473
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.13716110587120056
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12690533697605133
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.23873724043369293
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.13661299645900726
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.1767624467611313
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.1411452442407608
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.0701945349574089
epoch£º1291	 i:8 	 global-step:25828	 l-p:0.12844151258468628
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.14589034020900726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1604, 5.1600, 5.1604],
        [5.1604, 5.5524, 5.4586],
        [5.1604, 5.1592, 5.1603],
        [5.1604, 5.0792, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.12163301557302475 
model_pd.l_d.mean(): -18.959144592285156 
model_pd.lagr.mean(): -18.83751106262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5322], device='cuda:0')), ('power', tensor([-19.7101], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.12163301557302475
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.19832821190357208
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.14032131433486938
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.1263497769832611
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.11427415907382965
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.14216364920139313
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.1734904646873474
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.1359521597623825
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.13957999646663666
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.12818729877471924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1617, 5.1575, 5.1615],
        [5.1617, 4.8646, 4.7190],
        [5.1617, 5.0006, 5.0573],
        [5.1617, 5.0442, 5.1050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.09966392070055008 
model_pd.l_d.mean(): -20.338451385498047 
model_pd.lagr.mean(): -20.238786697387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4792], device='cuda:0')), ('power', tensor([-21.0503], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.09966392070055008
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.120637908577919
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.25022658705711365
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.151556596159935
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.1590898036956787
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.10726357996463776
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.12659788131713867
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.08251959830522537
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.20062701404094696
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.13258463144302368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 5.1570, 5.1579],
        [5.1579, 5.1082, 5.1461],
        [5.1579, 4.8585, 4.6968],
        [5.1579, 5.1574, 5.1579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.09473364800214767 
model_pd.l_d.mean(): -19.33759880065918 
model_pd.lagr.mean(): -19.24286460876465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5359], device='cuda:0')), ('power', tensor([-20.0964], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.09473364800214767
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.1577322781085968
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11378156393766403
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.24327728152275085
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12612897157669067
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.1440904140472412
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.1554572731256485
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12389747053384781
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.1495230346918106
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.1316102296113968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1570, 5.1547, 5.1569],
        [5.1570, 5.1432, 5.1557],
        [5.1570, 5.1063, 5.1448],
        [5.1570, 5.2332, 4.9564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.1688883751630783 
model_pd.l_d.mean(): -20.4480037689209 
model_pd.lagr.mean(): -20.279115676879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.1437], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.1688883751630783
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.07488255947828293
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.20879188179969788
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.12670652568340302
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.1259339451789856
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10799237340688705
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.1657247543334961
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.1686151921749115
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.15200085937976837
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.13902346789836884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1582, 5.0722, 5.1265],
        [5.1582, 5.0903, 5.1376],
        [5.1582, 5.1582, 5.1582],
        [5.1582, 5.1386, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.14628055691719055 
model_pd.l_d.mean(): -19.4099063873291 
model_pd.lagr.mean(): -19.263626098632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5223], device='cuda:0')), ('power', tensor([-20.1556], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.14628055691719055
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.10892008244991302
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.1578056365251541
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.12087574601173401
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.17414073646068573
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.1581515371799469
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.18471397459506989
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.1408771276473999
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.12252727150917053
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11218072474002838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[5.1636, 4.9103, 4.5980],
        [5.1636, 4.9098, 4.8882],
        [5.1636, 4.9080, 4.8833],
        [5.1636, 4.8638, 4.6853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.1221073716878891 
model_pd.l_d.mean(): -18.863771438598633 
model_pd.lagr.mean(): -18.74166488647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5388], device='cuda:0')), ('power', tensor([-19.6204], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.1221073716878891
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.12127482891082764
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.20283463597297668
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14767803251743317
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1989578902721405
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.18766549229621887
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.09843619167804718
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.08561976999044418
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.14760012924671173
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.10340014845132828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1623, 5.1625],
        [5.1625, 4.8859, 4.6019],
        [5.1625, 4.9706, 4.6287],
        [5.1625, 4.8724, 4.6185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.10747417062520981 
model_pd.l_d.mean(): -20.47293472290039 
model_pd.lagr.mean(): -20.365461349487305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4768], device='cuda:0')), ('power', tensor([-21.1838], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.10747417062520981
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.17757056653499603
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.2065555900335312
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.08172990381717682
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.10734359174966812
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.12339083105325699
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.19226162135601044
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.1400901973247528
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.20674952864646912
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.08279317617416382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1587, 5.1581, 5.1587],
        [5.1587, 4.8584, 4.6608],
        [5.1587, 5.1587, 5.1587],
        [5.1587, 5.1264, 5.1531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.12375703454017639 
model_pd.l_d.mean(): -20.93132972717285 
model_pd.lagr.mean(): -20.807573318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4030], device='cuda:0')), ('power', tensor([-21.5718], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.12375703454017639
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.12363594025373459
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.2164771854877472
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.1495867371559143
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.1279204785823822
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.1216081753373146
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12101007252931595
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.1584039032459259
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.1423712521791458
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.15695306658744812
